From 1ab783fe38545762e5f4a29de2fb8afdb5d3d195 Mon Sep 17 00:00:00 2001
From: yuanhecai <yuanhecai@loongson.cn>
Date: Fri, 11 Aug 2023 14:56:22 +0800
Subject: [PATCH] Merged the latest code from mips and loongarch.

---
 Makefile                                      |    2 +-
 configure                                     |  215 +-
 ffbuild/arch.mak                              |    4 +-
 ffbuild/common.mak                            |   18 +-
 fftools/ffmpeg.c                              |    2 +-
 libavcodec/cabac_functions.h                  |    3 +
 libavcodec/h264_cabac.c                       |    2 +
 libavcodec/h264chroma.c                       |    2 +
 libavcodec/h264chroma.h                       |    1 +
 libavcodec/h264dsp.c                          |    1 +
 libavcodec/h264dsp.h                          |    2 +
 libavcodec/h264pred.c                         |    2 +
 libavcodec/h264pred.h                         |    2 +
 libavcodec/h264qpel.c                         |    2 +
 libavcodec/h264qpel.h                         |    1 +
 libavcodec/hevcdsp.c                          |    2 +
 libavcodec/hevcdsp.h                          |    1 +
 libavcodec/hpeldsp.c                          |    2 +
 libavcodec/hpeldsp.h                          |    1 +
 libavcodec/idctdsp.c                          |    2 +
 libavcodec/idctdsp.h                          |    2 +
 libavcodec/loongarch/Makefile                 |   37 +
 libavcodec/loongarch/cabac.h                  |  238 +
 libavcodec/loongarch/h264_cabac.c             |  140 +
 libavcodec/loongarch/h264_deblock_lasx.c      |  147 +
 .../loongarch/h264_intrapred_init_loongarch.c |   66 +
 libavcodec/loongarch/h264_intrapred_lasx.h    |   35 +
 libavcodec/loongarch/h264chroma.S             |  966 ++++
 .../loongarch/h264chroma_init_loongarch.c     |   45 +
 libavcodec/loongarch/h264chroma_lasx.h        |   43 +
 libavcodec/loongarch/h264dsp.S                | 1977 ++++++++
 libavcodec/loongarch/h264dsp_init_loongarch.c |   95 +
 libavcodec/loongarch/h264dsp_lasx.c           |  782 +++
 libavcodec/loongarch/h264dsp_lasx.h           |  129 +
 libavcodec/loongarch/h264idct.S               |  659 +++
 libavcodec/loongarch/h264idct_c.c             |  183 +
 libavcodec/loongarch/h264intrapred.S          |  299 ++
 libavcodec/loongarch/h264qpel.S               | 1686 +++++++
 .../loongarch/h264qpel_init_loongarch.c       |  169 +
 libavcodec/loongarch/h264qpel_lasx.c          | 2038 ++++++++
 libavcodec/loongarch/h264qpel_lasx.h          |  159 +
 libavcodec/loongarch/h264qpel_lsx.c           |  488 ++
 libavcodec/loongarch/h264qpel_lsx.h           |  178 +
 libavcodec/loongarch/hevc_idct_lsx.c          |  842 ++++
 libavcodec/loongarch/hevc_lpf_sao_lsx.c       | 2485 ++++++++++
 libavcodec/loongarch/hevc_mc_bi_lsx.c         | 2289 +++++++++
 libavcodec/loongarch/hevc_mc_uni_lsx.c        | 1423 ++++++
 libavcodec/loongarch/hevc_mc_uniw_lsx.c       |  298 ++
 libavcodec/loongarch/hevcdsp_init_loongarch.c |  190 +
 libavcodec/loongarch/hevcdsp_lsx.c            | 3299 +++++++++++++
 libavcodec/loongarch/hevcdsp_lsx.h            |  230 +
 libavcodec/loongarch/hpeldsp_init_loongarch.c |   50 +
 libavcodec/loongarch/hpeldsp_lasx.c           | 1287 +++++
 libavcodec/loongarch/hpeldsp_lasx.h           |   58 +
 libavcodec/loongarch/idctdsp_init_loongarch.c |   45 +
 libavcodec/loongarch/idctdsp_lasx.c           |  124 +
 libavcodec/loongarch/idctdsp_loongarch.h      |   41 +
 libavcodec/loongarch/loongson_asm.S           |  946 ++++
 libavcodec/loongarch/simple_idct_lasx.c       |  297 ++
 libavcodec/loongarch/vc1dsp_init_loongarch.c  |   67 +
 libavcodec/loongarch/vc1dsp_lasx.c            | 1005 ++++
 libavcodec/loongarch/vc1dsp_loongarch.h       |   79 +
 libavcodec/loongarch/videodsp_init.c          |   45 +
 libavcodec/loongarch/vp8_lpf_lsx.c            |  591 +++
 libavcodec/loongarch/vp8_mc_lsx.c             |  951 ++++
 libavcodec/loongarch/vp8dsp_init_loongarch.c  |   63 +
 libavcodec/loongarch/vp8dsp_loongarch.h       |   90 +
 libavcodec/loongarch/vp9_idct_lsx.c           | 1411 ++++++
 libavcodec/loongarch/vp9_intra_lsx.c          |  653 +++
 libavcodec/loongarch/vp9_lpf_lsx.c            | 3141 ++++++++++++
 libavcodec/loongarch/vp9_mc_lsx.c             | 2480 ++++++++++
 libavcodec/loongarch/vp9dsp_init_loongarch.c  |  130 +
 libavcodec/loongarch/vp9dsp_loongarch.h       |  182 +
 libavcodec/mips/Makefile                      |    5 +-
 libavcodec/mips/aacdec_mips.c                 |    8 +-
 libavcodec/mips/aacpsdsp_mips.c               |   13 +-
 libavcodec/mips/aacpsy_mips.h                 |   14 +-
 libavcodec/mips/aacsbr_mips.c                 |    2 +-
 libavcodec/mips/blockdsp_init_mips.c          |   40 +-
 libavcodec/mips/blockdsp_mmi.c                |    8 +-
 libavcodec/mips/cabac.h                       |  162 +-
 libavcodec/mips/constants.c                   |   88 +-
 libavcodec/mips/constants.h                   |   88 +-
 libavcodec/mips/fft_mips.c                    |   12 +-
 libavcodec/mips/h263dsp_init_mips.c           |   18 +-
 libavcodec/mips/h264_deblock_msa.c            |  153 +
 libavcodec/mips/h264chroma_init_mips.c        |   54 +-
 libavcodec/mips/h264chroma_mmi.c              |  205 +-
 libavcodec/mips/h264dsp_init_mips.c           |  227 +-
 libavcodec/mips/h264dsp_mips.h                |   46 +-
 libavcodec/mips/h264dsp_mmi.c                 |  404 +-
 libavcodec/mips/h264dsp_msa.c                 |  605 +--
 libavcodec/mips/h264idct_msa.c                |   43 +-
 libavcodec/mips/h264pred_init_mips.c          |  210 +-
 libavcodec/mips/h264pred_mmi.c                |   49 +-
 libavcodec/mips/h264qpel_init_mips.c          |  412 +-
 libavcodec/mips/h264qpel_mmi.c                |   60 +-
 libavcodec/mips/h264qpel_msa.c                |   64 +-
 libavcodec/mips/hevc_idct_msa.c               |   21 +-
 libavcodec/mips/hevc_lpf_sao_msa.c            |  202 +-
 libavcodec/mips/hevc_macros_msa.h             |    9 +
 libavcodec/mips/hevc_mc_bi_msa.c              |  193 +-
 libavcodec/mips/hevc_mc_biw_msa.c             |  160 +-
 libavcodec/mips/hevc_mc_uni_msa.c             |   88 +
 libavcodec/mips/hevc_mc_uniw_msa.c            | 4215 ++++++++++-------
 libavcodec/mips/hevcdsp_init_mips.c           |  992 ++--
 libavcodec/mips/hevcdsp_mmi.c                 |  342 +-
 libavcodec/mips/hevcdsp_msa.c                 |   86 +-
 libavcodec/mips/hevcpred_init_mips.c          |   40 +-
 libavcodec/mips/hevcpred_msa.c                |   38 +-
 libavcodec/mips/hpeldsp_init_mips.c           |  180 +-
 libavcodec/mips/hpeldsp_mmi.c                 |   27 +-
 libavcodec/mips/hpeldsp_msa.c                 |   66 +-
 libavcodec/mips/idctdsp_init_mips.c           |   74 +-
 libavcodec/mips/idctdsp_mmi.c                 |    4 +-
 libavcodec/mips/idctdsp_msa.c                 |    9 +-
 libavcodec/mips/me_cmp_init_mips.c            |   50 +-
 libavcodec/mips/me_cmp_msa.c                  |  209 +-
 libavcodec/mips/mpegaudiodsp_mips_float.c     |  492 +-
 libavcodec/mips/mpegvideo_init_mips.c         |   48 +-
 libavcodec/mips/mpegvideo_mmi.c               |  114 +-
 libavcodec/mips/mpegvideoencdsp_init_mips.c   |   21 +-
 libavcodec/mips/pixblockdsp_init_mips.c       |   63 +-
 libavcodec/mips/pixblockdsp_mmi.c             |    8 +-
 libavcodec/mips/qpeldsp_init_mips.c           |  270 +-
 libavcodec/mips/qpeldsp_msa.c                 |   88 +-
 libavcodec/mips/sbrdsp_mips.c                 |   19 +-
 libavcodec/mips/simple_idct_mmi.c             |   65 +-
 libavcodec/mips/simple_idct_msa.c             |   98 +-
 libavcodec/mips/vc1dsp_init_mips.c            |  164 +-
 libavcodec/mips/vc1dsp_mips.h                 |   31 +-
 libavcodec/mips/vc1dsp_mmi.c                  |  218 +-
 libavcodec/mips/vc1dsp_msa.c                  |  461 ++
 libavcodec/mips/videodsp_init.c               |   10 +-
 libavcodec/mips/vp3dsp_idct_mmi.c             |  143 +-
 libavcodec/mips/vp3dsp_idct_msa.c             |   68 +-
 libavcodec/mips/vp3dsp_init_mips.c            |   44 +-
 libavcodec/mips/vp8_idct_msa.c                |    5 +-
 libavcodec/mips/vp8_mc_msa.c                  |    4 +-
 libavcodec/mips/vp8dsp_init_mips.c            |  240 +-
 libavcodec/mips/vp8dsp_mmi.c                  |  443 +-
 libavcodec/mips/vp9_idct_msa.c                |   13 +-
 libavcodec/mips/vp9_lpf_msa.c                 |    3 +-
 libavcodec/mips/vp9_mc_mmi.c                  |  138 +-
 libavcodec/mips/vp9_mc_msa.c                  |   16 +-
 libavcodec/mips/vp9dsp_init_mips.c            |   16 +-
 libavcodec/mips/wmv2dsp_init_mips.c           |   18 +-
 libavcodec/mips/wmv2dsp_mips.h                |    4 +-
 libavcodec/mips/wmv2dsp_mmi.c                 |    6 +-
 libavcodec/mips/xvid_idct_mmi.c               |    4 +-
 libavcodec/mips/xvididct_init_mips.c          |   31 +-
 libavcodec/mips/xvididct_mips.h               |    4 +-
 libavcodec/vc1dsp.c                           |    3 +-
 libavcodec/vc1dsp.h                           |    1 +
 libavcodec/videodsp.c                         |    2 +
 libavcodec/videodsp.h                         |    1 +
 libavcodec/vp8dsp.c                           |    2 +
 libavcodec/vp8dsp.h                           |    1 +
 libavcodec/vp9dsp.c                           |    1 +
 libavcodec/vp9dsp.h                           |    1 +
 libavutil/cpu.c                               |   20 +
 libavutil/cpu.h                               |    7 +
 libavutil/cpu_internal.h                      |    4 +
 libavutil/intmath.h                           |    3 +
 libavutil/loongarch/Makefile                  |    1 +
 libavutil/loongarch/cpu.c                     |   69 +
 libavutil/loongarch/cpu.h                     |   31 +
 libavutil/loongarch/intmath.h                 |   73 +
 libavutil/loongarch/loongson_intrinsics.h     | 1948 ++++++++
 libavutil/loongarch/timer.h                   |   48 +
 libavutil/mips/Makefile                       |    2 +-
 libavutil/mips/asmdefs.h                      |   50 +
 libavutil/mips/cpu.c                          |  134 +
 libavutil/mips/cpu.h                          |   28 +
 libavutil/mips/generic_macros_msa.h           |  255 +-
 libavutil/mips/mmiutils.h                     |  142 +-
 libavutil/tests/cpu.c                         |    6 +
 libavutil/timer.h                             |    2 +
 libswscale/loongarch/Makefile                 |   12 +
 libswscale/loongarch/input.S                  |  285 ++
 libswscale/loongarch/input_lasx.c             |  189 +
 libswscale/loongarch/output.S                 |  138 +
 libswscale/loongarch/output_lasx.c            | 1980 ++++++++
 libswscale/loongarch/output_lsx.c             | 1828 +++++++
 libswscale/loongarch/rgb2rgb_lasx.c           |   53 +
 libswscale/loongarch/swscale.S                | 1868 ++++++++
 libswscale/loongarch/swscale_init_loongarch.c |  147 +
 libswscale/loongarch/swscale_lasx.c           |  949 ++++
 libswscale/loongarch/swscale_loongarch.h      |  132 +
 libswscale/loongarch/swscale_lsx.c            |   57 +
 libswscale/loongarch/yuv2rgb_lasx.c           |  330 ++
 libswscale/loongarch/yuv2rgb_lsx.c            |  361 ++
 libswscale/mips/Makefile                      |    5 +
 libswscale/mips/rgb2rgb_init_mips.c           |   33 +
 libswscale/mips/rgb2rgb_mips.h                |   32 +
 libswscale/mips/rgb2rgb_msa.c                 |   51 +
 libswscale/mips/swscale_init_mips.c           |  243 +
 libswscale/mips/swscale_mips.h                |  417 ++
 libswscale/mips/swscale_msa.c                 | 2164 +++++++++
 libswscale/mips/yuv2rgb_msa.c                 |  251 +
 libswscale/rgb2rgb.c                          |    4 +
 libswscale/rgb2rgb.h                          |    2 +
 libswscale/swscale.c                          |    4 +
 libswscale/swscale_internal.h                 |    4 +
 libswscale/utils.c                            |   34 +-
 libswscale/yuv2rgb.c                          |    4 +
 tests/checkasm/checkasm.c                     |    6 +
 207 files changed, 57411 insertions(+), 5992 deletions(-)
 create mode 100644 libavcodec/loongarch/Makefile
 create mode 100644 libavcodec/loongarch/cabac.h
 create mode 100644 libavcodec/loongarch/h264_cabac.c
 create mode 100644 libavcodec/loongarch/h264_deblock_lasx.c
 create mode 100644 libavcodec/loongarch/h264_intrapred_init_loongarch.c
 create mode 100644 libavcodec/loongarch/h264_intrapred_lasx.h
 create mode 100644 libavcodec/loongarch/h264chroma.S
 create mode 100644 libavcodec/loongarch/h264chroma_init_loongarch.c
 create mode 100644 libavcodec/loongarch/h264chroma_lasx.h
 create mode 100644 libavcodec/loongarch/h264dsp.S
 create mode 100644 libavcodec/loongarch/h264dsp_init_loongarch.c
 create mode 100644 libavcodec/loongarch/h264dsp_lasx.c
 create mode 100644 libavcodec/loongarch/h264dsp_lasx.h
 create mode 100644 libavcodec/loongarch/h264idct.S
 create mode 100644 libavcodec/loongarch/h264idct_c.c
 create mode 100644 libavcodec/loongarch/h264intrapred.S
 create mode 100644 libavcodec/loongarch/h264qpel.S
 create mode 100644 libavcodec/loongarch/h264qpel_init_loongarch.c
 create mode 100644 libavcodec/loongarch/h264qpel_lasx.c
 create mode 100644 libavcodec/loongarch/h264qpel_lasx.h
 create mode 100644 libavcodec/loongarch/h264qpel_lsx.c
 create mode 100644 libavcodec/loongarch/h264qpel_lsx.h
 create mode 100644 libavcodec/loongarch/hevc_idct_lsx.c
 create mode 100644 libavcodec/loongarch/hevc_lpf_sao_lsx.c
 create mode 100644 libavcodec/loongarch/hevc_mc_bi_lsx.c
 create mode 100644 libavcodec/loongarch/hevc_mc_uni_lsx.c
 create mode 100644 libavcodec/loongarch/hevc_mc_uniw_lsx.c
 create mode 100644 libavcodec/loongarch/hevcdsp_init_loongarch.c
 create mode 100644 libavcodec/loongarch/hevcdsp_lsx.c
 create mode 100644 libavcodec/loongarch/hevcdsp_lsx.h
 create mode 100644 libavcodec/loongarch/hpeldsp_init_loongarch.c
 create mode 100644 libavcodec/loongarch/hpeldsp_lasx.c
 create mode 100644 libavcodec/loongarch/hpeldsp_lasx.h
 create mode 100644 libavcodec/loongarch/idctdsp_init_loongarch.c
 create mode 100644 libavcodec/loongarch/idctdsp_lasx.c
 create mode 100644 libavcodec/loongarch/idctdsp_loongarch.h
 create mode 100644 libavcodec/loongarch/loongson_asm.S
 create mode 100644 libavcodec/loongarch/simple_idct_lasx.c
 create mode 100644 libavcodec/loongarch/vc1dsp_init_loongarch.c
 create mode 100644 libavcodec/loongarch/vc1dsp_lasx.c
 create mode 100644 libavcodec/loongarch/vc1dsp_loongarch.h
 create mode 100644 libavcodec/loongarch/videodsp_init.c
 create mode 100644 libavcodec/loongarch/vp8_lpf_lsx.c
 create mode 100644 libavcodec/loongarch/vp8_mc_lsx.c
 create mode 100644 libavcodec/loongarch/vp8dsp_init_loongarch.c
 create mode 100644 libavcodec/loongarch/vp8dsp_loongarch.h
 create mode 100644 libavcodec/loongarch/vp9_idct_lsx.c
 create mode 100644 libavcodec/loongarch/vp9_intra_lsx.c
 create mode 100644 libavcodec/loongarch/vp9_lpf_lsx.c
 create mode 100644 libavcodec/loongarch/vp9_mc_lsx.c
 create mode 100644 libavcodec/loongarch/vp9dsp_init_loongarch.c
 create mode 100644 libavcodec/loongarch/vp9dsp_loongarch.h
 create mode 100644 libavcodec/mips/h264_deblock_msa.c
 create mode 100644 libavcodec/mips/vc1dsp_msa.c
 create mode 100644 libavutil/loongarch/Makefile
 create mode 100644 libavutil/loongarch/cpu.c
 create mode 100644 libavutil/loongarch/cpu.h
 create mode 100644 libavutil/loongarch/intmath.h
 create mode 100644 libavutil/loongarch/loongson_intrinsics.h
 create mode 100644 libavutil/loongarch/timer.h
 create mode 100644 libavutil/mips/cpu.c
 create mode 100644 libavutil/mips/cpu.h
 create mode 100644 libswscale/loongarch/Makefile
 create mode 100644 libswscale/loongarch/input.S
 create mode 100644 libswscale/loongarch/input_lasx.c
 create mode 100644 libswscale/loongarch/output.S
 create mode 100644 libswscale/loongarch/output_lasx.c
 create mode 100644 libswscale/loongarch/output_lsx.c
 create mode 100644 libswscale/loongarch/rgb2rgb_lasx.c
 create mode 100644 libswscale/loongarch/swscale.S
 create mode 100644 libswscale/loongarch/swscale_init_loongarch.c
 create mode 100644 libswscale/loongarch/swscale_lasx.c
 create mode 100644 libswscale/loongarch/swscale_loongarch.h
 create mode 100644 libswscale/loongarch/swscale_lsx.c
 create mode 100644 libswscale/loongarch/yuv2rgb_lasx.c
 create mode 100644 libswscale/loongarch/yuv2rgb_lsx.c
 create mode 100644 libswscale/mips/Makefile
 create mode 100644 libswscale/mips/rgb2rgb_init_mips.c
 create mode 100644 libswscale/mips/rgb2rgb_mips.h
 create mode 100644 libswscale/mips/rgb2rgb_msa.c
 create mode 100644 libswscale/mips/swscale_init_mips.c
 create mode 100644 libswscale/mips/swscale_mips.h
 create mode 100644 libswscale/mips/swscale_msa.c
 create mode 100644 libswscale/mips/yuv2rgb_msa.c

diff --git a/Makefile b/Makefile
index 532372c9c4..55e2b6f8d2 100644
--- a/Makefile
+++ b/Makefile
@@ -75,7 +75,7 @@ SUBDIR_VARS := CLEANFILES FFLIBS HOSTPROGS TESTPROGS TOOLS               \
                ARMV5TE-OBJS ARMV6-OBJS ARMV8-OBJS VFP-OBJS NEON-OBJS     \
                ALTIVEC-OBJS VSX-OBJS MMX-OBJS X86ASM-OBJS                \
                MIPSFPU-OBJS MIPSDSPR2-OBJS MIPSDSP-OBJS MSA-OBJS         \
-               MMI-OBJS OBJS SLIBOBJS HOSTOBJS TESTOBJS
+               MMI-OBJS LSX-OBJS LASX-OBJS OBJS SLIBOBJS HOSTOBJS TESTOBJS
 
 define RESET
 $(1) :=
diff --git a/configure b/configure
index 34c2adb4a4..5f3c516770 100755
--- a/configure
+++ b/configure
@@ -442,10 +442,11 @@ Optimization options (experts only):
   --disable-mipsdsp        disable MIPS DSP ASE R1 optimizations
   --disable-mipsdspr2      disable MIPS DSP ASE R2 optimizations
   --disable-msa            disable MSA optimizations
-  --disable-msa2           disable MSA2 optimizations
   --disable-mipsfpu        disable floating point MIPS optimizations
   --disable-mmi            disable Loongson SIMD optimizations
   --disable-fast-unaligned consider unaligned accesses slow
+  --disable-lsx            disable LSX optimizations
+  --disable-lasx           disable LASX optimizations
 
 Developer options (useful when working on FFmpeg itself):
   --disable-debug          disable debugging symbols
@@ -1983,6 +1984,9 @@ ARCH_LIST="
     x86
     x86_32
     x86_64
+    loongarch
+    loongarch32
+    loongarch64
 "
 
 ARCH_EXT_LIST_ARM="
@@ -2006,13 +2010,14 @@ ARCH_EXT_LIST_MIPS="
     mipsdsp
     mipsdspr2
     msa
-    msa2
 "
 
 ARCH_EXT_LIST_LOONGSON="
     loongson2
     loongson3
     mmi
+    lsx
+    lasx
 "
 
 ARCH_EXT_LIST_X86_SIMD="
@@ -2525,6 +2530,10 @@ power8_deps="vsx"
 
 loongson2_deps="mips"
 loongson3_deps="mips"
+mmi_deps_any="loongson2 loongson3"
+lsx_deps="loongarch"
+lasx_deps="lsx"
+
 mips32r2_deps="mips"
 mips32r5_deps="mips"
 mips32r6_deps="mips"
@@ -2533,9 +2542,7 @@ mips64r6_deps="mips"
 mipsfpu_deps="mips"
 mipsdsp_deps="mips"
 mipsdspr2_deps="mips"
-mmi_deps="mips"
 msa_deps="mipsfpu"
-msa2_deps="msa"
 
 cpunop_deps="i686"
 x86_64_select="i686"
@@ -2572,8 +2579,8 @@ for ext in $(filter_out mmx $ARCH_EXT_LIST_X86_SIMD); do
 done
 
 aligned_stack_if_any="aarch64 ppc x86"
-fast_64bit_if_any="aarch64 alpha ia64 mips64 parisc64 ppc64 sparc64 x86_64"
-fast_clz_if_any="aarch64 alpha avr32 mips ppc x86"
+fast_64bit_if_any="aarch64 alpha ia64 mips64 parisc64 ppc64 sparc64 x86_64 loongarch64"
+fast_clz_if_any="aarch64 alpha avr32 mips ppc x86 loongarch"
 fast_unaligned_if_any="aarch64 ppc x86"
 simd_align_16_if_any="altivec neon sse"
 simd_align_32_if_any="avx"
@@ -4773,6 +4780,9 @@ case "$arch" in
     arm*|iPad*|iPhone*)
         arch="arm"
     ;;
+    loongarch*)
+        arch="loongarch"
+    ;;
     mips*|IP*)
         case "$arch" in
         *el)
@@ -4917,8 +4927,6 @@ elif enabled bfin; then
 
 elif enabled mips; then
 
-    cpuflags="-march=$cpu"
-
     if [ "$cpu" != "generic" ]; then
         disable mips32r2
         disable mips32r5
@@ -4927,19 +4935,53 @@ elif enabled mips; then
         disable mips64r6
         disable loongson2
         disable loongson3
+        disable mipsdsp
+        disable mipsdspr2
+
+        cpuflags="-march=$cpu"
 
         case $cpu in
-            24kc|24kf*|24kec|34kc|1004kc|24kef*|34kf*|1004kf*|74kc|74kf)
+            # General ISA levels
+            mips1|mips3)
+            ;;
+            mips32r2)
                 enable mips32r2
-                disable msa
             ;;
-            p5600|i6400|p6600)
-                disable mipsdsp
-                disable mipsdspr2
+            mips32r5)
+                enable mips32r2
+                enable mips32r5
             ;;
-            loongson*)
-                enable loongson2
+            mips64r2|mips64r5)
+                enable mips64r2
                 enable loongson3
+            ;;
+            # Cores from MIPS(MTI)
+            24kc)
+                disable mipsfpu
+                enable mips32r2
+            ;;
+            24kf*|24kec|34kc|74Kc|1004kc)
+                enable mips32r2
+            ;;
+            24kef*|34kf*|1004kf*)
+                enable mipsdsp
+                enable mips32r2
+            ;;
+            p5600)
+                enable mips32r2
+                enable mips32r5
+                check_cflags "-mtune=p5600" && check_cflags "-msched-weight -mload-store-pairs -funroll-loops"
+            ;;
+            i6400)
+                enable mips64r6
+                check_cflags "-mtune=i6400 -mabi=64" && check_cflags "-msched-weight -mload-store-pairs -funroll-loops" && check_ldflags "-mabi=64"
+            ;;
+            p6600)
+                enable mips64r6
+                check_cflags "-mtune=p6600 -mabi=64" && check_cflags "-msched-weight -mload-store-pairs -funroll-loops" && check_ldflags "-mabi=64"
+            ;;
+            # Cores from Loongson
+            loongson2e|loongson2f|loongson3*)
                 enable local_aligned
                 enable simd_align_16
                 enable fast_64bit
@@ -4947,75 +4989,42 @@ elif enabled mips; then
                 enable fast_cmov
                 enable fast_unaligned
                 disable aligned_stack
-                disable mipsdsp
-                disable mipsdspr2
                 # When gcc version less than 5.3.0, add -fno-expensive-optimizations flag.
-                if [ $cc == gcc ]; then
-                    gcc_version=$(gcc -dumpversion)
-                    if [ "$(echo "$gcc_version 5.3.0" | tr " " "\n" | sort -rV | head -n 1)" == "$gcc_version" ]; then
-                        expensive_optimization_flag=""
-                    else
+                if test "$cc_type" = "gcc"; then
+                    case $gcc_basever in
+                        2|2.*|3.*|4.*|5.0|5.1|5.2)
                         expensive_optimization_flag="-fno-expensive-optimizations"
-                    fi
+                        ;;
+                        *)
+                        expensive_optimization_flag=""
+                        ;;
+                    esac
                 fi
+
                 case $cpu in
                     loongson3*)
+                        enable loongson3
                         cpuflags="-march=loongson3a -mhard-float $expensive_optimization_flag"
                     ;;
                     loongson2e)
+                        enable loongson2
                         cpuflags="-march=loongson2e -mhard-float $expensive_optimization_flag"
                     ;;
                     loongson2f)
+                        enable loongson2
                         cpuflags="-march=loongson2f -mhard-float $expensive_optimization_flag"
                     ;;
                 esac
             ;;
             *)
-                # Unknown CPU. Disable everything.
-                warn "unknown CPU. Disabling all MIPS optimizations."
-                disable mipsfpu
-                disable mipsdsp
-                disable mipsdspr2
-                disable msa
-                disable mmi
+                warn "unknown MIPS CPU"
             ;;
         esac
 
-        case $cpu in
-            24kc)
-                disable mipsfpu
-                disable mipsdsp
-                disable mipsdspr2
-            ;;
-            24kf*)
-                disable mipsdsp
-                disable mipsdspr2
-            ;;
-            24kec|34kc|1004kc)
-                disable mipsfpu
-                disable mipsdspr2
-            ;;
-            24kef*|34kf*|1004kf*)
-                disable mipsdspr2
-            ;;
-            74kc)
-                disable mipsfpu
-            ;;
-            p5600)
-                enable mips32r5
-                check_cflags "-mtune=p5600" && check_cflags "-msched-weight -mload-store-pairs -funroll-loops"
-            ;;
-            i6400)
-                enable mips64r6
-                check_cflags "-mtune=i6400 -mabi=64" && check_cflags "-msched-weight -mload-store-pairs -funroll-loops" && check_ldflags "-mabi=64"
-            ;;
-            p6600)
-                enable mips64r6
-                check_cflags "-mtune=p6600 -mabi=64" && check_cflags "-msched-weight -mload-store-pairs -funroll-loops" && check_ldflags "-mabi=64"
-            ;;
-        esac
     else
-        # We do not disable anything. Is up to the user to disable the unwanted features.
+        disable mipsdsp
+        disable mipsdspr2
+        # Disable DSP stuff for generic CPU, it can't be detected at runtime.
         warn 'generic cpu selected'
     fi
 
@@ -5120,6 +5129,20 @@ elif enabled x86; then
         ;;
     esac
 
+elif enabled loongarch; then
+    # TODO: pending to test.
+    enable local_aligned
+    enable simd_align_16
+    enable fast_64bit
+    enable fast_clz
+    enable fast_cmov
+    enable fast_unaligned
+    disable aligned_stack
+    case $cpu in
+        gs464v)
+            cpuflags="-march=$cpu"
+        ;;
+    esac
 fi
 
 if [ "$cpu" != generic ]; then
@@ -5202,6 +5225,11 @@ case "$arch" in
             objformat=elf64
         fi
     ;;
+    loongarch)
+        check_64bit loongarch32 loongarch64
+        enabled loongarch64 && disable loongarch32
+        enabled shared && enable_weak pic
+    ;;
 esac
 
 # OS specific
@@ -5756,28 +5784,45 @@ EOF
 
 elif enabled mips; then
 
-    enabled loongson2 && check_inline_asm loongson2 '"dmult.g $8, $9, $10"'
-    enabled loongson3 && check_inline_asm loongson3 '"gsldxc1 $f0, 0($2, $3)"'
-    enabled mmi && check_inline_asm mmi '"punpcklhw $f0, $f0, $f0"'
-
-    # Enable minimum ISA based on selected options
+    # Check toolchain ISA level
     if enabled mips64; then
-        enabled mips64r6 && check_inline_asm_flags mips64r6 '"dlsa $0, $0, $0, 1"' '-mips64r6'
-        enabled mips64r2 && check_inline_asm_flags mips64r2 '"dext $0, $0, 0, 1"' '-mips64r2'
-        disabled mips64r6 && disabled mips64r2 && check_inline_asm_flags mips64r1 '"daddi $0, $0, 0"' '-mips64'
+        enabled mips64r6 && check_inline_asm mips64r6 '"dlsa $0, $0, $0, 1"' &&
+            disable mips64r2
+
+        enabled mips64r2 && check_inline_asm mips64r2 '"dext $0, $0, 0, 1"'
+
+        disable mips32r6 && disable mips32r5 && disable mips32r2
     else
-        enabled mips32r6 && check_inline_asm_flags mips32r6 '"aui $0, $0, 0"' '-mips32r6'
-        enabled mips32r5 && check_inline_asm_flags mips32r5 '"eretnc"' '-mips32r5'
-        enabled mips32r2 && check_inline_asm_flags mips32r2 '"ext $0, $0, 0, 1"' '-mips32r2'
-        disabled mips32r6 && disabled mips32r5 && disabled mips32r2 && check_inline_asm_flags mips32r1 '"addi $0, $0, 0"' '-mips32'
+        enabled mips32r6 && check_inline_asm mips32r6 '"aui $0, $0, 0"' &&
+            disable mips32r5 && disable mips32r2
+
+        enabled mips32r5 && check_inline_asm mips32r5 '"eretnc"'
+        enabled mips32r2 && check_inline_asm mips32r2 '"ext $0, $0, 0, 1"'
+
+        disable mips64r6 && disable mips64r5 && disable mips64r2
     fi
 
-    enabled mipsfpu && check_inline_asm_flags mipsfpu '"cvt.d.l $f0, $f2"' '-mhard-float'
+    enabled mipsfpu && check_inline_asm mipsfpu '"cvt.d.l $f0, $f2"'
     enabled mipsfpu && (enabled mips32r5 || enabled mips32r6 || enabled mips64r6) && check_inline_asm_flags mipsfpu '"cvt.d.l $f0, $f1"' '-mfp64'
-    enabled mipsfpu && enabled msa && check_inline_asm_flags msa '"addvi.b $w0, $w1, 1"' '-mmsa' && check_headers msa.h || disable msa
+
     enabled mipsdsp && check_inline_asm_flags mipsdsp '"addu.qb $t0, $t1, $t2"' '-mdsp'
     enabled mipsdspr2 && check_inline_asm_flags mipsdspr2 '"absq_s.qb $t0, $t1"' '-mdspr2'
-    enabled msa && enabled msa2 && check_inline_asm_flags msa2 '"nxbits.any.b $w0, $w0"' '-mmsa2' && check_headers msa2.h || disable msa2
+
+    # MSA can be detected at runtime so we supply extra flags here
+    enabled mipsfpu && enabled msa && check_inline_asm msa '"addvi.b $w0, $w1, 1"' '-mmsa' && append MSAFLAGS '-mmsa'
+
+    # loongson2 have no switch cflag so we can only probe toolchain ability
+    enabled loongson2 && check_inline_asm loongson2 '"dmult.g $8, $9, $10"' && disable loongson3
+
+    # loongson3 is paired with MMI
+    enabled loongson3 && check_inline_asm loongson3 '"gsldxc1 $f0, 0($2, $3)"' '-mloongson-ext' && append MMIFLAGS '-mloongson-ext'
+
+    # MMI can be detected at runtime too
+    enabled mmi && check_inline_asm mmi '"pxor $f0, $f0, $f0"' '-mloongson-mmi' && append MMIFLAGS '-mloongson-mmi'
+
+    # TODO: The following code is intended to fix h264 decoding module compilation error,
+    # but it is not resonable. Maybe we should fix it in the feature.
+    enabled mmi && h264_decoder_select="$h264_decoder_select hpeldsp"
 
     if enabled bigendian && enabled msa; then
         disable msa
@@ -5896,6 +5941,9 @@ EOF
         ;;
     esac
 
+elif enabled loongarch; then
+    enabled lsx && check_inline_asm lsx '"vadd.b $vr0, $vr1, $vr2"' '-mlsx' && append LSXFLAGS '-mlsx'
+    enabled lasx && check_inline_asm lasx '"xvadd.b $xr0, $xr1, $xr2"' '-mlasx' && append LASXFLAGS '-mlasx'
 fi
 
 check_cc intrinsics_neon arm_neon.h "int16x8_t test = vdupq_n_s16(0)"
@@ -7155,7 +7203,6 @@ if enabled mips; then
     echo "MIPS DSP R1 enabled       ${mipsdsp-no}"
     echo "MIPS DSP R2 enabled       ${mipsdspr2-no}"
     echo "MIPS MSA enabled          ${msa-no}"
-    echo "MIPS MSA2 enabled         ${msa2-no}"
     echo "LOONGSON MMI enabled      ${mmi-no}"
 fi
 if enabled ppc; then
@@ -7165,6 +7212,10 @@ if enabled ppc; then
     echo "PPC 4xx optimizations     ${ppc4xx-no}"
     echo "dcbzl available           ${dcbzl-no}"
 fi
+if enabled loongarch; then
+    echo "LSX enabled               ${lsx-no}"
+    echo "LASX enabled              ${lasx-no}"
+fi
 echo "debug symbols             ${debug-no}"
 echo "strip symbols             ${stripping-no}"
 echo "optimize for size         ${small-no}"
@@ -7324,6 +7375,10 @@ LDSOFLAGS=$LDSOFLAGS
 SHFLAGS=$(echo $($ldflags_filter $SHFLAGS))
 ASMSTRIPFLAGS=$ASMSTRIPFLAGS
 X86ASMFLAGS=$X86ASMFLAGS
+MSAFLAGS=$MSAFLAGS
+MMIFLAGS=$MMIFLAGS
+LSXFLAGS=$LSXFLAGS
+LASXFLAGS=$LASXFLAGS
 BUILDSUF=$build_suffix
 PROGSSUF=$progs_suffix
 FULLNAME=$FULLNAME
diff --git a/ffbuild/arch.mak b/ffbuild/arch.mak
index e09006efca..997e31e85e 100644
--- a/ffbuild/arch.mak
+++ b/ffbuild/arch.mak
@@ -8,7 +8,9 @@ OBJS-$(HAVE_MIPSFPU)   += $(MIPSFPU-OBJS)    $(MIPSFPU-OBJS-yes)
 OBJS-$(HAVE_MIPSDSP)   += $(MIPSDSP-OBJS)    $(MIPSDSP-OBJS-yes)
 OBJS-$(HAVE_MIPSDSPR2) += $(MIPSDSPR2-OBJS)  $(MIPSDSPR2-OBJS-yes)
 OBJS-$(HAVE_MSA)       += $(MSA-OBJS)        $(MSA-OBJS-yes)
-OBJS-$(HAVE_MMI)   += $(MMI-OBJS)   $(MMI-OBJS-yes)
+OBJS-$(HAVE_MMI)       += $(MMI-OBJS)        $(MMI-OBJS-yes)
+OBJS-$(HAVE_LSX)       += $(LSX-OBJS)        $(LSX-OBJS-yes)
+OBJS-$(HAVE_LASX)      += $(LASX-OBJS)       $(LASX-OBJS-yes)
 
 OBJS-$(HAVE_ALTIVEC) += $(ALTIVEC-OBJS) $(ALTIVEC-OBJS-yes)
 OBJS-$(HAVE_VSX)     += $(VSX-OBJS) $(VSX-OBJS-yes)
diff --git a/ffbuild/common.mak b/ffbuild/common.mak
index 7355508ea0..a30e127e64 100644
--- a/ffbuild/common.mak
+++ b/ffbuild/common.mak
@@ -44,7 +44,7 @@ LDFLAGS    := $(ALLFFLIBS:%=$(LD_PATH)lib%) $(LDFLAGS)
 
 define COMPILE
        $(call $(1)DEP,$(1))
-       $($(1)) $($(1)FLAGS) $($(1)_DEPFLAGS) $($(1)_C) $($(1)_O) $(patsubst $(SRC_PATH)/%,$(SRC_LINK)/%,$<)
+       $($(1)) $($(1)FLAGS) $($(2)) $($(1)_DEPFLAGS) $($(1)_C) $($(1)_O) $(patsubst $(SRC_PATH)/%,$(SRC_LINK)/%,$<)
 endef
 
 COMPILE_C = $(call COMPILE,CC)
@@ -54,6 +54,22 @@ COMPILE_M = $(call COMPILE,OBJCC)
 COMPILE_X86ASM = $(call COMPILE,X86ASM)
 COMPILE_HOSTC = $(call COMPILE,HOSTCC)
 COMPILE_NVCC = $(call COMPILE,NVCC)
+COMPILE_MMI = $(call COMPILE,CC,MMIFLAGS)
+COMPILE_MSA = $(call COMPILE,CC,MSAFLAGS)
+COMPILE_LSX = $(call COMPILE,CC,LSXFLAGS)
+COMPILE_LASX = $(call COMPILE,CC,LASXFLAGS)
+
+%_mmi.o: %_mmi.c
+	$(COMPILE_MMI)
+
+%_msa.o: %_msa.c
+	$(COMPILE_MSA)
+
+%_lsx.o: %_lsx.c
+	$(COMPILE_LSX)
+
+%_lasx.o: %_lasx.c
+	$(COMPILE_LASX)
 
 %.o: %.c
 	$(COMPILE_C)
diff --git a/fftools/ffmpeg.c b/fftools/ffmpeg.c
index 01f04103cf..df66929597 100644
--- a/fftools/ffmpeg.c
+++ b/fftools/ffmpeg.c
@@ -1702,7 +1702,7 @@ static void print_report(int is_last_report, int64_t timer_start, int64_t cur_ti
             frame_number = ost->frame_number;
             fps = t > 1 ? frame_number / t : 0;
             av_bprintf(&buf, "frame=%5d fps=%3.*f q=%3.1f ",
-                     frame_number, fps < 9.95, fps, q);
+                     frame_number, fps < 100, fps, q);
             av_bprintf(&buf_script, "frame=%d\n", frame_number);
             av_bprintf(&buf_script, "fps=%.2f\n", fps);
             av_bprintf(&buf_script, "stream_%d_%d_q=%.1f\n",
diff --git a/libavcodec/cabac_functions.h b/libavcodec/cabac_functions.h
index bb2b4210b7..fad9d7c10f 100644
--- a/libavcodec/cabac_functions.h
+++ b/libavcodec/cabac_functions.h
@@ -48,6 +48,9 @@
 #if ARCH_MIPS
 #   include "mips/cabac.h"
 #endif
+#if ARCH_LOONGARCH64
+#   include "loongarch/cabac.h"
+#endif
 
 static const uint8_t * const ff_h264_norm_shift = ff_h264_cabac_tables + H264_NORM_SHIFT_OFFSET;
 static const uint8_t * const ff_h264_lps_range = ff_h264_cabac_tables + H264_LPS_RANGE_OFFSET;
diff --git a/libavcodec/h264_cabac.c b/libavcodec/h264_cabac.c
index 815149a501..d4ddcba9e7 100644
--- a/libavcodec/h264_cabac.c
+++ b/libavcodec/h264_cabac.c
@@ -44,6 +44,8 @@
 
 #if ARCH_X86
 #include "x86/h264_cabac.c"
+#elif ARCH_LOONGARCH64
+#include "loongarch/h264_cabac.c"
 #endif
 
 /* Cabac pre state table */
diff --git a/libavcodec/h264chroma.c b/libavcodec/h264chroma.c
index c2f1f30f5a..279a6ada7f 100644
--- a/libavcodec/h264chroma.c
+++ b/libavcodec/h264chroma.c
@@ -56,4 +56,6 @@ av_cold void ff_h264chroma_init(H264ChromaContext *c, int bit_depth)
         ff_h264chroma_init_x86(c, bit_depth);
     if (ARCH_MIPS)
         ff_h264chroma_init_mips(c, bit_depth);
+    if (ARCH_LOONGARCH)
+        ff_h264chroma_init_loongarch(c, bit_depth);
 }
diff --git a/libavcodec/h264chroma.h b/libavcodec/h264chroma.h
index 5c89fd12df..3259b4935f 100644
--- a/libavcodec/h264chroma.h
+++ b/libavcodec/h264chroma.h
@@ -36,5 +36,6 @@ void ff_h264chroma_init_arm(H264ChromaContext *c, int bit_depth);
 void ff_h264chroma_init_ppc(H264ChromaContext *c, int bit_depth);
 void ff_h264chroma_init_x86(H264ChromaContext *c, int bit_depth);
 void ff_h264chroma_init_mips(H264ChromaContext *c, int bit_depth);
+void ff_h264chroma_init_loongarch(H264ChromaContext *c, int bit_depth);
 
 #endif /* AVCODEC_H264CHROMA_H */
diff --git a/libavcodec/h264dsp.c b/libavcodec/h264dsp.c
index d26f552369..436ce68f0b 100644
--- a/libavcodec/h264dsp.c
+++ b/libavcodec/h264dsp.c
@@ -158,4 +158,5 @@ av_cold void ff_h264dsp_init(H264DSPContext *c, const int bit_depth,
     if (ARCH_PPC) ff_h264dsp_init_ppc(c, bit_depth, chroma_format_idc);
     if (ARCH_X86) ff_h264dsp_init_x86(c, bit_depth, chroma_format_idc);
     if (ARCH_MIPS) ff_h264dsp_init_mips(c, bit_depth, chroma_format_idc);
+    if (ARCH_LOONGARCH) ff_h264dsp_init_loongarch(c, bit_depth, chroma_format_idc);
 }
diff --git a/libavcodec/h264dsp.h b/libavcodec/h264dsp.h
index cbea3173c6..35744adbd1 100644
--- a/libavcodec/h264dsp.h
+++ b/libavcodec/h264dsp.h
@@ -129,5 +129,7 @@ void ff_h264dsp_init_x86(H264DSPContext *c, const int bit_depth,
                          const int chroma_format_idc);
 void ff_h264dsp_init_mips(H264DSPContext *c, const int bit_depth,
                           const int chroma_format_idc);
+void ff_h264dsp_init_loongarch(H264DSPContext *c, const int bit_depth,
+                               const int chroma_format_idc);
 
 #endif /* AVCODEC_H264DSP_H */
diff --git a/libavcodec/h264pred.c b/libavcodec/h264pred.c
index 5632a58fd7..718f2819ce 100644
--- a/libavcodec/h264pred.c
+++ b/libavcodec/h264pred.c
@@ -600,4 +600,6 @@ av_cold void ff_h264_pred_init(H264PredContext *h, int codec_id,
         ff_h264_pred_init_x86(h, codec_id, bit_depth, chroma_format_idc);
     if (ARCH_MIPS)
         ff_h264_pred_init_mips(h, codec_id, bit_depth, chroma_format_idc);
+    if (ARCH_LOONGARCH)
+        ff_h264_pred_init_loongarch(h, codec_id, bit_depth, chroma_format_idc);
 }
diff --git a/libavcodec/h264pred.h b/libavcodec/h264pred.h
index 2863dc9bd1..4583052dfe 100644
--- a/libavcodec/h264pred.h
+++ b/libavcodec/h264pred.h
@@ -122,5 +122,7 @@ void ff_h264_pred_init_x86(H264PredContext *h, int codec_id,
                            const int bit_depth, const int chroma_format_idc);
 void ff_h264_pred_init_mips(H264PredContext *h, int codec_id,
                             const int bit_depth, const int chroma_format_idc);
+void ff_h264_pred_init_loongarch(H264PredContext *h, int codec_id,
+                                 const int bit_depth, const int chroma_format_idc);
 
 #endif /* AVCODEC_H264PRED_H */
diff --git a/libavcodec/h264qpel.c b/libavcodec/h264qpel.c
index 50e82e23b0..9f8cfbb474 100644
--- a/libavcodec/h264qpel.c
+++ b/libavcodec/h264qpel.c
@@ -106,4 +106,6 @@ av_cold void ff_h264qpel_init(H264QpelContext *c, int bit_depth)
         ff_h264qpel_init_x86(c, bit_depth);
     if (ARCH_MIPS)
         ff_h264qpel_init_mips(c, bit_depth);
+    if (ARCH_LOONGARCH)
+        ff_h264qpel_init_loongarch(c, bit_depth);
 }
diff --git a/libavcodec/h264qpel.h b/libavcodec/h264qpel.h
index 7c57ad001c..0259e8de23 100644
--- a/libavcodec/h264qpel.h
+++ b/libavcodec/h264qpel.h
@@ -36,5 +36,6 @@ void ff_h264qpel_init_arm(H264QpelContext *c, int bit_depth);
 void ff_h264qpel_init_ppc(H264QpelContext *c, int bit_depth);
 void ff_h264qpel_init_x86(H264QpelContext *c, int bit_depth);
 void ff_h264qpel_init_mips(H264QpelContext *c, int bit_depth);
+void ff_h264qpel_init_loongarch(H264QpelContext *c, int bit_depth);
 
 #endif /* AVCODEC_H264QPEL_H */
diff --git a/libavcodec/hevcdsp.c b/libavcodec/hevcdsp.c
index 957e40d5ff..b26f7f5f38 100644
--- a/libavcodec/hevcdsp.c
+++ b/libavcodec/hevcdsp.c
@@ -265,4 +265,6 @@ int i = 0;
         ff_hevc_dsp_init_x86(hevcdsp, bit_depth);
     if (ARCH_MIPS)
         ff_hevc_dsp_init_mips(hevcdsp, bit_depth);
+    if (ARCH_LOONGARCH)
+        ff_hevc_dsp_init_loongarch(hevcdsp, bit_depth);
 }
diff --git a/libavcodec/hevcdsp.h b/libavcodec/hevcdsp.h
index 0ae67cba85..27432ebc8c 100644
--- a/libavcodec/hevcdsp.h
+++ b/libavcodec/hevcdsp.h
@@ -131,5 +131,6 @@ void ff_hevc_dsp_init_arm(HEVCDSPContext *c, const int bit_depth);
 void ff_hevc_dsp_init_ppc(HEVCDSPContext *c, const int bit_depth);
 void ff_hevc_dsp_init_x86(HEVCDSPContext *c, const int bit_depth);
 void ff_hevc_dsp_init_mips(HEVCDSPContext *c, const int bit_depth);
+void ff_hevc_dsp_init_loongarch(HEVCDSPContext *c, const int bit_depth);
 
 #endif /* AVCODEC_HEVCDSP_H */
diff --git a/libavcodec/hpeldsp.c b/libavcodec/hpeldsp.c
index 8e2fd8fcf5..681f839ff8 100644
--- a/libavcodec/hpeldsp.c
+++ b/libavcodec/hpeldsp.c
@@ -367,4 +367,6 @@ av_cold void ff_hpeldsp_init(HpelDSPContext *c, int flags)
         ff_hpeldsp_init_x86(c, flags);
     if (ARCH_MIPS)
         ff_hpeldsp_init_mips(c, flags);
+    if (ARCH_LOONGARCH)
+        ff_hpeldsp_init_loongarch(c, flags);
 }
diff --git a/libavcodec/hpeldsp.h b/libavcodec/hpeldsp.h
index 768139bfc9..45e81b10a5 100644
--- a/libavcodec/hpeldsp.h
+++ b/libavcodec/hpeldsp.h
@@ -102,5 +102,6 @@ void ff_hpeldsp_init_arm(HpelDSPContext *c, int flags);
 void ff_hpeldsp_init_ppc(HpelDSPContext *c, int flags);
 void ff_hpeldsp_init_x86(HpelDSPContext *c, int flags);
 void ff_hpeldsp_init_mips(HpelDSPContext *c, int flags);
+void ff_hpeldsp_init_loongarch(HpelDSPContext *c, int flags);
 
 #endif /* AVCODEC_HPELDSP_H */
diff --git a/libavcodec/idctdsp.c b/libavcodec/idctdsp.c
index 846ed0b0f8..71bd03c606 100644
--- a/libavcodec/idctdsp.c
+++ b/libavcodec/idctdsp.c
@@ -315,6 +315,8 @@ av_cold void ff_idctdsp_init(IDCTDSPContext *c, AVCodecContext *avctx)
         ff_idctdsp_init_x86(c, avctx, high_bit_depth);
     if (ARCH_MIPS)
         ff_idctdsp_init_mips(c, avctx, high_bit_depth);
+    if (ARCH_LOONGARCH)
+        ff_idctdsp_init_loongarch(c, avctx, high_bit_depth);
 
     ff_init_scantable_permutation(c->idct_permutation,
                                   c->perm_type);
diff --git a/libavcodec/idctdsp.h b/libavcodec/idctdsp.h
index ca21a31a02..014488aec3 100644
--- a/libavcodec/idctdsp.h
+++ b/libavcodec/idctdsp.h
@@ -118,5 +118,7 @@ void ff_idctdsp_init_x86(IDCTDSPContext *c, AVCodecContext *avctx,
                          unsigned high_bit_depth);
 void ff_idctdsp_init_mips(IDCTDSPContext *c, AVCodecContext *avctx,
                           unsigned high_bit_depth);
+void ff_idctdsp_init_loongarch(IDCTDSPContext *c, AVCodecContext *avctx,
+                               unsigned high_bit_depth);
 
 #endif /* AVCODEC_IDCTDSP_H */
diff --git a/libavcodec/loongarch/Makefile b/libavcodec/loongarch/Makefile
new file mode 100644
index 0000000000..4af72bc0a2
--- /dev/null
+++ b/libavcodec/loongarch/Makefile
@@ -0,0 +1,37 @@
+OBJS-$(CONFIG_H264CHROMA)             += loongarch/h264chroma_init_loongarch.o
+OBJS-$(CONFIG_H264QPEL)               += loongarch/h264qpel_init_loongarch.o
+OBJS-$(CONFIG_H264DSP)                += loongarch/h264dsp_init_loongarch.o
+OBJS-$(CONFIG_H264PRED)               += loongarch/h264_intrapred_init_loongarch.o
+OBJS-$(CONFIG_VP8_DECODER)            += loongarch/vp8dsp_init_loongarch.o
+OBJS-$(CONFIG_VP9_DECODER)            += loongarch/vp9dsp_init_loongarch.o
+OBJS-$(CONFIG_VC1DSP)                 += loongarch/vc1dsp_init_loongarch.o
+OBJS-$(CONFIG_HPELDSP)                += loongarch/hpeldsp_init_loongarch.o
+OBJS-$(CONFIG_IDCTDSP)                += loongarch/idctdsp_init_loongarch.o
+OBJS-$(CONFIG_VIDEODSP)               += loongarch/videodsp_init.o
+OBJS-$(CONFIG_HEVC_DECODER)           += loongarch/hevcdsp_init_loongarch.o
+LASX-OBJS-$(CONFIG_H264QPEL)          += loongarch/h264qpel_lasx.o
+LASX-OBJS-$(CONFIG_H264DSP)           += loongarch/h264dsp_lasx.o \
+                                         loongarch/h264_deblock_lasx.o
+LASX-OBJS-$(CONFIG_VC1_DECODER)       += loongarch/vc1dsp_lasx.o
+LASX-OBJS-$(CONFIG_HPELDSP)           += loongarch/hpeldsp_lasx.o
+LASX-OBJS-$(CONFIG_IDCTDSP)           += loongarch/simple_idct_lasx.o  \
+                                         loongarch/idctdsp_lasx.o
+LSX-OBJS-$(CONFIG_VP8_DECODER)        += loongarch/vp8_mc_lsx.o \
+                                         loongarch/vp8_lpf_lsx.o
+LSX-OBJS-$(CONFIG_VP9_DECODER)        += loongarch/vp9_mc_lsx.o \
+                                         loongarch/vp9_intra_lsx.o \
+                                         loongarch/vp9_lpf_lsx.o \
+                                         loongarch/vp9_idct_lsx.o
+LSX-OBJS-$(CONFIG_HEVC_DECODER)       += loongarch/hevcdsp_lsx.o \
+                                         loongarch/hevc_idct_lsx.o \
+                                         loongarch/hevc_lpf_sao_lsx.o \
+                                         loongarch/hevc_mc_bi_lsx.o \
+                                         loongarch/hevc_mc_uni_lsx.o \
+                                         loongarch/hevc_mc_uniw_lsx.o
+LSX-OBJS-$(CONFIG_H264DSP)            += loongarch/h264idct.o \
+                                         loongarch/h264idct_c.o \
+                                         loongarch/h264dsp.o
+LSX-OBJS-$(CONFIG_H264QPEL)           += loongarch/h264qpel.o \
+                                         loongarch/h264qpel_lsx.o
+LSX-OBJS-$(CONFIG_H264CHROMA)         += loongarch/h264chroma.o
+LSX-OBJS-$(CONFIG_H264PRED)           += loongarch/h264intrapred.o
diff --git a/libavcodec/loongarch/cabac.h b/libavcodec/loongarch/cabac.h
new file mode 100644
index 0000000000..e1c946fe16
--- /dev/null
+++ b/libavcodec/loongarch/cabac.h
@@ -0,0 +1,238 @@
+/*
+ * Loongson  optimized cabac
+ *
+ * Copyright (c) 2020 Loongson Technology Corporation Limited
+ * Contributed by Shiyou Yin <yinshiyou-hf@loongson.cn>
+ *                Gu Xiwei(guxiwei-hf@loongson.cn)
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef AVCODEC_LOONGARCH_CABAC_H
+#define AVCODEC_LOONGARCH_CABAC_H
+
+#include "libavcodec/cabac.h"
+#include "config.h"
+
+#define GET_CABAC_LOONGARCH_UNCBSR                                      \
+    "ld.bu        %[bit],        %[state],       0x0           \n\t"    \
+    "andi         %[tmp0],       %[c_range],     0xC0          \n\t"    \
+    "slli.d       %[tmp0],       %[tmp0],        0x01          \n\t"    \
+    "add.d        %[tmp0],       %[tmp0],        %[tables]     \n\t"    \
+    "add.d        %[tmp0],       %[tmp0],        %[bit]        \n\t"    \
+    /* tmp1: RangeLPS */                                                \
+    "ld.bu        %[tmp1],       %[tmp0],        %[lps_off]    \n\t"    \
+                                                                        \
+    "sub.d        %[c_range],    %[c_range],     %[tmp1]       \n\t"    \
+    "slli.d       %[tmp0],       %[c_range],     0x11          \n\t"    \
+    "bge          %[tmp0],       %[c_low],       1f            \n\t"    \
+    "move         %[c_range],    %[tmp1]                       \n\t"    \
+    "nor          %[bit],        %[bit],         %[bit]        \n\t"    \
+    "sub.d        %[c_low],      %[c_low],       %[tmp0]       \n\t"    \
+                                                                        \
+    "1:                                                        \n\t"    \
+    /* tmp1: *state */                                                  \
+    "add.d        %[tmp0],       %[tables],      %[bit]        \n\t"    \
+    "ld.bu        %[tmp1],       %[tmp0],        %[mlps_off]   \n\t"    \
+    /* tmp2: lps_mask */                                                \
+    "add.d        %[tmp0],       %[tables],      %[c_range]    \n\t"    \
+    "ld.bu        %[tmp2],       %[tmp0],        %[norm_off]   \n\t"    \
+                                                                        \
+    "andi         %[bit],        %[bit],         0x01          \n\t"    \
+    "st.b         %[tmp1],       %[state],       0x0           \n\t"    \
+    "sll.d        %[c_range],    %[c_range],     %[tmp2]       \n\t"    \
+    "sll.d        %[c_low],      %[c_low],       %[tmp2]       \n\t"    \
+                                                                        \
+    "and          %[tmp1],       %[c_low],       %[cabac_mask] \n\t"    \
+    "bnez         %[tmp1],       1f                            \n\t"    \
+    "ld.hu        %[tmp1],       %[c_bytestream], 0x0          \n\t"    \
+    "ctz.d        %[tmp0],       %[c_low]                      \n\t"    \
+    "addi.d       %[tmp2],       %[tmp0],        -16           \n\t"    \
+    "revb.2h      %[tmp0],       %[tmp1]                       \n\t"    \
+    "slli.d       %[tmp0],       %[tmp0],        0x01          \n\t"    \
+    "sub.d        %[tmp0],       %[tmp0],        %[cabac_mask] \n\t"    \
+    "sll.d        %[tmp0],       %[tmp0],        %[tmp2]       \n\t"    \
+    "add.d        %[c_low],      %[c_low],       %[tmp0]       \n\t"    \
+    "addi.d       %[c_bytestream], %[c_bytestream],     0x02   \n\t"    \
+    "1:                                                        \n\t"    \
+
+#define GET_CABAC_LOONGARCH                                             \
+    "ld.bu        %[bit],        %[state],       0x0           \n\t"    \
+    "andi         %[tmp0],       %[c_range],     0xC0          \n\t"    \
+    "slli.d       %[tmp0],       %[tmp0],        0x01          \n\t"    \
+    "add.d        %[tmp0],       %[tmp0],        %[tables]     \n\t"    \
+    "add.d        %[tmp0],       %[tmp0],        %[bit]        \n\t"    \
+    /* tmp1: RangeLPS */                                                \
+    "ld.bu        %[tmp1],       %[tmp0],        %[lps_off]    \n\t"    \
+                                                                        \
+    "sub.d        %[c_range],    %[c_range],     %[tmp1]       \n\t"    \
+    "slli.d       %[tmp0],       %[c_range],     0x11          \n\t"    \
+    "bge          %[tmp0],       %[c_low],       1f            \n\t"    \
+    "move         %[c_range],    %[tmp1]                       \n\t"    \
+    "nor          %[bit],        %[bit],         %[bit]        \n\t"    \
+    "sub.d        %[c_low],      %[c_low],       %[tmp0]       \n\t"    \
+                                                                        \
+    "1:                                                        \n\t"    \
+    /* tmp1: *state */                                                  \
+    "add.d        %[tmp0],       %[tables],      %[bit]        \n\t"    \
+    "ld.bu        %[tmp1],       %[tmp0],        %[mlps_off]   \n\t"    \
+    /* tmp2: lps_mask */                                                \
+    "add.d        %[tmp0],       %[tables],      %[c_range]    \n\t"    \
+    "ld.bu        %[tmp2],       %[tmp0],        %[norm_off]   \n\t"    \
+                                                                        \
+    "andi         %[bit],        %[bit],         0x01          \n\t"    \
+    "st.b         %[tmp1],       %[state],       0x0           \n\t"    \
+    "sll.d        %[c_range],    %[c_range],     %[tmp2]       \n\t"    \
+    "sll.d        %[c_low],      %[c_low],       %[tmp2]       \n\t"    \
+                                                                        \
+    "and          %[tmp1],       %[c_low],       %[cabac_mask] \n\t"    \
+    "bnez         %[tmp1],       1f                            \n\t"    \
+    "ld.hu        %[tmp1],       %[c_bytestream], 0x0          \n\t"    \
+    "ctz.d        %[tmp0],       %[c_low]                      \n\t"    \
+    "addi.d       %[tmp2],       %[tmp0],        -16           \n\t"    \
+    "revb.2h      %[tmp0],       %[tmp1]                       \n\t"    \
+    "slli.d       %[tmp0],       %[tmp0],        0x01          \n\t"    \
+    "sub.d        %[tmp0],       %[tmp0],        %[cabac_mask] \n\t"    \
+    "sll.d        %[tmp0],       %[tmp0],        %[tmp2]       \n\t"    \
+                                                                        \
+    "add.d        %[c_low],      %[c_low],       %[tmp0]       \n\t"    \
+                                                                        \
+    "slt      %[tmp0],  %[c_bytestream],  %[c_bytestream_end]  \n\t"    \
+    "add.d    %[c_bytestream], %[c_bytestream],     %[tmp0]    \n\t"    \
+    "add.d    %[c_bytestream], %[c_bytestream],     %[tmp0]    \n\t"    \
+    "1:                                                        \n\t"    \
+
+#define get_cabac_inline get_cabac_inline_loongarch
+static av_always_inline
+int get_cabac_inline_loongarch(CABACContext *c, uint8_t * const state)
+{
+    int64_t tmp0, tmp1, tmp2, bit;
+
+    __asm__ volatile (
+#if UNCHECKED_BITSTREAM_READER
+        GET_CABAC_LOONGARCH_UNCBSR
+#else
+        GET_CABAC_LOONGARCH
+#endif
+    : [bit]"=&r"(bit), [tmp0]"=&r"(tmp0), [tmp1]"=&r"(tmp1), [tmp2]"=&r"(tmp2),
+      [c_range]"+&r"(c->range), [c_low]"+&r"(c->low),
+      [c_bytestream]"+&r"(c->bytestream)
+    : [state]"r"(state), [tables]"r"(ff_h264_cabac_tables),
+#if !UNCHECKED_BITSTREAM_READER
+      [c_bytestream_end]"r"(c->bytestream_end),
+#endif
+      [lps_off]"i"(H264_LPS_RANGE_OFFSET),
+      [mlps_off]"i"(H264_MLPS_STATE_OFFSET + 128),
+      [norm_off]"i"(H264_NORM_SHIFT_OFFSET),
+      [cabac_mask]"r"(CABAC_MASK)
+    : "memory"
+    );
+
+    return bit;
+}
+
+#define get_cabac_bypass get_cabac_bypass_loongarch
+static av_always_inline int get_cabac_bypass_loongarch(CABACContext *c)
+{
+    int64_t tmp0, tmp1, tmp2;
+    int res = 0;
+    __asm__ volatile(
+        "slli.d     %[c_low],        %[c_low],        0x01                \n\t"
+        "and        %[tmp0],         %[c_low],        %[cabac_mask]       \n\t"
+        "bnez       %[tmp0],         1f                                   \n\t"
+        "ld.hu      %[tmp1],         %[c_bytestream], 0x0                 \n\t"
+#if UNCHECKED_BITSTREAM_READER
+        "addi.d     %[c_bytestream], %[c_bytestream], 0x02                \n\t"
+#else
+        "slt        %[tmp0],         %[c_bytestream], %[c_bytestream_end] \n\t"
+        "add.d      %[c_bytestream], %[c_bytestream], %[tmp0]             \n\t"
+        "add.d      %[c_bytestream], %[c_bytestream], %[tmp0]             \n\t"
+#endif
+        "revb.2h    %[tmp1],         %[tmp1]                              \n\t"
+        "slli.d     %[tmp1],         %[tmp1],         0x01                \n\t"
+        "sub.d      %[tmp1],         %[tmp1],         %[cabac_mask]       \n\t"
+        "add.d      %[c_low],        %[c_low],        %[tmp1]             \n\t"
+        "1:                                                               \n\t"
+        "slli.d     %[tmp1],         %[c_range],      0x11                \n\t"
+        "slt        %[tmp0],         %[c_low],        %[tmp1]             \n\t"
+        "sub.d      %[tmp1],         %[c_low],        %[tmp1]             \n\t"
+        "masknez    %[tmp2],         %[one],          %[tmp0]             \n\t"
+        "maskeqz    %[res],          %[res],          %[tmp0]             \n\t"
+        "or         %[res],          %[res],          %[tmp2]             \n\t"
+        "masknez    %[tmp2],         %[tmp1],         %[tmp0]             \n\t"
+        "maskeqz    %[c_low],        %[c_low],        %[tmp0]             \n\t"
+        "or         %[c_low],        %[c_low],        %[tmp2]             \n\t"
+        : [tmp0]"=&r"(tmp0), [tmp1]"=&r"(tmp1), [tmp2]"=&r"(tmp2),
+          [c_range]"+&r"(c->range), [c_low]"+&r"(c->low),
+          [c_bytestream]"+&r"(c->bytestream), [res]"+&r"(res)
+        : [cabac_mask]"r"(CABAC_MASK),
+#if !UNCHECKED_BITSTREAM_READER
+          [c_bytestream_end]"r"(c->bytestream_end),
+#endif
+          [one]"r"(0x01)
+        : "memory"
+    );
+    return res;
+}
+
+#define get_cabac_bypass_sign get_cabac_bypass_sign_loongarch
+static av_always_inline
+int get_cabac_bypass_sign_loongarch(CABACContext *c, int val)
+{
+    int64_t tmp0, tmp1;
+    int res = val;
+    __asm__ volatile(
+        "slli.d     %[c_low],        %[c_low],        0x01                \n\t"
+        "and        %[tmp0],         %[c_low],        %[cabac_mask]       \n\t"
+        "bnez       %[tmp0],         1f                                   \n\t"
+        "ld.hu      %[tmp1],         %[c_bytestream], 0x0                 \n\t"
+#if UNCHECKED_BITSTREAM_READER
+        "addi.d     %[c_bytestream], %[c_bytestream], 0x02                \n\t"
+#else
+        "slt        %[tmp0],         %[c_bytestream], %[c_bytestream_end] \n\t"
+        "add.d      %[c_bytestream], %[c_bytestream], %[tmp0]             \n\t"
+        "add.d      %[c_bytestream], %[c_bytestream], %[tmp0]             \n\t"
+#endif
+        "revb.2h    %[tmp1],         %[tmp1]                              \n\t"
+        "slli.d     %[tmp1],         %[tmp1],         0x01                \n\t"
+        "sub.d      %[tmp1],         %[tmp1],         %[cabac_mask]       \n\t"
+        "add.d      %[c_low],        %[c_low],        %[tmp1]             \n\t"
+        "1:                                                               \n\t"
+        "slli.d     %[tmp1],         %[c_range],      0x11                \n\t"
+        "slt        %[tmp0],         %[c_low],        %[tmp1]             \n\t"
+        "sub.d      %[tmp1],         %[c_low],        %[tmp1]             \n\t"
+        "masknez    %[tmp1],         %[tmp1],         %[tmp0]             \n\t"
+        "maskeqz    %[c_low],        %[c_low],        %[tmp0]             \n\t"
+        "or         %[c_low],        %[c_low],        %[tmp1]             \n\t"
+        "sub.d      %[tmp1],         %[zero],         %[res]              \n\t"
+        "maskeqz    %[tmp1],         %[tmp1],         %[tmp0]             \n\t"
+        "masknez    %[res],          %[res],          %[tmp0]             \n\t"
+        "or         %[res],          %[res],          %[tmp1]             \n\t"
+        : [tmp0]"=&r"(tmp0), [tmp1]"=&r"(tmp1), [res]"+&r"(res),
+          [c_range]"+&r"(c->range), [c_low]"+&r"(c->low),
+          [c_bytestream]"+&r"(c->bytestream)
+        : [cabac_mask]"r"(CABAC_MASK),
+#if !UNCHECKED_BITSTREAM_READER
+          [c_bytestream_end]"r"(c->bytestream_end),
+#endif
+          [zero]"r"(0x0)
+        : "memory"
+    );
+
+    return res;
+}
+#endif /* AVCODEC_LOONGARCH_CABAC_H */
diff --git a/libavcodec/loongarch/h264_cabac.c b/libavcodec/loongarch/h264_cabac.c
new file mode 100644
index 0000000000..d88743bed7
--- /dev/null
+++ b/libavcodec/loongarch/h264_cabac.c
@@ -0,0 +1,140 @@
+/*
+ * Loongson  optimized cabac
+ *
+ * Copyright (c) 2021 Loongson Technology Corporation Limited
+ * Contributed by Hao Chen <chenhao@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "libavcodec/cabac.h"
+#include "cabac.h"
+
+#define decode_significance decode_significance_loongarch
+static int decode_significance_loongarch(CABACContext *c, int max_coeff,
+    uint8_t *significant_coeff_ctx_base, int *index, int64_t last_off)
+{
+    void *end = significant_coeff_ctx_base + max_coeff - 1;
+    int64_t minusstart = -(int64_t)significant_coeff_ctx_base;
+    int64_t minusindex = 4 - (int64_t)index;
+    int64_t bit, tmp0, tmp1, tmp2, one = 1;
+    uint8_t *state = significant_coeff_ctx_base;
+
+    __asm__ volatile(
+    "3:"
+#if UNCHECKED_BITSTREAM_READER
+    GET_CABAC_LOONGARCH_UNCBSR
+#else
+    GET_CABAC_LOONGARCH
+#endif
+    "blt     %[bit],          %[one],            4f               \n\t"
+    "add.d   %[state],        %[state],          %[last_off]      \n\t"
+#if UNCHECKED_BITSTREAM_READER
+    GET_CABAC_LOONGARCH_UNCBSR
+#else
+    GET_CABAC_LOONGARCH
+#endif
+    "sub.d   %[state],        %[state],          %[last_off]      \n\t"
+    "add.d   %[tmp0],         %[state],          %[minusstart]    \n\t"
+    "st.w    %[tmp0],         %[index],          0                \n\t"
+    "bge     %[bit],          %[one],            5f               \n\t"
+    "addi.d  %[index],        %[index],          4                \n\t"
+    "4:                                                           \n\t"
+    "addi.d  %[state],        %[state],          1                \n\t"
+    "blt     %[state],        %[end],            3b               \n\t"
+    "add.d   %[tmp0],         %[state],          %[minusstart]    \n\t"
+    "st.w    %[tmp0],         %[index],          0                \n\t"
+    "5:                                                           \n\t"
+    "add.d   %[tmp0],         %[index],          %[minusindex]    \n\t"
+    "srli.d  %[tmp0],         %[tmp0],           2                \n\t"
+    : [bit]"=&r"(bit), [tmp0]"=&r"(tmp0), [tmp1]"=&r"(tmp1), [tmp2]"=&r"(tmp2),
+      [c_range]"+&r"(c->range), [c_low]"+&r"(c->low), [state]"+&r"(state),
+      [c_bytestream]"+&r"(c->bytestream), [index]"+&r"(index)
+    : [tables]"r"(ff_h264_cabac_tables), [end]"r"(end), [one]"r"(one),
+      [minusstart]"r"(minusstart), [minusindex]"r"(minusindex),
+      [last_off]"r"(last_off),
+#if !UNCHECKED_BITSTREAM_READER
+      [c_bytestream_end]"r"(c->bytestream_end),
+#endif
+      [lps_off]"i"(H264_LPS_RANGE_OFFSET),
+      [mlps_off]"i"(H264_MLPS_STATE_OFFSET + 128),
+      [norm_off]"i"(H264_NORM_SHIFT_OFFSET),
+      [cabac_mask]"r"(CABAC_MASK)
+    : "memory"
+    );
+
+    return (int)tmp0;
+}
+
+#define decode_significance_8x8 decode_significance_8x8_loongarch
+static int decode_significance_8x8_loongarch(
+    CABACContext *c, uint8_t *significant_coeff_ctx_base,
+    int *index, uint8_t *last_coeff_ctx_base, const uint8_t *sig_off)
+{
+    int64_t minusindex = 4 - (int64_t)index;
+    int64_t bit, tmp0, tmp1, tmp2, one = 1, end =  63, last = 0;
+    uint8_t *state = 0;
+    int64_t flag_offset = H264_LAST_COEFF_FLAG_OFFSET_8x8_OFFSET;
+
+    __asm__ volatile(
+    "3:                                                              \n\t"
+    "ldx.bu   %[tmp0],     %[sig_off],       %[last]                 \n\t"
+    "add.d    %[state],    %[tmp0], %[significant_coeff_ctx_base]    \n\t"
+#if UNCHECKED_BITSTREAM_READER
+    GET_CABAC_LOONGARCH_UNCBSR
+#else
+    GET_CABAC_LOONGARCH
+#endif
+    "blt      %[bit],      %[one],           4f                      \n\t"
+    "add.d    %[tmp0],     %[tables],        %[flag_offset]          \n\t"
+    "ldx.bu   %[tmp1],     %[tmp0],          %[last]                 \n\t"
+    "add.d    %[state],    %[tmp1],    %[last_coeff_ctx_base]        \n\t"
+#if UNCHECKED_BITSTREAM_READER
+    GET_CABAC_LOONGARCH_UNCBSR
+#else
+    GET_CABAC_LOONGARCH
+#endif
+    "st.w    %[last],      %[index],         0                       \n\t"
+    "bge     %[bit],       %[one],           5f                      \n\t"
+    "addi.d  %[index],     %[index],         4                       \n\t"
+    "4:                                                              \n\t"
+    "addi.d  %[last],      %[last],          1                       \n\t"
+    "blt     %[last],      %[end],           3b                      \n\t"
+    "st.w    %[last],      %[index],         0                       \n\t"
+    "5:                                                              \n\t"
+    "add.d   %[tmp0],      %[index],         %[minusindex]           \n\t"
+    "srli.d  %[tmp0],      %[tmp0],          2                       \n\t"
+    : [bit]"=&r"(bit), [tmp0]"=&r"(tmp0), [tmp1]"=&r"(tmp1),
+      [tmp2]"=&r"(tmp2), [c_range]"+&r"(c->range),
+      [c_low]"+&r"(c->low), [state]"+&r"(state), [last]"+&r"(last),
+      [c_bytestream]"+&r"(c->bytestream), [index]"+&r"(index)
+    : [tables]"r"(ff_h264_cabac_tables), [end]"r"(end),
+      [one]"r"(one), [minusindex]"r"(minusindex),
+      [last_coeff_ctx_base]"r"(last_coeff_ctx_base),
+      [flag_offset]"r"(flag_offset),
+#if !UNCHECKED_BITSTREAM_READER
+      [c_bytestream_end]"r"(c->bytestream_end),
+#endif
+      [lps_off]"i"(H264_LPS_RANGE_OFFSET), [sig_off]"r"(sig_off),
+      [mlps_off]"i"(H264_MLPS_STATE_OFFSET + 128),
+      [norm_off]"i"(H264_NORM_SHIFT_OFFSET),
+      [cabac_mask]"r"(CABAC_MASK),
+      [significant_coeff_ctx_base]"r"(significant_coeff_ctx_base)
+    );
+
+    return (int)tmp0;
+}
diff --git a/libavcodec/loongarch/h264_deblock_lasx.c b/libavcodec/loongarch/h264_deblock_lasx.c
new file mode 100644
index 0000000000..c89bea9a84
--- /dev/null
+++ b/libavcodec/loongarch/h264_deblock_lasx.c
@@ -0,0 +1,147 @@
+/*
+ * Copyright (c) 2021 Loongson Technology Corporation Limited
+ * Contributed by Xiwei Gu <guxiwei-hf@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "libavcodec/bit_depth_template.c"
+#include "h264dsp_lasx.h"
+#include "libavutil/loongarch/loongson_intrinsics.h"
+
+#define H264_LOOP_FILTER_STRENGTH_ITERATION_LASX(edges, step, mask_mv, dir, \
+                                                 d_idx, mask_dir)           \
+do {                                                                        \
+    int b_idx = 0; \
+    int step_x4 = step << 2; \
+    int d_idx_12 = d_idx + 12; \
+    int d_idx_52 = d_idx + 52; \
+    int d_idx_x4 = d_idx << 2; \
+    int d_idx_x4_48 = d_idx_x4 + 48; \
+    int dir_x32  = dir * 32; \
+    uint8_t *ref_t = (uint8_t*)ref; \
+    uint8_t *mv_t  = (uint8_t*)mv; \
+    uint8_t *nnz_t = (uint8_t*)nnz; \
+    uint8_t *bS_t  = (uint8_t*)bS; \
+    mask_mv <<= 3; \
+    for (; b_idx < edges; b_idx += step) { \
+        out &= mask_dir; \
+        if (!(mask_mv & b_idx)) { \
+            if (bidir) { \
+                ref2 = __lasx_xvldx(ref_t, d_idx_12); \
+                ref3 = __lasx_xvldx(ref_t, d_idx_52); \
+                ref0 = __lasx_xvld(ref_t, 12); \
+                ref1 = __lasx_xvld(ref_t, 52); \
+                ref2 = __lasx_xvilvl_w(ref3, ref2); \
+                ref0 = __lasx_xvilvl_w(ref0, ref0); \
+                ref1 = __lasx_xvilvl_w(ref1, ref1); \
+                ref3 = __lasx_xvshuf4i_w(ref2, 0xB1); \
+                ref0 = __lasx_xvsub_b(ref0, ref2); \
+                ref1 = __lasx_xvsub_b(ref1, ref3); \
+                ref0 = __lasx_xvor_v(ref0, ref1); \
+\
+                tmp2 = __lasx_xvldx(mv_t, d_idx_x4_48);   \
+                tmp3 = __lasx_xvld(mv_t, 48); \
+                tmp4 = __lasx_xvld(mv_t, 208); \
+                tmp5 = __lasx_xvld(mv_t + d_idx_x4, 208); \
+                DUP2_ARG3(__lasx_xvpermi_q, tmp2, tmp2, 0x20, tmp5, tmp5, \
+                          0x20, tmp2, tmp5); \
+                tmp3 =  __lasx_xvpermi_q(tmp4, tmp3, 0x20); \
+                tmp2 = __lasx_xvsub_h(tmp2, tmp3); \
+                tmp5 = __lasx_xvsub_h(tmp5, tmp3); \
+                DUP2_ARG2(__lasx_xvsat_h, tmp2, 7, tmp5, 7, tmp2, tmp5); \
+                tmp0 = __lasx_xvpickev_b(tmp5, tmp2); \
+                tmp0 = __lasx_xvpermi_d(tmp0, 0xd8); \
+                tmp0 = __lasx_xvadd_b(tmp0, cnst_1); \
+                tmp0 = __lasx_xvssub_bu(tmp0, cnst_0); \
+                tmp0 = __lasx_xvsat_h(tmp0, 7); \
+                tmp0 = __lasx_xvpickev_b(tmp0, tmp0); \
+                tmp0 = __lasx_xvpermi_d(tmp0, 0xd8); \
+                tmp1 = __lasx_xvpickod_d(tmp0, tmp0); \
+                out = __lasx_xvor_v(ref0, tmp0); \
+                tmp1 = __lasx_xvshuf4i_w(tmp1, 0xB1); \
+                out = __lasx_xvor_v(out, tmp1); \
+                tmp0 = __lasx_xvshuf4i_w(out, 0xB1); \
+                out = __lasx_xvmin_bu(out, tmp0); \
+            } else { \
+                ref0 = __lasx_xvldx(ref_t, d_idx_12); \
+                ref3 = __lasx_xvld(ref_t, 12); \
+                tmp2 = __lasx_xvldx(mv_t, d_idx_x4_48); \
+                tmp3 = __lasx_xvld(mv_t, 48); \
+                tmp4 = __lasx_xvsub_h(tmp3, tmp2); \
+                tmp1 = __lasx_xvsat_h(tmp4, 7); \
+                tmp1 = __lasx_xvpickev_b(tmp1, tmp1); \
+                tmp1 = __lasx_xvadd_b(tmp1, cnst_1); \
+                out = __lasx_xvssub_bu(tmp1, cnst_0); \
+                out = __lasx_xvsat_h(out, 7); \
+                out = __lasx_xvpickev_b(out, out); \
+                ref0 = __lasx_xvsub_b(ref3, ref0); \
+                out = __lasx_xvor_v(out, ref0); \
+            } \
+        } \
+        tmp0 = __lasx_xvld(nnz_t, 12); \
+        tmp1 = __lasx_xvldx(nnz_t, d_idx_12); \
+        tmp0 = __lasx_xvor_v(tmp0, tmp1); \
+        tmp0 = __lasx_xvmin_bu(tmp0, cnst_2); \
+        out  = __lasx_xvmin_bu(out, cnst_2); \
+        tmp0 = __lasx_xvslli_h(tmp0, 1); \
+        tmp0 = __lasx_xvmax_bu(out, tmp0); \
+        tmp0 = __lasx_vext2xv_hu_bu(tmp0); \
+        __lasx_xvstelm_d(tmp0, bS_t + dir_x32, 0, 0); \
+        ref_t += step; \
+        mv_t  += step_x4; \
+        nnz_t += step; \
+        bS_t  += step; \
+    } \
+} while(0)
+
+void ff_h264_loop_filter_strength_lasx(int16_t bS[2][4][4], uint8_t nnz[40],
+                                       int8_t ref[2][40], int16_t mv[2][40][2],
+                                       int bidir, int edges, int step,
+                                       int mask_mv0, int mask_mv1, int field)
+{
+    __m256i out;
+    __m256i ref0, ref1, ref2, ref3;
+    __m256i tmp0, tmp1;
+    __m256i tmp2, tmp3, tmp4, tmp5;
+    __m256i cnst_0, cnst_1, cnst_2;
+    __m256i zero = __lasx_xvldi(0);
+    __m256i one  = __lasx_xvnor_v(zero, zero);
+    int64_t cnst3 = 0x0206020602060206, cnst4 = 0x0103010301030103;
+    if (field) {
+        cnst_0 = __lasx_xvreplgr2vr_d(cnst3);
+        cnst_1 = __lasx_xvreplgr2vr_d(cnst4);
+        cnst_2 = __lasx_xvldi(0x01);
+    } else {
+        DUP2_ARG1(__lasx_xvldi, 0x06, 0x03, cnst_0, cnst_1);
+        cnst_2 = __lasx_xvldi(0x01);
+    }
+    step  <<= 3;
+    edges <<= 3;
+
+    H264_LOOP_FILTER_STRENGTH_ITERATION_LASX(edges, step, mask_mv1,
+                                             1, -8, zero);
+    H264_LOOP_FILTER_STRENGTH_ITERATION_LASX(32, 8, mask_mv0, 0, -1, one);
+
+    DUP2_ARG2(__lasx_xvld, (int8_t*)bS, 0, (int8_t*)bS, 16, tmp0, tmp1);
+    DUP2_ARG2(__lasx_xvilvh_d, tmp0, tmp0, tmp1, tmp1, tmp2, tmp3);
+    LASX_TRANSPOSE4x4_H(tmp0, tmp2, tmp1, tmp3, tmp2, tmp3, tmp4, tmp5);
+    __lasx_xvstelm_d(tmp2, (int8_t*)bS, 0, 0);
+    __lasx_xvstelm_d(tmp3, (int8_t*)bS + 8, 0, 0);
+    __lasx_xvstelm_d(tmp4, (int8_t*)bS + 16, 0, 0);
+    __lasx_xvstelm_d(tmp5, (int8_t*)bS + 24, 0, 0);
+}
diff --git a/libavcodec/loongarch/h264_intrapred_init_loongarch.c b/libavcodec/loongarch/h264_intrapred_init_loongarch.c
new file mode 100644
index 0000000000..e12ba66da3
--- /dev/null
+++ b/libavcodec/loongarch/h264_intrapred_init_loongarch.c
@@ -0,0 +1,66 @@
+/*
+ * Copyright (c) 2021 Loongson Technology Corporation Limited
+ * Contributed by Hao Chen <chenhao@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "libavutil/loongarch/cpu.h"
+#include "libavcodec/h264pred.h"
+#include "h264_intrapred_lasx.h"
+
+av_cold void ff_h264_pred_init_loongarch(H264PredContext *h, int codec_id,
+                                         const int bit_depth,
+                                         const int chroma_format_idc)
+{
+    int cpu_flags = av_get_cpu_flags();
+
+    if (bit_depth == 8) {
+        if (have_lsx(cpu_flags)) {
+            if (chroma_format_idc <= 1) {
+            }
+            if (codec_id == AV_CODEC_ID_VP7 || codec_id == AV_CODEC_ID_VP8) {
+            } else {
+                if (chroma_format_idc <= 1) {
+                }
+                if (codec_id == AV_CODEC_ID_SVQ3) {
+                    h->pred16x16[PLANE_PRED8x8] = ff_h264_pred16x16_plane_svq3_8_lsx;
+                } else if (codec_id == AV_CODEC_ID_RV40) {
+                    h->pred16x16[PLANE_PRED8x8] = ff_h264_pred16x16_plane_rv40_8_lsx;
+                } else {
+                    h->pred16x16[PLANE_PRED8x8] = ff_h264_pred16x16_plane_h264_8_lsx;
+                }
+            }
+        }
+        if (have_lasx(cpu_flags)) {
+            if (chroma_format_idc <= 1) {
+            }
+            if (codec_id == AV_CODEC_ID_VP7 || codec_id == AV_CODEC_ID_VP8) {
+            } else {
+                if (chroma_format_idc <= 1) {
+                }
+                if (codec_id == AV_CODEC_ID_SVQ3) {
+                    h->pred16x16[PLANE_PRED8x8] = ff_h264_pred16x16_plane_svq3_8_lasx;
+                } else if (codec_id == AV_CODEC_ID_RV40) {
+                    h->pred16x16[PLANE_PRED8x8] = ff_h264_pred16x16_plane_rv40_8_lasx;
+                } else {
+                    h->pred16x16[PLANE_PRED8x8] = ff_h264_pred16x16_plane_h264_8_lasx;
+                }
+            }
+        }
+    }
+}
diff --git a/libavcodec/loongarch/h264_intrapred_lasx.h b/libavcodec/loongarch/h264_intrapred_lasx.h
new file mode 100644
index 0000000000..6aff9d63bb
--- /dev/null
+++ b/libavcodec/loongarch/h264_intrapred_lasx.h
@@ -0,0 +1,35 @@
+/*
+ * Copyright (c) 2021 Loongson Technology Corporation Limited
+ * Contributed by Hao Chen <chenhao@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef AVCODEC_LOONGARCH_H264_INTRAPRED_LASX_H
+#define AVCODEC_LOONGARCH_H264_INTRAPRED_LASX_H
+
+#include "libavcodec/avcodec.h"
+
+void ff_h264_pred16x16_plane_h264_8_lsx(uint8_t *src, ptrdiff_t stride);
+void ff_h264_pred16x16_plane_rv40_8_lsx(uint8_t *src, ptrdiff_t stride);
+void ff_h264_pred16x16_plane_svq3_8_lsx(uint8_t *src, ptrdiff_t stride);
+
+void ff_h264_pred16x16_plane_h264_8_lasx(uint8_t *src, ptrdiff_t stride);
+void ff_h264_pred16x16_plane_rv40_8_lasx(uint8_t *src, ptrdiff_t stride);
+void ff_h264_pred16x16_plane_svq3_8_lasx(uint8_t *src, ptrdiff_t stride);
+
+#endif  // #ifndef AVCODEC_LOONGARCH_H264_INTRAPRED_LASX_H
diff --git a/libavcodec/loongarch/h264chroma.S b/libavcodec/loongarch/h264chroma.S
new file mode 100644
index 0000000000..81916c4cec
--- /dev/null
+++ b/libavcodec/loongarch/h264chroma.S
@@ -0,0 +1,966 @@
+/*
+ * Loongson LSX optimized h264chroma
+ *
+ * Copyright (c) 2022 Loongson Technology Corporation Limited
+ * Contributed by Lu Wang <wanglu@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "loongson_asm.S"
+
+/* void ff_put_h264_chroma_mc8_lsx(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
+                                   int h, int x, int y) */
+function ff_put_h264_chroma_mc8_lsx
+    li.d             t8,      8
+    sub.d            t1,      t8,     a4     // 8-x
+    sub.d            t2,      t8,     a5     // 8-y
+    mul.d            t3,      t1,     t2     // A
+    mul.d            t4,      a4,     t2     // B
+    mul.d            t5,      t1,     a5     // C
+    mul.d            t6,      a4,     a5     // D
+    add.d            t0,      t4,     t5     // E
+    vreplgr2vr.b     vr0,     t3
+    vreplgr2vr.b     vr1,     t4
+    vreplgr2vr.b     vr2,     t5
+    vreplgr2vr.b     vr3,     t6
+    vreplgr2vr.b     vr4,     t0
+    slli.d           t2,      a2,     1
+    add.d            t3,      t2,     a2
+    slli.d           t4,      a2,     2
+
+    bge              zero,    t6,     .ENDLOOP_D
+    move             t1,      a3
+    vilvl.b          vr9,     vr1,    vr0
+    vilvl.b          vr10,    vr3,    vr2
+.LOOP_D:
+    vld              vr5,     a1,     0
+    vld              vr6,     a1,     1
+    add.d            a1,      a1,     a2
+    vld              vr7,     a1,     0
+    vld              vr8,     a1,     1
+    vilvl.b          vr11,    vr6,    vr5
+    vilvl.b          vr12,    vr8,    vr7
+    vmulwev.h.bu     vr13,    vr9,    vr11
+    vmaddwod.h.bu    vr13,    vr9,    vr11
+    vmulwev.h.bu     vr14,    vr10,   vr12
+    vmaddwod.h.bu    vr14,    vr10,   vr12
+    vadd.h           vr13,    vr13,   vr14
+    vsrarni.b.h      vr13,    vr13,   6
+    vstelm.d         vr13,    a0,     0,    0
+    add.d            a0,      a0,     a2
+    add.d            a1,      a1,     a2
+    vld              vr5,     a1,     0
+    vld              vr6,     a1,     1
+    vilvl.b          vr11,    vr8,    vr7
+    vilvl.b          vr12,    vr6,    vr5
+    vmulwev.h.bu     vr13,    vr9,    vr11
+    vmaddwod.h.bu    vr13,    vr9,    vr11
+    vmulwev.h.bu     vr14,    vr10,   vr12
+    vmaddwod.h.bu    vr14,    vr10,   vr12
+    vadd.h           vr13,    vr13,   vr14
+    vsrarni.b.h      vr13,    vr13,   6
+    vstelm.d         vr13,    a0,     0,    0
+    add.d            a0,      a0,     a2
+    add.d            a1,      a1,     a2
+    vld              vr7,     a1,     0
+    vld              vr8,     a1,     1
+    vilvl.b          vr11,    vr6,    vr5
+    vilvl.b          vr12,    vr8,    vr7
+    vmulwev.h.bu     vr13,    vr9,    vr11
+    vmaddwod.h.bu    vr13,    vr9,    vr11
+    vmulwev.h.bu     vr14,    vr10,   vr12
+    vmaddwod.h.bu    vr14,    vr10,   vr12
+    vadd.h           vr13,    vr13,   vr14
+    vsrarni.b.h      vr13,    vr13,   6
+    vstelm.d         vr13,    a0,     0,    0
+    add.d            a0,      a0,     a2
+    add.d            a1,      a1,     a2
+    vld              vr5,     a1,     0
+    vld              vr6,     a1,     1
+    vilvl.b          vr11,    vr8,    vr7
+    vilvl.b          vr12,    vr6,    vr5
+    vmulwev.h.bu     vr13,    vr9,    vr11
+    vmaddwod.h.bu    vr13,    vr9,    vr11
+    vmulwev.h.bu     vr14,    vr10,   vr12
+    vmaddwod.h.bu    vr14,    vr10,   vr12
+    vadd.h           vr13,    vr13,   vr14
+    vsrarni.b.h      vr13,    vr13,   6
+    vstelm.d         vr13,    a0,     0,    0
+    add.d            a0,      a0,     a2
+
+    addi.d           t1,      t1,     -4
+    blt              zero,    t1,     .LOOP_D
+    b                .ENDLOOP
+.ENDLOOP_D:
+
+    bge              zero,    t0,     .ENDLOOP_E
+    move             t1,      a3
+    li.d             t7,      1
+    slt              t8,      zero,   t5
+    maskeqz          t5,      a2,     t8
+    masknez          t7,      t7,     t8
+    or               t7,      t7,     t5
+    vilvl.b          vr7,     vr4,    vr0
+.LOOP_E:
+    vld              vr5,     a1,     0
+    vldx             vr6,     a1,     t7
+    vilvl.b          vr5,     vr6,    vr5
+    vmulwev.h.bu     vr6,     vr7,    vr5
+    vmaddwod.h.bu    vr6,     vr7,    vr5
+    vsrarni.b.h      vr6,     vr6,    6
+    vstelm.d         vr6,     a0,     0,    0
+    add.d            a0,      a0,     a2
+    add.d            a1,      a1,     a2
+    vld              vr5,     a1,     0
+    vldx             vr6,     a1,     t7
+    vilvl.b          vr5,     vr6,    vr5
+    vmulwev.h.bu     vr6,     vr7,    vr5
+    vmaddwod.h.bu    vr6,     vr7,    vr5
+    vsrarni.b.h      vr6,     vr6,    6
+    vstelm.d         vr6,     a0,     0,    0
+    add.d            a0,      a0,     a2
+    add.d            a1,      a1,     a2
+    vld              vr5,     a1,     0
+    vldx             vr6,     a1,     t7
+    vilvl.b          vr5,     vr6,    vr5
+    vmulwev.h.bu     vr6,     vr7,    vr5
+    vmaddwod.h.bu    vr6,     vr7,    vr5
+    vsrarni.b.h      vr6,     vr6,    6
+    vstelm.d         vr6,     a0,     0,    0
+    add.d            a0,      a0,     a2
+    add.d            a1,      a1,     a2
+    vld              vr5,     a1,     0
+    vldx             vr6,     a1,     t7
+    vilvl.b          vr5,     vr6,    vr5
+    vmulwev.h.bu     vr6,     vr7,    vr5
+    vmaddwod.h.bu    vr6,     vr7,    vr5
+    vsrarni.b.h      vr6,     vr6,    6
+    vstelm.d         vr6,     a0,     0,    0
+    add.d            a0,      a0,     a2
+    add.d            a1,      a1,     a2
+
+    addi.d           t1,      t1,     -4
+    blt              zero,    t1,     .LOOP_E
+    b                .ENDLOOP
+.ENDLOOP_E:
+
+    move             t1,      a3
+.LOOP:
+    vld              vr5,     a1,     0
+    vmulwev.h.bu     vr6,     vr0,    vr5
+    vmulwod.h.bu     vr7,     vr0,    vr5
+    vsrarni.b.h      vr6,     vr6,    6
+    vsrarni.b.h      vr7,     vr7,    6
+    vilvl.b          vr6,     vr7,    vr6
+    vstelm.d         vr6,     a0,     0,    0
+    add.d            a0,      a0,     a2
+    vldx             vr5,     a1,     a2
+    vmulwev.h.bu     vr6,     vr0,    vr5
+    vmulwod.h.bu     vr7,     vr0,    vr5
+    vsrarni.b.h      vr6,     vr6,    6
+    vsrarni.b.h      vr7,     vr7,    6
+    vilvl.b          vr6,     vr7,    vr6
+    vstelm.d         vr6,     a0,     0,    0
+    add.d            a0,      a0,     a2
+    vldx             vr5,     a1,     t2
+    vmulwev.h.bu     vr6,     vr0,    vr5
+    vmulwod.h.bu     vr7,     vr0,    vr5
+    vsrarni.b.h      vr6,     vr6,    6
+    vsrarni.b.h      vr7,     vr7,    6
+    vilvl.b          vr6,     vr7,    vr6
+    vstelm.d         vr6,     a0,     0,    0
+    add.d            a0,      a0,     a2
+    vldx             vr5,     a1,     t3
+    vmulwev.h.bu     vr6,     vr0,    vr5
+    vmulwod.h.bu     vr7,     vr0,    vr5
+    vsrarni.b.h      vr6,     vr6,    6
+    vsrarni.b.h      vr7,     vr7,    6
+    vilvl.b          vr6,     vr7,    vr6
+    vstelm.d         vr6,     a0,     0,    0
+    add.d            a0,      a0,     a2
+    add.d            a1,      a1,     t4
+
+    addi.d           t1,      t1,     -4
+    blt              zero,    t1,     .LOOP
+.ENDLOOP:
+endfunc
+
+/* void ff_avg_h264_chroma_mc8_lsx(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
+                                   int h, int x, int y) */
+function ff_avg_h264_chroma_mc8_lsx
+    li.d             t8,      8
+    sub.d            t1,      t8,     a4     // 8-x
+    sub.d            t2,      t8,     a5     // 8-y
+    mul.d            t3,      t1,     t2     // A
+    mul.d            t4,      a4,     t2     // B
+    mul.d            t5,      t1,     a5     // C
+    mul.d            t6,      a4,     a5     // D
+    add.d            t0,      t4,     t5     // E
+    vreplgr2vr.b     vr0,     t3
+    vreplgr2vr.b     vr1,     t4
+    vreplgr2vr.b     vr2,     t5
+    vreplgr2vr.b     vr3,     t6
+    vreplgr2vr.b     vr4,     t0
+    slli.d           t2,      a2,     1
+    add.d            t3,      t2,     a2
+    slli.d           t4,      a2,     2
+
+    bge              zero,    t6,     .ENDLOOPD
+    move             t1,      a3
+    vilvl.b          vr9,     vr1,    vr0
+    vilvl.b          vr10,    vr3,    vr2
+.LOOPD:
+    vld              vr5,     a1,     0
+    vld              vr6,     a1,     1
+    add.d            a1,      a1,     a2
+    vld              vr7,     a1,     0
+    vld              vr8,     a1,     1
+    vld              vr11,    a0,     0
+    vilvl.b          vr12,    vr6,    vr5
+    vilvl.b          vr13,    vr8,    vr7
+    vmulwev.h.bu     vr14,    vr9,    vr12
+    vmaddwod.h.bu    vr14,    vr9,    vr12
+    vmulwev.h.bu     vr15,    vr10,   vr13
+    vmaddwod.h.bu    vr15,    vr10,   vr13
+    vadd.h           vr14,    vr14,   vr15
+    vsrari.h         vr14,    vr14,   6
+    vsllwil.hu.bu    vr11,    vr11,   0
+    vadd.h           vr11,    vr14,   vr11
+    vsrarni.b.h      vr11,    vr11,   1
+    vstelm.d         vr11,    a0,     0,    0
+    add.d            a0,      a0,     a2
+    add.d            a1,      a1,     a2
+    vld              vr5,     a1,     0
+    vld              vr6,     a1,     1
+    vld              vr11,    a0,     0
+    vilvl.b          vr12,    vr8,    vr7
+    vilvl.b          vr13,    vr6,    vr5
+    vmulwev.h.bu     vr14,    vr9,    vr12
+    vmaddwod.h.bu    vr14,    vr9,    vr12
+    vmulwev.h.bu     vr15,    vr10,   vr13
+    vmaddwod.h.bu    vr15,    vr10,   vr13
+    vadd.h           vr14,    vr14,   vr15
+    vsrari.h         vr14,    vr14,   6
+    vsllwil.hu.bu    vr11,    vr11,   0
+    vadd.h           vr11,    vr14,   vr11
+    vsrarni.b.h      vr11,    vr11,   1
+    vstelm.d         vr11,    a0,     0,    0
+    add.d            a0,      a0,     a2
+    add.d            a1,      a1,     a2
+    vld              vr7,     a1,     0
+    vld              vr8,     a1,     1
+    vld              vr11,    a0,     0
+    vilvl.b          vr12,    vr6,    vr5
+    vilvl.b          vr13,    vr8,    vr7
+    vmulwev.h.bu     vr14,    vr9,    vr12
+    vmaddwod.h.bu    vr14,    vr9,    vr12
+    vmulwev.h.bu     vr15,    vr10,   vr13
+    vmaddwod.h.bu    vr15,    vr10,   vr13
+    vadd.h           vr14,    vr14,   vr15
+    vsrari.h         vr14,    vr14,   6
+    vsllwil.hu.bu    vr11,    vr11,   0
+    vadd.h           vr11,    vr14,   vr11
+    vsrarni.b.h      vr11,    vr11,   1
+    vstelm.d         vr11,    a0,     0,    0
+    add.d            a0,      a0,     a2
+    add.d            a1,      a1,     a2
+    vld              vr5,     a1,     0
+    vld              vr6,     a1,     1
+    vld              vr11,    a0,     0
+    vilvl.b          vr12,    vr8,    vr7
+    vilvl.b          vr13,    vr6,    vr5
+    vmulwev.h.bu     vr14,    vr9,    vr12
+    vmaddwod.h.bu    vr14,    vr9,    vr12
+    vmulwev.h.bu     vr15,    vr10,   vr13
+    vmaddwod.h.bu    vr15,    vr10,   vr13
+    vadd.h           vr14,    vr14,   vr15
+    vsrari.h         vr14,    vr14,   6
+    vsllwil.hu.bu    vr11,    vr11,   0
+    vadd.h           vr11,    vr14,   vr11
+    vsrarni.b.h      vr11,    vr11,   1
+    vstelm.d         vr11,    a0,     0,    0
+    add.d            a0,      a0,     a2
+
+    addi.d           t1,      t1,     -4
+    blt              zero,    t1,     .LOOPD
+    b                .ENDLOOPELSE
+.ENDLOOPD:
+
+    bge              zero,    t0,     .ENDLOOPE
+    move             t1,      a3
+    li.d             t7,      1
+    slt              t8,      zero,   t5
+    maskeqz          t5,      a2,     t8
+    masknez          t7,      t7,     t8
+    or               t7,      t7,     t5
+    vilvl.b          vr7,     vr4,    vr0
+.LOOPE:
+    vld              vr5,     a1,     0
+    vldx             vr6,     a1,     t7
+    vld              vr8,     a0,     0
+    vilvl.b          vr5,     vr6,    vr5
+    vmulwev.h.bu     vr6,     vr7,    vr5
+    vmaddwod.h.bu    vr6,     vr7,    vr5
+    vsrari.h         vr6,     vr6,    6
+    vsllwil.hu.bu    vr8,     vr8,    0
+    vadd.h           vr8,     vr6,    vr8
+    vsrarni.b.h      vr8,     vr8,    1
+    vstelm.d         vr8,     a0,     0,    0
+    add.d            a0,      a0,     a2
+    add.d            a1,      a1,     a2
+    vld              vr5,     a1,     0
+    vldx             vr6,     a1,     t7
+    vld              vr8,     a0,     0
+    vilvl.b          vr5,     vr6,    vr5
+    vmulwev.h.bu     vr6,     vr7,    vr5
+    vmaddwod.h.bu    vr6,     vr7,    vr5
+    vsrari.h         vr6,     vr6,    6
+    vsllwil.hu.bu    vr8,     vr8,    0
+    vadd.h           vr8,     vr6,    vr8
+    vsrarni.b.h      vr8,     vr8,    1
+    vstelm.d         vr8,     a0,     0,    0
+    add.d            a0,      a0,     a2
+    add.d            a1,      a1,     a2
+    vld              vr5,     a1,     0
+    vldx             vr6,     a1,     t7
+    vld              vr8,     a0,     0
+    vilvl.b          vr5,     vr6,    vr5
+    vmulwev.h.bu     vr6,     vr7,    vr5
+    vmaddwod.h.bu    vr6,     vr7,    vr5
+    vsrari.h         vr6,     vr6,    6
+    vsllwil.hu.bu    vr8,     vr8,    0
+    vadd.h           vr8,     vr6,    vr8
+    vsrarni.b.h      vr8,     vr8,    1
+    vstelm.d         vr8,     a0,     0,    0
+    add.d            a0,      a0,     a2
+    add.d            a1,      a1,     a2
+    vld              vr5,     a1,     0
+    vldx             vr6,     a1,     t7
+    vld              vr8,     a0,     0
+    vilvl.b          vr5,     vr6,    vr5
+    vmulwev.h.bu     vr6,     vr7,    vr5
+    vmaddwod.h.bu    vr6,     vr7,    vr5
+    vsrari.h         vr6,     vr6,    6
+    vsllwil.hu.bu    vr8,     vr8,    0
+    vadd.h           vr8,     vr6,    vr8
+    vsrarni.b.h      vr8,     vr8,    1
+    vstelm.d         vr8,     a0,     0,    0
+    add.d            a0,      a0,     a2
+    add.d            a1,      a1,     a2
+
+    addi.d           t1,      t1,     -4
+    blt              zero,    t1,     .LOOPE
+    b                .ENDLOOPELSE
+.ENDLOOPE:
+
+    move             t1,      a3
+.LOOPELSE:
+    vld              vr5,     a1,     0
+    vld              vr8,     a0,     0
+    vmulwev.h.bu     vr6,     vr0,    vr5
+    vmulwod.h.bu     vr7,     vr0,    vr5
+    vilvl.h          vr6,     vr7,    vr6
+    vsrari.h         vr6,     vr6,    6
+    vsllwil.hu.bu    vr8,     vr8,    0
+    vadd.h           vr8,     vr6,    vr8
+    vsrarni.b.h      vr8,     vr8,    1
+    vstelm.d         vr8,     a0,     0,    0
+    add.d            a0,      a0,     a2
+    vldx             vr5,     a1,     a2
+    vld              vr8,     a0,     0
+    vmulwev.h.bu     vr6,     vr0,    vr5
+    vmulwod.h.bu     vr7,     vr0,    vr5
+    vilvl.h          vr6,     vr7,    vr6
+    vsrari.h         vr6,     vr6,    6
+    vsllwil.hu.bu    vr8,     vr8,    0
+    vadd.h           vr8,     vr6,    vr8
+    vsrarni.b.h      vr8,     vr8,    1
+    vstelm.d         vr8,     a0,     0,    0
+    add.d            a0,      a0,     a2
+    vldx             vr5,     a1,     t2
+    vld              vr8,     a0,     0
+    vmulwev.h.bu     vr6,     vr0,    vr5
+    vmulwod.h.bu     vr7,     vr0,    vr5
+    vilvl.h          vr6,     vr7,    vr6
+    vsrari.h         vr6,     vr6,    6
+    vsllwil.hu.bu    vr8,     vr8,    0
+    vadd.h           vr8,     vr6,    vr8
+    vsrarni.b.h      vr8,     vr8,    1
+    vstelm.d         vr8,     a0,     0,    0
+    add.d            a0,      a0,     a2
+    vldx             vr5,     a1,     t3
+    vld              vr8,     a0,     0
+    vmulwev.h.bu     vr6,     vr0,    vr5
+    vmulwod.h.bu     vr7,     vr0,    vr5
+    vilvl.h          vr6,     vr7,    vr6
+    vsrari.h         vr6,     vr6,    6
+    vsllwil.hu.bu    vr8,     vr8,    0
+    vadd.h           vr8,     vr6,    vr8
+    vsrarni.b.h      vr8,     vr8,    1
+    vstelm.d         vr8,     a0,     0,    0
+    add.d            a0,      a0,     a2
+    add.d            a1,      a1,     t4
+
+    addi.d           t1,      t1,     -4
+    blt              zero,    t1,     .LOOPELSE
+.ENDLOOPELSE:
+endfunc
+
+/* void ff_put_h264_chroma_mc4_lsx(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
+                                   int h, int x, int y) */
+function ff_put_h264_chroma_mc4_lsx
+    li.d             t8,      8
+    sub.d            t1,      t8,     a4     // 8-x
+    sub.d            t2,      t8,     a5     // 8-y
+    mul.d            t3,      t1,     t2     // A
+    mul.d            t4,      a4,     t2     // B
+    mul.d            t5,      t1,     a5     // C
+    mul.d            t6,      a4,     a5     // D
+    add.d            t0,      t4,     t5     // E
+    slli.d           t8,      a2,     1
+    vreplgr2vr.b     vr0,     t3
+    vreplgr2vr.b     vr1,     t4
+    vreplgr2vr.b     vr2,     t5
+    vreplgr2vr.b     vr3,     t6
+    vreplgr2vr.b     vr4,     t0
+
+    bge              zero,    t6,     .ENDPUT_D
+    move             t1,      a3
+    vilvl.b          vr9,     vr1,    vr0
+    vilvl.b          vr10,    vr3,    vr2
+.PUT_D:
+    vld              vr5,     a1,     0
+    vld              vr6,     a1,     1
+    add.d            a1,      a1,     a2
+    vld              vr7,     a1,     0
+    vld              vr8,     a1,     1
+    add.d            a1,      a1,     a2
+    vld              vr11,    a1,     0
+    vld              vr12,    a1,     1
+    vilvl.b          vr5,     vr6,    vr5
+    vilvl.b          vr7,     vr8,    vr7
+    vilvl.b          vr13,    vr12,   vr11
+    vilvl.d          vr5,     vr7,    vr5
+    vilvl.d          vr13,    vr13,   vr7
+    vmulwev.h.bu     vr14,    vr9,    vr5
+    vmaddwod.h.bu    vr14,    vr9,    vr5
+    vmulwev.h.bu     vr15,    vr10,   vr13
+    vmaddwod.h.bu    vr15,    vr10,   vr13
+    vadd.h           vr14,    vr14,   vr15
+    vsrarni.b.h      vr14,    vr14,   6
+    vstelm.w         vr14,    a0,     0,    0
+    add.d            a0,      a0,     a2
+    vstelm.w         vr14,    a0,     0,    1
+    add.d            a0,      a0,     a2
+    addi.d           t1,      t1,     -2
+    blt              zero,    t1,     .PUT_D
+    b                .ENDPUT
+.ENDPUT_D:
+
+    bge              zero,    t0,     .ENDPUT_E
+    move             t1,      a3
+    li.d             t7,      1
+    slt              t8,      zero,   t5
+    maskeqz          t5,      a2,     t8
+    masknez          t7,      t7,     t8
+    or               t7,      t7,     t5
+    vilvl.b          vr7,     vr4,    vr0
+.PUT_E:
+    vld              vr5,     a1,     0
+    vldx             vr6,     a1,     t7
+    vilvl.b          vr5,     vr6,    vr5
+    add.d            a1,      a1,     a2
+    vld              vr8,     a1,     0
+    vldx             vr9,     a1,     t7
+    vilvl.b          vr8,     vr9,    vr8
+    vilvl.d          vr5,     vr8,    vr5
+    vmulwev.h.bu     vr6,     vr7,    vr5
+    vmaddwod.h.bu    vr6,     vr7,    vr5
+    vsrarni.b.h      vr6,     vr6,    6
+    vstelm.w         vr6,     a0,     0,    0
+    add.d            a0,      a0,     a2
+    vstelm.w         vr6,     a0,     0,    1
+    add.d            a0,      a0,     a2
+    add.d            a1,      a1,     a2
+    addi.d           t1,      t1,     -2
+    blt              zero,    t1,     .PUT_E
+    b                .ENDPUT
+.ENDPUT_E:
+
+    move             t1,      a3
+.PUT:
+    vld              vr5,     a1,     0
+    vldx             vr8,     a1,     a2
+    vilvl.w          vr5,     vr8,    vr5
+    vmulwev.h.bu     vr6,     vr0,    vr5
+    vmulwod.h.bu     vr7,     vr0,    vr5
+    vsrarni.b.h      vr6,     vr6,    6
+    vsrarni.b.h      vr7,     vr7,    6
+    vilvl.b          vr6,     vr7,    vr6
+    vstelm.w         vr6,     a0,     0,    0
+    add.d            a0,      a0,     a2
+    vstelm.w         vr6,     a0,     0,    1
+    add.d            a0,      a0,     a2
+    add.d            a1,      a1,     t8
+    addi.d           t1,      t1,     -2
+    blt              zero,    t1,     .PUT
+.ENDPUT:
+endfunc
+
+/* void ff_put_h264_chroma_mc8_lasx(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
+                                    int h, int x, int y) */
+function ff_put_h264_chroma_mc8_lasx
+    li.d             t8,      8
+    sub.d            t1,      t8,     a4     // 8-x
+    sub.d            t2,      t8,     a5     // 8-y
+    mul.d            t3,      t1,     t2     // A
+    mul.d            t4,      a4,     t2     // B
+    mul.d            t5,      t1,     a5     // C
+    mul.d            t6,      a4,     a5     // D
+    add.d            t0,      t4,     t5     // E
+    xvreplgr2vr.b    xr0,     t3
+    xvreplgr2vr.b    xr1,     t4
+    xvreplgr2vr.b    xr2,     t5
+    xvreplgr2vr.b    xr3,     t6
+    xvreplgr2vr.b    xr4,     t0
+    slli.d           t2,      a2,     1
+    add.d            t3,      t2,     a2
+    slli.d           t4,      a2,     2
+
+    bge              zero,    t6,     .ENDLOOP_DA
+    move             t1,      a3
+    xvilvl.b         xr9,     xr1,    xr0
+    xvilvl.b         xr10,    xr3,    xr2
+.LOOP_DA:
+    fld.d            f5,      a1,     0
+    fld.d            f6,      a1,     1
+    add.d            a1,      a1,     a2
+    fld.d            f7,      a1,     0
+    fld.d            f8,      a1,     1
+    add.d            a1,      a1,     a2
+    fld.d            f13,     a1,     0
+    fld.d            f14,     a1,     1
+    add.d            a1,      a1,     a2
+    fld.d            f15,     a1,     0
+    fld.d            f16,     a1,     1
+    add.d            a1,      a1,     a2
+    fld.d            f17,     a1,     0
+    fld.d            f18,     a1,     1
+    vilvl.b          vr11,    vr6,    vr5
+    vilvl.b          vr12,    vr8,    vr7
+    vilvl.b          vr14,    vr14,   vr13
+    vilvl.b          vr15,    vr16,   vr15
+    vilvl.b          vr16,    vr18,   vr17
+    xvpermi.q        xr11,    xr12,   0x02
+    xvpermi.q        xr12,    xr14,   0x02
+    xvpermi.q        xr14,    xr15,   0x02
+    xvpermi.q        xr15,    xr16,   0x02
+
+    xvmulwev.h.bu    xr19,    xr9,    xr11
+    xvmaddwod.h.bu   xr19,    xr9,    xr11
+    xvmulwev.h.bu    xr20,    xr10,   xr12
+    xvmaddwod.h.bu   xr20,    xr10,   xr12
+    xvadd.h          xr21,    xr19,   xr20
+    xvsrarni.b.h     xr21,    xr21,   6
+    vstelm.d         vr21,    a0,     0,    0
+    add.d            a0,      a0,     a2
+    xvstelm.d        xr21,    a0,     0,    2
+    add.d            a0,      a0,     a2
+    xvmulwev.h.bu    xr13,    xr9,    xr14
+    xvmaddwod.h.bu   xr13,    xr9,    xr14
+    xvmulwev.h.bu    xr14,    xr10,   xr15
+    xvmaddwod.h.bu   xr14,    xr10,   xr15
+    xvadd.h          xr13,    xr13,   xr14
+    xvsrarni.b.h     xr13,    xr13,   6
+    vstelm.d         vr13,    a0,     0,    0
+    add.d            a0,      a0,     a2
+    xvstelm.d        xr13,    a0,     0,    2
+    add.d            a0,      a0,     a2
+
+    addi.d           t1,      t1,     -4
+    blt              zero,    t1,     .LOOP_DA
+    b                .ENDLOOPA
+.ENDLOOP_DA:
+
+    bge              zero,    t0,     .ENDLOOP_EA
+    move             t1,      a3
+    li.d             t7,      1
+    slt              t8,      zero,   t5
+    maskeqz          t5,      a2,     t8
+    masknez          t7,      t7,     t8
+    or               t7,      t7,     t5
+    xvilvl.b         xr7,     xr4,    xr0
+.LOOP_EA:
+    fld.d            f5,      a1,     0
+    fldx.d           f6,      a1,     t7
+    add.d            a1,      a1,     a2
+    fld.d            f9,      a1,     0
+    fldx.d           f10,     a1,     t7
+    add.d            a1,      a1,     a2
+    fld.d            f11,     a1,     0
+    fldx.d           f12,     a1,     t7
+    add.d            a1,      a1,     a2
+    fld.d            f13,     a1,     0
+    fldx.d           f14,     a1,     t7
+    vilvl.b          vr5,     vr6,    vr5
+    vilvl.b          vr9,     vr10,   vr9
+    vilvl.b          vr11,    vr12,   vr11
+    vilvl.b          vr13,    vr14,   vr13
+    xvpermi.q        xr5,     xr9,    0x02
+    xvpermi.q        xr11,    xr13,   0x02
+
+    xvmulwev.h.bu    xr8,     xr7,    xr5
+    xvmaddwod.h.bu   xr8,     xr7,    xr5
+    xvmulwev.h.bu    xr6,     xr7,    xr11
+    xvmaddwod.h.bu   xr6,     xr7,    xr11
+    xvsrarni.b.h     xr8,     xr8,    6
+    vstelm.d         vr8,     a0,     0,    0
+    add.d            a0,      a0,     a2
+    xvstelm.d        xr8,     a0,     0,    2
+    add.d            a0,      a0,     a2
+    xvsrarni.b.h     xr6,     xr6,    6
+    vstelm.d         vr6,     a0,     0,    0
+    add.d            a0,      a0,     a2
+    xvstelm.d        xr6,     a0,     0,    2
+    add.d            a0,      a0,     a2
+    add.d            a1,      a1,     a2
+
+    addi.d           t1,      t1,     -4
+    blt              zero,    t1,     .LOOP_EA
+    b                .ENDLOOPA
+.ENDLOOP_EA:
+
+    move             t1,      a3
+.LOOPA:
+    fld.d            f5,      a1,     0
+    fldx.d           f6,      a1,     a2
+    fldx.d           f7,      a1,     t2
+    fldx.d           f8,      a1,     t3
+    vilvl.d          vr5,     vr6,    vr5
+    vilvl.d          vr7,     vr8,    vr7
+    xvpermi.q        xr5,     xr7,    0x02
+    xvmulwev.h.bu    xr6,     xr0,    xr5
+    xvmulwod.h.bu    xr7,     xr0,    xr5
+    xvilvl.h         xr8,     xr7,    xr6
+    xvilvh.h         xr9,     xr7,    xr6
+    xvsrarni.b.h     xr9,     xr8,    6
+    vstelm.d         vr9,     a0,     0,    0
+    add.d            a0,      a0,     a2
+    vstelm.d         vr9,     a0,     0,    1
+    add.d            a0,      a0,     a2
+    xvstelm.d        xr9,     a0,     0,    2
+    add.d            a0,      a0,     a2
+    xvstelm.d        xr9,     a0,     0,    3
+    add.d            a0,      a0,     a2
+    add.d            a1,      a1,     t4
+
+    addi.d           t1,      t1,     -4
+    blt              zero,    t1,     .LOOPA
+.ENDLOOPA:
+endfunc
+
+/* void ff_avg_h264_chroma_mc8_lasx(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
+                                    int h, int x, int y) */
+function ff_avg_h264_chroma_mc8_lasx
+    li.d             t8,      8
+    sub.d            t1,      t8,     a4     // 8-x
+    sub.d            t2,      t8,     a5     // 8-y
+    mul.d            t3,      t1,     t2     // A
+    mul.d            t4,      a4,     t2     // B
+    mul.d            t5,      t1,     a5     // C
+    mul.d            t6,      a4,     a5     // D
+    add.d            t0,      t4,     t5     // E
+    xvreplgr2vr.b    xr0,     t3
+    xvreplgr2vr.b    xr1,     t4
+    xvreplgr2vr.b    xr2,     t5
+    xvreplgr2vr.b    xr3,     t6
+    xvreplgr2vr.b    xr4,     t0
+    slli.d           t2,      a2,     1
+    add.d            t3,      t2,     a2
+    slli.d           t4,      a2,     2
+
+    bge              zero,    t6,     .ENDLOOPDA
+    move             t1,      a3
+    xvilvl.b         xr9,     xr1,    xr0
+    xvilvl.b         xr10,    xr3,    xr2
+.LOOPDA:
+    fld.d            f5,      a1,     0
+    fld.d            f6,      a1,     1
+    add.d            a1,      a1,     a2
+    fld.d            f7,      a1,     0
+    fld.d            f8,      a1,     1
+    add.d            a1,      a1,     a2
+    fld.d            f11,     a1,     0
+    fld.d            f12,     a1,     1
+    add.d            a1,      a1,     a2
+    fld.d            f13,     a1,     0
+    fld.d            f14,     a1,     1
+    add.d            a1,      a1,     a2
+    fld.d            f15,     a1,     0
+    fld.d            f16,     a1,     1
+    fld.d            f17,     a0,     0
+    fldx.d           f18,     a0,     a2
+    fldx.d           f19,     a0,     t2
+    fldx.d           f20,     a0,     t3
+    vilvl.b          vr5,     vr6,    vr5
+    vilvl.b          vr7,     vr8,    vr7
+    vilvl.b          vr11,    vr12,   vr11
+    vilvl.b          vr13,    vr14,   vr13
+    vilvl.b          vr16,    vr16,   vr15
+    xvpermi.q        xr5,     xr7,    0x02
+    xvpermi.q        xr7,     xr11,   0x02
+    xvpermi.q        xr11,    xr13,   0x02
+    xvpermi.q        xr13,    xr16,   0x02
+    xvpermi.q        xr17,    xr18,   0x02
+    xvpermi.q        xr19,    xr20,   0x02
+
+    xvmulwev.h.bu    xr14,    xr9,    xr5
+    xvmaddwod.h.bu   xr14,    xr9,    xr5
+    xvmulwev.h.bu    xr15,    xr10,   xr7
+    xvmaddwod.h.bu   xr15,    xr10,   xr7
+    xvadd.h          xr14,    xr14,   xr15
+    xvsrari.h        xr14,    xr14,   6
+    xvsllwil.hu.bu   xr17,    xr17,   0
+    xvadd.h          xr20,    xr14,   xr17
+    xvsrarni.b.h     xr20,    xr20,   1
+    xvstelm.d        xr20,    a0,     0,    0
+    add.d            a0,      a0,     a2
+    xvstelm.d        xr20,    a0,     0,    2
+    add.d            a0,      a0,     a2
+    xvmulwev.h.bu    xr14,    xr9,    xr11
+    xvmaddwod.h.bu   xr14,    xr9,    xr11
+    xvmulwev.h.bu    xr15,    xr10,   xr13
+    xvmaddwod.h.bu   xr15,    xr10,   xr13
+    xvadd.h          xr14,    xr14,   xr15
+    xvsrari.h        xr14,    xr14,   6
+    xvsllwil.hu.bu   xr19,    xr19,   0
+    xvadd.h          xr21,    xr14,   xr19
+    xvsrarni.b.h     xr21,    xr21,   1
+    xvstelm.d        xr21,    a0,     0,    0
+    add.d            a0,      a0,     a2
+    xvstelm.d        xr21,    a0,     0,    2
+    add.d            a0,      a0,     a2
+
+    addi.d           t1,      t1,     -4
+    blt              zero,    t1,     .LOOPDA
+    b                .ENDLOOPELSEA
+.ENDLOOPDA:
+
+    bge              zero,    t0,     .ENDLOOPEA
+    move             t1,      a3
+    li.d             t7,      1
+    slt              t8,      zero,   t5
+    maskeqz          t5,      a2,     t8
+    masknez          t7,      t7,     t8
+    or               t7,      t7,     t5
+    xvilvl.b         xr7,     xr4,    xr0
+.LOOPEA:
+    fld.d            f5,      a1,     0
+    fldx.d           f6,      a1,     t7
+    add.d            a1,      a1,     a2
+    fld.d            f8,      a1,     0
+    fldx.d           f9,      a1,     t7
+    add.d            a1,      a1,     a2
+    fld.d            f10,     a1,     0
+    fldx.d           f11,     a1,     t7
+    add.d            a1,      a1,     a2
+    fld.d            f12,     a1,     0
+    fldx.d           f13,     a1,     t7
+    add.d            a1,      a1,     a2
+    fld.d            f14,     a0,     0
+    fldx.d           f15,     a0,     a2
+    fldx.d           f16,     a0,     t2
+    fldx.d           f17,     a0,     t3
+    vilvl.b          vr5,     vr6,    vr5
+    vilvl.b          vr8,     vr9,    vr8
+    vilvl.b          vr10,    vr11,   vr10
+    vilvl.b          vr12,    vr13,   vr12
+    xvpermi.q        xr5,     xr8,    0x02
+    xvpermi.q        xr10,    xr12,   0x02
+    xvpermi.q        xr14,    xr15,   0x02
+    xvpermi.q        xr16,    xr17,   0x02
+
+    xvmulwev.h.bu    xr6,     xr7,    xr5
+    xvmaddwod.h.bu   xr6,     xr7,    xr5
+    xvsrari.h        xr6,     xr6,    6
+    xvsllwil.hu.bu   xr14,    xr14,   0
+    xvadd.h          xr8,     xr6,    xr14
+    xvsrarni.b.h     xr8,     xr8,    1
+    xvstelm.d        xr8,     a0,     0,    0
+    add.d            a0,      a0,     a2
+    xvstelm.d        xr8,     a0,     0,    2
+    add.d            a0,      a0,     a2
+    xvmulwev.h.bu    xr6,     xr7,    xr10
+    xvmaddwod.h.bu   xr6,     xr7,    xr10
+    xvsrari.h        xr6,     xr6,    6
+    xvsllwil.hu.bu   xr16,    xr16,   0
+    xvadd.h          xr8,     xr6,    xr16
+    xvsrarni.b.h     xr8,     xr8,    1
+    xvstelm.d        xr8,     a0,     0,    0
+    add.d            a0,      a0,     a2
+    xvstelm.d        xr8,     a0,     0,    2
+    add.d            a0,      a0,     a2
+
+    addi.d           t1,      t1,     -4
+    blt              zero,    t1,     .LOOPEA
+    b                .ENDLOOPELSEA
+.ENDLOOPEA:
+
+    move             t1,      a3
+.LOOPELSEA:
+    fld.d            f5,      a1,     0
+    fldx.d           f6,      a1,     a2
+    fldx.d           f7,      a1,     t2
+    fldx.d           f8,      a1,     t3
+    fld.d            f9,      a0,     0
+    fldx.d           f10,     a0,     a2
+    fldx.d           f11,     a0,     t2
+    fldx.d           f12,     a0,     t3
+    xvpermi.q        xr5,     xr6,    0x02
+    xvpermi.q        xr7,     xr8,    0x02
+    xvpermi.q        xr9,     xr10,   0x02
+    xvpermi.q        xr11,    xr12,   0x02
+
+    xvmulwev.h.bu    xr12,    xr0,    xr5
+    xvmulwod.h.bu    xr13,    xr0,    xr5
+    xvilvl.h         xr12,    xr13,   xr12
+    xvsrari.h        xr12,    xr12,   6
+    xvsllwil.hu.bu   xr9,     xr9,    0
+    xvadd.h          xr9,     xr12,   xr9
+    xvsrarni.b.h     xr9,     xr9,    1
+    xvstelm.d        xr9,     a0,     0,    0
+    add.d            a0,      a0,     a2
+    xvstelm.d        xr9,     a0,     0,    2
+    add.d            a0,      a0,     a2
+    xvmulwev.h.bu    xr12,    xr0,    xr7
+    xvmulwod.h.bu    xr13,    xr0,    xr7
+    xvilvl.h         xr12,    xr13,   xr12
+    xvsrari.h        xr12,    xr12,   6
+    xvsllwil.hu.bu   xr11,    xr11,   0
+    xvadd.h          xr13,    xr12,   xr11
+    xvsrarni.b.h     xr13,    xr13,   1
+    xvstelm.d        xr13,    a0,     0,    0
+    add.d            a0,      a0,     a2
+    xvstelm.d        xr13,    a0,     0,    2
+    add.d            a0,      a0,     a2
+    add.d            a1,      a1,     t4
+
+    addi.d           t1,      t1,     -4
+    blt              zero,    t1,     .LOOPELSEA
+.ENDLOOPELSEA:
+endfunc
+
+/* void ff_put_h264_chroma_mc4_lasx(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
+                                    int h, int x, int y) */
+function ff_put_h264_chroma_mc4_lasx
+    li.d             t8,      8
+    sub.d            t1,      t8,     a4     // 8-x
+    sub.d            t2,      t8,     a5     // 8-y
+    mul.d            t3,      t1,     t2     // A
+    mul.d            t4,      a4,     t2     // B
+    mul.d            t5,      t1,     a5     // C
+    mul.d            t6,      a4,     a5     // D
+    add.d            t0,      t4,     t5     // E
+    slli.d           t8,      a2,     1
+    vreplgr2vr.b     vr0,     t3
+    vreplgr2vr.b     vr1,     t4
+    vreplgr2vr.b     vr2,     t5
+    vreplgr2vr.b     vr3,     t6
+    vreplgr2vr.b     vr4,     t0
+
+    bge              zero,    t6,     .ENDPUT_DA
+    move             t1,      a3
+    vilvl.b          vr9,     vr1,    vr0
+    vilvl.b          vr10,    vr3,    vr2
+.PUT_DA:
+    fld.d            f5,      a1,     0
+    fld.d            f6,      a1,     1
+    add.d            a1,      a1,     a2
+    fld.d            f7,      a1,     0
+    fld.d            f8,      a1,     1
+    add.d            a1,      a1,     a2
+    fld.d            f11,     a1,     0
+    fld.d            f12,     a1,     1
+    vilvl.b          vr5,     vr6,    vr5
+    vilvl.b          vr7,     vr8,    vr7
+    vilvl.b          vr13,    vr12,   vr11
+    vilvl.d          vr5,     vr7,    vr5
+    vilvl.d          vr13,    vr13,   vr7
+    vmulwev.h.bu     vr14,    vr9,    vr5
+    vmaddwod.h.bu    vr14,    vr9,    vr5
+    vmulwev.h.bu     vr15,    vr10,   vr13
+    vmaddwod.h.bu    vr15,    vr10,   vr13
+    xvadd.h          xr14,    xr14,   xr15
+    vsrarni.b.h      vr16,    vr14,   6
+    vstelm.w         vr16,    a0,     0,    0
+    add.d            a0,      a0,     a2
+    vstelm.w         vr16,    a0,     0,    1
+    add.d            a0,      a0,     a2
+    addi.d           t1,      t1,     -2
+    blt              zero,    t1,     .PUT_DA
+    b                .ENDPUTA
+.ENDPUT_DA:
+
+    bge              zero,    t0,     .ENDPUT_EA
+    move             t1,      a3
+    li.d             t7,      1
+    slt              t8,      zero,   t5
+    maskeqz          t5,      a2,     t8
+    masknez          t7,      t7,     t8
+    or               t7,      t7,     t5
+    vilvl.b          vr7,     vr4,    vr0
+.PUT_EA:
+    fld.d            f5,      a1,     0
+    fldx.d           f6,      a1,     t7
+    vilvl.b          vr5,     vr6,    vr5
+    add.d            a1,      a1,     a2
+    fld.d            f8,      a1,     0
+    fldx.d           f9,      a1,     t7
+    vilvl.b          vr8,     vr9,    vr8
+    vilvl.d          vr5,     vr8,    vr5
+    vmulwev.h.bu     vr6,     vr7,    vr5
+    vmaddwod.h.bu    vr6,     vr7,    vr5
+    vsrarni.b.h      vr6,     vr6,    6
+    vstelm.w         vr6,     a0,     0,    0
+    add.d            a0,      a0,     a2
+    vstelm.w         vr6,     a0,     0,    1
+    add.d            a0,      a0,     a2
+    add.d            a1,      a1,     a2
+    addi.d           t1,      t1,     -2
+    blt              zero,    t1,     .PUT_EA
+    b                .ENDPUTA
+.ENDPUT_EA:
+
+    move             t1,      a3
+.PUTA:
+    fld.d            f5,      a1,     0
+    fldx.d           f8,      a1,     a2
+    vilvl.w          vr5,     vr8,    vr5
+    vmulwev.h.bu     vr6,     vr0,    vr5
+    vmulwod.h.bu     vr7,     vr0,    vr5
+    vilvl.h          vr6,     vr7,    vr6
+    vsrarni.b.h      vr6,     vr6,    6
+    vstelm.w         vr6,     a0,     0,    0
+    add.d            a0,      a0,     a2
+    vstelm.w         vr6,     a0,     0,    1
+    add.d            a0,      a0,     a2
+    add.d            a1,      a1,     t8
+    addi.d           t1,      t1,     -2
+    blt              zero,    t1,     .PUTA
+.ENDPUTA:
+endfunc
diff --git a/libavcodec/loongarch/h264chroma_init_loongarch.c b/libavcodec/loongarch/h264chroma_init_loongarch.c
new file mode 100644
index 0000000000..9388a35da4
--- /dev/null
+++ b/libavcodec/loongarch/h264chroma_init_loongarch.c
@@ -0,0 +1,45 @@
+/*
+ * Copyright (c) 2020 Loongson Technology Corporation Limited
+ * Contributed by Shiyou Yin <yinshiyou-hf@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "h264chroma_lasx.h"
+#include "libavutil/attributes.h"
+#include "libavutil/loongarch/cpu.h"
+#include "libavcodec/h264chroma.h"
+
+av_cold void ff_h264chroma_init_loongarch(H264ChromaContext *c, int bit_depth)
+{
+    int cpu_flags = av_get_cpu_flags();
+    if (have_lsx(cpu_flags)) {
+        if (bit_depth <= 8) {
+            c->put_h264_chroma_pixels_tab[0] = ff_put_h264_chroma_mc8_lsx;
+            c->avg_h264_chroma_pixels_tab[0] = ff_avg_h264_chroma_mc8_lsx;
+            c->put_h264_chroma_pixels_tab[1] = ff_put_h264_chroma_mc4_lsx;
+        }
+    }
+
+    if (have_lasx(cpu_flags)) {
+        if (bit_depth <= 8) {
+            c->put_h264_chroma_pixels_tab[0] = ff_put_h264_chroma_mc8_lasx;
+            c->avg_h264_chroma_pixels_tab[0] = ff_avg_h264_chroma_mc8_lasx;
+            c->put_h264_chroma_pixels_tab[1] = ff_put_h264_chroma_mc4_lasx;
+        }
+    }
+}
diff --git a/libavcodec/loongarch/h264chroma_lasx.h b/libavcodec/loongarch/h264chroma_lasx.h
new file mode 100644
index 0000000000..608c14d590
--- /dev/null
+++ b/libavcodec/loongarch/h264chroma_lasx.h
@@ -0,0 +1,43 @@
+/*
+ * Copyright (c) 2020 Loongson Technology Corporation Limited
+ * Contributed by Shiyou Yin <yinshiyou-hf@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef AVCODEC_LOONGARCH_H264CHROMA_LASX_H
+#define AVCODEC_LOONGARCH_H264CHROMA_LASX_H
+
+#include <stdint.h>
+#include <stddef.h>
+#include "libavcodec/h264.h"
+
+void ff_put_h264_chroma_mc8_lsx(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
+        int h, int x, int y);
+void ff_avg_h264_chroma_mc8_lsx(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
+        int h, int x, int y);
+void ff_put_h264_chroma_mc4_lsx(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
+        int h, int x, int y);
+
+void ff_put_h264_chroma_mc4_lasx(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
+        int h, int x, int y);
+void ff_put_h264_chroma_mc8_lasx(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
+        int h, int x, int y);
+void ff_avg_h264_chroma_mc8_lasx(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
+        int h, int x, int y);
+
+#endif /* AVCODEC_LOONGARCH_H264CHROMA_LASX_H */
diff --git a/libavcodec/loongarch/h264dsp.S b/libavcodec/loongarch/h264dsp.S
new file mode 100644
index 0000000000..e543503a69
--- /dev/null
+++ b/libavcodec/loongarch/h264dsp.S
@@ -0,0 +1,1977 @@
+/*
+ * Loongson LSX/LASX optimized h264dsp
+ *
+ * Copyright (c) 2023 Loongson Technology Corporation Limited
+ * Contributed by Hao Chen <chenhao@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "loongson_asm.S"
+
+const vec_shuf
+.rept 2
+.byte 0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3
+.endr
+endconst
+
+.macro AVC_LPF_P1_OR_Q1 _in0, _in1, _in2, _in3, _in4, _in5, _out, _tmp0, _tmp1
+    vavgr.hu       \_tmp0,   \_in0,  \_in1
+    vslli.h        \_tmp1,   \_in2,  1
+    vsub.h         \_tmp0,   \_tmp0, \_tmp1
+    vavg.h         \_tmp0,   \_in3,  \_tmp0
+    vclip.h        \_tmp0,   \_tmp0, \_in4,  \_in5
+    vadd.h         \_out,    \_in2,  \_tmp0
+.endm
+
+.macro AVC_LPF_P0Q0 _in0, _in1, _in2, _in3, _in4, _in5, _out0,   \
+                    _out1, _tmp0, _tmp1
+    vsub.h         \_tmp0,   \_in0,  \_in1
+    vsub.h         \_tmp1,   \_in2,  \_in3
+    vslli.h        \_tmp0,   \_tmp0, 2
+    vaddi.hu       \_tmp1,   \_tmp1, 4
+    vadd.h         \_tmp0,   \_tmp0, \_tmp1
+    vsrai.h        \_tmp0,   \_tmp0, 3
+    vclip.h        \_tmp0,   \_tmp0, \_in4,  \_in5
+    vadd.h         \_out0,   \_in1,  \_tmp0
+    vsub.h         \_out1,   \_in0,  \_tmp0
+    vclip255.h     \_out0,   \_out0
+    vclip255.h     \_out1,   \_out1
+.endm
+
+.macro SAVE_REG
+    addi.d          sp,     sp,    -64
+    fst.d           f24,    sp,    0
+    fst.d           f25,    sp,    8
+    fst.d           f26,    sp,    16
+    fst.d           f27,    sp,    24
+    fst.d           f28,    sp,    32
+    fst.d           f29,    sp,    40
+    fst.d           f30,    sp,    48
+    fst.d           f31,    sp,    56
+.endm
+
+.macro RESTORE_REG
+    fld.d           f24,    sp,    0
+    fld.d           f25,    sp,    8
+    fld.d           f26,    sp,    16
+    fld.d           f27,    sp,    24
+    fld.d           f28,    sp,    32
+    fld.d           f29,    sp,    40
+    fld.d           f30,    sp,    48
+    fld.d           f31,    sp,    56
+    addi.d          sp,     sp,    64
+.endm
+
+.macro load_double _in0, _in1, _in2, _in3, _src, _str0, _str1, _str2
+    fld.d           \_in0,    \_src,    0
+    fldx.d          \_in1,    \_src,    \_str0
+    fldx.d          \_in2,    \_src,    \_str1
+    fldx.d          \_in3,    \_src,    \_str2
+.endm
+
+.macro store_double _in0, _in1, _in2, _in3, _dst, _str0, _str1, _str2
+    fst.d           \_in0,    \_dst,    0
+    fstx.d          \_in1,    \_dst,    \_str0
+    fstx.d          \_in2,    \_dst,    \_str1
+    fstx.d          \_in3,    \_dst,    \_str2
+.endm
+
+function ff_h264_h_lpf_luma_8_lsx
+    slli.d          t0,     a1,    1   //img_width_2x
+    slli.d          t1,     a1,    2   //img_width_4x
+    slli.d          t2,     a1,    3   //img_width_8x
+    SAVE_REG
+    la.local        t4,     vec_shuf
+    add.d           t3,     t0,    a1  //img_width_3x
+    vldrepl.w       vr0,    a4,    0   //tmp_vec0
+    vld             vr1,    t4,    0  //tc_vec
+    vshuf.b         vr1,    vr0,   vr0,   vr1   //tc_vec
+    vslti.b         vr2,    vr1,   0
+    vxori.b         vr2,    vr2,   255
+    vandi.b         vr2,    vr2,   1    //bs_vec
+    vsetnez.v       $fcc0,  vr2
+    bceqz           $fcc0,  .END_LUMA_8
+    vldi            vr0,    0            //zero
+    addi.d          t4,     a0,    -4    //src
+    vslt.bu         vr3,    vr0,   vr2   //is_bs_greater_than0
+    add.d           t5,     t4,    t2    //src_tmp
+    vld             vr4,    t4,    0    //row0
+    vldx            vr5,    t4,    a1   //row1
+    vldx            vr6,    t4,    t0   //row2
+    vldx            vr7,    t4,    t3   //row3
+    add.d           t6,     t4,    t1   // src += img_width_4x
+    vld             vr8,    t6,    0    //row4
+    vldx            vr9,    t6,    a1   //row5
+    vldx            vr10,   t6,    t0   //row6
+    vldx            vr11,   t6,    t3   //row7
+    vld             vr12,   t5,    0    //row8
+    vldx            vr13,   t5,    a1   //row9
+    vldx            vr14,   t5,    t0   //row10
+    vldx            vr15,   t5,    t3   //row11
+    add.d           t6,     t5,    t1   // src_tmp += img_width_4x
+    vld             vr16,   t6,    0    //row12
+    vldx            vr17,   t6,    a1   //row13
+    vldx            vr18,   t6,    t0   //row14
+    vldx            vr19,   t6,    t3   //row15
+    LSX_TRANSPOSE16X8_B vr4, vr5, vr6, vr7, vr8, vr9, vr10, vr11,        \
+                        vr12, vr13, vr14, vr15, vr16, vr17, vr18, vr19,  \
+                        vr10, vr11, vr12, vr13, vr14, vr15, vr16, vr17,  \
+                        vr20, vr21, vr22, vr23, vr24, vr25, vr26, vr27
+    //vr10: p3_org, vr11: p2_org, vr12: p1_org, vr13: p0_org
+    //vr14: q0_org, vr15: q1_org, vr16: q2_org, vr17: q3_org
+    vabsd.bu        vr20,   vr13,  vr14    //p0_asub_q0
+    vabsd.bu        vr21,   vr12,  vr13    //p1_asub_p0
+    vabsd.bu        vr22,   vr15,  vr14    //q1_asub_q0
+
+    vreplgr2vr.b    vr4,    a2          //alpha
+    vreplgr2vr.b    vr5,    a3          //beta
+
+    vslt.bu         vr6,    vr20,  vr4   //is_less_than_alpha
+    vslt.bu         vr7,    vr21,  vr5   //is_less_than_beta
+    vand.v          vr8,    vr6,   vr7   //is_less_than
+    vslt.bu         vr7,    vr22,  vr5   //is_less_than_beta
+    vand.v          vr8,    vr7,   vr8   //is_less_than
+    vand.v          vr8,    vr8,   vr3   //is_less_than
+    vsetnez.v       $fcc0,  vr8
+    bceqz           $fcc0,  .END_LUMA_8
+    vneg.b          vr9,    vr1          //neg_tc_h
+    vsllwil.hu.bu   vr18,   vr1,   0     //tc_h.0
+    vexth.hu.bu     vr19,   vr1          //tc_h.1
+    vexth.h.b       vr2,    vr9          //neg_tc_h.1
+    vsllwil.h.b     vr9,    vr9,   0     //neg_tc_h.0
+
+    vsllwil.hu.bu   vr23,   vr12,  0     //p1_org_h.0
+    vexth.hu.bu     vr3,    vr12         //p1_org_h.1
+    vsllwil.hu.bu   vr24,   vr13,  0     //p0_org_h.0
+    vexth.hu.bu     vr4,    vr13         //p0_org_h.1
+    vsllwil.hu.bu   vr25,   vr14,  0     //q0_org_h.0
+    vexth.hu.bu     vr6,    vr14         //q0_org_h.1
+
+    vabsd.bu        vr0,    vr11,  vr13  //p2_asub_p0
+    vslt.bu         vr7,    vr0,   vr5
+    vand.v          vr7,    vr8,   vr7   //is_less_than_beta
+    vsetnez.v       $fcc0,  vr7
+    bceqz           $fcc0,  .END_LUMA_BETA
+    vsllwil.hu.bu   vr26,   vr11,  0   //p2_org_h.0
+    vexth.hu.bu     vr0,    vr11       //p2_org_h.1
+    AVC_LPF_P1_OR_Q1 vr24, vr25, vr23, vr26, vr9, vr18, vr27, vr28, vr29
+    AVC_LPF_P1_OR_Q1 vr4, vr6, vr3, vr0, vr2, vr19, vr28, vr29, vr30
+    vpickev.b       vr27,   vr28,  vr27
+    vbitsel.v       vr12,   vr12,  vr27,  vr7
+    vandi.b         vr7,    vr7,   1
+    vadd.b          vr1,    vr1,   vr7
+.END_LUMA_BETA:
+    vabsd.bu        vr26,   vr16,  vr14  //q2_asub_q0
+    vslt.bu         vr7,    vr26,  vr5
+    vand.v          vr7,    vr7,   vr8
+    vsllwil.hu.bu   vr27,   vr15,  0     //q1_org_h.0
+    vexth.hu.bu     vr26,   vr15         //q1_org_h.1
+    vsetnez.v       $fcc0,  vr7
+    bceqz           $fcc0,  .END_LUMA_BETA_SEC
+    vsllwil.hu.bu   vr28,   vr16,  0     //q2_org_h.0
+    vexth.hu.bu     vr0,    vr16         //q2_org_h.1
+    AVC_LPF_P1_OR_Q1 vr24, vr25, vr27, vr28, vr9, vr18, vr29, vr30, vr31
+    AVC_LPF_P1_OR_Q1 vr4, vr6, vr26, vr0, vr2, vr19, vr22, vr30, vr31
+    vpickev.b       vr29,   vr22,  vr29
+    vbitsel.v       vr15,   vr15,  vr29,  vr7
+    vandi.b         vr7,    vr7,   1
+    vadd.b          vr1,    vr1,   vr7
+.END_LUMA_BETA_SEC:
+    vneg.b          vr22,   vr1    //neg_thresh_h
+    vsllwil.h.b     vr28,   vr22,  0  //neg_thresh_h.0
+    vexth.h.b       vr29,   vr22     //neg_thresh_h.1
+    vsllwil.hu.bu   vr18,   vr1,   0  //tc_h.0
+    vexth.hu.bu     vr1,    vr1       //tc_h.1
+    AVC_LPF_P0Q0 vr25, vr24, vr23, vr27, vr28, vr18, vr30, vr31, vr0, vr2
+    AVC_LPF_P0Q0 vr6, vr4, vr3, vr26, vr29, vr1, vr20, vr21, vr0, vr2
+    vpickev.b       vr30,   vr20,  vr30  //p0_h
+    vpickev.b       vr31,   vr21,  vr31  //q0_h
+    vbitsel.v       vr13,   vr13,  vr30,  vr8  //p0_org
+    vbitsel.v       vr14,   vr14,  vr31,  vr8  //q0_org
+
+    vilvl.b         vr4,    vr12,  vr10   // row0.0
+    vilvl.b         vr5,    vr16,  vr14   // row0.1
+    vilvl.b         vr6,    vr13,  vr11   // row2.0
+    vilvl.b         vr7,    vr17,  vr15   // row2.1
+
+    vilvh.b         vr8,    vr12,  vr10   // row1.0
+    vilvh.b         vr9,    vr16,  vr14   // row1.1
+    vilvh.b         vr10,   vr13,  vr11   // row3.0
+    vilvh.b         vr11,   vr17,  vr15   // row3.1
+
+    vilvl.b         vr12,   vr6,   vr4    // row4.0
+    vilvl.b         vr13,   vr7,   vr5    // row4.1
+    vilvl.b         vr14,   vr10,  vr8    // row6.0
+    vilvl.b         vr15,   vr11,  vr9    // row6.1
+
+    vilvh.b         vr16,   vr6,   vr4    // row5.0
+    vilvh.b         vr17,   vr7,   vr5    // row5.1
+    vilvh.b         vr18,   vr10,  vr8    // row7.0
+    vilvh.b         vr19,   vr11,  vr9    // row7.1
+
+    vilvl.w         vr4,    vr13,  vr12   // row4: 0, 4, 1, 5
+    vilvh.w         vr5,    vr13,  vr12   // row4: 2, 6, 3, 7
+    vilvl.w         vr6,    vr17,  vr16   // row5: 0, 4, 1, 5
+    vilvh.w         vr7,    vr17,  vr16   // row5: 2, 6, 3, 7
+
+    vilvl.w         vr8,    vr15,  vr14   // row6: 0, 4, 1, 5
+    vilvh.w         vr9,    vr15,  vr14   // row6: 2, 6, 3, 7
+    vilvl.w         vr10,   vr19,  vr18   // row7: 0, 4, 1, 5
+    vilvh.w         vr11,   vr19,  vr18   // row7: 2, 6, 3, 7
+
+    vbsrl.v         vr20,   vr4,   8
+    vbsrl.v         vr21,   vr5,   8
+    vbsrl.v         vr22,   vr6,   8
+    vbsrl.v         vr23,   vr7,   8
+
+    vbsrl.v         vr24,   vr8,   8
+    vbsrl.v         vr25,   vr9,   8
+    vbsrl.v         vr26,   vr10,  8
+    vbsrl.v         vr27,   vr11,  8
+
+    store_double f4, f20, f5, f21, t4, a1, t0, t3
+    add.d           t4,     t4,    t1
+    store_double f6, f22, f7, f23, t4, a1, t0, t3
+    add.d           t4,     t4,    t1
+    store_double f8, f24, f9, f25, t4, a1, t0, t3
+    add.d           t4,     t4,    t1
+    store_double f10, f26, f11, f27, t4, a1, t0, t3
+.END_LUMA_8:
+    RESTORE_REG
+endfunc
+
+function ff_h264_v_lpf_luma_8_lsx
+    slli.d          t0,     a1,    1   //img_width_2x
+    la.local        t4,     vec_shuf
+    vldrepl.w       vr0,    a4,    0   //tmp_vec0
+    vld             vr1,    t4,    0  //tc_vec
+    add.d           t1,     t0,    a1  //img_width_3x
+    vshuf.b         vr1,    vr0,   vr0,   vr1   //tc_vec
+    addi.d          sp,     sp,    -24
+    fst.d           f24,    sp,    0
+    fst.d           f25,    sp,    8
+    fst.d           f26,    sp,    16
+    vslti.b         vr2,    vr1,   0
+    vxori.b         vr2,    vr2,   255
+    vandi.b         vr2,    vr2,   1    //bs_vec
+    vsetnez.v       $fcc0,  vr2
+    bceqz           $fcc0,  .END_V_LUMA_8
+    sub.d           t2,     a0,    t1   //data - img_width_3x
+    vreplgr2vr.b    vr4,    a2          //alpha
+    vreplgr2vr.b    vr5,    a3          //beta
+    vldi            vr0,    0           //zero
+    vld             vr10,   t2,    0    //p2_org
+    vldx            vr11,   t2,    a1   //p1_org
+    vldx            vr12,   t2,    t0   //p0_org
+    vld             vr13,   a0,    0    //q0_org
+    vldx            vr14,   a0,    a1   //q1_org
+
+    vslt.bu         vr0,    vr0,   vr2   //is_bs_greater_than0
+    vabsd.bu        vr16,   vr11,  vr12  //p1_asub_p0
+    vabsd.bu        vr15,   vr12,  vr13  //p0_asub_q0
+    vabsd.bu        vr17,   vr14,  vr13  //q1_asub_q0
+
+    vslt.bu         vr6,    vr15,  vr4   //is_less_than_alpha
+    vslt.bu         vr7,    vr16,  vr5   //is_less_than_beta
+    vand.v          vr8,    vr6,   vr7   //is_less_than
+    vslt.bu         vr7,    vr17,  vr5   //is_less_than_beta
+    vand.v          vr8,    vr7,   vr8
+    vand.v          vr8,    vr8,   vr0  //is_less_than
+
+    vsetnez.v       $fcc0,  vr8
+    bceqz           $fcc0,  .END_V_LUMA_8
+    vldx            vr15,   a0,    t0    //q2_org
+    vneg.b          vr0,    vr1          //neg_tc_h
+    vsllwil.h.b     vr18,   vr1,   0     //tc_h.0
+    vexth.h.b       vr19,   vr1          //tc_h.1
+    vsllwil.h.b     vr9,    vr0,   0     //neg_tc_h.0
+    vexth.h.b       vr2,    vr0          //neg_tc_h.1
+
+    vsllwil.hu.bu   vr16,   vr11,  0     //p1_org_h.0
+    vexth.hu.bu     vr17,   vr11         //p1_org_h.1
+    vsllwil.hu.bu   vr20,   vr12,  0     //p0_org_h.0
+    vexth.hu.bu     vr21,   vr12         //p0_org_h.1
+    vsllwil.hu.bu   vr22,   vr13,  0     //q0_org_h.0
+    vexth.hu.bu     vr23,   vr13         //q0_org_h.1
+
+    vabsd.bu        vr0,    vr10,  vr12  //p2_asub_p0
+    vslt.bu         vr7,    vr0,   vr5   //is_less_than_beta
+    vand.v          vr7,    vr7,   vr8   //is_less_than_beta
+
+    vsetnez.v       $fcc0,  vr8
+    bceqz           $fcc0,  .END_V_LESS_BETA
+    vsllwil.hu.bu   vr3,    vr10,  0   //p2_org_h.0
+    vexth.hu.bu     vr4,    vr10       //p2_org_h.1
+    AVC_LPF_P1_OR_Q1 vr20, vr22, vr16, vr3, vr9, vr18, vr24, vr0, vr26
+    AVC_LPF_P1_OR_Q1 vr21, vr23, vr17, vr4, vr2, vr19, vr25, vr0, vr26
+    vpickev.b       vr24,   vr25,  vr24
+    vbitsel.v       vr24,   vr11,  vr24,   vr7
+    addi.d          t3,     t2,    16
+    vstx            vr24,   t2,    a1
+    vandi.b         vr7,    vr7,   1
+    vadd.b          vr1,    vr7,   vr1
+.END_V_LESS_BETA:
+    vabsd.bu        vr0,    vr15,  vr13   //q2_asub_q0
+    vslt.bu         vr7,    vr0,   vr5    //is_less_than_beta
+    vand.v          vr7,    vr7,   vr8    //is_less_than_beta
+    vsllwil.hu.bu   vr3,    vr14,  0     //q1_org_h.0
+    vexth.hu.bu     vr4,    vr14         //q1_org_h.1
+
+    vsetnez.v       $fcc0,  vr7
+    bceqz           $fcc0,  .END_V_LESS_BETA_SEC
+    vsllwil.hu.bu   vr11,   vr15,  0     //q2_org_h.0
+    vexth.hu.bu     vr15,   vr15         //q2_org_h.1
+    AVC_LPF_P1_OR_Q1 vr20, vr22, vr3, vr11, vr9, vr18, vr24, vr0, vr26
+    AVC_LPF_P1_OR_Q1 vr21, vr23, vr4, vr15, vr2, vr19, vr25, vr0, vr26
+    vpickev.b       vr24,   vr25,  vr24
+    vbitsel.v       vr24,   vr14,  vr24,   vr7
+    vstx            vr24,   a0,    a1
+    vandi.b         vr7,    vr7,   1
+    vadd.b          vr1,    vr1,   vr7
+.END_V_LESS_BETA_SEC:
+    vneg.b          vr0,    vr1
+    vsllwil.h.b     vr9,    vr0,   0    //neg_thresh_h.0
+    vexth.h.b       vr2,    vr0         //neg_thresh_h.1
+    vsllwil.hu.bu   vr18,   vr1,   0    //tc_h.0
+    vexth.hu.bu     vr19,   vr1         //tc_h.1
+    AVC_LPF_P0Q0 vr22, vr20, vr16, vr3, vr9, vr18, vr11, vr15, vr0, vr26
+    AVC_LPF_P0Q0 vr23, vr21, vr17, vr4, vr2, vr19, vr10, vr14, vr0, vr26
+    vpickev.b       vr11,   vr10,  vr11  //p0_h
+    vpickev.b       vr15,   vr14,  vr15  //q0_h
+    vbitsel.v       vr11,   vr12,  vr11,   vr8  //p0_h
+    vbitsel.v       vr15,   vr13,  vr15,   vr8  //q0_h
+    vstx            vr11,   t2,    t0
+    vst             vr15,   a0,    0
+.END_V_LUMA_8:
+    fld.d           f24,    sp,    0
+    fld.d           f25,    sp,    8
+    fld.d           f26,    sp,    16
+    addi.d          sp,     sp,    24
+endfunc
+
+const chroma_shuf
+.byte 0, 0, 1, 1, 2, 2, 3, 3, 0, 0, 1, 1, 2, 2, 3, 3
+endconst
+
+function ff_h264_h_lpf_chroma_8_lsx
+    slli.d          t0,     a1,    1   //img_width_2x
+    slli.d          t1,     a1,    2   //img_width_4x
+    la.local        t4,     chroma_shuf
+    add.d           t2,     t0,    a1  //img_width_3x
+    vldrepl.w       vr0,    a4,    0   //tmp_vec0
+    vld             vr1,    t4,    0  //tc_vec
+    vshuf.b         vr1,    vr0,   vr0,   vr1   //tc_vec
+    vslti.b         vr2,    vr1,   0
+    vxori.b         vr2,    vr2,   255
+    vandi.b         vr2,    vr2,   1    //bs_vec
+    vsetnez.v       $fcc0,  vr2
+    bceqz           $fcc0,  .END_CHROMA_8
+    vldi            vr0,    0
+    addi.d          t4,     a0,    -2
+    vslt.bu         vr3,    vr0,   vr2   //is_bs_greater_than0
+    add.d           t5,     t4,    t1
+    vld             vr4,    t4,    0    //row0
+    vldx            vr5,    t4,    a1   //row1
+    vldx            vr6,    t4,    t0   //row2
+    vldx            vr7,    t4,    t2   //row3
+    vld             vr8,    t5,    0    //row4
+    vldx            vr9,    t5,    a1   //row5
+    vldx            vr10,   t5,    t0   //row6
+    vldx            vr11,   t5,    t2   //row7
+    vilvl.b         vr12,   vr6,   vr4  //p1_org
+    vilvl.b         vr13,   vr7,   vr5  //p0_org
+    vilvl.b         vr14,   vr10,  vr8  //q0_org
+    vilvl.b         vr15,   vr11,  vr9  //q1_org
+    vilvl.b         vr4,    vr13,  vr12 //row0
+    vilvl.b         vr5,    vr15,  vr14 //row1
+    vilvl.w         vr6,    vr5,   vr4  //row2
+    vilvh.w         vr7,    vr5,   vr4  //row3
+    vilvl.d         vr12,   vr6,   vr6  //p1_org
+    vilvh.d         vr13,   vr6,   vr6  //p0_org
+    vilvl.d         vr14,   vr7,   vr7  //q0_org
+    vilvh.d         vr15,   vr7,   vr7  //q1_org
+
+    vabsd.bu        vr20,   vr13,  vr14  //p0_asub_q0
+    vabsd.bu        vr21,   vr12,  vr13  //p1_asub_p0
+    vabsd.bu        vr22,   vr15,  vr14  //q1_asub_q0
+
+    vreplgr2vr.b    vr4,    a2     //alpha
+    vreplgr2vr.b    vr5,    a3     //beta
+
+    vslt.bu         vr6,    vr20,  vr4  //is_less_than_alpha
+    vslt.bu         vr7,    vr21,  vr5  //is_less_than_beta
+    vand.v          vr8,    vr6,   vr7   //is_less_than
+    vslt.bu         vr7,    vr22,  vr5   //is_less_than_beta
+    vand.v          vr8,    vr7,   vr8   //is_less_than
+    vand.v          vr8,    vr8,   vr3   //is_less_than
+    vsetnez.v       $fcc0,  vr8
+    bceqz           $fcc0,  .END_CHROMA_8
+
+    vneg.b          vr9,    vr1          //neg_tc_h
+    vexth.hu.bu     vr3,    vr12         //p1_org_h
+    vexth.hu.bu     vr4,    vr13         //p0_org_h.1
+    vexth.hu.bu     vr5,    vr14         //q0_org_h.1
+    vexth.hu.bu     vr6,    vr15         //q1_org_h.1
+
+    vexth.hu.bu     vr18,   vr1          //tc_h.1
+    vexth.h.b       vr2,    vr9          //neg_tc_h.1
+
+    AVC_LPF_P0Q0 vr5, vr4, vr3, vr6, vr2, vr18, vr10, vr11, vr16, vr17
+    vpickev.b       vr10,   vr10,   vr10  //p0_h
+    vpickev.b       vr11,   vr11,   vr11  //q0_h
+    vbitsel.v       vr13,   vr13,   vr10,   vr8
+    vbitsel.v       vr14,   vr14,   vr11,   vr8
+    vilvl.b         vr15,   vr14,   vr13
+    addi.d          t4,     t4,     1
+    add.d           t5,     t4,     a1
+    add.d           t6,     t4,     t0
+    add.d           t7,     t4,     t2
+    vstelm.h        vr15,   t4,     0,    0
+    vstelm.h        vr15,   t5,     0,    1
+    vstelm.h        vr15,   t6,     0,    2
+    vstelm.h        vr15,   t7,     0,    3
+    add.d           t4,     t4,     t1
+    add.d           t5,     t4,     a1
+    add.d           t6,     t4,     t0
+    add.d           t7,     t4,     t2
+    vstelm.h        vr15,   t4,     0,    4
+    vstelm.h        vr15,   t5,     0,    5
+    vstelm.h        vr15,   t6,     0,    6
+    vstelm.h        vr15,   t7,     0,    7
+.END_CHROMA_8:
+endfunc
+
+function ff_h264_v_lpf_chroma_8_lsx
+    slli.d          t0,     a1,    1   //img_width_2x
+    la.local        t4,     chroma_shuf
+    vldrepl.w       vr0,    a4,    0   //tmp_vec0
+    vld             vr1,    t4,    0  //tc_vec
+    vshuf.b         vr1,    vr0,   vr0,   vr1   //tc_vec
+    vslti.b         vr2,    vr1,   0
+    vxori.b         vr2,    vr2,   255
+    vandi.b         vr2,    vr2,   1    //bs_vec
+    vsetnez.v       $fcc0,  vr2
+    bceqz           $fcc0,  .END_CHROMA_V_8
+    vldi            vr0,    0
+    sub.d           t4,     a0,    t0
+    vslt.bu         vr3,    vr0,   vr2   //is_bs_greater_than0
+    vld             vr12,   t4,    0    //p1_org
+    vldx            vr13,   t4,    a1   //p0_org
+    vld             vr14,   a0,    0    //q0_org
+    vldx            vr15,   a0,    a1   //q1_org
+
+    vabsd.bu        vr20,   vr13,  vr14  //p0_asub_q0
+    vabsd.bu        vr21,   vr12,  vr13  //p1_asub_p0
+    vabsd.bu        vr22,   vr15,  vr14  //q1_asub_q0
+
+    vreplgr2vr.b    vr4,    a2     //alpha
+    vreplgr2vr.b    vr5,    a3     //beta
+
+    vslt.bu         vr6,    vr20,  vr4  //is_less_than_alpha
+    vslt.bu         vr7,    vr21,  vr5  //is_less_than_beta
+    vand.v          vr8,    vr6,   vr7   //is_less_than
+    vslt.bu         vr7,    vr22,  vr5   //is_less_than_beta
+    vand.v          vr8,    vr7,   vr8   //is_less_than
+    vand.v          vr8,    vr8,   vr3   //is_less_than
+    vsetnez.v       $fcc0,  vr8
+    bceqz           $fcc0,  .END_CHROMA_V_8
+
+    vneg.b          vr9,    vr1          //neg_tc_h
+    vsllwil.hu.bu   vr3,    vr12,   0    //p1_org_h
+    vsllwil.hu.bu   vr4,    vr13,   0    //p0_org_h.1
+    vsllwil.hu.bu   vr5,    vr14,   0    //q0_org_h.1
+    vsllwil.hu.bu   vr6,    vr15,   0    //q1_org_h.1
+
+    vexth.hu.bu     vr18,   vr1          //tc_h.1
+    vexth.h.b       vr2,    vr9          //neg_tc_h.1
+
+    AVC_LPF_P0Q0 vr5, vr4, vr3, vr6, vr2, vr18, vr10, vr11, vr16, vr17
+    vpickev.b       vr10,   vr10,   vr10  //p0_h
+    vpickev.b       vr11,   vr11,   vr11  //q0_h
+    vbitsel.v       vr10,   vr13,   vr10,   vr8
+    vbitsel.v       vr11,   vr14,   vr11,   vr8
+    fstx.d          f10,    t4,     a1
+    fst.d           f11,    a0,     0
+.END_CHROMA_V_8:
+endfunc
+
+.macro AVC_LPF_P0P1P2_OR_Q0Q1Q2 _in0, _in1, _in2, _in3, _in4, _in5  \
+                                _out0, _out1, _out2, _tmp0, _const3
+    vadd.h          \_tmp0,  \_in1,  \_in2
+    vadd.h          \_tmp0,  \_tmp0, \_in3
+    vslli.h         \_out2,  \_in0,  1
+    vslli.h         \_out0,  \_tmp0, 1
+    vadd.h          \_out0,  \_out0, \_in4
+    vadd.h          \_out1,  \_in4,  \_tmp0
+    vadd.h          \_out0,  \_out0, \_in5
+    vmadd.h         \_out2,  \_in4,  \_const3
+    vsrar.h         \_out0,  \_out0, \_const3
+    vadd.h          \_out2,  \_out2, \_tmp0
+    vsrari.h        \_out1,  \_out1, 2
+    vsrar.h         \_out2,  \_out2, \_const3
+.endm
+
+.macro AVC_LPF_P0_OR_Q0 _in0, _in1, _in2, _out0, _tmp0
+    vslli.h         \_tmp0,  \_in2,  1
+    vadd.h          \_out0,  \_in0,  \_in1
+    vadd.h          \_out0,  \_out0, \_tmp0
+    vsrari.h        \_out0,  \_out0, 2
+.endm
+
+////LSX optimization is enough for this function.
+function ff_h264_h_lpf_luma_intra_8_lsx
+    slli.d          t0,     a1,    1   //img_width_2x
+    slli.d          t1,     a1,    2   //img_width_4x
+    addi.d          t4,     a0,    -4   //src
+    SAVE_REG
+    add.d           t2,     t0,    a1   //img_width_3x
+    add.d           t5,     t4,    t1
+    vld             vr0,    t4,    0    //row0
+    vldx            vr1,    t4,    a1   //row1
+    vldx            vr2,    t4,    t0   //row2
+    vldx            vr3,    t4,    t2   //row3
+    add.d           t6,     t5,    t1
+    vld             vr4,    t5,    0    //row4
+    vldx            vr5,    t5,    a1   //row5
+    vldx            vr6,    t5,    t0   //row6
+    vldx            vr7,    t5,    t2   //row7
+    add.d           t7,     t6,    t1
+    vld             vr8,    t6,    0    //row8
+    vldx            vr9,    t6,    a1   //row9
+    vldx            vr10,   t6,    t0   //row10
+    vldx            vr11,   t6,    t2   //row11
+    vld             vr12,   t7,    0    //row12
+    vldx            vr13,   t7,    a1   //row13
+    vldx            vr14,   t7,    t0   //row14
+    vldx            vr15,   t7,    t2   //row15
+    LSX_TRANSPOSE16X8_B vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7,       \
+                        vr8, vr9, vr10, vr11, vr12, vr13, vr14, vr15, \
+                        vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7,       \
+                        vr16, vr17, vr18, vr19, vr20, vr21, vr22, vr23
+    // vr0: p3_org, vr1: p2_org, vr2: p1_org, vr3: p0_org
+    // vr4: q0_org, vr5: q1_org, vr6: q2_org, vr7: q3_org
+
+    vreplgr2vr.b    vr16,   a2    //alpha_in
+    vreplgr2vr.b    vr17,   a3    //beta_in
+    vabsd.bu        vr10,   vr3,   vr4    //p0_asub_q0
+    vabsd.bu        vr11,   vr2,   vr3    //p1_asub_p0
+    vabsd.bu        vr12,   vr5,   vr4    //q1_asub_q0
+
+    vslt.bu         vr8,    vr10,  vr16  //is_less_than_alpha
+    vslt.bu         vr9,    vr11,  vr17  //is_less_than_beta
+    vand.v          vr18,   vr8,   vr9   //is_less_than
+    vslt.bu         vr9,    vr12,  vr17  //is_less_than_beta
+    vand.v          vr18,   vr18,  vr9   //is_less_than
+
+    vsetnez.v       $fcc0,  vr18
+    bceqz           $fcc0,  .END_H_INTRA_8
+    vsrli.b         vr16,   vr16,  2     //less_alpha_shift2_add2
+    vaddi.bu        vr16,   vr16,  2
+    vslt.bu         vr16,   vr10,  vr16
+    vsllwil.hu.bu   vr10,   vr2,   0   //p1_org_h.0
+    vexth.hu.bu     vr11,   vr2        //p1_org_h.1
+    vsllwil.hu.bu   vr12,   vr3,   0   //p0_org_h.0
+    vexth.hu.bu     vr13,   vr3        //p0_org_h.1
+
+    vsllwil.hu.bu   vr14,   vr4,   0   //q0_org_h.0
+    vexth.hu.bu     vr15,   vr4        //q0_org_h.1
+    vsllwil.hu.bu   vr19,   vr5,   0   //q1_org_h.0
+    vexth.hu.bu     vr20,   vr5        //q1_org_h.1
+
+    vabsd.bu        vr21,   vr1,   vr3  //p2_asub_p0
+    vslt.bu         vr9,    vr21,  vr17  //is_less_than_beta
+    vand.v          vr9,    vr9,   vr16
+    vxori.b         vr22,   vr9,   0xff  //negate_is_less_than_beta
+    vand.v          vr9,    vr9,   vr18
+    vand.v          vr22,   vr22,  vr18
+
+    vsetnez.v       $fcc0,  vr9
+    bceqz           $fcc0,  .END_H_INTRA_LESS_BETA
+    vsllwil.hu.bu   vr23,   vr1,   0   //p2_org_h.0
+    vexth.hu.bu     vr24,   vr1        //p2_org_h.1
+    vsllwil.hu.bu   vr25,   vr0,   0   //p3_org_h.0
+    vexth.hu.bu     vr26,   vr0        //p3_org_h.1
+    vldi            vr27,   0x403
+
+    AVC_LPF_P0P1P2_OR_Q0Q1Q2 vr25, vr12, vr14, vr10, vr23, vr19, vr28, vr29, vr30, vr31, vr27
+    AVC_LPF_P0P1P2_OR_Q0Q1Q2 vr26, vr13, vr15, vr11, vr24, vr20, vr23, vr25, vr21, vr31, vr27
+    vpickev.b       vr28,   vr23,  vr28  //p0_h
+    vpickev.b       vr29,   vr25,  vr29  //p1_h
+    vpickev.b       vr30,   vr21,  vr30  //p2_h
+    vbitsel.v       vr3,    vr3,   vr28,   vr9
+    vbitsel.v       vr2,    vr2,   vr29,   vr9
+    vbitsel.v       vr1,    vr1,   vr30,   vr9
+.END_H_INTRA_LESS_BETA:
+    AVC_LPF_P0_OR_Q0 vr12, vr19, vr10, vr23, vr25
+    AVC_LPF_P0_OR_Q0 vr13, vr20, vr11, vr24, vr25
+    //vr23: p0_h.0   vr24: p0_h.1
+    vpickev.b       vr23,   vr24,  vr23
+    vbitsel.v       vr3,    vr3,   vr23,   vr22
+
+    vabsd.bu        vr21,   vr6,   vr4   //q2_asub_q0
+    vslt.bu         vr9,    vr21,  vr17  //is_less_than_beta
+    vand.v          vr9,    vr9,   vr16
+    vxori.b         vr22,   vr9,   0xff   //negate_is_less_than_beta
+    vand.v          vr9,    vr9,   vr18
+    vand.v          vr22,   vr22,  vr18
+
+    vsetnez.v       $fcc0,  vr9
+    bceqz           $fcc0,  .END_H_INTRA_LESS_BETA_SEC
+    vsllwil.hu.bu   vr23,   vr6,   0   //q2_org_h.0
+    vexth.hu.bu     vr24,   vr6        //q2_org_h.1
+    vsllwil.hu.bu   vr25,   vr7,   0   //q3_org_h.0
+    vexth.hu.bu     vr26,   vr7        //q3_org_h.1
+    vldi            vr27,   0x403
+
+    AVC_LPF_P0P1P2_OR_Q0Q1Q2 vr25, vr14, vr12, vr19, vr23, vr10, vr28, vr29, vr30, vr31, vr27
+    AVC_LPF_P0P1P2_OR_Q0Q1Q2 vr26, vr15, vr13, vr20, vr24, vr11, vr23, vr25, vr21, vr31, vr27
+    vpickev.b       vr28,   vr23,  vr28  //q0_h
+    vpickev.b       vr29,   vr25,  vr29  //q1_h
+    vpickev.b       vr30,   vr21,  vr30  //q2_h
+    vbitsel.v       vr4,    vr4,   vr28,   vr9
+    vbitsel.v       vr5,    vr5,   vr29,   vr9
+    vbitsel.v       vr6,    vr6,   vr30,   vr9
+.END_H_INTRA_LESS_BETA_SEC:
+    AVC_LPF_P0_OR_Q0 vr14, vr10, vr19, vr23, vr25
+    AVC_LPF_P0_OR_Q0 vr15, vr11, vr20, vr24, vr25
+    vpickev.b       vr23,   vr24,  vr23
+    vbitsel.v       vr4,    vr4,   vr23,   vr22
+
+    vilvl.b         vr14,   vr2,   vr0   // row0.0
+    vilvl.b         vr15,   vr6,   vr4   // row0.1
+    vilvl.b         vr16,   vr3,   vr1   // row2.0
+    vilvl.b         vr17,   vr7,   vr5   // row2.1
+
+    vilvh.b         vr18,   vr2,   vr0   // row1.0
+    vilvh.b         vr19,   vr6,   vr4   // row1.1
+    vilvh.b         vr20,   vr3,   vr1   // row3.0
+    vilvh.b         vr21,   vr7,   vr5   // row3.1
+
+    vilvl.b         vr2,    vr16,   vr14    // row4.0
+    vilvl.b         vr3,    vr17,   vr15    // row4.1
+    vilvl.b         vr4,    vr20,   vr18    // row6.0
+    vilvl.b         vr5,    vr21,   vr19    // row6.1
+
+    vilvh.b         vr6,    vr16,   vr14    // row5.0
+    vilvh.b         vr7,    vr17,   vr15    // row5.1
+    vilvh.b         vr8,    vr20,   vr18    // row7.0
+    vilvh.b         vr9,    vr21,   vr19    // row7.1
+
+    vilvl.w         vr14,   vr3,    vr2   // row4: 0, 4, 1, 5
+    vilvh.w         vr15,   vr3,    vr2   // row4: 2, 6, 3, 7
+    vilvl.w         vr16,   vr7,    vr6   // row5: 0, 4, 1, 5
+    vilvh.w         vr17,   vr7,    vr6   // row5: 2, 6, 3, 7
+
+    vilvl.w         vr18,   vr5,    vr4   // row6: 0, 4, 1, 5
+    vilvh.w         vr19,   vr5,    vr4   // row6: 2, 6, 3, 7
+    vilvl.w         vr20,   vr9,    vr8   // row7: 0, 4, 1, 5
+    vilvh.w         vr21,   vr9,    vr8   // row7: 2, 6, 3, 7
+
+    vbsrl.v         vr0,    vr14,   8
+    vbsrl.v         vr1,    vr15,   8
+    vbsrl.v         vr2,    vr16,   8
+    vbsrl.v         vr3,    vr17,   8
+
+    vbsrl.v         vr4,    vr18,   8
+    vbsrl.v         vr5,    vr19,   8
+    vbsrl.v         vr6,    vr20,   8
+    vbsrl.v         vr7,    vr21,   8
+
+    store_double f14, f0, f15, f1, t4, a1, t0, t2
+    store_double f16, f2, f17, f3, t5, a1, t0, t2
+    store_double f18, f4, f19, f5, t6, a1, t0, t2
+    store_double f20, f6, f21, f7, t7, a1, t0, t2
+.END_H_INTRA_8:
+    RESTORE_REG
+endfunc
+
+//LSX optimization is enough for this function.
+function ff_h264_v_lpf_luma_intra_8_lsx
+    slli.d          t0,     a1,    1   //img_width_2x
+    add.d           t1,     t0,    a1  //img_width_3x
+    SAVE_REG
+    sub.d           t4,     a0,    t1  //src - img_width_3x
+
+    vld             vr0,    a0,    0   //q0_org
+    vldx            vr1,    a0,    a1  //q1_org
+    vldx            vr2,    t4,    a1  //p1_org
+    vldx            vr3,    t4,    t0  //p0_org
+
+    vreplgr2vr.b    vr4,    a2   //alpha
+    vreplgr2vr.b    vr5,    a3   //beta
+
+    vabsd.bu        vr6,    vr3,   vr0   //p0_asub_q0
+    vabsd.bu        vr7,    vr2,   vr3   //p1_asub_p0
+    vabsd.bu        vr8,    vr1,   vr0   //q1_asub_q0
+
+    vslt.bu         vr9,    vr6,   vr4  //is_less_than_alpha
+    vslt.bu         vr10,   vr7,   vr5  //is_less_than_beta
+    vand.v          vr11,   vr9,   vr10  //is_less_than
+    vslt.bu         vr10,   vr8,   vr5
+    vand.v          vr11,   vr10,  vr11
+
+    vsetnez.v       $fcc0,  vr11
+    bceqz           $fcc0,  .END_V_INTRA_8
+
+    vld             vr12,   t4,    0   //p2_org
+    vldx            vr13,   a0,    t0  //q2_org
+    vsrli.b         vr14,   vr4,   2   //is_alpha_shift2_add2
+    vsllwil.hu.bu   vr15,   vr2,   0  //p1_org_h.0
+    vexth.hu.bu     vr16,   vr2       //p1_org_h.1
+    vaddi.bu        vr14,   vr14,  2
+    vsllwil.hu.bu   vr17,   vr3,   0  //p0_org_h.0
+    vexth.hu.bu     vr18,   vr3       //p0_org_h.1
+    vslt.bu         vr14,   vr6,   vr14
+    vsllwil.hu.bu   vr19,   vr0,   0  //q0_org_h.0
+    vexth.hu.bu     vr20,   vr0       //q0_org_h.1
+    vsllwil.hu.bu   vr21,   vr1,   0  //q1_org_h.0
+    vexth.hu.bu     vr22,   vr1       //q1_org_h.1
+
+    vabsd.bu        vr23,   vr12,  vr3  //p2_asub_p0
+    vslt.bu         vr10,   vr23,  vr5  //is_less_than_beta
+    vand.v          vr10,   vr10,  vr14
+    vxori.b         vr23,   vr10,  0xff //negate_is_less_than_beta
+    vand.v          vr10,   vr10,  vr11
+    vand.v          vr23,   vr23,  vr11
+
+    vsetnez.v       $fcc0,  vr10
+    bceqz           $fcc0,  .END_V_INTRA_LESS_BETA
+    sub.d           t5,     t4,    a1
+    vld             vr24,   t5,    0  //p3_org
+    vsllwil.hu.bu   vr26,   vr12,  0  //p2_org_h.0
+    vexth.hu.bu     vr27,   vr12      //p2_org_h.1
+    vsllwil.hu.bu   vr28,   vr24,  0  //p3_org_h.0
+    vexth.hu.bu     vr29,   vr24      //p3_org_h.1
+    vldi            vr4,    0x403
+
+    AVC_LPF_P0P1P2_OR_Q0Q1Q2 vr28, vr17, vr19, vr15, vr26, vr21, vr25, vr30, vr31, vr24, vr4
+    AVC_LPF_P0P1P2_OR_Q0Q1Q2 vr29, vr18, vr20, vr16, vr27, vr22, vr6, vr7, vr8, vr24, vr4
+
+    vpickev.b       vr25,   vr6,   vr25  //p0_h
+    vpickev.b       vr30,   vr7,   vr30  //p1_h
+    vpickev.b       vr31,   vr8,   vr31  //p2_h
+
+    vbitsel.v       vr3,    vr3,   vr25,   vr10
+    vbitsel.v       vr2,    vr2,   vr30,   vr10
+    vbitsel.v       vr12,   vr12,  vr31,   vr10
+
+    vstx            vr2,    t4,    a1
+    vst             vr12,   t4,    0
+.END_V_INTRA_LESS_BETA:
+    AVC_LPF_P0_OR_Q0 vr17, vr21, vr15, vr24, vr30
+    AVC_LPF_P0_OR_Q0 vr18, vr22, vr16, vr25, vr30
+    vpickev.b       vr24,   vr25,  vr24
+    vbitsel.v       vr3,    vr3,   vr24,   vr23
+    vstx            vr3,    t4,    t0
+
+    vabsd.bu        vr23,   vr13,  vr0   //q2_asub_q0
+    vslt.bu         vr10,   vr23,  vr5   //is_less_than_beta
+    vand.v          vr10,   vr10,  vr14
+    vxori.b         vr23,   vr10,  0xff  //negate_is_less_than_beta
+    vand.v          vr10,   vr10,  vr11
+    vand.v          vr23,   vr23,  vr11
+
+    vsetnez.v       $fcc0,  vr10
+    bceqz           $fcc0,  .END_V_INTRA_LESS_BETA_SEC
+    vldx            vr24,   a0,    t1  //q3_org
+
+    vsllwil.hu.bu   vr26,   vr13,  0  //q2_org_h.0
+    vexth.hu.bu     vr27,   vr13      //q2_org_h.1
+    vsllwil.hu.bu   vr28,   vr24,  0  //q3_org_h.0
+    vexth.hu.bu     vr29,   vr24      //q3_org_h.1
+    vldi            vr4,    0x403
+
+    AVC_LPF_P0P1P2_OR_Q0Q1Q2 vr28, vr19, vr17, vr21, vr26, vr15, vr25, vr30, vr31, vr24, vr4
+    AVC_LPF_P0P1P2_OR_Q0Q1Q2 vr29, vr20, vr18, vr22, vr27, vr16, vr6, vr7, vr8, vr24, vr4
+
+    vpickev.b       vr25,   vr6,   vr25
+    vpickev.b       vr30,   vr7,   vr30
+    vpickev.b       vr31,   vr8,   vr31
+
+    vbitsel.v       vr0,    vr0,   vr25,   vr10
+    vbitsel.v       vr1,    vr1,   vr30,   vr10
+    vbitsel.v       vr13,   vr13,  vr31,   vr10
+    vstx            vr1,    a0,    a1
+    vstx            vr13,   a0,    t0
+.END_V_INTRA_LESS_BETA_SEC:
+    AVC_LPF_P0_OR_Q0 vr19, vr15, vr21, vr24, vr30
+    AVC_LPF_P0_OR_Q0 vr20, vr16, vr22, vr25, vr30
+    vpickev.b       vr24,   vr25,  vr24
+    vbitsel.v       vr0,    vr0,   vr24,   vr23
+    vst             vr0,    a0,    0
+.END_V_INTRA_8:
+    RESTORE_REG
+endfunc
+
+function ff_h264_h_lpf_chroma_intra_8_lsx
+    addi.d          t4,     a0,    -2
+    slli.d          t0,     a1,    1   //img_2x
+    slli.d          t2,     a1,    2   //img_4x
+    add.d           t1,     t0,    a1  //img_3x
+
+    add.d           t5,     t4,    t2
+    fld.s           f0,     t4,    0   //row0
+    fldx.s          f1,     t4,    a1  //row1
+    fldx.s          f2,     t4,    t0  //row2
+    fldx.s          f3,     t4,    t1  //row3
+    fld.s           f4,     t5,    0   //row4
+    fldx.s          f5,     t5,    a1  //row5
+    fldx.s          f6,     t5,    t0  //row6
+    fldx.s          f7,     t5,    t1  //row7
+
+    vilvl.b         vr8,    vr2,   vr0  //p1_org
+    vilvl.b         vr9,    vr3,   vr1  //p0_org
+    vilvl.b         vr10,   vr6,   vr4  //q0_org
+    vilvl.b         vr11,   vr7,   vr5  //q1_org
+
+    vilvl.b         vr0,    vr9,   vr8
+    vilvl.b         vr1,    vr11,  vr10
+    vilvl.w         vr2,    vr1,   vr0
+    vilvh.w         vr3,    vr1,   vr0
+
+    vilvl.d         vr8,    vr2,   vr2   //p1_org
+    vilvh.d         vr9,    vr2,   vr2   //p0_org
+    vilvl.d         vr10,   vr3,   vr3   //q0_org
+    vilvh.d         vr11,   vr3,   vr3   //q1_org
+
+    vreplgr2vr.b    vr0,    a2   //alpha
+    vreplgr2vr.b    vr1,    a3   //beta
+
+    vabsd.bu        vr2,    vr9,   vr10  //p0_asub_q0
+    vabsd.bu        vr3,    vr8,   vr9   //p1_asub_p0
+    vabsd.bu        vr4,    vr11,  vr10  //q1_asub_q0
+
+    vslt.bu         vr5,    vr2,   vr0  //is_less_than_alpha
+    vslt.bu         vr6,    vr3,   vr1  //is_less_than_beta
+    vand.v          vr7,    vr5,   vr6   //is_less_than
+    vslt.bu         vr6,    vr4,   vr1
+    vand.v          vr7,    vr7,   vr6
+
+    vsetnez.v       $fcc0,  vr7
+    bceqz           $fcc0,  .END_H_CHROMA_INTRA_8
+
+    vexth.hu.bu     vr12,   vr8   //p1_org_h
+    vexth.hu.bu     vr13,   vr9   //p0_org_h
+    vexth.hu.bu     vr14,   vr10  //q0_org_h
+    vexth.hu.bu     vr15,   vr11  //q1_org_h
+
+    AVC_LPF_P0_OR_Q0 vr13, vr15, vr12, vr16, vr18
+    AVC_LPF_P0_OR_Q0 vr14, vr12, vr15, vr17, vr18
+
+    vpickev.b       vr18,   vr16,   vr16
+    vpickev.b       vr19,   vr17,   vr17
+    vbitsel.v       vr9,    vr9,    vr18,   vr7
+    vbitsel.v       vr10,   vr10,   vr19,   vr7
+.END_H_CHROMA_INTRA_8:
+    vilvl.b         vr11,   vr10,   vr9
+    addi.d          t4,     t4,     1
+    vstelm.h        vr11,   t4,     0,      0
+    add.d           t4,     t4,     a1
+    vstelm.h        vr11,   t4,     0,      1
+    add.d           t4,     t4,     a1
+    vstelm.h        vr11,   t4,     0,      2
+    add.d           t4,     t4,     a1
+    vstelm.h        vr11,   t4,     0,      3
+    add.d           t4,     t4,     a1
+    vstelm.h        vr11,   t4,     0,      4
+    add.d           t4,     t4,     a1
+    vstelm.h        vr11,   t4,     0,      5
+    add.d           t4,     t4,     a1
+    vstelm.h        vr11,   t4,     0,      6
+    add.d           t4,     t4,     a1
+    vstelm.h        vr11,   t4,     0,      7
+endfunc
+
+function ff_h264_v_lpf_chroma_intra_8_lsx
+    slli.d          t0,     a1,     1    //img_width_2x
+    sub.d           t2,     a0,     a1
+    sub.d           t1,     a0,     t0   //data - img_width_2x
+
+    vreplgr2vr.b    vr0,    a2
+    vreplgr2vr.b    vr1,    a3
+
+    vld             vr2,    t1,     0   //p1_org
+    vldx            vr3,    t1,     a1  //p0_org
+    vld             vr4,    a0,     0   //q0_org
+    vldx            vr5,    a0,     a1  //q1_org
+
+    vabsd.bu        vr6,    vr3,    vr4  //p0_asub_q0
+    vabsd.bu        vr7,    vr2,    vr3  //p1_asub_p0
+    vabsd.bu        vr8,    vr5,    vr4  //q1_asub_q0
+
+    vslt.bu         vr9,    vr6,    vr0  //is_less_than_alpha
+    vslt.bu         vr10,   vr7,    vr1  //is_less_than_beta
+    vand.v          vr11,   vr9,    vr10  //is_less_than
+    vslt.bu         vr10,   vr8,    vr1
+    vand.v          vr11,   vr10,   vr11
+
+    vsetnez.v       $fcc0,  vr11
+    bceqz           $fcc0,  .END_V_CHROMA_INTRA_8
+
+    vsllwil.hu.bu   vr6,    vr2,    0  //p1_org_h.0
+    vsllwil.hu.bu   vr8,    vr3,    0  //p0_org_h.0
+    vsllwil.hu.bu   vr13,   vr4,    0  //q0_org_h.0
+    vsllwil.hu.bu   vr15,   vr5,    0  //q1_org_h.0
+
+    AVC_LPF_P0_OR_Q0 vr8, vr15, vr6, vr17, vr23
+    AVC_LPF_P0_OR_Q0 vr13, vr6, vr15, vr18, vr23
+
+    vpickev.b       vr19,   vr17,   vr17
+    vpickev.b       vr20,   vr18,   vr18
+    vbitsel.v       vr3,    vr3,    vr19,   vr11
+    vbitsel.v       vr4,    vr4,    vr20,   vr11
+
+    vstelm.d        vr3,    t2,     0,      0
+    vstelm.d        vr4,    a0,     0,      0
+.END_V_CHROMA_INTRA_8:
+endfunc
+
+.macro biweight_calc _in0, _in1, _in2, _in3, _reg0, _reg1, _reg2,\
+                     _out0, _out1, _out2, _out3
+    vmov             \_out0,   \_reg0
+    vmov             \_out1,   \_reg0
+    vmov             \_out2,   \_reg0
+    vmov             \_out3,   \_reg0
+    vmaddwev.h.bu.b  \_out0,   \_in0,     \_reg1
+    vmaddwev.h.bu.b  \_out1,   \_in1,     \_reg1
+    vmaddwev.h.bu.b  \_out2,   \_in2,     \_reg1
+    vmaddwev.h.bu.b  \_out3,   \_in3,     \_reg1
+    vmaddwod.h.bu.b  \_out0,   \_in0,     \_reg1
+    vmaddwod.h.bu.b  \_out1,   \_in1,     \_reg1
+    vmaddwod.h.bu.b  \_out2,   \_in2,     \_reg1
+    vmaddwod.h.bu.b  \_out3,   \_in3,     \_reg1
+
+    vssran.bu.h      \_out0,   \_out0,    \_reg2
+    vssran.bu.h      \_out1,   \_out1,    \_reg2
+    vssran.bu.h      \_out2,   \_out2,    \_reg2
+    vssran.bu.h      \_out3,   \_out3,    \_reg2
+.endm
+
+.macro biweight_load_8
+    load_double f0, f1, f2, f3, a1, a2, t0, t1
+    load_double f10, f11, f12, f13, a0, a2, t0, t1
+
+    vilvl.d          vr0,    vr1,    vr0  //src0
+    vilvl.d          vr2,    vr3,    vr2  //src2
+    vilvl.d          vr10,   vr11,   vr10 //dst0
+    vilvl.d          vr12,   vr13,   vr12 //dst2
+
+    vilvl.b          vr1,    vr10,   vr0  //vec0.0
+    vilvh.b          vr3,    vr10,   vr0  //vec0.1
+    vilvl.b          vr5,    vr12,   vr2  //vec1.0
+    vilvh.b          vr7,    vr12,   vr2  //vec1.1
+.endm
+
+.macro biweight_8
+    biweight_calc vr1, vr3, vr5, vr7, vr8, vr20, vr9, vr0, vr2, vr4, vr6
+    vilvl.d          vr0,    vr2,   vr0
+    vilvl.d          vr2,    vr6,   vr4
+
+    vbsrl.v          vr1,    vr0,   8
+    vbsrl.v          vr3,    vr2,   8
+
+    store_double f0, f1, f2, f3, a0, a2, t0, t1
+.endm
+
+.macro biweight_load2_8
+    biweight_load_8
+    load_double f0, f2, f4, f6, t4, a2, t0, t1
+    load_double f14, f15, f16, f17, t5, a2, t0, t1
+
+    vilvl.d          vr0,    vr2,    vr0  //src4
+    vilvl.d          vr4,    vr6,    vr4  //src6
+    vilvl.d          vr14,   vr15,   vr14 //dst4
+    vilvl.d          vr16,   vr17,   vr16 //dst6
+
+    vilvl.b          vr11,   vr14,   vr0  //vec4.0
+    vilvh.b          vr13,   vr14,   vr0  //vec4.1
+    vilvl.b          vr15,   vr16,   vr4  //vec6.0
+    vilvh.b          vr17,   vr16,   vr4  //vec6.1
+.endm
+
+.macro biweight2_8
+    biweight_8
+    biweight_calc vr11, vr13, vr15, vr17, vr8, vr20, vr9, \
+                  vr10, vr12, vr14, vr16
+    vilvl.d          vr10,   vr12,  vr10
+    vilvl.d          vr12,   vr16,  vr14
+
+    vbsrl.v          vr11,   vr10,  8
+    vbsrl.v          vr13,   vr12,  8
+
+    store_double f10, f11, f12, f13, t5, a2, t0, t1
+.endm
+
+.macro biweight_load_16
+    add.d           t4,     a1,     t2
+    vld             vr0,    a1,     0
+    vldx            vr1,    a1,     a2
+    vldx            vr2,    a1,     t0
+    vldx            vr3,    a1,     t1
+    vld             vr4,    t4,     0
+    vldx            vr5,    t4,     a2
+    vldx            vr6,    t4,     t0
+    vldx            vr7,    t4,     t1
+
+    add.d           t5,     a0,     t2
+    vld             vr10,   a0,     0
+    vldx            vr11,   a0,     a2
+    vldx            vr12,   a0,     t0
+    vldx            vr13,   a0,     t1
+    vld             vr14,   t5,     0
+    vldx            vr15,   t5,     a2
+    vldx            vr16,   t5,     t0
+    vldx            vr17,   t5,     t1
+
+    vilvl.b         vr18,   vr10,   vr0
+    vilvl.b         vr19,   vr11,   vr1
+    vilvl.b         vr21,   vr12,   vr2
+    vilvl.b         vr22,   vr13,   vr3
+    vilvh.b         vr0,    vr10,   vr0
+    vilvh.b         vr1,    vr11,   vr1
+    vilvh.b         vr2,    vr12,   vr2
+    vilvh.b         vr3,    vr13,   vr3
+
+    vilvl.b         vr10,   vr14,   vr4
+    vilvl.b         vr11,   vr15,   vr5
+    vilvl.b         vr12,   vr16,   vr6
+    vilvl.b         vr13,   vr17,   vr7
+    vilvh.b         vr14,   vr14,   vr4
+    vilvh.b         vr15,   vr15,   vr5
+    vilvh.b         vr16,   vr16,   vr6
+    vilvh.b         vr17,   vr17,   vr7
+.endm
+
+.macro biweight_16
+    biweight_calc vr18, vr19, vr21, vr22, vr8, vr20, vr9, vr4, vr5, vr6, vr7
+    biweight_calc vr0, vr1, vr2, vr3, vr8, vr20, vr9, vr18, vr19, vr21, vr22
+    biweight_calc vr10, vr11, vr12, vr13, vr8, vr20, vr9, vr0, vr1, vr2, vr3
+    biweight_calc vr14, vr15, vr16, vr17, vr8, vr20, vr9, vr10, vr11, vr12, vr13
+
+    vilvl.d         vr4,    vr18,   vr4
+    vilvl.d         vr5,    vr19,   vr5
+    vilvl.d         vr6,    vr21,   vr6
+    vilvl.d         vr7,    vr22,   vr7
+    vilvl.d         vr0,    vr10,   vr0
+    vilvl.d         vr1,    vr11,   vr1
+    vilvl.d         vr2,    vr12,   vr2
+    vilvl.d         vr3,    vr13,   vr3
+
+    vst             vr4,    a0,     0
+    vstx            vr5,    a0,     a2
+    vstx            vr6,    a0,     t0
+    vstx            vr7,    a0,     t1
+    vst             vr0,    t5,     0
+    vstx            vr1,    t5,     a2
+    vstx            vr2,    t5,     t0
+    vstx            vr3,    t5,     t1
+.endm
+
+.macro biweight_func   w
+function ff_biweight_h264_pixels\w\()_8_lsx
+    slli.d           t0,     a2,     1
+    slli.d           t2,     a2,     2
+    add.d            t1,     t0,     a2
+    addi.d           a7,     a7,     1
+    ori              a7,     a7,     1
+    sll.d            a7,     a7,     a4
+    addi.d           a4,     a4,     1
+
+    vreplgr2vr.b     vr0,    a6    //tmp0
+    vreplgr2vr.b     vr1,    a5    //tmp1
+    vreplgr2vr.h     vr8,    a7    //offset
+    vreplgr2vr.h     vr9,    a4    //denom
+    vilvh.b          vr20,   vr1,    vr0    //wgt
+.endm
+
+biweight_func  8
+    addi.d           t3,     zero,  8
+    biweight_load_8
+    biweight_8
+    blt              a3,     t3,    .END_BIWEIGHT_H264_PIXELS8
+    addi.d           t3,     zero,  16
+    add.d            a1,     a1,    t2
+    add.d            a0,     a0,    t2
+    biweight_load_8
+    biweight_8
+    blt              a3,     t3,    .END_BIWEIGHT_H264_PIXELS8
+    add.d            a1,     a1,    t2
+    add.d            a0,     a0,    t2
+    add.d            t4,     a1,    t2
+    add.d            t5,     a0,    t2
+    biweight_load2_8
+    biweight2_8
+.END_BIWEIGHT_H264_PIXELS8:
+endfunc
+
+biweight_func 16
+    addi.d           t6,     zero,  16
+    biweight_load_16
+    biweight_16
+
+    bne             a3,     t6,     .END_BIWEIGHT_PIXELS16
+    add.d           a1,     t4,     t2
+    add.d           a0,     t5,     t2
+    biweight_load_16
+    biweight_16
+.END_BIWEIGHT_PIXELS16:
+endfunc
+
+.macro biweight_calc_4 _in0, _out0
+    vmov             \_out0,  vr8
+    vmaddwev.h.bu.b  \_out0,  \_in0,  vr20
+    vmaddwod.h.bu.b  \_out0,  \_in0,  vr20
+    vssran.bu.h      \_out0,  \_out0, vr9
+.endm
+
+//LSX optimization is sufficient for this function.
+biweight_func 4
+    addi.d           t3,     zero,  4
+    fld.s            f0,     a1,     0
+    fldx.s           f1,     a1,     a2
+    fld.s            f10,    a0,     0
+    fldx.s           f11,    a0,     a2
+    vilvl.w          vr2,    vr1,    vr0
+    vilvl.w          vr12,   vr11,   vr10
+    vilvl.b          vr0,    vr12,   vr2
+
+    biweight_calc_4 vr0, vr1
+    vbsrl.v          vr2,    vr1,    4
+    fst.s            f1,     a0,     0
+    fstx.s           f2,     a0,     a2
+
+    blt              a3,     t3,     .END_BIWEIGHT_H264_PIXELS4
+    addi.d           t3,     zero,   8
+    fldx.s           f0,     a1,     t0
+    fldx.s           f1,     a1,     t1
+    fldx.s           f10,    a0,     t0
+    fldx.s           f11,    a0,     t1
+    vilvl.w          vr2,    vr1,    vr0
+    vilvl.w          vr12,   vr11,   vr10
+    vilvl.b          vr0,    vr12,   vr2
+
+    biweight_calc_4 vr0, vr1
+    vbsrl.v          vr2,    vr1,    4
+    fstx.s           f1,     a0,     t0
+    fstx.s           f2,     a0,     t1
+    blt              a3,     t3,     .END_BIWEIGHT_H264_PIXELS4
+    add.d            a1,     a1,     t2
+    add.d            a0,     a0,     t2
+    fld.s            f0,     a1,     0
+    fldx.s           f1,     a1,     a2
+    fldx.s           f2,     a1,     t0
+    fldx.s           f3,     a1,     t1
+    fld.s            f10,    a0,     0
+    fldx.s           f11,    a0,     a2
+    fldx.s           f12,    a0,     t0
+    fldx.s           f13,    a0,     t1
+    vilvl.w          vr4,    vr1,    vr0
+    vilvl.w          vr5,    vr3,    vr2
+    vilvl.w          vr14,   vr11,   vr10
+    vilvl.w          vr15,   vr13,   vr12
+
+    vilvl.b          vr0,    vr14,   vr4
+    vilvl.b          vr10,   vr15,   vr5
+
+    vmov             vr1,    vr8
+    vmov             vr11,   vr8
+    vmaddwev.h.bu.b  vr1,    vr0,    vr20
+    vmaddwev.h.bu.b  vr11,   vr10,   vr20
+    vmaddwod.h.bu.b  vr1,    vr0,    vr20
+    vmaddwod.h.bu.b  vr11,   vr10,   vr20
+
+    vssran.bu.h      vr0,    vr1,    vr9   //vec0
+    vssran.bu.h      vr10,   vr11,   vr9   //vec0
+    vbsrl.v          vr2,    vr0,    4
+    vbsrl.v          vr12,   vr10,   4
+
+    fst.s            f0,     a0,     0
+    fstx.s           f2,     a0,     a2
+    fstx.s           f10,    a0,     t0
+    fstx.s           f12,    a0,     t1
+.END_BIWEIGHT_H264_PIXELS4:
+endfunc
+
+.macro biweight_func_lasx   w
+function ff_biweight_h264_pixels\w\()_8_lasx
+    slli.d           t0,     a2,     1
+    slli.d           t2,     a2,     2
+    add.d            t1,     t0,     a2
+    addi.d           a7,     a7,     1
+    ori              a7,     a7,     1
+    sll.d            a7,     a7,     a4
+    addi.d           a4,     a4,     1
+
+    xvreplgr2vr.b    xr0,    a6    //tmp0
+    xvreplgr2vr.b    xr1,    a5    //tmp1
+    xvreplgr2vr.h    xr8,    a7    //offset
+    xvreplgr2vr.h    xr9,    a4    //denom
+    xvilvh.b         xr20,   xr1,    xr0    //wgt
+.endm
+
+.macro biweight_calc_lasx _in0, _in1, _reg0, _reg1, _reg2, _out0, _out1
+    xmov              \_out0,   \_reg0
+    xmov              \_out1,   \_reg0
+    xvmaddwev.h.bu.b  \_out0,   \_in0,     \_reg1
+    xvmaddwev.h.bu.b  \_out1,   \_in1,     \_reg1
+    xvmaddwod.h.bu.b  \_out0,   \_in0,     \_reg1
+    xvmaddwod.h.bu.b  \_out1,   \_in1,     \_reg1
+
+    xvssran.bu.h      \_out0,   \_out0,    \_reg2
+    xvssran.bu.h      \_out1,   \_out1,    \_reg2
+.endm
+
+.macro biweight_load_lasx_8
+    load_double f0, f1, f2, f3, a1, a2, t0, t1
+    load_double f10, f11, f12, f13, a0, a2, t0, t1
+
+    vilvl.d          vr0,    vr1,    vr0  //src0
+    vilvl.d          vr2,    vr3,    vr2  //src2
+    vilvl.d          vr10,   vr11,   vr10 //dst0
+    vilvl.d          vr12,   vr13,   vr12 //dst2
+
+    xvpermi.q        xr2,    xr0,    0x20
+    xvpermi.q        xr12,   xr10,   0x20
+
+    xvilvl.b         xr0,    xr12,   xr2
+    xvilvh.b         xr1,    xr12,   xr2
+.endm
+
+.macro biweight_lasx_8
+    biweight_calc_lasx xr0, xr1, xr8, xr20, xr9, xr2, xr3
+    xvilvl.d         xr0,    xr3,    xr2
+    xvpermi.d        xr2,    xr0,    0x4E
+    vbsrl.v          vr1,    vr0,    8
+    vbsrl.v          vr3,    vr2,    8
+
+    store_double f0, f1, f2, f3, a0, a2, t0, t1
+.endm
+
+biweight_func_lasx   8
+    addi.d           t3,     zero,   8
+    biweight_load_lasx_8
+    biweight_lasx_8
+    blt              a3,     t3,     .END_BIWEIGHT_H264_PIXELS8_LASX
+    addi.d           t3,     zero,   16
+    add.d            a1,     a1,     t2
+    add.d            a0,     a0,     t2
+    biweight_load_lasx_8
+    biweight_lasx_8
+    blt              a3,     t3,     .END_BIWEIGHT_H264_PIXELS8_LASX
+    add.d            a1,     a1,     t2
+    add.d            a0,     a0,     t2
+    add.d            t4,     a1,     t2
+    add.d            t5,     a0,     t2
+    biweight_load_lasx_8
+    load_double f4, f5, f6, f7, t4, a2, t0, t1
+    load_double f14, f15, f16, f17, t5, a2, t0, t1
+    vilvl.d          vr4,    vr5,    vr4  //src4
+    vilvl.d          vr6,    vr7,    vr6  //src6
+    vilvl.d          vr14,   vr15,   vr14 //dst4
+    vilvl.d          vr16,   vr17,   vr16 //dst6
+    xvpermi.q        xr6,    xr4,    0x20
+    xvpermi.q        xr16,   xr14,   0x20
+    xvilvl.b         xr10,   xr16,   xr6
+    xvilvh.b         xr11,   xr16,   xr6
+    biweight_lasx_8
+    biweight_calc_lasx xr10, xr11, xr8, xr20, xr9, xr12, xr13
+    xvilvl.d         xr10,   xr13,   xr12
+    xvpermi.d        xr12,   xr10,   0x4E
+    vbsrl.v          vr11,   vr10,   8
+    vbsrl.v          vr13,   vr12,   8
+    store_double f10, f11, f12, f13, t5, a2, t0, t1
+.END_BIWEIGHT_H264_PIXELS8_LASX:
+endfunc
+
+.macro biweight_load_lasx_16
+    add.d            t4,     a1,     t2
+    vld              vr0,    a1,     0
+    vldx             vr1,    a1,     a2
+    vldx             vr2,    a1,     t0
+    vldx             vr3,    a1,     t1
+    vld              vr4,    t4,     0
+    vldx             vr5,    t4,     a2
+    vldx             vr6,    t4,     t0
+    vldx             vr7,    t4,     t1
+
+    add.d            t5,     a0,     t2
+    vld              vr10,   a0,     0
+    vldx             vr11,   a0,     a2
+    vldx             vr12,   a0,     t0
+    vldx             vr13,   a0,     t1
+    vld              vr14,   t5,     0
+    vldx             vr15,   t5,     a2
+    vldx             vr16,   t5,     t0
+    vldx             vr17,   t5,     t1
+
+    xvpermi.q        xr1,    xr0,    0x20
+    xvpermi.q        xr3,    xr2,    0x20
+    xvpermi.q        xr5,    xr4,    0x20
+    xvpermi.q        xr7,    xr6,    0x20
+
+    xvpermi.q        xr11,   xr10,   0x20
+    xvpermi.q        xr13,   xr12,   0x20
+    xvpermi.q        xr15,   xr14,   0x20
+    xvpermi.q        xr17,   xr16,   0x20
+
+    xvilvl.b         xr0,    xr11,   xr1   //vec0
+    xvilvl.b         xr2,    xr13,   xr3   //vec2
+    xvilvl.b         xr4,    xr15,   xr5   //vec4
+    xvilvl.b         xr6,    xr17,   xr7   //vec6
+
+    xvilvh.b         xr10,   xr11,   xr1   //vec1
+    xvilvh.b         xr12,   xr13,   xr3   //vec2
+    xvilvh.b         xr14,   xr15,   xr5   //vec5
+    xvilvh.b         xr16,   xr17,   xr7   //vec7
+.endm
+
+.macro biweight_lasx_16
+    biweight_calc_lasx xr0, xr2, xr8, xr20, xr9, xr1, xr3
+    biweight_calc_lasx xr4, xr6, xr8, xr20, xr9, xr5, xr7
+    biweight_calc_lasx xr10, xr12, xr8, xr20, xr9, xr11, xr13
+    biweight_calc_lasx xr14, xr16, xr8, xr20, xr9, xr15, xr17
+    xvilvl.d         xr0,    xr11,   xr1
+    xvilvl.d         xr2,    xr13,   xr3
+    xvilvl.d         xr4,    xr15,   xr5
+    xvilvl.d         xr6,    xr17,   xr7
+
+    xvpermi.d        xr1,    xr0,    0x4E
+    xvpermi.d        xr3,    xr2,    0x4E
+    xvpermi.d        xr5,    xr4,    0x4E
+    xvpermi.d        xr7,    xr6,    0x4E
+    vst              vr0,    a0,     0
+    vstx             vr1,    a0,     a2
+    vstx             vr2,    a0,     t0
+    vstx             vr3,    a0,     t1
+    vst              vr4,    t5,     0
+    vstx             vr5,    t5,     a2
+    vstx             vr6,    t5,     t0
+    vstx             vr7,    t5,     t1
+.endm
+
+biweight_func_lasx   16
+    addi.d           t6,     zero,   16
+    biweight_load_lasx_16
+    biweight_lasx_16
+    bne              a3,     t6,     .END_BIWEIGHT_PIXELS16_LASX
+    add.d            a1,     t4,     t2
+    add.d            a0,     t5,     t2
+    biweight_load_lasx_16
+    biweight_lasx_16
+.END_BIWEIGHT_PIXELS16_LASX:
+endfunc
+
+.macro weight_func  w
+function ff_weight_h264_pixels\w\()_8_lsx
+    slli.d           t0,     a1,     1
+    slli.d           t2,     a1,     2
+    add.d            t1,     t0,     a1
+
+    sll.d            a5,     a5,     a3
+    vreplgr2vr.h     vr20,   a4      //weight
+    vreplgr2vr.h     vr8,    a5      //offset
+    vreplgr2vr.h     vr9,    a3      //log2_denom
+.endm
+
+.macro weight_load_16
+    add.d            t4,     a0,     t2
+    vld              vr0,    a0,     0
+    vldx             vr1,    a0,     a1
+    vldx             vr2,    a0,     t0
+    vldx             vr3,    a0,     t1
+    vld              vr4,    t4,     0
+    vldx             vr5,    t4,     a1
+    vldx             vr6,    t4,     t0
+    vldx             vr7,    t4,     t1
+
+    vilvl.b          vr10,   vr23,   vr0
+    vilvl.b          vr11,   vr23,   vr1
+    vilvl.b          vr12,   vr23,   vr2
+    vilvl.b          vr13,   vr23,   vr3
+    vilvl.b          vr14,   vr23,   vr4
+    vilvl.b          vr15,   vr23,   vr5
+    vilvl.b          vr16,   vr23,   vr6
+    vilvl.b          vr17,   vr23,   vr7
+.endm
+
+.macro weight_extend_16
+    vilvl.b          vr10,   vr23,   vr0
+    vilvl.b          vr11,   vr23,   vr1
+    vilvl.b          vr12,   vr23,   vr2
+    vilvl.b          vr13,   vr23,   vr3
+    vilvl.b          vr14,   vr23,   vr4
+    vilvl.b          vr15,   vr23,   vr5
+    vilvl.b          vr16,   vr23,   vr6
+    vilvl.b          vr17,   vr23,   vr7
+
+    vilvh.b          vr18,   vr23,   vr0
+    vilvh.b          vr19,   vr23,   vr1
+    vilvh.b          vr21,   vr23,   vr2
+    vilvh.b          vr22,   vr23,   vr3
+    vilvh.b          vr0,    vr23,   vr4
+    vilvh.b          vr1,    vr23,   vr5
+    vilvh.b          vr2,    vr23,   vr6
+    vilvh.b          vr3,    vr23,   vr7
+.endm
+
+.macro weight_calc _in0, _in1, _in2, _in3, _reg0, _reg1, _reg2, \
+                   _out0, _out1, _out2, _out3
+    vmul.h          \_in0,   \_in0,  \_reg1
+    vmul.h          \_in1,   \_in1,  \_reg1
+    vmul.h          \_in2,   \_in2,  \_reg1
+    vmul.h          \_in3,   \_in3,  \_reg1
+    vsadd.h         \_out0,  \_reg0, \_in0
+    vsadd.h         \_out1,  \_reg0, \_in1
+    vsadd.h         \_out2,  \_reg0, \_in2
+    vsadd.h         \_out3,  \_reg0, \_in3
+    vssrarn.bu.h    \_out0,  \_out0, \_reg2
+    vssrarn.bu.h    \_out1,  \_out1, \_reg2
+    vssrarn.bu.h    \_out2,  \_out2, \_reg2
+    vssrarn.bu.h    \_out3,  \_out3, \_reg2
+.endm
+
+.macro weight_16
+    weight_calc vr10, vr11, vr12, vr13, vr8, vr20, vr9, vr10, vr11, vr12, vr13
+    weight_calc vr14, vr15, vr16, vr17, vr8, vr20, vr9, vr14, vr15, vr16, vr17
+    weight_calc vr18, vr19, vr21, vr22, vr8, vr20, vr9, vr4, vr5, vr6, vr7
+    weight_calc vr0, vr1, vr2, vr3, vr8, vr20, vr9, vr0, vr1, vr2, vr3
+
+    vilvl.d          vr10,   vr4,    vr10
+    vilvl.d          vr11,   vr5,    vr11
+    vilvl.d          vr12,   vr6,    vr12
+    vilvl.d          vr13,   vr7,    vr13
+    vilvl.d          vr14,   vr0,    vr14
+    vilvl.d          vr15,   vr1,    vr15
+    vilvl.d          vr16,   vr2,    vr16
+    vilvl.d          vr17,   vr3,    vr17
+
+    vst              vr10,   a0,     0
+    vstx             vr11,   a0,     a1
+    vstx             vr12,   a0,     t0
+    vstx             vr13,   a0,     t1
+    vst              vr14,   t4,     0
+    vstx             vr15,   t4,     a1
+    vstx             vr16,   t4,     t0
+    vstx             vr17,   t4,     t1
+.endm
+
+weight_func 16
+    vldi            vr23,    0
+    addi.d          t3,      zero,   16
+    weight_load_16
+    weight_extend_16
+    weight_16
+    bne             a2,      t3,     .END_WEIGHT_H264_PIXELS16_8
+    add.d           a0,      t4,     t2
+    weight_load_16
+    weight_extend_16
+    weight_16
+.END_WEIGHT_H264_PIXELS16_8:
+endfunc
+
+.macro weight_load_8
+    load_double f0, f1, f2, f3, a0, a1, t0, t1
+.endm
+
+.macro weight_extend_8
+    vilvl.b          vr10,   vr21,   vr0
+    vilvl.b          vr11,   vr21,   vr1
+    vilvl.b          vr12,   vr21,   vr2
+    vilvl.b          vr13,   vr21,   vr3
+.endm
+
+.macro weight_8
+    weight_calc vr10, vr11, vr12, vr13, vr8, vr20, vr9, vr0, vr1, vr2, vr3
+    store_double f0, f1, f2, f3, a0, a1, t0, t1
+.endm
+
+weight_func 8
+    vldi             vr21,   0
+    addi.d           t3,     zero,   8
+    weight_load_8
+    weight_extend_8
+    weight_8
+    blt              a2,     t3,     .END_WEIGHT_H264_PIXELS8
+    add.d            a0,     a0,     t2
+    addi.d           t3,     zero,   16
+    weight_load_8
+    weight_extend_8
+    weight_8
+    blt              a2,     t3,     .END_WEIGHT_H264_PIXELS8
+    add.d            a0,     a0,     t2
+    add.d            t4,     a0,     t2
+    weight_load_8
+    load_double f4, f5, f6, f7, t4, a1, t0, t1
+    weight_extend_8
+    vilvl.b          vr14,   vr21,   vr4
+    vilvl.b          vr15,   vr21,   vr5
+    vilvl.b          vr16,   vr21,   vr6
+    vilvl.b          vr17,   vr21,   vr7
+    weight_8
+    weight_calc vr14, vr15, vr16, vr17, vr8, vr20, vr9, vr4, vr5, vr6, vr7
+    store_double f4, f5, f6, f7, t4, a1, t0, t1
+.END_WEIGHT_H264_PIXELS8:
+endfunc
+
+.macro weight_func_lasx  w
+function ff_weight_h264_pixels\w\()_8_lasx
+    slli.d           t0,     a1,     1
+    slli.d           t2,     a1,     2
+    add.d            t1,     t0,     a1
+
+    sll.d            a5,     a5,     a3
+    xvreplgr2vr.h    xr20,   a4      //weight
+    xvreplgr2vr.h    xr8,    a5      //offset
+    xvreplgr2vr.h    xr9,    a3      //log2_denom
+.endm
+
+.macro weight_calc_lasx _in0, _in1, _reg0, _reg1, _reg2, _out0, _out1
+    xvmul.h          \_out0, \_in0,  \_reg1
+    xvmul.h          \_out1, \_in1,  \_reg1
+    xvsadd.h         \_out0, \_reg0, \_out0
+    xvsadd.h         \_out1, \_reg0, \_out1
+    xvssrarn.bu.h    \_out0, \_out0, \_reg2
+    xvssrarn.bu.h    \_out1, \_out1, \_reg2
+.endm
+
+.macro weight_load_lasx_8
+    load_double f0, f1, f2, f3, a0, a1, t0, t1
+    vilvl.d          vr4,    vr1,    vr0
+    vilvl.d          vr5,    vr3,    vr2
+    vext2xv.hu.bu    xr6,    xr4
+    vext2xv.hu.bu    xr7,    xr5
+.endm
+
+.macro weight_lasx_8
+    weight_calc_lasx xr6, xr7, xr8, xr20, xr9, xr1, xr3
+    xvpermi.d        xr2,    xr1,    0x2
+    xvpermi.d        xr4,    xr3,    0x2
+    store_double f1, f2, f3, f4, a0, a1, t0, t1
+.endm
+
+weight_func_lasx 8
+    addi.d           t3,     zero,   8
+    weight_load_lasx_8
+    weight_lasx_8
+    blt              a2,     t3,     .END_WEIGHT_H264_PIXELS8_LASX
+    add.d            a0,     a0,     t2
+    addi.d           t3,     zero,   16
+    weight_load_lasx_8
+    weight_lasx_8
+    blt              a2,     t3,     .END_WEIGHT_H264_PIXELS8_LASX
+    add.d            a0,     a0,     t2
+    add.d            t4,     a0,     t2
+    weight_load_lasx_8
+    load_double f14, f15, f16, f17, t4, a1, t0, t1
+    vilvl.d          vr4,    vr15,   vr14
+    vilvl.d          vr5,    vr17,   vr16
+    vext2xv.hu.bu    xr10,   xr4
+    vext2xv.hu.bu    xr11,   xr5
+    weight_lasx_8
+    weight_calc_lasx xr10, xr11, xr8, xr20, xr9, xr4, xr6
+    xvpermi.d        xr5,   xr4,     0x2
+    xvpermi.d        xr7,   xr6,     0x2
+    store_double f4, f5, f6, f7, t4, a1, t0, t1
+.END_WEIGHT_H264_PIXELS8_LASX:
+endfunc
+
+.macro weight_load_lasx_16
+    add.d            t4,     a0,     t2
+    vld              vr0,    a0,     0
+    vldx             vr1,    a0,     a1
+    vldx             vr2,    a0,     t0
+    vldx             vr3,    a0,     t1
+    vld              vr4,    t4,     0
+    vldx             vr5,    t4,     a1
+    vldx             vr6,    t4,     t0
+    vldx             vr7,    t4,     t1
+
+    vext2xv.hu.bu    xr0,    xr0
+    vext2xv.hu.bu    xr1,    xr1
+    vext2xv.hu.bu    xr2,    xr2
+    vext2xv.hu.bu    xr3,    xr3
+    vext2xv.hu.bu    xr4,    xr4
+    vext2xv.hu.bu    xr5,    xr5
+    vext2xv.hu.bu    xr6,    xr6
+    vext2xv.hu.bu    xr7,    xr7
+.endm
+
+.macro weight_lasx_16
+    weight_calc_lasx xr0, xr1, xr8, xr20, xr9, xr10, xr11
+    weight_calc_lasx xr2, xr3, xr8, xr20, xr9, xr12, xr13
+    weight_calc_lasx xr4, xr5, xr8, xr20, xr9, xr14, xr15
+    weight_calc_lasx xr6, xr7, xr8, xr20, xr9, xr16, xr17
+    xvpermi.d        xr10,   xr10,   0xD8
+    xvpermi.d        xr11,   xr11,   0xD8
+    xvpermi.d        xr12,   xr12,   0xD8
+    xvpermi.d        xr13,   xr13,   0xD8
+    xvpermi.d        xr14,   xr14,   0xD8
+    xvpermi.d        xr15,   xr15,   0xD8
+    xvpermi.d        xr16,   xr16,   0xD8
+    xvpermi.d        xr17,   xr17,   0xD8
+
+    vst              vr10,   a0,     0
+    vstx             vr11,   a0,     a1
+    vstx             vr12,   a0,     t0
+    vstx             vr13,   a0,     t1
+    vst              vr14,   t4,     0
+    vstx             vr15,   t4,     a1
+    vstx             vr16,   t4,     t0
+    vstx             vr17,   t4,     t1
+.endm
+
+weight_func_lasx 16
+    addi.d           t3,     zero,   16
+    weight_load_lasx_16
+    weight_lasx_16
+    bne              a2,     t3,     .END_WEIGHT_H264_PIXELS16_8_LASX
+    add.d            a0,     t4,     t2
+    weight_load_lasx_16
+    weight_lasx_16
+.END_WEIGHT_H264_PIXELS16_8_LASX:
+endfunc
+
+//LSX optimization is enough for this function.
+function ff_weight_h264_pixels4_8_lsx
+    add.d            t0,     a0,     a1
+    addi.d           t3,     zero,   4
+
+    sll.d            a5,     a5,     a3
+    vreplgr2vr.h     vr20,   a4      //weight
+    vreplgr2vr.h     vr8,    a5      //offset
+    vreplgr2vr.h     vr9,    a3      //log2_denom
+    vldi             vr21,   0
+
+    fld.s            f0,     a0,     0
+    fldx.s           f1,     a0,     a1
+    vilvl.w          vr4,    vr1,    vr0
+    vilvl.b          vr5,    vr21,   vr4
+    vmul.h           vr10,   vr5,    vr20
+    vsadd.h          vr0,    vr8,    vr10
+    vssrarn.bu.h     vr0,    vr0,    vr9
+
+    fst.s            f0,     a0,     0
+    vstelm.w         vr0,    t0,     0,    1
+    blt              a2,     t3,    .END_WEIGHT_H264_PIXELS4
+    add.d            a0,     t0,     a1
+    addi.d           t3,     zero,   8
+    fld.s            f0,     a0,     0
+    fldx.s           f1,     a0,     a1
+    add.d            t0,     a0,     a1
+    vilvl.w          vr4,    vr1,    vr0
+    vilvl.b          vr5,    vr21,   vr4
+
+    vmul.h           vr10,   vr5,    vr20
+    vsadd.h          vr0,    vr8,    vr10
+    vssrarn.bu.h     vr0,    vr0,    vr9
+
+    fst.s            f0,     a0,     0
+    vstelm.w         vr0,    t0,     0,    1
+    blt              a2,     t3,    .END_WEIGHT_H264_PIXELS4
+    add.d            a0,     t0,     a1
+    add.d            t0,     a0,     a1
+    add.d            t1,     t0,     a1
+    add.d            t2,     t1,     a1
+
+    fld.s            f0,     a0,     0
+    fld.s            f1,     t0,     0
+    fld.s            f2,     t1,     0
+    fld.s            f3,     t2,     0
+
+    vilvl.w          vr4,    vr1,    vr0
+    vilvl.w          vr5,    vr3,    vr2
+    vilvl.b          vr6,    vr21,   vr4
+    vilvl.b          vr7,    vr21,   vr5
+
+    vmul.h           vr10,   vr6,    vr20
+    vmul.h           vr11,   vr7,    vr20
+    vsadd.h          vr0,    vr8,    vr10
+    vsadd.h          vr1,    vr8,    vr11
+    vssrarn.bu.h     vr10,   vr0,    vr9
+    vssrarn.bu.h     vr11,   vr1,    vr9
+
+    fst.s            f10,    a0,     0
+    vstelm.w         vr10,   t0,     0,    1
+    fst.s            f11,    t1,     0
+    vstelm.w         vr11,   t2,     0,    1
+.END_WEIGHT_H264_PIXELS4:
+endfunc
+
+function ff_h264_add_pixels4_8_lsx
+    slli.d           t0,     a2,     1
+    add.d            t1,     t0,     a2
+    vld              vr0,    a1,     0
+    vld              vr1,    a1,     16
+    vldi             vr2,    0
+    fld.s            f3,     a0,     0
+    fldx.s           f4,     a0,     a2
+    fldx.s           f5,     a0,     t0
+    fldx.s           f6,     a0,     t1
+    vilvl.w          vr7,    vr4,    vr3
+    vilvl.w          vr8,    vr6,    vr5
+    vilvl.b          vr9,    vr2,    vr7
+    vilvl.b          vr10,   vr2,    vr8
+    vadd.h           vr11,   vr0,    vr9
+    vadd.h           vr12,   vr1,    vr10
+    vpickev.b        vr0,    vr12,   vr11
+    vbsrl.v          vr3,    vr0,    4
+    vbsrl.v          vr4,    vr0,    8
+    vbsrl.v          vr5,    vr0,    12
+    fst.s            f0,     a0,     0
+    fstx.s           f3,     a0,     a2
+    fstx.s           f4,     a0,     t0
+    fstx.s           f5,     a0,     t1
+    vst              vr2,    a1,     0
+    vst              vr2,    a1,     16
+endfunc
+
+function ff_h264_add_pixels8_8_lsx
+    slli.d           t0,     a2,     1
+    slli.d           t2,     a2,     2
+    add.d            t1,     t0,     a2
+    add.d            t3,     a0,     t2
+    vldi             vr0,    0
+    vld              vr1,    a1,     0
+    vld              vr2,    a1,     16
+    vld              vr3,    a1,     32
+    vld              vr4,    a1,     48
+    vld              vr5,    a1,     64
+    vld              vr6,    a1,     80
+    vld              vr7,    a1,     96
+    vld              vr8,    a1,     112
+    load_double f10, f11, f12, f13, a0, a2, t0, t1
+    load_double f14, f15, f16, f17, t3, a2, t0, t1
+    vilvl.b          vr10,   vr0,    vr10
+    vilvl.b          vr11,   vr0,    vr11
+    vilvl.b          vr12,   vr0,    vr12
+    vilvl.b          vr13,   vr0,    vr13
+    vilvl.b          vr14,   vr0,    vr14
+    vilvl.b          vr15,   vr0,    vr15
+    vilvl.b          vr16,   vr0,    vr16
+    vilvl.b          vr17,   vr0,    vr17
+    vadd.h           vr1,    vr1,    vr10
+    vadd.h           vr2,    vr2,    vr11
+    vadd.h           vr3,    vr3,    vr12
+    vadd.h           vr4,    vr4,    vr13
+    vadd.h           vr5,    vr5,    vr14
+    vadd.h           vr6,    vr6,    vr15
+    vadd.h           vr7,    vr7,    vr16
+    vadd.h           vr8,    vr8,    vr17
+    vpickev.b        vr10,   vr2,    vr1
+    vpickev.b        vr12,   vr4,    vr3
+    vpickev.b        vr14,   vr6,    vr5
+    vpickev.b        vr16,   vr8,    vr7
+    vbsrl.v          vr11,   vr10,   8
+    vbsrl.v          vr13,   vr12,   8
+    vbsrl.v          vr15,   vr14,   8
+    vbsrl.v          vr17,   vr16,   8
+    vst              vr0,    a1,     0
+    vst              vr0,    a1,     16
+    vst              vr0,    a1,     32
+    vst              vr0,    a1,     48
+    vst              vr0,    a1,     64
+    vst              vr0,    a1,     80
+    vst              vr0,    a1,     96
+    vst              vr0,    a1,     112
+    store_double f10, f11, f12, f13, a0, a2, t0, t1
+    store_double f14, f15, f16, f17, t3, a2, t0, t1
+endfunc
+
+const cnst_value
+.byte 6, 2, 6, 2, 6, 2, 6, 2, 6, 2, 6, 2, 6, 2, 6, 2
+.byte 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1
+endconst
+
+function ff_h264_loop_filter_strength_lsx
+    vldi             vr0,    0
+    ldptr.w          t0,     sp,     0   //mask_mv1
+    ldptr.w          t1,     sp,     8   //field
+    beqz             t1,     .FIELD
+    la.local         t2,     cnst_value
+    vld              vr1,    t2,     0
+    vld              vr2,    t2,     16
+    b                .END_FIELD
+.FIELD:
+    vldi             vr1,    0x06
+    vldi             vr2,    0x03
+.END_FIELD:
+    vldi             vr3,    0x01
+    slli.d           a6,     a6,     3  //step <<= 3
+    slli.d           a5,     a5,     3  //edges <<= 3
+    move             t3,     zero
+    slli.d           t4,     a6,     2
+    move             t5,     a2
+    move             t6,     a3
+    move             t7,     a1
+    move             t8,     a0
+    slli.d           t0,     t0,     3
+.ITERATION_FIR:
+    bge              t3,     a5,     .END_ITERATION_FIR
+    vand.v           vr20,   vr20,   vr0
+    and              t2,     t0,     t3
+    bnez             t2,     .MASK_MV_FIR
+    beqz             a4,     .BIDIR_FIR
+    vld              vr4,    t5,     4
+    vld              vr5,    t5,     44
+    vld              vr6,    t5,     12
+    vld              vr7,    t5,     52
+    vilvl.w          vr4,    vr5,    vr4
+    vilvl.w          vr6,    vr6,    vr6
+    vilvl.w          vr7,    vr7,    vr7
+    vshuf4i.h        vr5,    vr4,    0x4e
+    vsub.b           vr6,    vr6,    vr4
+    vsub.b           vr7,    vr7,    vr5
+    vor.v            vr6,    vr6,    vr7
+    vld              vr10,   t6,     16
+    vld              vr11,   t6,     48
+    vld              vr12,   t6,     208
+    vld              vr8,    t6,     176
+    vsub.h           vr13,   vr10,   vr11
+    vsub.h           vr14,   vr10,   vr12
+    vsub.h           vr15,   vr8,    vr11
+    vsub.h           vr16,   vr8,    vr12
+    vssrarni.b.h     vr14,   vr13,   0
+    vssrarni.b.h     vr16,   vr15,   0
+    vadd.b           vr14,   vr2,    vr14
+    vadd.b           vr16,   vr2,    vr16
+    vssub.bu         vr14,   vr14,   vr1
+    vssub.bu         vr16,   vr16,   vr1
+    vssrarni.b.h     vr14,   vr14,   0
+    vssrarni.b.h     vr16,   vr16,   0
+    vor.v            vr20,   vr6,    vr14
+    vshuf4i.h        vr16,   vr16,   0x4e
+    vor.v            vr20,   vr20,   vr16
+    vshuf4i.h        vr21,   vr20,   0x4e
+    vmin.bu          vr20,   vr20,   vr21
+    b                .MASK_MV_FIR
+.BIDIR_FIR:
+    vld              vr4,    t5,     4
+    vld              vr5,    t5,     12
+    vld              vr10,   t6,     16
+    vld              vr11,   t6,     48
+    vsub.h           vr12,   vr11,   vr10
+    vssrarni.b.h     vr12,   vr12,   0
+    vadd.b           vr13,   vr12,   vr2
+    vssub.bu         vr14,   vr13,   vr1
+    vsat.h           vr15,   vr14,   7
+    vpickev.b        vr20,   vr15,   vr15
+    vsub.b           vr6,    vr5,    vr4
+    vor.v            vr20,   vr20,   vr6
+.MASK_MV_FIR:
+    vld              vr4,    t7,     12
+    vld              vr5,    t7,     4
+    vor.v            vr6,    vr4,    vr5
+    vmin.bu          vr6,    vr6,    vr3
+    vmin.bu          vr20,   vr20,   vr3
+    vslli.h          vr6,    vr6,    1
+    vmax.bu          vr6,    vr20,   vr6
+    vilvl.b          vr7,    vr0,    vr6
+    add.d            t3,     t3,     a6
+    fst.d            f7,     t8,     32
+    add.d            t5,     t5,     a6
+    add.d            t6,     t6,     t4
+    add.d            t7,     t7,     a6
+    add.d            t8,     t8,     a6
+    b                .ITERATION_FIR
+.END_ITERATION_FIR:
+    move             t3,     zero
+    addi.d           a5,     zero,   32
+    vldi             vr21,   0xff
+    move             t5,     a2
+    move             t6,     a3
+    move             t7,     a1
+    move             t8,     a0
+    slli.d           a7,     a7,     3
+.ITERATION_SEC:
+    bge              t3,     a5,     .END_ITERATION_SEC
+    vand.v           vr20,   vr20,   vr21
+    and              t2,     a7,     t3
+    bnez             t2,     .MASK_MV_SEC
+    beqz             a4,     .BIDIR_SEC
+    vld              vr4,    t5,     11
+    vld              vr5,    t5,     51
+    vld              vr6,    t5,     12
+    vld              vr7,    t5,     52
+    vilvl.w          vr4,    vr5,    vr4
+    vilvl.w          vr6,    vr6,    vr6
+    vilvl.w          vr7,    vr7,    vr7
+    vshuf4i.h        vr5,    vr4,    0x4e
+    vsub.b           vr6,    vr6,    vr4
+    vsub.b           vr7,    vr7,    vr5
+    vor.v            vr6,    vr6,    vr7
+    vld              vr10,   t6,     44
+    vld              vr11,   t6,     48
+    vld              vr12,   t6,     208
+    vld              vr8,    t6,     204
+    vsub.h           vr13,   vr10,   vr11
+    vsub.h           vr14,   vr10,   vr12
+    vsub.h           vr15,   vr8,    vr11
+    vsub.h           vr16,   vr8,    vr12
+    vssrarni.b.h     vr14,   vr13,   0
+    vssrarni.b.h     vr16,   vr15,   0
+    vadd.b           vr14,   vr2,    vr14
+    vadd.b           vr16,   vr2,    vr16
+    vssub.bu         vr14,   vr14,   vr1
+    vssub.bu         vr16,   vr16,   vr1
+    vssrarni.b.h     vr14,   vr14,   0
+    vssrarni.b.h     vr16,   vr16,   0
+    vor.v            vr20,   vr6,    vr14
+    vshuf4i.h        vr16,   vr16,   0x4e
+    vor.v            vr20,   vr20,   vr16
+    vshuf4i.h        vr22,   vr20,   0x4e
+    vmin.bu          vr20,   vr20,   vr22
+    b                .MASK_MV_SEC
+.BIDIR_SEC:
+    vld              vr4,    t5,     11
+    vld              vr5,    t5,     12
+    vld              vr10,   t6,     44
+    vld              vr11,   t6,     48
+    vsub.h           vr12,   vr11,   vr10
+    vssrarni.b.h     vr12,   vr12,   0
+    vadd.b           vr13,   vr12,   vr2
+    vssub.bu         vr14,   vr13,   vr1
+    vssrarni.b.h     vr14,   vr14,   0
+    vsub.b           vr6,    vr5,    vr4
+    vor.v            vr20,   vr14,   vr6
+.MASK_MV_SEC:
+    vld              vr4,    t7,     12
+    vld              vr5,    t7,     11
+    vor.v            vr6,    vr4,    vr5
+    vmin.bu          vr6,    vr6,    vr3
+    vmin.bu          vr20,   vr20,   vr3
+    vslli.h          vr6,    vr6,    1
+    vmax.bu          vr6,    vr20,   vr6
+    vilvl.b          vr7,    vr0,    vr6
+    addi.d           t3,     t3,     8
+    fst.d            f7,     t8,     0
+    addi.d           t5,     t5,     8
+    addi.d           t6,     t6,     32
+    addi.d           t7,     t7,     8
+    addi.d           t8,     t8,     8
+    b                .ITERATION_SEC
+.END_ITERATION_SEC:
+    vld              vr4,    a0,     0
+    vld              vr5,    a0,     16
+    vilvh.d          vr6,    vr4,    vr4
+    vilvh.d          vr7,    vr5,    vr5
+    LSX_TRANSPOSE4x4_H vr4, vr6, vr5, vr7, vr6, vr7, vr8, vr9, vr10, vr11
+    vilvl.d          vr4,    vr7,    vr6
+    vilvl.d          vr5,    vr9,    vr8
+    vst              vr4,    a0,     0
+    vst              vr5,    a0,     16
+endfunc
diff --git a/libavcodec/loongarch/h264dsp_init_loongarch.c b/libavcodec/loongarch/h264dsp_init_loongarch.c
new file mode 100644
index 0000000000..c5de9b79cb
--- /dev/null
+++ b/libavcodec/loongarch/h264dsp_init_loongarch.c
@@ -0,0 +1,95 @@
+/*
+ * Copyright (c) 2021 Loongson Technology Corporation Limited
+ * Contributed by Shiyou Yin <yinshiyou-hf@loongson.cn>
+ *                Xiwei  Gu  <guxiwei-hf@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "libavutil/loongarch/cpu.h"
+#include "h264dsp_lasx.h"
+
+av_cold void ff_h264dsp_init_loongarch(H264DSPContext *c, const int bit_depth,
+                                       const int chroma_format_idc)
+{
+    int cpu_flags = av_get_cpu_flags();
+
+    if (have_lsx(cpu_flags)) {
+        if (chroma_format_idc <= 1)
+            c->h264_loop_filter_strength = ff_h264_loop_filter_strength_lsx;
+        if (bit_depth == 8) {
+            c->h264_idct_add     = ff_h264_idct_add_8_lsx;
+            c->h264_idct8_add    = ff_h264_idct8_add_8_lsx;
+            c->h264_idct_dc_add  = ff_h264_idct_dc_add_8_lsx;
+            c->h264_idct8_dc_add = ff_h264_idct8_dc_add_8_lsx;
+
+            if (chroma_format_idc <= 1) {
+                c->h264_idct_add8 = ff_h264_idct_add8_8_lsx;
+                c->h264_h_loop_filter_chroma = ff_h264_h_lpf_chroma_8_lsx;
+                c->h264_h_loop_filter_chroma_intra = ff_h264_h_lpf_chroma_intra_8_lsx;
+            } else
+                c->h264_idct_add8 = ff_h264_idct_add8_422_8_lsx;
+
+            c->h264_idct_add16 = ff_h264_idct_add16_8_lsx;
+            c->h264_idct8_add4 = ff_h264_idct8_add4_8_lsx;
+            c->h264_luma_dc_dequant_idct = ff_h264_luma_dc_dequant_idct_8_lsx;
+            c->h264_idct_add16intra = ff_h264_idct_add16_intra_8_lsx;
+
+            c->h264_add_pixels4_clear = ff_h264_add_pixels4_8_lsx;
+            c->h264_add_pixels8_clear = ff_h264_add_pixels8_8_lsx;
+            c->h264_v_loop_filter_luma = ff_h264_v_lpf_luma_8_lsx;
+            c->h264_h_loop_filter_luma = ff_h264_h_lpf_luma_8_lsx;
+            c->h264_v_loop_filter_luma_intra = ff_h264_v_lpf_luma_intra_8_lsx;
+            c->h264_h_loop_filter_luma_intra = ff_h264_h_lpf_luma_intra_8_lsx;
+            c->h264_v_loop_filter_chroma = ff_h264_v_lpf_chroma_8_lsx;
+
+            c->h264_v_loop_filter_chroma_intra = ff_h264_v_lpf_chroma_intra_8_lsx;
+
+            c->biweight_h264_pixels_tab[0] = ff_biweight_h264_pixels16_8_lsx;
+            c->biweight_h264_pixels_tab[1] = ff_biweight_h264_pixels8_8_lsx;
+            c->biweight_h264_pixels_tab[2] = ff_biweight_h264_pixels4_8_lsx;
+            c->weight_h264_pixels_tab[0] = ff_weight_h264_pixels16_8_lsx;
+            c->weight_h264_pixels_tab[1] = ff_weight_h264_pixels8_8_lsx;
+            c->weight_h264_pixels_tab[2] = ff_weight_h264_pixels4_8_lsx;
+            c->h264_idct8_add    = ff_h264_idct8_add_8_lsx;
+            c->h264_idct8_dc_add = ff_h264_idct8_dc_add_8_lsx;
+        }
+    }
+    if (have_lasx(cpu_flags)) {
+        if (chroma_format_idc <= 1)
+            c->h264_loop_filter_strength = ff_h264_loop_filter_strength_lasx;
+        if (bit_depth == 8) {
+            c->h264_add_pixels4_clear = ff_h264_add_pixels4_8_lasx;
+            c->h264_add_pixels8_clear = ff_h264_add_pixels8_8_lasx;
+            c->h264_v_loop_filter_luma = ff_h264_v_lpf_luma_8_lasx;
+            c->h264_h_loop_filter_luma = ff_h264_h_lpf_luma_8_lasx;
+            c->h264_v_loop_filter_luma_intra = ff_h264_v_lpf_luma_intra_8_lasx;
+            c->h264_h_loop_filter_luma_intra = ff_h264_h_lpf_luma_intra_8_lasx;
+
+            /* Weighted MC */
+            c->weight_h264_pixels_tab[0] = ff_weight_h264_pixels16_8_lasx;
+            c->weight_h264_pixels_tab[1] = ff_weight_h264_pixels8_8_lasx;
+
+            c->biweight_h264_pixels_tab[0] = ff_biweight_h264_pixels16_8_lasx;
+            c->biweight_h264_pixels_tab[1] = ff_biweight_h264_pixels8_8_lasx;
+
+            c->h264_idct8_add    = ff_h264_idct8_add_8_lasx;
+            c->h264_idct8_dc_add = ff_h264_idct8_dc_add_8_lasx;
+            c->h264_idct8_add4 = ff_h264_idct8_add4_8_lasx;
+        }
+    }
+}
diff --git a/libavcodec/loongarch/h264dsp_lasx.c b/libavcodec/loongarch/h264dsp_lasx.c
new file mode 100644
index 0000000000..7b0200cac7
--- /dev/null
+++ b/libavcodec/loongarch/h264dsp_lasx.c
@@ -0,0 +1,782 @@
+/*
+ * Loongson LASX optimized h264dsp
+ *
+ * Copyright (c) 2021 Loongson Technology Corporation Limited
+ * Contributed by Shiyou Yin <yinshiyou-hf@loongson.cn>
+ *                Xiwei  Gu  <guxiwei-hf@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "libavutil/loongarch/loongson_intrinsics.h"
+#include "h264dsp_lasx.h"
+
+#define AVC_LPF_P1_OR_Q1(p0_or_q0_org_in, q0_or_p0_org_in,   \
+                         p1_or_q1_org_in, p2_or_q2_org_in,   \
+                         neg_tc_in, tc_in, p1_or_q1_out)     \
+{                                                            \
+    __m256i clip3, temp;                                     \
+                                                             \
+    clip3 = __lasx_xvavgr_hu(p0_or_q0_org_in,                \
+                             q0_or_p0_org_in);               \
+    temp = __lasx_xvslli_h(p1_or_q1_org_in, 1);              \
+    clip3 = __lasx_xvsub_h(clip3, temp);                     \
+    clip3 = __lasx_xvavg_h(p2_or_q2_org_in, clip3);          \
+    clip3 = __lasx_xvclip_h(clip3, neg_tc_in, tc_in);        \
+    p1_or_q1_out = __lasx_xvadd_h(p1_or_q1_org_in, clip3);   \
+}
+
+#define AVC_LPF_P0Q0(q0_or_p0_org_in, p0_or_q0_org_in,       \
+                     p1_or_q1_org_in, q1_or_p1_org_in,       \
+                     neg_threshold_in, threshold_in,         \
+                     p0_or_q0_out, q0_or_p0_out)             \
+{                                                            \
+    __m256i q0_sub_p0, p1_sub_q1, delta;                     \
+                                                             \
+    q0_sub_p0 = __lasx_xvsub_h(q0_or_p0_org_in,              \
+                               p0_or_q0_org_in);             \
+    p1_sub_q1 = __lasx_xvsub_h(p1_or_q1_org_in,              \
+                               q1_or_p1_org_in);             \
+    q0_sub_p0 = __lasx_xvslli_h(q0_sub_p0, 2);               \
+    p1_sub_q1 = __lasx_xvaddi_hu(p1_sub_q1, 4);              \
+    delta = __lasx_xvadd_h(q0_sub_p0, p1_sub_q1);            \
+    delta = __lasx_xvsrai_h(delta, 3);                       \
+    delta = __lasx_xvclip_h(delta, neg_threshold_in,         \
+           threshold_in);                                    \
+    p0_or_q0_out = __lasx_xvadd_h(p0_or_q0_org_in, delta);   \
+    q0_or_p0_out = __lasx_xvsub_h(q0_or_p0_org_in, delta);   \
+                                                             \
+    p0_or_q0_out = __lasx_xvclip255_h(p0_or_q0_out);         \
+    q0_or_p0_out = __lasx_xvclip255_h(q0_or_p0_out);         \
+}
+
+void ff_h264_h_lpf_luma_8_lasx(uint8_t *data, int img_width,
+                               int alpha_in, int beta_in, int8_t *tc)
+{
+    int img_width_2x = img_width << 1;
+    int img_width_4x = img_width << 2;
+    int img_width_8x = img_width << 3;
+    int img_width_3x = img_width_2x + img_width;
+    __m256i tmp_vec0, bs_vec;
+    __m256i tc_vec = {0x0101010100000000, 0x0303030302020202,
+                      0x0101010100000000, 0x0303030302020202};
+
+    tmp_vec0 = __lasx_xvldrepl_w((uint32_t*)tc, 0);
+    tc_vec   = __lasx_xvshuf_b(tmp_vec0, tmp_vec0, tc_vec);
+    bs_vec   = __lasx_xvslti_b(tc_vec, 0);
+    bs_vec   = __lasx_xvxori_b(bs_vec, 255);
+    bs_vec   = __lasx_xvandi_b(bs_vec, 1);
+
+    if (__lasx_xbnz_v(bs_vec)) {
+        uint8_t *src = data - 4;
+        __m256i p3_org, p2_org, p1_org, p0_org, q0_org, q1_org, q2_org, q3_org;
+        __m256i p0_asub_q0, p1_asub_p0, q1_asub_q0, alpha, beta;
+        __m256i is_less_than, is_less_than_beta, is_less_than_alpha;
+        __m256i is_bs_greater_than0;
+        __m256i zero = __lasx_xvldi(0);
+
+        is_bs_greater_than0 = __lasx_xvslt_bu(zero, bs_vec);
+
+        {
+            uint8_t *src_tmp = src + img_width_8x;
+            __m256i row0, row1, row2, row3, row4, row5, row6, row7;
+            __m256i row8, row9, row10, row11, row12, row13, row14, row15;
+
+            DUP4_ARG2(__lasx_xvldx, src, 0, src, img_width, src, img_width_2x,
+                      src, img_width_3x, row0, row1, row2, row3);
+            src += img_width_4x;
+            DUP4_ARG2(__lasx_xvldx, src, 0, src, img_width, src, img_width_2x,
+                      src, img_width_3x, row4, row5, row6, row7);
+            src -= img_width_4x;
+            DUP4_ARG2(__lasx_xvldx, src_tmp, 0, src_tmp, img_width, src_tmp,
+                      img_width_2x, src_tmp, img_width_3x,
+                      row8, row9, row10, row11);
+            src_tmp += img_width_4x;
+            DUP4_ARG2(__lasx_xvldx, src_tmp, 0, src_tmp, img_width, src_tmp,
+                      img_width_2x, src_tmp, img_width_3x,
+                      row12, row13, row14, row15);
+            src_tmp -= img_width_4x;
+
+            LASX_TRANSPOSE16x8_B(row0, row1, row2, row3, row4, row5, row6,
+                                 row7, row8, row9, row10, row11,
+                                 row12, row13, row14, row15,
+                                 p3_org, p2_org, p1_org, p0_org,
+                                 q0_org, q1_org, q2_org, q3_org);
+        }
+
+        p0_asub_q0 = __lasx_xvabsd_bu(p0_org, q0_org);
+        p1_asub_p0 = __lasx_xvabsd_bu(p1_org, p0_org);
+        q1_asub_q0 = __lasx_xvabsd_bu(q1_org, q0_org);
+
+        alpha = __lasx_xvreplgr2vr_b(alpha_in);
+        beta  = __lasx_xvreplgr2vr_b(beta_in);
+
+        is_less_than_alpha = __lasx_xvslt_bu(p0_asub_q0, alpha);
+        is_less_than_beta  = __lasx_xvslt_bu(p1_asub_p0, beta);
+        is_less_than       = is_less_than_alpha & is_less_than_beta;
+        is_less_than_beta  = __lasx_xvslt_bu(q1_asub_q0, beta);
+        is_less_than       = is_less_than_beta & is_less_than;
+        is_less_than       = is_less_than & is_bs_greater_than0;
+
+        if (__lasx_xbnz_v(is_less_than)) {
+            __m256i neg_tc_h, tc_h, p1_org_h, p0_org_h, q0_org_h, q1_org_h;
+            __m256i p2_asub_p0, q2_asub_q0;
+
+            neg_tc_h = __lasx_xvneg_b(tc_vec);
+            neg_tc_h = __lasx_vext2xv_h_b(neg_tc_h);
+            tc_h     = __lasx_vext2xv_hu_bu(tc_vec);
+            p1_org_h = __lasx_vext2xv_hu_bu(p1_org);
+            p0_org_h = __lasx_vext2xv_hu_bu(p0_org);
+            q0_org_h = __lasx_vext2xv_hu_bu(q0_org);
+
+            p2_asub_p0 = __lasx_xvabsd_bu(p2_org, p0_org);
+            is_less_than_beta = __lasx_xvslt_bu(p2_asub_p0, beta);
+            is_less_than_beta = is_less_than_beta & is_less_than;
+
+            if (__lasx_xbnz_v(is_less_than_beta)) {
+                __m256i p2_org_h, p1_h;
+
+                p2_org_h = __lasx_vext2xv_hu_bu(p2_org);
+                AVC_LPF_P1_OR_Q1(p0_org_h, q0_org_h, p1_org_h, p2_org_h,
+                                 neg_tc_h, tc_h, p1_h);
+                p1_h = __lasx_xvpickev_b(p1_h, p1_h);
+                p1_h = __lasx_xvpermi_d(p1_h, 0xd8);
+                p1_org = __lasx_xvbitsel_v(p1_org, p1_h, is_less_than_beta);
+                is_less_than_beta = __lasx_xvandi_b(is_less_than_beta, 1);
+                tc_vec = __lasx_xvadd_b(tc_vec, is_less_than_beta);
+            }
+
+            q2_asub_q0 = __lasx_xvabsd_bu(q2_org, q0_org);
+            is_less_than_beta = __lasx_xvslt_bu(q2_asub_q0, beta);
+            is_less_than_beta = is_less_than_beta & is_less_than;
+
+            q1_org_h = __lasx_vext2xv_hu_bu(q1_org);
+
+            if (__lasx_xbnz_v(is_less_than_beta)) {
+                __m256i q2_org_h, q1_h;
+
+                q2_org_h = __lasx_vext2xv_hu_bu(q2_org);
+                AVC_LPF_P1_OR_Q1(p0_org_h, q0_org_h, q1_org_h, q2_org_h,
+                                 neg_tc_h, tc_h, q1_h);
+                q1_h = __lasx_xvpickev_b(q1_h, q1_h);
+                q1_h = __lasx_xvpermi_d(q1_h, 0xd8);
+                q1_org = __lasx_xvbitsel_v(q1_org, q1_h, is_less_than_beta);
+
+                is_less_than_beta = __lasx_xvandi_b(is_less_than_beta, 1);
+                tc_vec = __lasx_xvadd_b(tc_vec, is_less_than_beta);
+            }
+
+            {
+                __m256i neg_thresh_h, p0_h, q0_h;
+
+                neg_thresh_h = __lasx_xvneg_b(tc_vec);
+                neg_thresh_h = __lasx_vext2xv_h_b(neg_thresh_h);
+                tc_h         = __lasx_vext2xv_hu_bu(tc_vec);
+
+                AVC_LPF_P0Q0(q0_org_h, p0_org_h, p1_org_h, q1_org_h,
+                             neg_thresh_h, tc_h, p0_h, q0_h);
+                DUP2_ARG2(__lasx_xvpickev_b, p0_h, p0_h, q0_h, q0_h,
+                          p0_h, q0_h);
+                DUP2_ARG2(__lasx_xvpermi_d, p0_h, 0xd8, q0_h, 0xd8,
+                          p0_h, q0_h);
+                p0_org = __lasx_xvbitsel_v(p0_org, p0_h, is_less_than);
+                q0_org = __lasx_xvbitsel_v(q0_org, q0_h, is_less_than);
+            }
+
+            {
+                __m256i row0, row1, row2, row3, row4, row5, row6, row7;
+                __m256i control = {0x0000000400000000, 0x0000000500000001,
+                                   0x0000000600000002, 0x0000000700000003};
+
+                DUP4_ARG3(__lasx_xvpermi_q, p0_org, q3_org, 0x02, p1_org,
+                          q2_org, 0x02, p2_org, q1_org, 0x02, p3_org,
+                          q0_org, 0x02, p0_org, p1_org, p2_org, p3_org);
+                DUP2_ARG2(__lasx_xvilvl_b, p1_org, p3_org, p0_org, p2_org,
+                          row0, row2);
+                DUP2_ARG2(__lasx_xvilvh_b, p1_org, p3_org, p0_org, p2_org,
+                          row1, row3);
+                DUP2_ARG2(__lasx_xvilvl_b, row2, row0, row3, row1, row4, row6);
+                DUP2_ARG2(__lasx_xvilvh_b, row2, row0, row3, row1, row5, row7);
+                DUP4_ARG2(__lasx_xvperm_w, row4, control, row5, control, row6,
+                          control, row7, control, row4, row5, row6, row7);
+                __lasx_xvstelm_d(row4, src, 0, 0);
+                __lasx_xvstelm_d(row4, src + img_width, 0, 1);
+                src += img_width_2x;
+                __lasx_xvstelm_d(row4, src, 0, 2);
+                __lasx_xvstelm_d(row4, src + img_width, 0, 3);
+                src += img_width_2x;
+                __lasx_xvstelm_d(row5, src, 0, 0);
+                __lasx_xvstelm_d(row5, src + img_width, 0, 1);
+                src += img_width_2x;
+                __lasx_xvstelm_d(row5, src, 0, 2);
+                __lasx_xvstelm_d(row5, src + img_width, 0, 3);
+                src += img_width_2x;
+                __lasx_xvstelm_d(row6, src, 0, 0);
+                __lasx_xvstelm_d(row6, src + img_width, 0, 1);
+                src += img_width_2x;
+                __lasx_xvstelm_d(row6, src, 0, 2);
+                __lasx_xvstelm_d(row6, src + img_width, 0, 3);
+                src += img_width_2x;
+                __lasx_xvstelm_d(row7, src, 0, 0);
+                __lasx_xvstelm_d(row7, src + img_width, 0, 1);
+                src += img_width_2x;
+                __lasx_xvstelm_d(row7, src, 0, 2);
+                __lasx_xvstelm_d(row7, src + img_width, 0, 3);
+            }
+        }
+    }
+}
+
+void ff_h264_v_lpf_luma_8_lasx(uint8_t *data, int img_width,
+                                   int alpha_in, int beta_in, int8_t *tc)
+{
+    int img_width_2x = img_width << 1;
+    int img_width_3x = img_width + img_width_2x;
+    __m256i tmp_vec0, bs_vec;
+    __m256i tc_vec = {0x0101010100000000, 0x0303030302020202,
+                      0x0101010100000000, 0x0303030302020202};
+
+    tmp_vec0 = __lasx_xvldrepl_w((uint32_t*)tc, 0);
+    tc_vec   = __lasx_xvshuf_b(tmp_vec0, tmp_vec0, tc_vec);
+    bs_vec   = __lasx_xvslti_b(tc_vec, 0);
+    bs_vec   = __lasx_xvxori_b(bs_vec, 255);
+    bs_vec   = __lasx_xvandi_b(bs_vec, 1);
+
+    if (__lasx_xbnz_v(bs_vec)) {
+        __m256i p2_org, p1_org, p0_org, q0_org, q1_org, q2_org;
+        __m256i p0_asub_q0, p1_asub_p0, q1_asub_q0, alpha, beta;
+        __m256i is_less_than, is_less_than_beta, is_less_than_alpha;
+        __m256i p1_org_h, p0_org_h, q0_org_h, q1_org_h;
+        __m256i is_bs_greater_than0;
+        __m256i zero = __lasx_xvldi(0);
+
+        alpha = __lasx_xvreplgr2vr_b(alpha_in);
+        beta  = __lasx_xvreplgr2vr_b(beta_in);
+
+        DUP2_ARG2(__lasx_xvldx, data, -img_width_3x, data, -img_width_2x,
+                  p2_org, p1_org);
+        p0_org = __lasx_xvldx(data, -img_width);
+        DUP2_ARG2(__lasx_xvldx, data, 0, data, img_width, q0_org, q1_org);
+
+        is_bs_greater_than0 = __lasx_xvslt_bu(zero, bs_vec);
+        p0_asub_q0 = __lasx_xvabsd_bu(p0_org, q0_org);
+        p1_asub_p0 = __lasx_xvabsd_bu(p1_org, p0_org);
+        q1_asub_q0 = __lasx_xvabsd_bu(q1_org, q0_org);
+
+        is_less_than_alpha = __lasx_xvslt_bu(p0_asub_q0, alpha);
+        is_less_than_beta  = __lasx_xvslt_bu(p1_asub_p0, beta);
+        is_less_than       = is_less_than_alpha & is_less_than_beta;
+        is_less_than_beta  = __lasx_xvslt_bu(q1_asub_q0, beta);
+        is_less_than       = is_less_than_beta & is_less_than;
+        is_less_than       = is_less_than & is_bs_greater_than0;
+
+        if (__lasx_xbnz_v(is_less_than)) {
+            __m256i neg_tc_h, tc_h, p2_asub_p0, q2_asub_q0;
+
+            q2_org = __lasx_xvldx(data, img_width_2x);
+
+            neg_tc_h = __lasx_xvneg_b(tc_vec);
+            neg_tc_h = __lasx_vext2xv_h_b(neg_tc_h);
+            tc_h     = __lasx_vext2xv_hu_bu(tc_vec);
+            p1_org_h = __lasx_vext2xv_hu_bu(p1_org);
+            p0_org_h = __lasx_vext2xv_hu_bu(p0_org);
+            q0_org_h = __lasx_vext2xv_hu_bu(q0_org);
+
+            p2_asub_p0        = __lasx_xvabsd_bu(p2_org, p0_org);
+            is_less_than_beta = __lasx_xvslt_bu(p2_asub_p0, beta);
+            is_less_than_beta = is_less_than_beta & is_less_than;
+
+            if (__lasx_xbnz_v(is_less_than_beta)) {
+                __m256i p1_h, p2_org_h;
+
+                p2_org_h = __lasx_vext2xv_hu_bu(p2_org);
+                AVC_LPF_P1_OR_Q1(p0_org_h, q0_org_h, p1_org_h, p2_org_h,
+                                 neg_tc_h, tc_h, p1_h);
+                p1_h = __lasx_xvpickev_b(p1_h, p1_h);
+                p1_h = __lasx_xvpermi_d(p1_h, 0xd8);
+                p1_h   = __lasx_xvbitsel_v(p1_org, p1_h, is_less_than_beta);
+                p1_org = __lasx_xvpermi_q(p1_org, p1_h, 0x30);
+                __lasx_xvst(p1_org, data - img_width_2x, 0);
+
+                is_less_than_beta = __lasx_xvandi_b(is_less_than_beta, 1);
+                tc_vec = __lasx_xvadd_b(tc_vec, is_less_than_beta);
+            }
+
+            q2_asub_q0 = __lasx_xvabsd_bu(q2_org, q0_org);
+            is_less_than_beta = __lasx_xvslt_bu(q2_asub_q0, beta);
+            is_less_than_beta = is_less_than_beta & is_less_than;
+
+            q1_org_h = __lasx_vext2xv_hu_bu(q1_org);
+
+            if (__lasx_xbnz_v(is_less_than_beta)) {
+                __m256i q1_h, q2_org_h;
+
+                q2_org_h = __lasx_vext2xv_hu_bu(q2_org);
+                AVC_LPF_P1_OR_Q1(p0_org_h, q0_org_h, q1_org_h, q2_org_h,
+                                 neg_tc_h, tc_h, q1_h);
+                q1_h = __lasx_xvpickev_b(q1_h, q1_h);
+                q1_h = __lasx_xvpermi_d(q1_h, 0xd8);
+                q1_h = __lasx_xvbitsel_v(q1_org, q1_h, is_less_than_beta);
+                q1_org = __lasx_xvpermi_q(q1_org, q1_h, 0x30);
+                __lasx_xvst(q1_org, data + img_width, 0);
+
+                is_less_than_beta = __lasx_xvandi_b(is_less_than_beta, 1);
+                tc_vec = __lasx_xvadd_b(tc_vec, is_less_than_beta);
+
+            }
+
+            {
+                __m256i neg_thresh_h, p0_h, q0_h;
+
+                neg_thresh_h = __lasx_xvneg_b(tc_vec);
+                neg_thresh_h = __lasx_vext2xv_h_b(neg_thresh_h);
+                tc_h         = __lasx_vext2xv_hu_bu(tc_vec);
+
+                AVC_LPF_P0Q0(q0_org_h, p0_org_h, p1_org_h, q1_org_h,
+                             neg_thresh_h, tc_h, p0_h, q0_h);
+                DUP2_ARG2(__lasx_xvpickev_b, p0_h, p0_h, q0_h, q0_h,
+                          p0_h, q0_h);
+                DUP2_ARG2(__lasx_xvpermi_d, p0_h, 0Xd8, q0_h, 0xd8,
+                          p0_h, q0_h);
+                p0_h = __lasx_xvbitsel_v(p0_org, p0_h, is_less_than);
+                q0_h = __lasx_xvbitsel_v(q0_org, q0_h, is_less_than);
+                p0_org = __lasx_xvpermi_q(p0_org, p0_h, 0x30);
+                q0_org = __lasx_xvpermi_q(q0_org, q0_h, 0x30);
+                __lasx_xvst(p0_org, data - img_width, 0);
+                __lasx_xvst(q0_org, data, 0);
+            }
+        }
+    }
+}
+
+#define AVC_LPF_P0P1P2_OR_Q0Q1Q2(p3_or_q3_org_in, p0_or_q0_org_in,          \
+                                 q3_or_p3_org_in, p1_or_q1_org_in,          \
+                                 p2_or_q2_org_in, q1_or_p1_org_in,          \
+                                 p0_or_q0_out, p1_or_q1_out, p2_or_q2_out)  \
+{                                                                           \
+    __m256i threshold;                                                      \
+    __m256i const2, const3 = __lasx_xvldi(0);                               \
+                                                                            \
+    const2 = __lasx_xvaddi_hu(const3, 2);                                   \
+    const3 = __lasx_xvaddi_hu(const3, 3);                                   \
+    threshold = __lasx_xvadd_h(p0_or_q0_org_in, q3_or_p3_org_in);           \
+    threshold = __lasx_xvadd_h(p1_or_q1_org_in, threshold);                 \
+                                                                            \
+    p0_or_q0_out = __lasx_xvslli_h(threshold, 1);                           \
+    p0_or_q0_out = __lasx_xvadd_h(p0_or_q0_out, p2_or_q2_org_in);           \
+    p0_or_q0_out = __lasx_xvadd_h(p0_or_q0_out, q1_or_p1_org_in);           \
+    p0_or_q0_out = __lasx_xvsrar_h(p0_or_q0_out, const3);                   \
+                                                                            \
+    p1_or_q1_out = __lasx_xvadd_h(p2_or_q2_org_in, threshold);              \
+    p1_or_q1_out = __lasx_xvsrar_h(p1_or_q1_out, const2);                   \
+                                                                            \
+    p2_or_q2_out = __lasx_xvmul_h(p2_or_q2_org_in, const3);                 \
+    p2_or_q2_out = __lasx_xvadd_h(p2_or_q2_out, p3_or_q3_org_in);           \
+    p2_or_q2_out = __lasx_xvadd_h(p2_or_q2_out, p3_or_q3_org_in);           \
+    p2_or_q2_out = __lasx_xvadd_h(p2_or_q2_out, threshold);                 \
+    p2_or_q2_out = __lasx_xvsrar_h(p2_or_q2_out, const3);                   \
+}
+
+/* data[-u32_img_width] = (uint8_t)((2 * p1 + p0 + q1 + 2) >> 2); */
+#define AVC_LPF_P0_OR_Q0(p0_or_q0_org_in, q1_or_p1_org_in,             \
+                         p1_or_q1_org_in, p0_or_q0_out)                \
+{                                                                      \
+    __m256i const2 = __lasx_xvldi(0);                                  \
+    const2 = __lasx_xvaddi_hu(const2, 2);                              \
+    p0_or_q0_out = __lasx_xvadd_h(p0_or_q0_org_in, q1_or_p1_org_in);   \
+    p0_or_q0_out = __lasx_xvadd_h(p0_or_q0_out, p1_or_q1_org_in);      \
+    p0_or_q0_out = __lasx_xvadd_h(p0_or_q0_out, p1_or_q1_org_in);      \
+    p0_or_q0_out = __lasx_xvsrar_h(p0_or_q0_out, const2);              \
+}
+
+void ff_h264_h_lpf_luma_intra_8_lasx(uint8_t *data, int img_width,
+                                     int alpha_in, int beta_in)
+{
+    int img_width_2x = img_width << 1;
+    int img_width_4x = img_width << 2;
+    int img_width_3x = img_width_2x + img_width;
+    uint8_t *src = data - 4;
+    __m256i p0_asub_q0, p1_asub_p0, q1_asub_q0, alpha, beta;
+    __m256i is_less_than, is_less_than_beta, is_less_than_alpha;
+    __m256i p3_org, p2_org, p1_org, p0_org, q0_org, q1_org, q2_org, q3_org;
+    __m256i zero = __lasx_xvldi(0);
+
+    {
+        __m256i row0, row1, row2, row3, row4, row5, row6, row7;
+        __m256i row8, row9, row10, row11, row12, row13, row14, row15;
+
+        DUP4_ARG2(__lasx_xvldx, src, 0, src, img_width, src, img_width_2x,
+                  src, img_width_3x, row0, row1, row2, row3);
+        src += img_width_4x;
+        DUP4_ARG2(__lasx_xvldx, src, 0, src, img_width, src, img_width_2x,
+                  src, img_width_3x, row4, row5, row6, row7);
+        src += img_width_4x;
+        DUP4_ARG2(__lasx_xvldx, src, 0, src, img_width, src, img_width_2x,
+                  src, img_width_3x, row8, row9, row10, row11);
+        src += img_width_4x;
+        DUP4_ARG2(__lasx_xvldx, src, 0, src, img_width, src, img_width_2x,
+                  src, img_width_3x, row12, row13, row14, row15);
+        src += img_width_4x;
+
+        LASX_TRANSPOSE16x8_B(row0, row1, row2, row3,
+                             row4, row5, row6, row7,
+                             row8, row9, row10, row11,
+                             row12, row13, row14, row15,
+                             p3_org, p2_org, p1_org, p0_org,
+                             q0_org, q1_org, q2_org, q3_org);
+    }
+
+    alpha = __lasx_xvreplgr2vr_b(alpha_in);
+    beta  = __lasx_xvreplgr2vr_b(beta_in);
+    p0_asub_q0 = __lasx_xvabsd_bu(p0_org, q0_org);
+    p1_asub_p0 = __lasx_xvabsd_bu(p1_org, p0_org);
+    q1_asub_q0 = __lasx_xvabsd_bu(q1_org, q0_org);
+
+    is_less_than_alpha = __lasx_xvslt_bu(p0_asub_q0, alpha);
+    is_less_than_beta  = __lasx_xvslt_bu(p1_asub_p0, beta);
+    is_less_than       = is_less_than_beta & is_less_than_alpha;
+    is_less_than_beta  = __lasx_xvslt_bu(q1_asub_q0, beta);
+    is_less_than       = is_less_than_beta & is_less_than;
+    is_less_than       = __lasx_xvpermi_q(zero, is_less_than, 0x30);
+
+    if (__lasx_xbnz_v(is_less_than)) {
+        __m256i p2_asub_p0, q2_asub_q0, p0_h, q0_h, negate_is_less_than_beta;
+        __m256i p1_org_h, p0_org_h, q0_org_h, q1_org_h;
+        __m256i less_alpha_shift2_add2 = __lasx_xvsrli_b(alpha, 2);
+
+        less_alpha_shift2_add2 = __lasx_xvaddi_bu(less_alpha_shift2_add2, 2);
+        less_alpha_shift2_add2 = __lasx_xvslt_bu(p0_asub_q0,
+                                                 less_alpha_shift2_add2);
+
+        p1_org_h = __lasx_vext2xv_hu_bu(p1_org);
+        p0_org_h = __lasx_vext2xv_hu_bu(p0_org);
+        q0_org_h = __lasx_vext2xv_hu_bu(q0_org);
+        q1_org_h = __lasx_vext2xv_hu_bu(q1_org);
+
+        p2_asub_p0               = __lasx_xvabsd_bu(p2_org, p0_org);
+        is_less_than_beta        = __lasx_xvslt_bu(p2_asub_p0, beta);
+        is_less_than_beta        = is_less_than_beta & less_alpha_shift2_add2;
+        negate_is_less_than_beta = __lasx_xvxori_b(is_less_than_beta, 0xff);
+        is_less_than_beta        = is_less_than_beta & is_less_than;
+        negate_is_less_than_beta = negate_is_less_than_beta & is_less_than;
+
+        /* combine and store */
+        if (__lasx_xbnz_v(is_less_than_beta)) {
+            __m256i p2_org_h, p3_org_h, p1_h, p2_h;
+
+            p2_org_h   = __lasx_vext2xv_hu_bu(p2_org);
+            p3_org_h   = __lasx_vext2xv_hu_bu(p3_org);
+
+            AVC_LPF_P0P1P2_OR_Q0Q1Q2(p3_org_h, p0_org_h, q0_org_h, p1_org_h,
+                                     p2_org_h, q1_org_h, p0_h, p1_h, p2_h);
+
+            p0_h = __lasx_xvpickev_b(p0_h, p0_h);
+            p0_h = __lasx_xvpermi_d(p0_h, 0xd8);
+            DUP2_ARG2(__lasx_xvpickev_b, p1_h, p1_h, p2_h, p2_h, p1_h, p2_h);
+            DUP2_ARG2(__lasx_xvpermi_d, p1_h, 0xd8, p2_h, 0xd8, p1_h, p2_h);
+            p0_org = __lasx_xvbitsel_v(p0_org, p0_h, is_less_than_beta);
+            p1_org = __lasx_xvbitsel_v(p1_org, p1_h, is_less_than_beta);
+            p2_org = __lasx_xvbitsel_v(p2_org, p2_h, is_less_than_beta);
+        }
+
+        AVC_LPF_P0_OR_Q0(p0_org_h, q1_org_h, p1_org_h, p0_h);
+        /* combine */
+        p0_h = __lasx_xvpickev_b(p0_h, p0_h);
+        p0_h = __lasx_xvpermi_d(p0_h, 0xd8);
+        p0_org = __lasx_xvbitsel_v(p0_org, p0_h, negate_is_less_than_beta);
+
+        /* if (tmpFlag && (unsigned)ABS(q2-q0) < thresholds->beta_in) */
+        q2_asub_q0 = __lasx_xvabsd_bu(q2_org, q0_org);
+        is_less_than_beta = __lasx_xvslt_bu(q2_asub_q0, beta);
+        is_less_than_beta = is_less_than_beta & less_alpha_shift2_add2;
+        negate_is_less_than_beta = __lasx_xvxori_b(is_less_than_beta, 0xff);
+        is_less_than_beta = is_less_than_beta & is_less_than;
+        negate_is_less_than_beta = negate_is_less_than_beta & is_less_than;
+
+        /* combine and store */
+        if (__lasx_xbnz_v(is_less_than_beta)) {
+            __m256i q2_org_h, q3_org_h, q1_h, q2_h;
+
+            q2_org_h   = __lasx_vext2xv_hu_bu(q2_org);
+            q3_org_h   = __lasx_vext2xv_hu_bu(q3_org);
+
+            AVC_LPF_P0P1P2_OR_Q0Q1Q2(q3_org_h, q0_org_h, p0_org_h, q1_org_h,
+                                     q2_org_h, p1_org_h, q0_h, q1_h, q2_h);
+
+            q0_h = __lasx_xvpickev_b(q0_h, q0_h);
+            q0_h = __lasx_xvpermi_d(q0_h, 0xd8);
+            DUP2_ARG2(__lasx_xvpickev_b, q1_h, q1_h, q2_h, q2_h, q1_h, q2_h);
+            DUP2_ARG2(__lasx_xvpermi_d, q1_h, 0xd8, q2_h, 0xd8, q1_h, q2_h);
+            q0_org = __lasx_xvbitsel_v(q0_org, q0_h, is_less_than_beta);
+            q1_org = __lasx_xvbitsel_v(q1_org, q1_h, is_less_than_beta);
+            q2_org = __lasx_xvbitsel_v(q2_org, q2_h, is_less_than_beta);
+
+        }
+
+        AVC_LPF_P0_OR_Q0(q0_org_h, p1_org_h, q1_org_h, q0_h);
+
+        /* combine */
+        q0_h = __lasx_xvpickev_b(q0_h, q0_h);
+        q0_h = __lasx_xvpermi_d(q0_h, 0xd8);
+        q0_org = __lasx_xvbitsel_v(q0_org, q0_h, negate_is_less_than_beta);
+
+        /* transpose and store */
+        {
+            __m256i row0, row1, row2, row3, row4, row5, row6, row7;
+            __m256i control = {0x0000000400000000, 0x0000000500000001,
+                               0x0000000600000002, 0x0000000700000003};
+
+            DUP4_ARG3(__lasx_xvpermi_q, p0_org, q3_org, 0x02, p1_org, q2_org,
+                      0x02, p2_org, q1_org, 0x02, p3_org, q0_org, 0x02,
+                      p0_org, p1_org, p2_org, p3_org);
+            DUP2_ARG2(__lasx_xvilvl_b, p1_org, p3_org, p0_org, p2_org,
+                      row0, row2);
+            DUP2_ARG2(__lasx_xvilvh_b, p1_org, p3_org, p0_org, p2_org,
+                      row1, row3);
+            DUP2_ARG2(__lasx_xvilvl_b, row2, row0, row3, row1, row4, row6);
+            DUP2_ARG2(__lasx_xvilvh_b, row2, row0, row3, row1, row5, row7);
+            DUP4_ARG2(__lasx_xvperm_w, row4, control, row5, control, row6,
+                      control, row7, control, row4, row5, row6, row7);
+            src = data - 4;
+            __lasx_xvstelm_d(row4, src, 0, 0);
+            __lasx_xvstelm_d(row4, src + img_width, 0, 1);
+            src += img_width_2x;
+            __lasx_xvstelm_d(row4, src, 0, 2);
+            __lasx_xvstelm_d(row4, src + img_width, 0, 3);
+            src += img_width_2x;
+            __lasx_xvstelm_d(row5, src, 0, 0);
+            __lasx_xvstelm_d(row5, src + img_width, 0, 1);
+            src += img_width_2x;
+            __lasx_xvstelm_d(row5, src, 0, 2);
+            __lasx_xvstelm_d(row5, src + img_width, 0, 3);
+            src += img_width_2x;
+            __lasx_xvstelm_d(row6, src, 0, 0);
+            __lasx_xvstelm_d(row6, src + img_width, 0, 1);
+            src += img_width_2x;
+            __lasx_xvstelm_d(row6, src, 0, 2);
+            __lasx_xvstelm_d(row6, src + img_width, 0, 3);
+            src += img_width_2x;
+            __lasx_xvstelm_d(row7, src, 0, 0);
+            __lasx_xvstelm_d(row7, src + img_width, 0, 1);
+            src += img_width_2x;
+            __lasx_xvstelm_d(row7, src, 0, 2);
+            __lasx_xvstelm_d(row7, src + img_width, 0, 3);
+        }
+    }
+}
+
+void ff_h264_v_lpf_luma_intra_8_lasx(uint8_t *data, int img_width,
+                                     int alpha_in, int beta_in)
+{
+    int img_width_2x = img_width << 1;
+    int img_width_3x = img_width_2x + img_width;
+    uint8_t *src = data - img_width_2x;
+    __m256i p0_asub_q0, p1_asub_p0, q1_asub_q0, alpha, beta;
+    __m256i is_less_than, is_less_than_beta, is_less_than_alpha;
+    __m256i p1_org, p0_org, q0_org, q1_org;
+    __m256i zero = __lasx_xvldi(0);
+
+    DUP4_ARG2(__lasx_xvldx, src, 0, src, img_width, src, img_width_2x,
+              src, img_width_3x, p1_org, p0_org, q0_org, q1_org);
+    alpha = __lasx_xvreplgr2vr_b(alpha_in);
+    beta  = __lasx_xvreplgr2vr_b(beta_in);
+    p0_asub_q0 = __lasx_xvabsd_bu(p0_org, q0_org);
+    p1_asub_p0 = __lasx_xvabsd_bu(p1_org, p0_org);
+    q1_asub_q0 = __lasx_xvabsd_bu(q1_org, q0_org);
+
+    is_less_than_alpha = __lasx_xvslt_bu(p0_asub_q0, alpha);
+    is_less_than_beta  = __lasx_xvslt_bu(p1_asub_p0, beta);
+    is_less_than       = is_less_than_beta & is_less_than_alpha;
+    is_less_than_beta  = __lasx_xvslt_bu(q1_asub_q0, beta);
+    is_less_than       = is_less_than_beta & is_less_than;
+    is_less_than       = __lasx_xvpermi_q(zero, is_less_than, 0x30);
+
+    if (__lasx_xbnz_v(is_less_than)) {
+        __m256i p2_asub_p0, q2_asub_q0, p0_h, q0_h, negate_is_less_than_beta;
+        __m256i p1_org_h, p0_org_h, q0_org_h, q1_org_h;
+        __m256i p2_org = __lasx_xvldx(src, -img_width);
+        __m256i q2_org = __lasx_xvldx(data, img_width_2x);
+        __m256i less_alpha_shift2_add2 = __lasx_xvsrli_b(alpha, 2);
+        less_alpha_shift2_add2 = __lasx_xvaddi_bu(less_alpha_shift2_add2, 2);
+        less_alpha_shift2_add2 = __lasx_xvslt_bu(p0_asub_q0,
+                                                 less_alpha_shift2_add2);
+
+        p1_org_h = __lasx_vext2xv_hu_bu(p1_org);
+        p0_org_h = __lasx_vext2xv_hu_bu(p0_org);
+        q0_org_h = __lasx_vext2xv_hu_bu(q0_org);
+        q1_org_h = __lasx_vext2xv_hu_bu(q1_org);
+
+        p2_asub_p0               = __lasx_xvabsd_bu(p2_org, p0_org);
+        is_less_than_beta        = __lasx_xvslt_bu(p2_asub_p0, beta);
+        is_less_than_beta        = is_less_than_beta & less_alpha_shift2_add2;
+        negate_is_less_than_beta = __lasx_xvxori_b(is_less_than_beta, 0xff);
+        is_less_than_beta        = is_less_than_beta & is_less_than;
+        negate_is_less_than_beta = negate_is_less_than_beta & is_less_than;
+
+        /* combine and store */
+        if (__lasx_xbnz_v(is_less_than_beta)) {
+            __m256i p2_org_h, p3_org_h, p1_h, p2_h;
+            __m256i p3_org = __lasx_xvldx(src, -img_width_2x);
+
+            p2_org_h   = __lasx_vext2xv_hu_bu(p2_org);
+            p3_org_h   = __lasx_vext2xv_hu_bu(p3_org);
+
+            AVC_LPF_P0P1P2_OR_Q0Q1Q2(p3_org_h, p0_org_h, q0_org_h, p1_org_h,
+                                     p2_org_h, q1_org_h, p0_h, p1_h, p2_h);
+
+            p0_h = __lasx_xvpickev_b(p0_h, p0_h);
+            p0_h =  __lasx_xvpermi_d(p0_h, 0xd8);
+            DUP2_ARG2(__lasx_xvpickev_b, p1_h, p1_h, p2_h, p2_h, p1_h, p2_h);
+            DUP2_ARG2(__lasx_xvpermi_d, p1_h, 0xd8, p2_h, 0xd8, p1_h, p2_h);
+            p0_org = __lasx_xvbitsel_v(p0_org, p0_h, is_less_than_beta);
+            p1_org = __lasx_xvbitsel_v(p1_org, p1_h, is_less_than_beta);
+            p2_org = __lasx_xvbitsel_v(p2_org, p2_h, is_less_than_beta);
+
+            __lasx_xvst(p1_org, src, 0);
+            __lasx_xvst(p2_org, src - img_width, 0);
+        }
+
+        AVC_LPF_P0_OR_Q0(p0_org_h, q1_org_h, p1_org_h, p0_h);
+        /* combine */
+        p0_h = __lasx_xvpickev_b(p0_h, p0_h);
+        p0_h = __lasx_xvpermi_d(p0_h, 0xd8);
+        p0_org = __lasx_xvbitsel_v(p0_org, p0_h, negate_is_less_than_beta);
+        __lasx_xvst(p0_org, data - img_width, 0);
+
+        /* if (tmpFlag && (unsigned)ABS(q2-q0) < thresholds->beta_in) */
+        q2_asub_q0 = __lasx_xvabsd_bu(q2_org, q0_org);
+        is_less_than_beta = __lasx_xvslt_bu(q2_asub_q0, beta);
+        is_less_than_beta = is_less_than_beta & less_alpha_shift2_add2;
+        negate_is_less_than_beta = __lasx_xvxori_b(is_less_than_beta, 0xff);
+        is_less_than_beta = is_less_than_beta & is_less_than;
+        negate_is_less_than_beta = negate_is_less_than_beta & is_less_than;
+
+        /* combine and store */
+        if (__lasx_xbnz_v(is_less_than_beta)) {
+            __m256i q2_org_h, q3_org_h, q1_h, q2_h;
+            __m256i q3_org = __lasx_xvldx(data, img_width_2x + img_width);
+
+            q2_org_h   = __lasx_vext2xv_hu_bu(q2_org);
+            q3_org_h   = __lasx_vext2xv_hu_bu(q3_org);
+
+            AVC_LPF_P0P1P2_OR_Q0Q1Q2(q3_org_h, q0_org_h, p0_org_h, q1_org_h,
+                                     q2_org_h, p1_org_h, q0_h, q1_h, q2_h);
+
+            q0_h = __lasx_xvpickev_b(q0_h, q0_h);
+            q0_h = __lasx_xvpermi_d(q0_h, 0xd8);
+            DUP2_ARG2(__lasx_xvpickev_b, q1_h, q1_h, q2_h, q2_h, q1_h, q2_h);
+            DUP2_ARG2(__lasx_xvpermi_d, q1_h, 0xd8, q2_h, 0xd8, q1_h, q2_h);
+            q0_org = __lasx_xvbitsel_v(q0_org, q0_h, is_less_than_beta);
+            q1_org = __lasx_xvbitsel_v(q1_org, q1_h, is_less_than_beta);
+            q2_org = __lasx_xvbitsel_v(q2_org, q2_h, is_less_than_beta);
+
+            __lasx_xvst(q1_org, data + img_width, 0);
+            __lasx_xvst(q2_org, data + img_width_2x, 0);
+        }
+
+        AVC_LPF_P0_OR_Q0(q0_org_h, p1_org_h, q1_org_h, q0_h);
+
+        /* combine */
+        q0_h = __lasx_xvpickev_b(q0_h, q0_h);
+        q0_h = __lasx_xvpermi_d(q0_h, 0xd8);
+        q0_org = __lasx_xvbitsel_v(q0_org, q0_h, negate_is_less_than_beta);
+
+        __lasx_xvst(q0_org, data, 0);
+    }
+}
+
+void ff_h264_add_pixels4_8_lasx(uint8_t *_dst, int16_t *_src, int stride)
+{
+    __m256i src0, dst0, dst1, dst2, dst3, zero;
+    __m256i tmp0, tmp1;
+    uint8_t* _dst1 = _dst + stride;
+    uint8_t* _dst2 = _dst1 + stride;
+    uint8_t* _dst3 = _dst2 + stride;
+
+    src0 = __lasx_xvld(_src, 0);
+    dst0 = __lasx_xvldrepl_w(_dst, 0);
+    dst1 = __lasx_xvldrepl_w(_dst1, 0);
+    dst2 = __lasx_xvldrepl_w(_dst2, 0);
+    dst3 = __lasx_xvldrepl_w(_dst3, 0);
+    tmp0 = __lasx_xvilvl_w(dst1, dst0);
+    tmp1 = __lasx_xvilvl_w(dst3, dst2);
+    dst0 = __lasx_xvilvl_d(tmp1, tmp0);
+    tmp0 = __lasx_vext2xv_hu_bu(dst0);
+    zero = __lasx_xvldi(0);
+    tmp1 = __lasx_xvadd_h(src0, tmp0);
+    dst0 = __lasx_xvpickev_b(tmp1, tmp1);
+    __lasx_xvstelm_w(dst0, _dst, 0, 0);
+    __lasx_xvstelm_w(dst0, _dst1, 0, 1);
+    __lasx_xvstelm_w(dst0, _dst2, 0, 4);
+    __lasx_xvstelm_w(dst0, _dst3, 0, 5);
+    __lasx_xvst(zero, _src, 0);
+}
+
+void ff_h264_add_pixels8_8_lasx(uint8_t *_dst, int16_t *_src, int stride)
+{
+    __m256i src0, src1, src2, src3;
+    __m256i dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    __m256i tmp0, tmp1, tmp2, tmp3;
+    __m256i zero = __lasx_xvldi(0);
+    uint8_t *_dst1 = _dst + stride;
+    uint8_t *_dst2 = _dst1 + stride;
+    uint8_t *_dst3 = _dst2 + stride;
+    uint8_t *_dst4 = _dst3 + stride;
+    uint8_t *_dst5 = _dst4 + stride;
+    uint8_t *_dst6 = _dst5 + stride;
+    uint8_t *_dst7 = _dst6 + stride;
+
+    src0 = __lasx_xvld(_src, 0);
+    src1 = __lasx_xvld(_src, 32);
+    src2 = __lasx_xvld(_src, 64);
+    src3 = __lasx_xvld(_src, 96);
+    dst0 = __lasx_xvldrepl_d(_dst, 0);
+    dst1 = __lasx_xvldrepl_d(_dst1, 0);
+    dst2 = __lasx_xvldrepl_d(_dst2, 0);
+    dst3 = __lasx_xvldrepl_d(_dst3, 0);
+    dst4 = __lasx_xvldrepl_d(_dst4, 0);
+    dst5 = __lasx_xvldrepl_d(_dst5, 0);
+    dst6 = __lasx_xvldrepl_d(_dst6, 0);
+    dst7 = __lasx_xvldrepl_d(_dst7, 0);
+    tmp0 = __lasx_xvilvl_d(dst1, dst0);
+    tmp1 = __lasx_xvilvl_d(dst3, dst2);
+    tmp2 = __lasx_xvilvl_d(dst5, dst4);
+    tmp3 = __lasx_xvilvl_d(dst7, dst6);
+    dst0 = __lasx_vext2xv_hu_bu(tmp0);
+    dst1 = __lasx_vext2xv_hu_bu(tmp1);
+    dst1 = __lasx_vext2xv_hu_bu(tmp1);
+    dst2 = __lasx_vext2xv_hu_bu(tmp2);
+    dst3 = __lasx_vext2xv_hu_bu(tmp3);
+    tmp0 = __lasx_xvadd_h(src0, dst0);
+    tmp1 = __lasx_xvadd_h(src1, dst1);
+    tmp2 = __lasx_xvadd_h(src2, dst2);
+    tmp3 = __lasx_xvadd_h(src3, dst3);
+    dst1 = __lasx_xvpickev_b(tmp1, tmp0);
+    dst2 = __lasx_xvpickev_b(tmp3, tmp2);
+    __lasx_xvst(zero, _src, 0);
+    __lasx_xvst(zero, _src, 32);
+    __lasx_xvst(zero, _src, 64);
+    __lasx_xvst(zero, _src, 96);
+    __lasx_xvstelm_d(dst1, _dst, 0, 0);
+    __lasx_xvstelm_d(dst1, _dst1, 0, 2);
+    __lasx_xvstelm_d(dst1, _dst2, 0, 1);
+    __lasx_xvstelm_d(dst1, _dst3, 0, 3);
+    __lasx_xvstelm_d(dst2, _dst4, 0, 0);
+    __lasx_xvstelm_d(dst2, _dst5, 0, 2);
+    __lasx_xvstelm_d(dst2, _dst6, 0, 1);
+    __lasx_xvstelm_d(dst2, _dst7, 0, 3);
+}
diff --git a/libavcodec/loongarch/h264dsp_lasx.h b/libavcodec/loongarch/h264dsp_lasx.h
new file mode 100644
index 0000000000..3a506b09df
--- /dev/null
+++ b/libavcodec/loongarch/h264dsp_lasx.h
@@ -0,0 +1,129 @@
+/*
+ * Copyright (c) 2021 Loongson Technology Corporation Limited
+ * Contributed by Shiyou Yin <yinshiyou-hf@loongson.cn>
+ *                Xiwei  Gu  <guxiwei-hf@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef AVCODEC_LOONGARCH_H264DSP_LASX_H
+#define AVCODEC_LOONGARCH_H264DSP_LASX_H
+
+#include "libavcodec/h264dec.h"
+
+void ff_h264_idct_add_8_lsx(uint8_t *dst, int16_t *src, int dst_stride);
+void ff_h264_idct8_add_8_lsx(uint8_t *dst, int16_t *src, int dst_stride);
+void ff_h264_idct_dc_add_8_lsx(uint8_t *dst, int16_t *src, int dst_stride);
+void ff_h264_idct8_dc_add_8_lsx(uint8_t *dst, int16_t *src, int dst_stride);
+void ff_h264_luma_dc_dequant_idct_8_lsx(int16_t *_output, int16_t *_input, int qmul);
+void ff_h264_idct_add16_8_lsx(uint8_t *dst, const int32_t *blk_offset,
+                              int16_t *block, int32_t dst_stride,
+                              const uint8_t nzc[15 * 8]);
+void ff_h264_idct8_add4_8_lsx(uint8_t *dst, const int32_t *blk_offset,
+                              int16_t *block, int32_t dst_stride,
+                              const uint8_t nzc[15 * 8]);
+void ff_h264_idct_add8_8_lsx(uint8_t **dst, const int32_t *blk_offset,
+                             int16_t *block, int32_t dst_stride,
+                             const uint8_t nzc[15 * 8]);
+void ff_h264_idct_add8_422_8_lsx(uint8_t **dst, const int32_t *blk_offset,
+                                 int16_t *block, int32_t dst_stride,
+                                 const uint8_t nzc[15 * 8]);
+void ff_h264_idct_add16_intra_8_lsx(uint8_t *dst, const int32_t *blk_offset,
+                                    int16_t *block, int32_t dst_stride,
+                                    const uint8_t nzc[15 * 8]);
+
+void ff_h264_h_lpf_luma_8_lsx(uint8_t *src, int stride,
+                              int alpha, int beta, int8_t *tc0);
+void ff_h264_v_lpf_luma_8_lsx(uint8_t *src, int stride,
+                              int alpha, int beta, int8_t *tc0);
+void ff_h264_h_lpf_luma_intra_8_lsx(uint8_t *src, int stride,
+                                    int alpha, int beta);
+void ff_h264_v_lpf_luma_intra_8_lsx(uint8_t *src, int stride,
+                                    int alpha, int beta);
+void ff_h264_h_lpf_chroma_8_lsx(uint8_t *src, int stride,
+                                int alpha, int beta, int8_t *tc0);
+void ff_h264_v_lpf_chroma_8_lsx(uint8_t *src, int stride,
+                                int alpha, int beta, int8_t *tc0);
+void ff_h264_h_lpf_chroma_intra_8_lsx(uint8_t *src, int stride,
+                                      int alpha, int beta);
+void ff_h264_v_lpf_chroma_intra_8_lsx(uint8_t *src, int stride,
+                                      int alpha, int beta);
+void ff_biweight_h264_pixels16_8_lsx(uint8_t *dst, uint8_t *src,
+                                     ptrdiff_t stride, int height,
+                                     int log2_denom, int weight_dst,
+                                     int weight_src, int offset_in);
+void ff_biweight_h264_pixels8_8_lsx(uint8_t *dst, uint8_t *src,
+                                    ptrdiff_t stride, int height,
+                                    int log2_denom, int weight_dst,
+                                    int weight_src, int offset);
+void ff_biweight_h264_pixels4_8_lsx(uint8_t *dst, uint8_t *src,
+                                    ptrdiff_t stride, int height,
+                                    int log2_denom, int weight_dst,
+                                    int weight_src, int offset);
+void ff_weight_h264_pixels16_8_lsx(uint8_t *src, ptrdiff_t stride,
+                                   int height, int log2_denom,
+                                   int weight_src, int offset_in);
+void ff_weight_h264_pixels8_8_lsx(uint8_t *src, ptrdiff_t stride,
+                                  int height, int log2_denom,
+                                  int weight_src, int offset);
+void ff_weight_h264_pixels4_8_lsx(uint8_t *src, ptrdiff_t stride,
+                                  int height, int log2_denom,
+                                  int weight_src, int offset);
+void ff_h264_add_pixels4_8_lsx(uint8_t *_dst, int16_t *_src, int stride);
+void ff_h264_add_pixels8_8_lsx(uint8_t *_dst, int16_t *_src, int stride);
+void ff_h264_loop_filter_strength_lsx(int16_t bS[2][4][4], uint8_t nnz[40],
+                                      int8_t ref[2][40], int16_t mv[2][40][2],
+                                      int bidir, int edges, int step,
+                                      int mask_mv0, int mask_mv1, int field);
+
+void ff_h264_h_lpf_luma_8_lasx(uint8_t *src, int stride,
+                               int alpha, int beta, int8_t *tc0);
+void ff_h264_v_lpf_luma_8_lasx(uint8_t *src, int stride,
+                               int alpha, int beta, int8_t *tc0);
+void ff_h264_h_lpf_luma_intra_8_lasx(uint8_t *src, int stride,
+                                     int alpha, int beta);
+void ff_h264_v_lpf_luma_intra_8_lasx(uint8_t *src, int stride,
+                                     int alpha, int beta);
+void ff_biweight_h264_pixels16_8_lasx(uint8_t *dst, uint8_t *src,
+                                      ptrdiff_t stride, int height,
+                                      int log2_denom, int weight_dst,
+                                      int weight_src, int offset_in);
+void ff_biweight_h264_pixels8_8_lasx(uint8_t *dst, uint8_t *src,
+                                     ptrdiff_t stride, int height,
+                                     int log2_denom, int weight_dst,
+                                     int weight_src, int offset);
+void ff_weight_h264_pixels16_8_lasx(uint8_t *src, ptrdiff_t stride,
+                                    int height, int log2_denom,
+                                    int weight_src, int offset_in);
+void ff_weight_h264_pixels8_8_lasx(uint8_t *src, ptrdiff_t stride,
+                                   int height, int log2_denom,
+                                   int weight_src, int offset);
+void ff_h264_add_pixels4_8_lasx(uint8_t *_dst, int16_t *_src, int stride);
+
+void ff_h264_add_pixels8_8_lasx(uint8_t *_dst, int16_t *_src, int stride);
+void ff_h264_idct8_add_8_lasx(uint8_t *dst, int16_t *src, int32_t dst_stride);
+void ff_h264_idct8_dc_add_8_lasx(uint8_t *dst, int16_t *src,
+                                  int32_t dst_stride);
+void ff_h264_idct8_add4_8_lasx(uint8_t *dst, const int32_t *blk_offset,
+                               int16_t *block, int32_t dst_stride,
+                               const uint8_t nzc[15 * 8]);
+void ff_h264_loop_filter_strength_lasx(int16_t bS[2][4][4], uint8_t nnz[40],
+                                       int8_t ref[2][40], int16_t mv[2][40][2],
+                                       int bidir, int edges, int step,
+                                       int mask_mv0, int mask_mv1, int field);
+
+#endif  // #ifndef AVCODEC_LOONGARCH_H264DSP_LASX_H
diff --git a/libavcodec/loongarch/h264idct.S b/libavcodec/loongarch/h264idct.S
new file mode 100644
index 0000000000..4507187802
--- /dev/null
+++ b/libavcodec/loongarch/h264idct.S
@@ -0,0 +1,659 @@
+/*
+ * Loongson LASX optimized h264idct
+ *
+ * Copyright (c) 2022 Loongson Technology Corporation Limited
+ * Contributed by Shiyou Yin <yinshiyou-hf@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "loongson_asm.S"
+
+/*
+ * #define FUNC2(a, b, c)  FUNC3(a, b, c)
+ * #define FUNCC(a) FUNC2(a, BIT_DEPTH, _c)
+ * void FUNCC(ff_h264_idct_add)(uint8_t *_dst, int16_t *_block, int stride)
+ * LSX optimization is enough for this function.
+ */
+function ff_h264_idct_add_8_lsx
+    fld.d         f0,     a1,    0
+    fld.d         f1,     a1,    8
+    fld.d         f2,     a1,    16
+    fld.d         f3,     a1,    24
+    vxor.v        vr7,    vr7,   vr7
+    add.d         t2,     a2,    a2
+    add.d         t3,     t2,    a2
+    vst           vr7,    a1,    0
+    vst           vr7,    a1,    16
+
+    vadd.h        vr4,    vr0,   vr2
+    vsub.h        vr5,    vr0,   vr2
+    vsrai.h       vr6,    vr1,   1
+    vsrai.h       vr7,    vr3,   1
+    vsub.h        vr6,    vr6,   vr3
+    vadd.h        vr7,    vr1,   vr7
+    LSX_BUTTERFLY_4_H vr4, vr5, vr6, vr7,  vr0, vr1, vr2, vr3
+    LSX_TRANSPOSE4x4_H vr0, vr1, vr2, vr3,  vr0, vr1, vr2, vr3,  vr4, vr5
+    vadd.h        vr4,    vr0,   vr2
+    vsub.h        vr5,    vr0,   vr2
+    vsrai.h       vr6,    vr1,   1
+    vsrai.h       vr7,    vr3,   1
+    vsub.h        vr6,    vr6,   vr3
+    vadd.h        vr7,    vr1,   vr7
+    LSX_BUTTERFLY_4_H vr4, vr5, vr6, vr7,  vr0, vr1, vr2, vr3
+
+    fld.s         f4,     a0,    0
+    fldx.s        f5,     a0,    a2
+    fldx.s        f6,     a0,    t2
+    fldx.s        f7,     a0,    t3
+
+    vsrari.h      vr0,    vr0,   6
+    vsrari.h      vr1,    vr1,   6
+    vsrari.h      vr2,    vr2,   6
+    vsrari.h      vr3,    vr3,   6
+
+    vsllwil.hu.bu vr4,    vr4,   0
+    vsllwil.hu.bu vr5,    vr5,   0
+    vsllwil.hu.bu vr6,    vr6,   0
+    vsllwil.hu.bu vr7,    vr7,   0
+    vadd.h        vr0,    vr0,   vr4
+    vadd.h        vr1,    vr1,   vr5
+    vadd.h        vr2,    vr2,   vr6
+    vadd.h        vr3,    vr3,   vr7
+    vssrarni.bu.h vr1,    vr0,   0
+    vssrarni.bu.h vr3,    vr2,   0
+
+    vbsrl.v       vr0,    vr1,   8
+    vbsrl.v       vr2,    vr3,   8
+    fst.s         f1,     a0,    0
+    fstx.s        f0,     a0,    a2
+    fstx.s        f3,     a0,    t2
+    fstx.s        f2,     a0,    t3
+endfunc
+
+/*
+ * #define FUNC2(a, b, c)  FUNC3(a, b, c)
+ * #define FUNCC(a) FUNC2(a, BIT_DEPTH, _c)
+ * void FUNCC(ff_h264_idct8_add)(uint8_t *_dst, int16_t *_block, int stride)
+ */
+function ff_h264_idct8_add_8_lsx
+    ld.h          t0,     a1,    0
+    add.d         t2,     a2,    a2
+    add.d         t3,     t2,    a2
+    add.d         t4,     t3,    a2
+    add.d         t5,     t4,    a2
+    add.d         t6,     t5,    a2
+    add.d         t7,     t6,    a2
+    addi.w        t0,     t0,    32
+    st.h          t0,     a1,    0
+
+    vld           vr0,    a1,    0
+    vld           vr1,    a1,    16
+    vld           vr2,    a1,    32
+    vld           vr3,    a1,    48
+    vld           vr4,    a1,    64
+    vld           vr5,    a1,    80
+    vld           vr6,    a1,    96
+    vld           vr7,    a1,    112
+    vxor.v        vr8,    vr8,   vr8
+    vst           vr8,    a1,    0
+    vst           vr8,    a1,    16
+    vst           vr8,    a1,    32
+    vst           vr8,    a1,    48
+    vst           vr8,    a1,    64
+    vst           vr8,    a1,    80
+    vst           vr8,    a1,    96
+    vst           vr8,    a1,    112
+
+    vadd.h        vr18,   vr0,   vr4
+    vsub.h        vr19,   vr0,   vr4
+    vsrai.h       vr20,   vr2,   1
+    vsrai.h       vr21,   vr6,   1
+    vsub.h        vr20,   vr20,  vr6
+    vadd.h        vr21,   vr21,  vr2
+    LSX_BUTTERFLY_4_H vr18, vr19, vr20, vr21,  vr10, vr12, vr14, vr16
+    vsrai.h       vr11,   vr7,   1
+    vsrai.h       vr13,   vr3,   1
+    vsrai.h       vr15,   vr5,   1
+    vsrai.h       vr17,   vr1,   1
+    vsub.h        vr11,   vr5,   vr11
+    vsub.h        vr13,   vr7,   vr13
+    vadd.h        vr15,   vr7,   vr15
+    vadd.h        vr17,   vr5,   vr17
+    vsub.h        vr11,   vr11,  vr7
+    vsub.h        vr13,   vr13,  vr3
+    vadd.h        vr15,   vr15,  vr5
+    vadd.h        vr17,   vr17,  vr1
+    vsub.h        vr11,   vr11,  vr3
+    vadd.h        vr13,   vr13,  vr1
+    vsub.h        vr15,   vr15,  vr1
+    vadd.h        vr17,   vr17,  vr3
+    vsrai.h       vr18,   vr11,  2
+    vsrai.h       vr19,   vr13,  2
+    vsrai.h       vr20,   vr15,  2
+    vsrai.h       vr21,   vr17,  2
+    vadd.h        vr11,   vr11,  vr21
+    vadd.h        vr13,   vr13,  vr20
+    vsub.h        vr15,   vr19,  vr15
+    vsub.h        vr17,   vr17,  vr18
+    LSX_BUTTERFLY_8_H vr10, vr16, vr12, vr14, vr13, vr15, vr11, vr17, \
+                      vr0,  vr3,  vr1,  vr2,  vr5,  vr6,  vr4,  vr7
+
+    LSX_TRANSPOSE8x8_H vr0,  vr1,  vr2,  vr3,  vr4,  vr5,  vr6,  vr7, \
+                       vr0,  vr1,  vr2,  vr3,  vr4,  vr5,  vr6,  vr7, \
+                       vr10, vr11, vr12, vr13, vr14, vr15, vr16, vr17
+    vexth.w.h     vr20,   vr0
+    vexth.w.h     vr21,   vr1
+    vexth.w.h     vr22,   vr2
+    vexth.w.h     vr23,   vr3
+    vexth.w.h     vr8,    vr4
+    vexth.w.h     vr9,    vr5
+    vexth.w.h     vr18,   vr6
+    vexth.w.h     vr19,   vr7
+    vsllwil.w.h   vr0,    vr0,   0
+    vsllwil.w.h   vr1,    vr1,   0
+    vsllwil.w.h   vr2,    vr2,   0
+    vsllwil.w.h   vr3,    vr3,   0
+    vsllwil.w.h   vr4,    vr4,   0
+    vsllwil.w.h   vr5,    vr5,   0
+    vsllwil.w.h   vr6,    vr6,   0
+    vsllwil.w.h   vr7,    vr7,   0
+
+    vadd.w        vr11,   vr0,   vr4
+    vsub.w        vr13,   vr0,   vr4
+    vsrai.w       vr15,   vr2,   1
+    vsrai.w       vr17,   vr6,   1
+    vsub.w        vr15,   vr15,  vr6
+    vadd.w        vr17,   vr17,  vr2
+    LSX_BUTTERFLY_4_W vr11, vr13, vr15, vr17,  vr10, vr12, vr14, vr16
+    vsrai.w       vr11,   vr7,   1
+    vsrai.w       vr13,   vr3,   1
+    vsrai.w       vr15,   vr5,   1
+    vsrai.w       vr17,   vr1,   1
+    vsub.w        vr11,   vr5,   vr11
+    vsub.w        vr13,   vr7,   vr13
+    vadd.w        vr15,   vr7,   vr15
+    vadd.w        vr17,   vr5,   vr17
+    vsub.w        vr11,   vr11,  vr7
+    vsub.w        vr13,   vr13,  vr3
+    vadd.w        vr15,   vr15,  vr5
+    vadd.w        vr17,   vr17,  vr1
+    vsub.w        vr11,   vr11,  vr3
+    vadd.w        vr13,   vr13,  vr1
+    vsub.w        vr15,   vr15,  vr1
+    vadd.w        vr17,   vr17,  vr3
+    vsrai.w       vr0,    vr11,  2
+    vsrai.w       vr1,    vr13,  2
+    vsrai.w       vr2,    vr15,  2
+    vsrai.w       vr3,    vr17,  2
+    vadd.w        vr11,   vr11,  vr3
+    vadd.w        vr13,   vr13,  vr2
+    vsub.w        vr15,   vr1,   vr15
+    vsub.w        vr17,   vr17,  vr0
+    LSX_BUTTERFLY_8_W vr10, vr12, vr14, vr16, vr11, vr13, vr15, vr17, \
+                      vr0,  vr1,  vr2,  vr3,  vr4,  vr5,  vr6,  vr7
+
+    vadd.w        vr11,    vr20,  vr8
+    vsub.w        vr13,    vr20,  vr8
+    vsrai.w       vr15,    vr22,  1
+    vsrai.w       vr17,    vr18,  1
+    vsub.w        vr15,    vr15,  vr18
+    vadd.w        vr17,    vr17,  vr22
+    LSX_BUTTERFLY_4_W vr11, vr13, vr15, vr17,  vr10, vr12, vr14, vr16
+    vsrai.w       vr11,   vr19,  1
+    vsrai.w       vr13,   vr23,  1
+    vsrai.w       vr15,   vr9,   1
+    vsrai.w       vr17,   vr21,  1
+    vsub.w        vr11,   vr9,   vr11
+    vsub.w        vr13,   vr19,  vr13
+    vadd.w        vr15,   vr19,  vr15
+    vadd.w        vr17,   vr9,   vr17
+    vsub.w        vr11,   vr11,  vr19
+    vsub.w        vr13,   vr13,  vr23
+    vadd.w        vr15,   vr15,  vr9
+    vadd.w        vr17,   vr17,  vr21
+    vsub.w        vr11,   vr11,  vr23
+    vadd.w        vr13,   vr13,  vr21
+    vsub.w        vr15,   vr15,  vr21
+    vadd.w        vr17,   vr17,  vr23
+    vsrai.w       vr20,   vr11,  2
+    vsrai.w       vr21,   vr13,  2
+    vsrai.w       vr22,   vr15,  2
+    vsrai.w       vr23,   vr17,  2
+    vadd.w        vr11,   vr11,  vr23
+    vadd.w        vr13,   vr13,  vr22
+    vsub.w        vr15,   vr21,  vr15
+    vsub.w        vr17,   vr17,  vr20
+    LSX_BUTTERFLY_8_W vr10, vr12, vr14, vr16, vr11, vr13, vr15, vr17, \
+                      vr20, vr21, vr22, vr23, vr8, vr9, vr18, vr19
+
+    vld           vr10,   a0,    0
+    vldx          vr11,   a0,    a2
+    vldx          vr12,   a0,    t2
+    vldx          vr13,   a0,    t3
+    vldx          vr14,   a0,    t4
+    vldx          vr15,   a0,    t5
+    vldx          vr16,   a0,    t6
+    vldx          vr17,   a0,    t7
+    vsrani.h.w    vr20,   vr0,   6
+    vsrani.h.w    vr21,   vr1,   6
+    vsrani.h.w    vr22,   vr2,   6
+    vsrani.h.w    vr23,   vr3,   6
+    vsrani.h.w    vr8,    vr4,   6
+    vsrani.h.w    vr9,    vr5,   6
+    vsrani.h.w    vr18,   vr6,   6
+    vsrani.h.w    vr19,   vr7,   6
+    vsllwil.hu.bu vr10,   vr10,  0
+    vsllwil.hu.bu vr11,   vr11,  0
+    vsllwil.hu.bu vr12,   vr12,  0
+    vsllwil.hu.bu vr13,   vr13,  0
+    vsllwil.hu.bu vr14,   vr14,  0
+    vsllwil.hu.bu vr15,   vr15,  0
+    vsllwil.hu.bu vr16,   vr16,  0
+    vsllwil.hu.bu vr17,   vr17,  0
+
+    vadd.h        vr0,    vr20,  vr10
+    vadd.h        vr1,    vr21,  vr11
+    vadd.h        vr2,    vr22,  vr12
+    vadd.h        vr3,    vr23,  vr13
+    vadd.h        vr4,    vr8,   vr14
+    vadd.h        vr5,    vr9,   vr15
+    vadd.h        vr6,    vr18,  vr16
+    vadd.h        vr7,    vr19,  vr17
+    vssrarni.bu.h vr1,    vr0,   0
+    vssrarni.bu.h vr3,    vr2,   0
+    vssrarni.bu.h vr5,    vr4,   0
+    vssrarni.bu.h vr7,    vr6,   0
+    vbsrl.v       vr0,    vr1,   8
+    vbsrl.v       vr2,    vr3,   8
+    vbsrl.v       vr4,    vr5,   8
+    vbsrl.v       vr6,    vr7,   8
+    fst.d         f1,     a0,    0
+    fstx.d        f0,     a0,    a2
+    fstx.d        f3,     a0,    t2
+    fstx.d        f2,     a0,    t3
+    fstx.d        f5,     a0,    t4
+    fstx.d        f4,     a0,    t5
+    fstx.d        f7,     a0,    t6
+    fstx.d        f6,     a0,    t7
+endfunc
+
+/*
+ * #define FUNC2(a, b, c)  FUNC3(a, b, c)
+ * #define FUNCC(a) FUNC2(a, BIT_DEPTH, _c)
+ * void FUNCC(ff_h264_idct8_add)(uint8_t *_dst, int16_t *_block, int stride)
+ */
+function ff_h264_idct8_add_8_lasx
+    ld.h          t0,     a1,    0
+    add.d         t2,     a2,    a2
+    add.d         t3,     t2,    a2
+    add.d         t4,     t3,    a2
+    add.d         t5,     t4,    a2
+    add.d         t6,     t5,    a2
+    add.d         t7,     t6,    a2
+    addi.w        t0,     t0,    32
+    st.h          t0,     a1,    0
+
+    vld           vr0,    a1,    0
+    vld           vr1,    a1,    16
+    vld           vr2,    a1,    32
+    vld           vr3,    a1,    48
+    vld           vr4,    a1,    64
+    vld           vr5,    a1,    80
+    vld           vr6,    a1,    96
+    vld           vr7,    a1,    112
+    xvxor.v       xr8,    xr8,   xr8
+    xvst          xr8,    a1,    0
+    xvst          xr8,    a1,    32
+    xvst          xr8,    a1,    64
+    xvst          xr8,    a1,    96
+
+    vadd.h        vr18,   vr0,   vr4
+    vsub.h        vr19,   vr0,   vr4
+    vsrai.h       vr20,   vr2,   1
+    vsrai.h       vr21,   vr6,   1
+    vsub.h        vr20,   vr20,  vr6
+    vadd.h        vr21,   vr21,  vr2
+    LSX_BUTTERFLY_4_H vr18, vr19, vr20, vr21,  vr10, vr12, vr14, vr16
+    vsrai.h       vr11,   vr7,   1
+    vsrai.h       vr13,   vr3,   1
+    vsrai.h       vr15,   vr5,   1
+    vsrai.h       vr17,   vr1,   1
+    vsub.h        vr11,   vr5,   vr11
+    vsub.h        vr13,   vr7,   vr13
+    vadd.h        vr15,   vr7,   vr15
+    vadd.h        vr17,   vr5,   vr17
+    vsub.h        vr11,   vr11,  vr7
+    vsub.h        vr13,   vr13,  vr3
+    vadd.h        vr15,   vr15,  vr5
+    vadd.h        vr17,   vr17,  vr1
+    vsub.h        vr11,   vr11,  vr3
+    vadd.h        vr13,   vr13,  vr1
+    vsub.h        vr15,   vr15,  vr1
+    vadd.h        vr17,   vr17,  vr3
+    vsrai.h       vr18,   vr11,  2
+    vsrai.h       vr19,   vr13,  2
+    vsrai.h       vr20,   vr15,  2
+    vsrai.h       vr21,   vr17,  2
+    vadd.h        vr11,   vr11,  vr21
+    vadd.h        vr13,   vr13,  vr20
+    vsub.h        vr15,   vr19,  vr15
+    vsub.h        vr17,   vr17,  vr18
+    LSX_BUTTERFLY_8_H vr10, vr16, vr12, vr14, vr13, vr15, vr11, vr17, \
+                      vr0,  vr3,  vr1,  vr2,  vr5,  vr6,  vr4,  vr7
+
+    LSX_TRANSPOSE8x8_H vr0,  vr1,  vr2,  vr3,  vr4,  vr5,  vr6,  vr7, \
+                       vr0,  vr1,  vr2,  vr3,  vr4,  vr5,  vr6,  vr7, \
+                       vr10, vr11, vr12, vr13, vr14, vr15, vr16, vr17
+    vext2xv.w.h   xr0,    xr0
+    vext2xv.w.h   xr1,    xr1
+    vext2xv.w.h   xr2,    xr2
+    vext2xv.w.h   xr3,    xr3
+    vext2xv.w.h   xr4,    xr4
+    vext2xv.w.h   xr5,    xr5
+    vext2xv.w.h   xr6,    xr6
+    vext2xv.w.h   xr7,    xr7
+
+    xvadd.w       xr11,   xr0,   xr4
+    xvsub.w       xr13,   xr0,   xr4
+    xvsrai.w      xr15,   xr2,   1
+    xvsrai.w      xr17,   xr6,   1
+    xvsub.w       xr15,   xr15,  xr6
+    xvadd.w       xr17,   xr17,  xr2
+    LASX_BUTTERFLY_4_W xr11, xr13, xr15, xr17,  xr10, xr12, xr14, xr16
+    xvsrai.w      xr11,   xr7,   1
+    xvsrai.w      xr13,   xr3,   1
+    xvsrai.w      xr15,   xr5,   1
+    xvsrai.w      xr17,   xr1,   1
+    xvsub.w       xr11,   xr5,   xr11
+    xvsub.w       xr13,   xr7,   xr13
+    xvadd.w       xr15,   xr7,   xr15
+    xvadd.w       xr17,   xr5,   xr17
+    xvsub.w       xr11,   xr11,  xr7
+    xvsub.w       xr13,   xr13,  xr3
+    xvadd.w       xr15,   xr15,  xr5
+    xvadd.w       xr17,   xr17,  xr1
+    xvsub.w       xr11,   xr11,  xr3
+    xvadd.w       xr13,   xr13,  xr1
+    xvsub.w       xr15,   xr15,  xr1
+    xvadd.w       xr17,   xr17,  xr3
+    xvsrai.w      xr0,    xr11,  2
+    xvsrai.w      xr1,    xr13,  2
+    xvsrai.w      xr2,    xr15,  2
+    xvsrai.w      xr3,    xr17,  2
+    xvadd.w       xr11,   xr11,  xr3
+    xvadd.w       xr13,   xr13,  xr2
+    xvsub.w       xr15,   xr1,   xr15
+    xvsub.w       xr17,   xr17,  xr0
+    LASX_BUTTERFLY_8_W xr10, xr12, xr14, xr16, xr11, xr13, xr15, xr17, \
+                       xr0,  xr1,  xr2,  xr3,  xr4,  xr5,  xr6,  xr7
+
+    vld           vr10,   a0,    0
+    vldx          vr11,   a0,    a2
+    vldx          vr12,   a0,    t2
+    vldx          vr13,   a0,    t3
+    vldx          vr14,   a0,    t4
+    vldx          vr15,   a0,    t5
+    vldx          vr16,   a0,    t6
+    vldx          vr17,   a0,    t7
+    xvldi         xr8,    0x806     //"xvldi.w xr8 6"
+    xvsran.h.w    xr0,    xr0,   xr8
+    xvsran.h.w    xr1,    xr1,   xr8
+    xvsran.h.w    xr2,    xr2,   xr8
+    xvsran.h.w    xr3,    xr3,   xr8
+    xvsran.h.w    xr4,    xr4,   xr8
+    xvsran.h.w    xr5,    xr5,   xr8
+    xvsran.h.w    xr6,    xr6,   xr8
+    xvsran.h.w    xr7,    xr7,   xr8
+    xvpermi.d     xr0,    xr0,   0x08
+    xvpermi.d     xr1,    xr1,   0x08
+    xvpermi.d     xr2,    xr2,   0x08
+    xvpermi.d     xr3,    xr3,   0x08
+    xvpermi.d     xr4,    xr4,   0x08
+    xvpermi.d     xr5,    xr5,   0x08
+    xvpermi.d     xr6,    xr6,   0x08
+    xvpermi.d     xr7,    xr7,   0x08
+
+    vsllwil.hu.bu vr10,   vr10,  0
+    vsllwil.hu.bu vr11,   vr11,  0
+    vsllwil.hu.bu vr12,   vr12,  0
+    vsllwil.hu.bu vr13,   vr13,  0
+    vsllwil.hu.bu vr14,   vr14,  0
+    vsllwil.hu.bu vr15,   vr15,  0
+    vsllwil.hu.bu vr16,   vr16,  0
+    vsllwil.hu.bu vr17,   vr17,  0
+
+    vadd.h        vr0,    vr0,   vr10
+    vadd.h        vr1,    vr1,   vr11
+    vadd.h        vr2,    vr2,   vr12
+    vadd.h        vr3,    vr3,   vr13
+    vadd.h        vr4,    vr4,   vr14
+    vadd.h        vr5,    vr5,   vr15
+    vadd.h        vr6,    vr6,   vr16
+    vadd.h        vr7,    vr7,   vr17
+    vssrarni.bu.h vr1,    vr0,   0
+    vssrarni.bu.h vr3,    vr2,   0
+    vssrarni.bu.h vr5,    vr4,   0
+    vssrarni.bu.h vr7,    vr6,   0
+    vbsrl.v       vr0,    vr1,   8
+    vbsrl.v       vr2,    vr3,   8
+    vbsrl.v       vr4,    vr5,   8
+    vbsrl.v       vr6,    vr7,   8
+    fst.d         f1,     a0,    0
+    fstx.d        f0,     a0,    a2
+    fstx.d        f3,     a0,    t2
+    fstx.d        f2,     a0,    t3
+    fstx.d        f5,     a0,    t4
+    fstx.d        f4,     a0,    t5
+    fstx.d        f7,     a0,    t6
+    fstx.d        f6,     a0,    t7
+endfunc
+
+/*
+ * #define FUNC2(a, b, c)  FUNC3(a, b, c)
+ * #define FUNCC(a) FUNC2(a, BIT_DEPTH, _c)
+ * void FUNCC(ff_h264_idct_dc_add)(uint8_t *_dst, int16_t *_block, int stride)
+ * LSX optimization is enough for this function.
+ */
+function ff_h264_idct_dc_add_8_lsx
+    vldrepl.h     vr4,    a1,    0
+    add.d         t2,     a2,    a2
+    add.d         t3,     t2,    a2
+    fld.s         f0,     a0,    0
+    fldx.s        f1,     a0,    a2
+    fldx.s        f2,     a0,    t2
+    fldx.s        f3,     a0,    t3
+    st.h          zero,   a1,    0
+
+    vsrari.h      vr4,    vr4,   6
+    vilvl.w       vr0,    vr1,   vr0
+    vilvl.w       vr1,    vr3,   vr2
+    vsllwil.hu.bu vr0,    vr0,   0
+    vsllwil.hu.bu vr1,    vr1,   0
+    vadd.h        vr0,    vr0,   vr4
+    vadd.h        vr1,    vr1,   vr4
+    vssrarni.bu.h vr1,    vr0,   0
+
+    vbsrl.v       vr2,    vr1,   4
+    vbsrl.v       vr3,    vr1,   8
+    vbsrl.v       vr4,    vr1,   12
+    fst.s         f1,     a0,    0
+    fstx.s        f2,     a0,    a2
+    fstx.s        f3,     a0,    t2
+    fstx.s        f4,     a0,    t3
+endfunc
+
+/*
+ * #define FUNC2(a, b, c)  FUNC3(a, b, c)
+ * #define FUNCC(a) FUNC2(a, BIT_DEPTH, _c)
+ * void FUNCC(ff_h264_idct8_dc_add)(uint8_t *_dst, int16_t *_block, int stride)
+ */
+function ff_h264_idct8_dc_add_8_lsx
+    vldrepl.h     vr8,    a1,    0
+    add.d         t2,     a2,    a2
+    add.d         t3,     t2,    a2
+    add.d         t4,     t3,    a2
+    add.d         t5,     t4,    a2
+    add.d         t6,     t5,    a2
+    add.d         t7,     t6,    a2
+
+    fld.d         f0,     a0,    0
+    fldx.d        f1,     a0,    a2
+    fldx.d        f2,     a0,    t2
+    fldx.d        f3,     a0,    t3
+    fldx.d        f4,     a0,    t4
+    fldx.d        f5,     a0,    t5
+    fldx.d        f6,     a0,    t6
+    fldx.d        f7,     a0,    t7
+    st.h          zero,   a1,    0
+
+    vsrari.h      vr8,    vr8,   6
+    vsllwil.hu.bu vr0,    vr0,   0
+    vsllwil.hu.bu vr1,    vr1,   0
+    vsllwil.hu.bu vr2,    vr2,   0
+    vsllwil.hu.bu vr3,    vr3,   0
+    vsllwil.hu.bu vr4,    vr4,   0
+    vsllwil.hu.bu vr5,    vr5,   0
+    vsllwil.hu.bu vr6,    vr6,   0
+    vsllwil.hu.bu vr7,    vr7,   0
+    vadd.h        vr0,    vr0,   vr8
+    vadd.h        vr1,    vr1,   vr8
+    vadd.h        vr2,    vr2,   vr8
+    vadd.h        vr3,    vr3,   vr8
+    vadd.h        vr4,    vr4,   vr8
+    vadd.h        vr5,    vr5,   vr8
+    vadd.h        vr6,    vr6,   vr8
+    vadd.h        vr7,    vr7,   vr8
+    vssrarni.bu.h vr1,    vr0,   0
+    vssrarni.bu.h vr3,    vr2,   0
+    vssrarni.bu.h vr5,    vr4,   0
+    vssrarni.bu.h vr7,    vr6,   0
+
+    vbsrl.v       vr0,    vr1,   8
+    vbsrl.v       vr2,    vr3,   8
+    vbsrl.v       vr4,    vr5,   8
+    vbsrl.v       vr6,    vr7,   8
+    fst.d         f1,     a0,    0
+    fstx.d        f0,     a0,    a2
+    fstx.d        f3,     a0,    t2
+    fstx.d        f2,     a0,    t3
+    fstx.d        f5,     a0,    t4
+    fstx.d        f4,     a0,    t5
+    fstx.d        f7,     a0,    t6
+    fstx.d        f6,     a0,    t7
+endfunc
+function ff_h264_idct8_dc_add_8_lasx
+    xvldrepl.h    xr8,    a1,    0
+    add.d         t2,     a2,    a2
+    add.d         t3,     t2,    a2
+    add.d         t4,     t3,    a2
+    add.d         t5,     t4,    a2
+    add.d         t6,     t5,    a2
+    add.d         t7,     t6,    a2
+
+    fld.d         f0,     a0,    0
+    fldx.d        f1,     a0,    a2
+    fldx.d        f2,     a0,    t2
+    fldx.d        f3,     a0,    t3
+    fldx.d        f4,     a0,    t4
+    fldx.d        f5,     a0,    t5
+    fldx.d        f6,     a0,    t6
+    fldx.d        f7,     a0,    t7
+    st.h          zero,   a1,    0
+
+    xvsrari.h     xr8,    xr8,   6
+    xvpermi.q     xr1,    xr0,   0x20
+    xvpermi.q     xr3,    xr2,   0x20
+    xvpermi.q     xr5,    xr4,   0x20
+    xvpermi.q     xr7,    xr6,   0x20
+    xvsllwil.hu.bu xr1,   xr1,   0
+    xvsllwil.hu.bu xr3,   xr3,   0
+    xvsllwil.hu.bu xr5,   xr5,   0
+    xvsllwil.hu.bu xr7,   xr7,   0
+    xvadd.h       xr1,    xr1,   xr8
+    xvadd.h       xr3,    xr3,   xr8
+    xvadd.h       xr5,    xr5,   xr8
+    xvadd.h       xr7,    xr7,   xr8
+
+    xvssrarni.bu.h xr3,   xr1,   0
+    xvssrarni.bu.h xr7,   xr5,   0
+
+    xvpermi.q     xr1,    xr3,   0x11
+    xvpermi.q     xr5,    xr7,   0x11
+    xvbsrl.v      xr0,    xr1,   8
+    xvbsrl.v      xr2,    xr3,   8
+    xvbsrl.v      xr4,    xr5,   8
+    xvbsrl.v      xr6,    xr7,   8
+
+    fst.d         f3,     a0,    0
+    fstx.d        f1,     a0,    a2
+    fstx.d        f2,     a0,    t2
+    fstx.d        f0,     a0,    t3
+    fstx.d        f7,     a0,    t4
+    fstx.d        f5,     a0,    t5
+    fstx.d        f6,     a0,    t6
+    fstx.d        f4,     a0,    t7
+endfunc
+
+/**
+ * IDCT transforms the 16 dc values and dequantizes them.
+ * @param qmul quantization parameter
+ * void FUNCC(ff_h264_luma_dc_dequant_idct)(int16_t *_output, int16_t *_input, int qmul){
+ * LSX optimization is enough for this function.
+ */
+function ff_h264_luma_dc_dequant_idct_8_lsx
+    vld           vr0,    a1,    0
+    vld           vr1,    a1,    8
+    vld           vr2,    a1,    16
+    vld           vr3,    a1,    24
+    vreplgr2vr.w  vr8,    a2
+    LSX_TRANSPOSE4x4_H vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr7, vr9, vr10
+    LSX_BUTTERFLY_4_H  vr4, vr6, vr7, vr5, vr0, vr3, vr2, vr1
+    LSX_BUTTERFLY_4_H  vr0, vr1, vr2, vr3, vr4, vr7, vr6, vr5
+    LSX_TRANSPOSE4x4_H vr4, vr5, vr6, vr7, vr0, vr1, vr2, vr3, vr9, vr10
+    LSX_BUTTERFLY_4_H  vr0, vr1, vr3, vr2, vr4, vr7, vr6, vr5
+    LSX_BUTTERFLY_4_H  vr4, vr5, vr6, vr7, vr0, vr1, vr2, vr3
+    vsllwil.w.h   vr0,    vr0,   0
+    vsllwil.w.h   vr1,    vr1,   0
+    vsllwil.w.h   vr2,    vr2,   0
+    vsllwil.w.h   vr3,    vr3,   0
+    vmul.w        vr0,    vr0,   vr8
+    vmul.w        vr1,    vr1,   vr8
+    vmul.w        vr2,    vr2,   vr8
+    vmul.w        vr3,    vr3,   vr8
+    vsrarni.h.w   vr1,    vr0,   8
+    vsrarni.h.w   vr3,    vr2,   8
+
+    vstelm.h      vr1,    a0,    0,   0
+    vstelm.h      vr1,    a0,    32,  4
+    vstelm.h      vr1,    a0,    64,  1
+    vstelm.h      vr1,    a0,    96,  5
+    vstelm.h      vr3,    a0,    128, 0
+    vstelm.h      vr3,    a0,    160, 4
+    vstelm.h      vr3,    a0,    192, 1
+    vstelm.h      vr3,    a0,    224, 5
+    addi.d        a0,     a0,    256
+    vstelm.h      vr1,    a0,    0,   2
+    vstelm.h      vr1,    a0,    32,  6
+    vstelm.h      vr1,    a0,    64,  3
+    vstelm.h      vr1,    a0,    96,  7
+    vstelm.h      vr3,    a0,    128, 2
+    vstelm.h      vr3,    a0,    160, 6
+    vstelm.h      vr3,    a0,    192, 3
+    vstelm.h      vr3,    a0,    224, 7
+endfunc
+
diff --git a/libavcodec/loongarch/h264idct_c.c b/libavcodec/loongarch/h264idct_c.c
new file mode 100644
index 0000000000..f6c934521e
--- /dev/null
+++ b/libavcodec/loongarch/h264idct_c.c
@@ -0,0 +1,183 @@
+/*
+ * Loongson LASX optimized h264idct
+ *
+ * Copyright (c) 2021 Loongson Technology Corporation Limited
+ * Contributed by Shiyou Yin <yinshiyou-hf@loongson.cn>
+ *                Xiwei  Gu  <guxiwei-hf@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "h264dsp_lasx.h"
+#include "libavcodec/bit_depth_template.c"
+
+void ff_h264_idct_add16_8_lsx(uint8_t *dst, const int32_t *blk_offset,
+                              int16_t *block, int32_t dst_stride,
+                              const uint8_t nzc[15 * 8])
+{
+    int32_t i;
+
+    for (i = 0; i < 16; i++) {
+        int32_t nnz = nzc[scan8[i]];
+
+        if (nnz == 1 && ((dctcoef *) block)[i * 16]) {
+            ff_h264_idct_dc_add_8_lsx(dst + blk_offset[i],
+                                      block + i * 16 * sizeof(pixel),
+                                      dst_stride);
+	} else if (nnz) {
+            ff_h264_idct_add_8_lsx(dst + blk_offset[i],
+                                   block + i * 16 * sizeof(pixel),
+                                   dst_stride);
+        }
+    }
+}
+
+void ff_h264_idct8_add4_8_lsx(uint8_t *dst, const int32_t *blk_offset,
+                              int16_t *block, int32_t dst_stride,
+                              const uint8_t nzc[15 * 8])
+{
+    int32_t cnt;
+
+    for (cnt = 0; cnt < 16; cnt += 4) {
+        int32_t nnz = nzc[scan8[cnt]];
+
+        if (nnz == 1 && ((dctcoef *) block)[cnt * 16]) {
+            ff_h264_idct8_dc_add_8_lsx(dst + blk_offset[cnt],
+                                        block + cnt * 16 * sizeof(pixel),
+                                        dst_stride);
+        } else if (nnz) {
+            ff_h264_idct8_add_8_lsx(dst + blk_offset[cnt],
+                                     block + cnt * 16 * sizeof(pixel),
+                                     dst_stride);
+        }
+    }
+}
+
+void ff_h264_idct8_add4_8_lasx(uint8_t *dst, const int32_t *blk_offset,
+                               int16_t *block, int32_t dst_stride,
+                               const uint8_t nzc[15 * 8])
+{
+    int32_t cnt;
+
+    for (cnt = 0; cnt < 16; cnt += 4) {
+        int32_t nnz = nzc[scan8[cnt]];
+
+        if (nnz == 1 && ((dctcoef *) block)[cnt * 16]) {
+            ff_h264_idct8_dc_add_8_lasx(dst + blk_offset[cnt],
+                                        block + cnt * 16 * sizeof(pixel),
+                                        dst_stride);
+        } else if (nnz) {
+            ff_h264_idct8_add_8_lasx(dst + blk_offset[cnt],
+                                     block + cnt * 16 * sizeof(pixel),
+                                     dst_stride);
+        }
+    }
+}
+
+void ff_h264_idct_add8_8_lsx(uint8_t **dst, const int32_t *blk_offset,
+                             int16_t *block, int32_t dst_stride,
+                             const uint8_t nzc[15 * 8])
+{
+    int32_t i;
+
+    for (i = 16; i < 20; i++) {
+        if (nzc[scan8[i]])
+            ff_h264_idct_add_8_lsx(dst[0] + blk_offset[i],
+                                   block + i * 16 * sizeof(pixel),
+                                   dst_stride);
+        else if (((dctcoef *) block)[i * 16])
+            ff_h264_idct_dc_add_8_lsx(dst[0] + blk_offset[i],
+                                      block + i * 16 * sizeof(pixel),
+                                      dst_stride);
+    }
+    for (i = 32; i < 36; i++) {
+        if (nzc[scan8[i]])
+            ff_h264_idct_add_8_lsx(dst[1] + blk_offset[i],
+                                   block + i * 16 * sizeof(pixel),
+                                   dst_stride);
+        else if (((dctcoef *) block)[i * 16])
+            ff_h264_idct_dc_add_8_lsx(dst[1] + blk_offset[i],
+                                      block + i * 16 * sizeof(pixel),
+                                      dst_stride);
+    }
+}
+
+void ff_h264_idct_add8_422_8_lsx(uint8_t **dst, const int32_t *blk_offset,
+                                 int16_t *block, int32_t dst_stride,
+                                 const uint8_t nzc[15 * 8])
+{
+    int32_t i;
+
+    for (i = 16; i < 20; i++) {
+        if (nzc[scan8[i]])
+            ff_h264_idct_add_8_lsx(dst[0] + blk_offset[i],
+                                   block + i * 16 * sizeof(pixel),
+                                   dst_stride);
+        else if (((dctcoef *) block)[i * 16])
+            ff_h264_idct_dc_add_8_lsx(dst[0] + blk_offset[i],
+                                      block + i * 16 * sizeof(pixel),
+                                      dst_stride);
+    }
+    for (i = 20; i < 24; i++) {
+        if (nzc[scan8[i + 4]])
+            ff_h264_idct_add_8_lsx(dst[0] + blk_offset[i + 4],
+                                   block + i * 16 * sizeof(pixel),
+                                   dst_stride);
+        else if (((dctcoef *) block)[i * 16])
+            ff_h264_idct_dc_add_8_lsx(dst[0] + blk_offset[i + 4],
+                                      block + i * 16 * sizeof(pixel),
+                                      dst_stride);
+    }
+    for (i = 32; i < 36; i++) {
+        if (nzc[scan8[i]])
+            ff_h264_idct_add_8_lsx(dst[1] + blk_offset[i],
+                                   block + i * 16 * sizeof(pixel),
+                                   dst_stride);
+        else if (((dctcoef *) block)[i * 16])
+            ff_h264_idct_dc_add_8_lsx(dst[1] + blk_offset[i],
+                                      block + i * 16 * sizeof(pixel),
+                                      dst_stride);
+    }
+    for (i = 36; i < 40; i++) {
+        if (nzc[scan8[i + 4]])
+            ff_h264_idct_add_8_lsx(dst[1] + blk_offset[i + 4],
+                                   block + i * 16 * sizeof(pixel),
+                                   dst_stride);
+        else if (((dctcoef *) block)[i * 16])
+            ff_h264_idct_dc_add_8_lsx(dst[1] + blk_offset[i + 4],
+                                      block + i * 16 * sizeof(pixel),
+                                      dst_stride);
+    }
+}
+
+void ff_h264_idct_add16_intra_8_lsx(uint8_t *dst, const int32_t *blk_offset,
+                                    int16_t *block, int32_t dst_stride,
+                                    const uint8_t nzc[15 * 8])
+{
+    int32_t i;
+
+    for (i = 0; i < 16; i++) {
+        if (nzc[scan8[i]])
+            ff_h264_idct_add_8_lsx(dst + blk_offset[i],
+                                   block + i * 16 * sizeof(pixel), dst_stride);
+        else if (((dctcoef *) block)[i * 16])
+            ff_h264_idct_dc_add_8_lsx(dst + blk_offset[i],
+                                      block + i * 16 * sizeof(pixel),
+                                      dst_stride);
+    }
+}
+
diff --git a/libavcodec/loongarch/h264intrapred.S b/libavcodec/loongarch/h264intrapred.S
new file mode 100644
index 0000000000..4b0de26965
--- /dev/null
+++ b/libavcodec/loongarch/h264intrapred.S
@@ -0,0 +1,299 @@
+/*
+ * Loongson LSX optimized h264intrapred
+ *
+ * Copyright (c) 2022 Loongson Technology Corporation Limited
+ * Contributed by Lu Wang <wanglu@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "loongson_asm.S"
+
+const shufa
+.byte 6, 5, 4, 3, 2, 1, 0
+endconst
+
+const mulk
+.byte 2, 0, 3, 0, 4, 0, 5, 0, 6, 0, 7, 0, 8, 0
+endconst
+
+const mulh
+.byte 0, 0, 1, 0,  2,  0,  3, 0,  4, 0,  5, 0,  6, 0,  7, 0
+.byte 8, 0, 9, 0, 10,  0, 11, 0, 12, 0, 13, 0, 14, 0, 15, 0
+endconst
+
+.macro PRED16X16_PLANE
+    slli.d        t6,    a1,    1
+    slli.d        t4,    a1,    3
+    addi.d        t0,    a0,    7
+    sub.d         t0,    t0,    a1
+    add.d         t1,    a0,    t4
+    addi.d        t1,    t1,    -1
+    sub.d         t2,    t1,    t6
+
+    ld.bu         t3,    t0,    1
+    ld.bu         t4,    t0,    -1
+    ld.bu         t5,    t1,    0
+    ld.bu         t7,    t2,    0
+    sub.d         t3,    t3,    t4
+    sub.d         t4,    t5,    t7
+
+    la.local      t5,    mulk
+    vld           vr0,   t5,    0
+    fld.d         f1,    t0,    2
+    fld.d         f2,    t0,    -8
+    la.local      t5,    shufa
+    fld.d         f3,    t5,    0
+    vshuf.b       vr2,   vr2,   vr2,   vr3
+    vilvl.b       vr1,   vr1,   vr2
+    vhsubw.hu.bu  vr1,   vr1,   vr1
+    vmul.h        vr0,   vr0,   vr1
+    vhaddw.w.h    vr1,   vr0,   vr0
+    vhaddw.d.w    vr0,   vr1,   vr1
+    vhaddw.q.d    vr1,   vr0,   vr0
+    vpickve2gr.w  t5,    vr1,   0
+    add.d         t3,    t3,    t5
+//2
+    sub.d         t2,    t2,    a1
+    ld.bu         t8,    t2,    0
+    ldx.bu        t7,    t1,    a1
+    sub.d         t5,    t7,    t8
+    slli.d        t5,    t5,    1
+
+//3&4
+    add.d         t1,    t1,    t6
+    sub.d         t2,    t2,    a1
+    ld.bu         t8,    t2,    0
+    ld.bu         t7,    t1,    0
+    sub.d         t7,    t7,    t8
+    slli.d        t8,    t7,    1
+    add.d         t7,    t7,    t8
+    add.d         t5,    t5,    t7
+    sub.d         t2,    t2,    a1
+    ld.bu         t8,    t2,    0
+    ldx.bu        t7,    t1,    a1
+    sub.d         t7,    t7,    t8
+    slli.d        t7,    t7,    2
+    add.d         t5,    t5,    t7
+
+//5&6
+    add.d         t1,    t1,    t6
+    sub.d         t2,    t2,    a1
+    ld.bu         t8,    t2,    0
+    ld.bu         t7,    t1,    0
+    sub.d         t7,    t7,    t8
+    slli.d        t8,    t7,    2
+    add.d         t7,    t7,    t8
+    add.d         t5,    t5,    t7
+    sub.d         t2,    t2,    a1
+    ld.bu         t8,    t2,    0
+    ldx.bu        t7,    t1,    a1
+    sub.d         t7,    t7,    t8
+    slli.d        t8,    t7,    1
+    slli.d        t7,    t7,    2
+    add.d         t7,    t7,    t8
+    add.d         t5,    t5,    t7
+
+//7&8
+    add.d         t1,    t1,    t6
+    sub.d         t2,    t2,    a1
+    ld.bu         t8,    t2,    0
+    ld.bu         t7,    t1,    0
+    sub.d         t7,    t7,    t8
+    slli.d        t8,    t7,    3
+    sub.d         t7,    t8,    t7
+    add.d         t5,    t5,    t7
+    sub.d         t2,    t2,    a1
+    ld.bu         t8,    t2,    0
+    ldx.bu        t7,    t1,    a1
+    sub.d         t7,    t7,    t8
+    slli.d        t7,    t7,    3
+    add.d         t5,    t5,    t7
+    add.d         t4,    t4,    t5
+    add.d         t1,    t1,    a1
+.endm
+
+.macro PRED16X16_PLANE_END
+    ld.bu         t7,    t1,    0
+    ld.bu         t8,    t2,    16
+    add.d         t5,    t7,    t8
+    addi.d        t5,    t5,    1
+    slli.d        t5,    t5,    4
+    add.d         t7,    t3,    t4
+    slli.d        t8,    t7,    3
+    sub.d         t7,    t8,    t7
+    sub.d         t5,    t5,    t7
+
+    la.local      t8,    mulh
+    vld           vr3,   t8,    0
+    slli.d        t8,    t3,    3
+    vreplgr2vr.h  vr4,   t3
+    vreplgr2vr.h  vr9,   t8
+    vmul.h        vr5,   vr3,   vr4
+
+.rept 16
+    move          t7,    t5
+    add.d         t5,    t5,    t4
+    vreplgr2vr.h  vr6,   t7
+    vadd.h        vr7,   vr6,   vr5
+    vadd.h        vr8,   vr9,   vr7
+    vssrani.bu.h  vr8,   vr7,   5
+    vst           vr8,   a0,    0
+    add.d         a0,    a0,    a1
+.endr
+.endm
+
+.macro PRED16X16_PLANE_END_LASX
+    ld.bu         t7,    t1,    0
+    ld.bu         t8,    t2,    16
+    add.d         t5,    t7,    t8
+    addi.d        t5,    t5,    1
+    slli.d        t5,    t5,    4
+    add.d         t7,    t3,    t4
+    slli.d        t8,    t7,    3
+    sub.d         t7,    t8,    t7
+    sub.d         t5,    t5,    t7
+
+    la.local      t8,    mulh
+    xvld          xr3,   t8,    0
+    xvreplgr2vr.h xr4,   t3
+    xvmul.h       xr5,   xr3,   xr4
+
+.rept 8
+    move          t7,    t5
+    add.d         t5,    t5,    t4
+    xvreplgr2vr.h xr6,   t7
+    xvreplgr2vr.h xr8,   t5
+    add.d         t5,    t5,    t4
+    xvadd.h       xr7,   xr6,   xr5
+    xvadd.h       xr9,   xr8,   xr5
+
+    xvssrani.bu.h xr9,   xr7,   5
+    vstelm.d      vr9,   a0,    0,    0
+    xvstelm.d     xr9,   a0,    8,    2
+    add.d         a0,    a0,    a1
+    vstelm.d      vr9,   a0,    0,    1
+    xvstelm.d     xr9,   a0,    8,    3
+    add.d         a0,    a0,    a1
+.endr
+.endm
+
+/* void ff_h264_pred16x16_plane_h264_8_lsx(uint8_t *src, ptrdiff_t stride)
+ */
+function ff_h264_pred16x16_plane_h264_8_lsx
+    PRED16X16_PLANE
+
+    slli.d        t7,    t3,    2
+    add.d         t3,    t3,    t7
+    addi.d        t3,    t3,    32
+    srai.d        t3,    t3,    6
+    slli.d        t7,    t4,    2
+    add.d         t4,    t4,    t7
+    addi.d        t4,    t4,    32
+    srai.d        t4,    t4,    6
+
+    PRED16X16_PLANE_END
+endfunc
+
+/* void ff_h264_pred16x16_plane_rv40_8_lsx(uint8_t *src, ptrdiff_t stride)
+ */
+function ff_h264_pred16x16_plane_rv40_8_lsx
+    PRED16X16_PLANE
+
+    srai.d        t7,    t3,    2
+    add.d         t3,    t3,    t7
+    srai.d        t3,    t3,    4
+    srai.d        t7,    t4,    2
+    add.d         t4,    t4,    t7
+    srai.d        t4,    t4,    4
+
+    PRED16X16_PLANE_END
+endfunc
+
+/* void ff_h264_pred16x16_plane_svq3_8_lsx(uint8_t *src, ptrdiff_t stride)
+ */
+function ff_h264_pred16x16_plane_svq3_8_lsx
+    PRED16X16_PLANE
+
+    li.d          t6,    4
+    li.d          t7,    5
+    li.d          t8,    16
+    div.d         t3,    t3,    t6
+    mul.d         t3,    t3,    t7
+    div.d         t3,    t3,    t8
+    div.d         t4,    t4,    t6
+    mul.d         t4,    t4,    t7
+    div.d         t4,    t4,    t8
+    move          t7,    t3
+    move          t3,    t4
+    move          t4,    t7
+
+    PRED16X16_PLANE_END
+endfunc
+
+/* void ff_h264_pred16x16_plane_h264_8_lasx(uint8_t *src, ptrdiff_t stride)
+ */
+function ff_h264_pred16x16_plane_h264_8_lasx
+    PRED16X16_PLANE
+
+    slli.d        t7,    t3,    2
+    add.d         t3,    t3,    t7
+    addi.d        t3,    t3,    32
+    srai.d        t3,    t3,    6
+    slli.d        t7,    t4,    2
+    add.d         t4,    t4,    t7
+    addi.d        t4,    t4,    32
+    srai.d        t4,    t4,    6
+
+    PRED16X16_PLANE_END_LASX
+endfunc
+
+/* void ff_h264_pred16x16_plane_rv40_8_lasx(uint8_t *src, ptrdiff_t stride)
+ */
+function ff_h264_pred16x16_plane_rv40_8_lasx
+    PRED16X16_PLANE
+
+    srai.d        t7,    t3,    2
+    add.d         t3,    t3,    t7
+    srai.d        t3,    t3,    4
+    srai.d        t7,    t4,    2
+    add.d         t4,    t4,    t7
+    srai.d        t4,    t4,    4
+
+    PRED16X16_PLANE_END_LASX
+endfunc
+
+/* void ff_h264_pred16x16_plane_svq3_8_lasx(uint8_t *src, ptrdiff_t stride)
+ */
+function ff_h264_pred16x16_plane_svq3_8_lasx
+    PRED16X16_PLANE
+
+    li.d          t5,    4
+    li.d          t7,    5
+    li.d          t8,    16
+    div.d         t3,    t3,    t5
+    mul.d         t3,    t3,    t7
+    div.d         t3,    t3,    t8
+    div.d         t4,    t4,    t5
+    mul.d         t4,    t4,    t7
+    div.d         t4,    t4,    t8
+    move          t7,    t3
+    move          t3,    t4
+    move          t4,    t7
+
+    PRED16X16_PLANE_END_LASX
+endfunc
diff --git a/libavcodec/loongarch/h264qpel.S b/libavcodec/loongarch/h264qpel.S
new file mode 100644
index 0000000000..a9920b45c6
--- /dev/null
+++ b/libavcodec/loongarch/h264qpel.S
@@ -0,0 +1,1686 @@
+/*
+ * Loongson LSX optimized h264qpel
+ *
+ * Copyright (c) 2022 Loongson Technology Corporation Limited
+ * Contributed by Hecai Yuan <yuanhecai@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "loongson_asm.S"
+
+.macro VLD_QPEL8_H_SSRANI_LSX in0, in1, in2, in3, in4
+    vld           vr0,    \in4,   0
+    vldx          vr1,    \in4,   a2
+    QPEL8_H_LSX   \in0,   \in1
+    vssrani.bu.h  \in0,   \in2,   5
+    vssrani.bu.h  \in1,   \in3,   5
+.endm
+
+.macro VLDX_QPEL8_H_SSRANI_LSX in0, in1, in2, in3, in4
+    vldx          vr0,    \in4,   t1
+    vldx          vr1,    \in4,   t2
+    QPEL8_H_LSX   \in0,   \in1
+    vssrani.bu.h  \in0,   \in2,   5
+    vssrani.bu.h  \in1,   \in3,   5
+.endm
+
+.macro VLD_DOUBLE_QPEL8_H_SSRANI_LSX in0, in1, in2, in3, in4, in5, in6, in7, in8
+    vld           vr0,    \in8,   0
+    vldx          vr1,    \in8,   a2
+    QPEL8_H_LSX   \in0,   \in1
+    vssrani.bu.h  \in0,   \in4,   5
+    vssrani.bu.h  \in1,   \in5,   5
+    vldx          vr0,    \in8,   t1
+    vldx          vr1,    \in8,   t2
+    QPEL8_H_LSX   \in2,   \in3
+    vssrani.bu.h  \in2,   \in6,   5
+    vssrani.bu.h  \in3,   \in7,   5
+.endm
+
+function ff_put_h264_qpel16_mc00_lsx
+    slli.d        t0,     a2,     1
+    add.d         t1,     t0,     a2
+    slli.d        t2,     t0,     1
+.rept 4
+    vld           vr0,    a1,     0
+    vldx          vr1,    a1,     a2
+    vldx          vr2,    a1,     t0
+    vldx          vr3,    a1,     t1
+    add.d         a1,     a1,     t2
+    vst           vr0,    a0,     0
+    vstx          vr1,    a0,     a2
+    vstx          vr2,    a0,     t0
+    vstx          vr3,    a0,     t1
+    add.d         a0,     a0,     t2
+.endr
+endfunc
+
+.macro QPEL8_H_LSX out0, out1
+    vbsrl.v       vr2,    vr0,    1
+    vbsrl.v       vr3,    vr1,    1
+    vbsrl.v       vr4,    vr0,    2
+    vbsrl.v       vr5,    vr1,    2
+    vbsrl.v       vr6,    vr0,    3
+    vbsrl.v       vr7,    vr1,    3
+    vbsrl.v       vr8,    vr0,    4
+    vbsrl.v       vr9,    vr1,    4
+    vbsrl.v       vr10,   vr0,    5
+    vbsrl.v       vr11,   vr1,    5
+
+    vilvl.b       vr6,    vr4,    vr6
+    vilvl.b       vr7,    vr5,    vr7
+    vilvl.b       vr8,    vr2,    vr8
+    vilvl.b       vr9,    vr3,    vr9
+    vilvl.b       vr10,   vr0,    vr10
+    vilvl.b       vr11,   vr1,    vr11
+    vhaddw.hu.bu  vr6,    vr6,    vr6
+    vhaddw.hu.bu  vr7,    vr7,    vr7
+    vhaddw.hu.bu  vr8,    vr8,    vr8
+    vhaddw.hu.bu  vr9,    vr9,    vr9
+    vhaddw.hu.bu  vr10,   vr10,   vr10
+    vhaddw.hu.bu  vr11,   vr11,   vr11
+    vmul.h        vr2,    vr6,    vr20
+    vmul.h        vr3,    vr7,    vr20
+    vmul.h        vr4,    vr8,    vr21
+    vmul.h        vr5,    vr9,    vr21
+    vssub.h       vr2,    vr2,    vr4
+    vssub.h       vr3,    vr3,    vr5
+    vsadd.h       vr2,    vr2,    vr10
+    vsadd.h       vr3,    vr3,    vr11
+    vsadd.h       \out0,  vr2,    vr22
+    vsadd.h       \out1,  vr3,    vr22
+.endm
+
+.macro VLD_DOUBLE_QPEL8_H_LSX in0, in1, in2, in3, in4
+    vld           vr0,    \in4,   0
+    vldx          vr1,    \in4,   a2
+    QPEL8_H_LSX   \in0,   \in1
+    vldx          vr0,    \in4,   t1
+    vldx          vr1,    \in4,   t2
+    QPEL8_H_LSX   \in2,   \in3
+.endm
+
+.macro put_h264_qpel16 in0
+function ff_put_h264_qpel16_mc\in0\()_lsx
+.ifc \in0, 10
+    addi.d        t8,     a1,     0
+.else
+    addi.d        t8,     a1,     1
+.endif
+    vldi          vr20,   0x414
+    vldi          vr21,   0x405
+    vldi          vr22,   0x410
+    slli.d        t1,     a2,     1
+    add.d         t2,     t1,     a2
+    addi.d        t0,     a1,     -2   // t0 = src - 2
+    addi.d        a1,     t0,     8    // a1 = t0 + 8
+.rept 4
+    VLD_DOUBLE_QPEL8_H_LSX vr12, vr13, vr14, vr15, t0
+    VLD_QPEL8_H_SSRANI_LSX vr2, vr3, vr12, vr13, a1
+    vld           vr10,   t8,     0
+    vldx          vr11,   t8,     a2
+    vavgr.bu      vr0,    vr2,    vr10
+    vavgr.bu      vr1,    vr3,    vr11
+    vst           vr0,    a0,     0
+    vstx          vr1,    a0,     a2
+    VLDX_QPEL8_H_SSRANI_LSX vr4, vr5, vr14, vr15, a1
+    vldx          vr12,   t8,     t1
+    vldx          vr13,   t8,     t2
+    vavgr.bu      vr2,    vr4,    vr12
+    vavgr.bu      vr3,    vr5,    vr13
+    vstx          vr2,    a0,     t1
+    vstx          vr3,    a0,     t2
+    alsl.d        a0,     a2,     a0,    2
+    alsl.d        t8,     a2,     t8,    2
+    alsl.d        a1,     a2,     a1,    2
+    alsl.d        t0,     a2,     t0,    2
+.endr
+endfunc
+.endm
+
+put_h264_qpel16 10
+put_h264_qpel16 30
+
+function ff_put_h264_qpel16_mc20_lsx
+    vldi          vr20,   0x414
+    vldi          vr21,   0x405
+    vldi          vr22,   0x410
+    slli.d        t1,     a2,     1
+    add.d         t2,     t1,     a2
+    addi.d        t0,     a1,     -2   // t0 = src - 2
+    addi.d        a1,     t0,     8    // a1 = t0 + 8
+.rept 4
+    VLD_DOUBLE_QPEL8_H_LSX vr12, vr13, vr14, vr15, t0
+    VLD_QPEL8_H_SSRANI_LSX vr2, vr3, vr12, vr13, a1
+    vst           vr2,    a0,     0
+    vstx          vr3,    a0,     a2
+    VLDX_QPEL8_H_SSRANI_LSX vr4, vr5, vr14, vr15, a1
+    vstx          vr4,    a0,     t1
+    vstx          vr5,    a0,     t2
+    alsl.d        a0,     a2,     a0,    2
+    alsl.d        a1,     a2,     a1,    2
+    alsl.d        t0,     a2,     t0,    2
+.endr
+endfunc
+
+.macro QPEL8_V_LSX in0, in1, in2, in3, in4, in5, in6
+    vilvl.b       vr7,    \in3,   \in2
+    vilvl.b       vr8,    \in4,   \in3
+    vilvl.b       vr9,    \in4,   \in1
+    vilvl.b       vr10,   \in5,   \in2
+    vilvl.b       vr11,   \in5,   \in0
+    vilvl.b       vr12,   \in6,   \in1
+    vhaddw.hu.bu  vr7,    vr7,    vr7
+    vhaddw.hu.bu  vr8,    vr8,    vr8
+    vhaddw.hu.bu  vr9,    vr9,    vr9
+    vhaddw.hu.bu  vr10,   vr10,   vr10
+    vhaddw.hu.bu  vr11,   vr11,   vr11
+    vhaddw.hu.bu  vr12,   vr12,   vr12
+    vmul.h        vr7,    vr7,    vr20
+    vmul.h        vr8,    vr8,    vr20
+    vmul.h        vr9,    vr9,    vr21
+    vmul.h        vr10,   vr10,   vr21
+    vssub.h       vr7,    vr7,    vr9
+    vssub.h       vr8,    vr8,    vr10
+    vsadd.h       vr7,    vr7,    vr11
+    vsadd.h       vr8,    vr8,    vr12
+    vsadd.h       vr7,    vr7,    vr22
+    vsadd.h       vr8,    vr8,    vr22
+
+    vilvh.b       vr13,   \in3,   \in2
+    vilvh.b       vr14,   \in4,   \in3
+    vilvh.b       vr15,   \in4,   \in1
+    vilvh.b       vr16,   \in5,   \in2
+    vilvh.b       vr17,   \in5,   \in0
+    vilvh.b       vr18,   \in6,   \in1
+    vhaddw.hu.bu  vr13,   vr13,   vr13
+    vhaddw.hu.bu  vr14,   vr14,   vr14
+    vhaddw.hu.bu  vr15,   vr15,   vr15
+    vhaddw.hu.bu  vr16,   vr16,   vr16
+    vhaddw.hu.bu  vr17,   vr17,   vr17
+    vhaddw.hu.bu  vr18,   vr18,   vr18
+    vmul.h        vr13,   vr13,   vr20
+    vmul.h        vr14,   vr14,   vr20
+    vmul.h        vr15,   vr15,   vr21
+    vmul.h        vr16,   vr16,   vr21
+    vssub.h       vr13,   vr13,   vr15
+    vssub.h       vr14,   vr14,   vr16
+    vsadd.h       vr13,   vr13,   vr17
+    vsadd.h       vr14,   vr14,   vr18
+    vsadd.h       vr13,   vr13,   vr22
+    vsadd.h       vr14,   vr14,   vr22
+    vssrani.bu.h  vr13,   vr7,    5
+    vssrani.bu.h  vr14,   vr8,    5
+.endm
+
+.macro put_h264_qpel16_mc1 in0
+function ff_put_h264_qpel16_mc\in0\()_lsx
+    slli.d        t0,     a2,     1
+    add.d         t1,     t0,     a2
+    sub.d         t2,     a1,     t0  // t2 = src - 2 * stride
+    vldi          vr20,   0x414
+    vldi          vr21,   0x405
+    vldi          vr22,   0x410
+
+    vld           vr0,    t2,     0
+    vldx          vr1,    t2,     a2
+    vldx          vr2,    t2,     t0
+    vldx          vr3,    t2,     t1
+    alsl.d        t2,     a2,     t2,    2  // t2 = t2 + 4 * stride
+    vld           vr4,    t2,     0
+    vldx          vr5,    t2,     a2
+    vldx          vr6,    t2,     t0
+    QPEL8_V_LSX vr0, vr1, vr2, vr3, vr4, vr5, vr6
+.ifc \in0, 01
+    vavgr.bu      vr13,   vr2,    vr13
+    vavgr.bu      vr14,   vr3,    vr14
+.else
+    vavgr.bu      vr13,   vr3,    vr13
+    vavgr.bu      vr14,   vr4,    vr14
+.endif
+    vst           vr13,   a0,     0
+    vstx          vr14,   a0,     a2
+
+    vldx          vr0,    t2,     t1
+    alsl.d        t2,     a2,     t2,    2  // t2 = t2 + 4 *stride
+    vld           vr1,    t2,     0
+    QPEL8_V_LSX vr2, vr3, vr4, vr5, vr6, vr0, vr1
+.ifc \in0, 01
+    vavgr.bu      vr13,   vr4,    vr13
+    vavgr.bu      vr14,   vr5,    vr14
+.else
+    vavgr.bu      vr13,   vr5,    vr13
+    vavgr.bu      vr14,   vr6,    vr14
+.endif
+    vstx          vr13,   a0,     t0
+    vstx          vr14,   a0,     t1
+
+    alsl.d        a0,     a2,     a0,    2  // dst = dst + 4 * stride
+
+    vldx          vr2,    t2,     a2
+    vldx          vr3,    t2,     t0
+    QPEL8_V_LSX vr4, vr5, vr6, vr0, vr1, vr2, vr3
+.ifc \in0, 01
+    vavgr.bu      vr13,   vr6,    vr13
+    vavgr.bu      vr14,   vr0,    vr14
+.else
+    vavgr.bu      vr13,   vr0,    vr13
+    vavgr.bu      vr14,   vr1,    vr14
+.endif
+    vst           vr13,   a0,     0
+    vstx          vr14,   a0,     a2
+
+    vldx          vr4,    t2,     t1
+    alsl.d        t2,     a2,     t2,    2 // t2 = t2 + 4 * stride
+    vld           vr5,    t2,     0
+    QPEL8_V_LSX vr6, vr0, vr1, vr2, vr3, vr4, vr5
+.ifc \in0, 01
+    vavgr.bu      vr13,   vr1,    vr13
+    vavgr.bu      vr14,   vr2,    vr14
+.else
+    vavgr.bu      vr13,   vr2,    vr13
+    vavgr.bu      vr14,   vr3,    vr14
+.endif
+    vstx          vr13,   a0,     t0
+    vstx          vr14,   a0,     t1
+
+    alsl.d        a0,     a2,     a0,    2  // dst = dst + 4 * stride
+
+    vldx          vr6,    t2,     a2
+    vldx          vr0,    t2,     t0
+    QPEL8_V_LSX vr1, vr2, vr3, vr4, vr5, vr6, vr0
+.ifc \in0, 01
+    vavgr.bu      vr13,   vr3,    vr13
+    vavgr.bu      vr14,   vr4,    vr14
+.else
+    vavgr.bu      vr13,   vr4,    vr13
+    vavgr.bu      vr14,   vr5,    vr14
+.endif
+    vst           vr13,   a0,     0
+    vstx          vr14,   a0,     a2
+
+    vldx          vr1,    t2,     t1
+    alsl.d        t2,     a2,     t2,    2  // t2 = t2 + 4 * stride
+    vld           vr2,    t2,     0
+    QPEL8_V_LSX vr3, vr4, vr5, vr6, vr0, vr1, vr2
+.ifc \in0, 01
+    vavgr.bu      vr13,   vr5,    vr13
+    vavgr.bu      vr14,   vr6,    vr14
+.else
+    vavgr.bu      vr13,   vr6,    vr13
+    vavgr.bu      vr14,   vr0,    vr14
+.endif
+    vstx          vr13,   a0,     t0
+    vstx          vr14,   a0,     t1
+
+    alsl.d        a0,     a2,     a0,    2  // dst = dst + 4 * stride
+
+    vldx          vr3,    t2,     a2
+    vldx          vr4,    t2,     t0
+    QPEL8_V_LSX vr5, vr6, vr0, vr1, vr2, vr3, vr4
+.ifc \in0, 01
+    vavgr.bu      vr13,   vr0,    vr13
+    vavgr.bu      vr14,   vr1,    vr14
+.else
+    vavgr.bu      vr13,   vr1,    vr13
+    vavgr.bu      vr14,   vr2,    vr14
+.endif
+    vst           vr13,   a0,     0
+    vstx          vr14,   a0,     a2
+
+    vldx          vr5,    t2,     t1
+    alsl.d        t2,     a2,     t2,    2 // t2 = t2 + 4 * stride
+    vld           vr6,    t2,     0
+    QPEL8_V_LSX vr0, vr1, vr2, vr3, vr4, vr5, vr6
+.ifc \in0, 01
+    vavgr.bu      vr13,   vr2,    vr13
+    vavgr.bu      vr14,   vr3,    vr14
+.else
+    vavgr.bu      vr13,   vr3,    vr13
+    vavgr.bu      vr14,   vr4,    vr14
+.endif
+    vstx          vr13,   a0,     t0
+    vstx          vr14,   a0,     t1
+endfunc
+.endm
+
+put_h264_qpel16_mc1 01
+put_h264_qpel16_mc1 03
+
+.macro VST_QPEL8_V_LOWPASS_LSX in0, in1, in2, in3, in4, in5, in6, in7, in8
+    QPEL8_V_LSX \in0, \in1, \in2, \in3, \in4, \in5, \in6
+    vavgr.bu      vr13,   \in7,   vr13
+    vavgr.bu      vr14,   \in8,   vr14
+    vst           vr13,   a0,     0
+    vstx          vr14,   a0,     a2
+.endm
+
+.macro VSTX_QPEL8_V_LOWPASS_LSX in0, in1, in2, in3, in4, in5, in6, in7, in8
+    QPEL8_V_LSX \in0, \in1, \in2, \in3, \in4, \in5, \in6
+    vavgr.bu      vr13,   \in7,   vr13
+    vavgr.bu      vr14,   \in8,   vr14
+    vstx          vr13,   a0,     t1
+    vstx          vr14,   a0,     t2
+.endm
+
+function ff_put_h264_qpel16_mc11_lsx
+    addi.d        sp,     sp,     -64
+    fst.d         f24,    sp,     0
+    fst.d         f25,    sp,     8
+    fst.d         f26,    sp,     16
+    fst.d         f27,    sp,     24
+    fst.d         f28,    sp,     32
+    fst.d         f29,    sp,     40
+    fst.d         f30,    sp,     48
+    fst.d         f31,    sp,     56
+    slli.d        t1,     a2,     1
+    add.d         t2,     t1,     a2
+    slli.d        t6,     t1,     1
+    vldi          vr20,   0x414
+    vldi          vr21,   0x405
+    vldi          vr22,   0x410
+    sub.d         t4,     a1,     t1   // t4 = src - 2 * stride
+    addi.d        t0,     a1,     -2   // t0 = src - 2
+    addi.d        a1,     t0,     8    // a1 = t0 + 8
+.rept 2
+    VLD_DOUBLE_QPEL8_H_LSX vr12, vr13, vr14, vr15, t0
+    alsl.d        t0,     a2,     t0,    2
+    VLD_DOUBLE_QPEL8_H_LSX vr16, vr17, vr18, vr19, t0
+    VLD_DOUBLE_QPEL8_H_SSRANI_LSX vr23, vr24, vr25, vr26, vr12, vr13, \
+                                  vr14, vr15, a1
+    alsl.d        a1,     a2,     a1,    2
+    VLD_DOUBLE_QPEL8_H_SSRANI_LSX vr27, vr28, vr29, vr30, vr16, vr17, \
+                                  vr18, vr19, a1
+
+    vld           vr0,    t4,     0      // t4 = src - 2 * stride
+    vldx          vr1,    t4,     a2
+    vldx          vr2,    t4,     t1
+    vldx          vr3,    t4,     t2
+    alsl.d        t4,     a2,     t4,    2  // src + 2 *stride
+    vld           vr4,    t4,     0
+    vldx          vr5,    t4,     a2
+    vldx          vr6,    t4,     t1
+    VST_QPEL8_V_LOWPASS_LSX vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr23, vr24
+    vldx          vr0,    t4,     t2
+    alsl.d        t4,     a2,     t4,    2  // src + 6 *stride
+    vld           vr1,    t4,     0
+    VSTX_QPEL8_V_LOWPASS_LSX vr2, vr3, vr4, vr5, vr6, vr0, vr1, vr25, vr26
+    alsl.d        a0,     a2,     a0,    2  // dst = dst + 4 * stride
+    vldx          vr2,    t4,     a2
+    vldx          vr3,    t4,     t1
+    VST_QPEL8_V_LOWPASS_LSX vr4, vr5, vr6, vr0, vr1, vr2, vr3, vr27, vr28
+    vldx          vr4,    t4,     t2
+    alsl.d        t4,     a2,     t4,    2  // src + 10 *stride
+    vld           vr5,    t4,     0
+    VSTX_QPEL8_V_LOWPASS_LSX vr6, vr0, vr1, vr2, vr3, vr4, vr5, vr29, vr30
+    alsl.d        t0,     a2,     t0,    2
+    alsl.d        a1,     a2,     a1,    2   // a1 = src + 8 * stride
+    alsl.d        a0,     a2,     a0,    2   // dst = dst + 8 * stride
+    sub.d         t4,     t4,     t6
+.endr
+    fld.d         f24,    sp,     0
+    fld.d         f25,    sp,     8
+    fld.d         f26,    sp,     16
+    fld.d         f27,    sp,     24
+    fld.d         f28,    sp,     32
+    fld.d         f29,    sp,     40
+    fld.d         f30,    sp,     48
+    fld.d         f31,    sp,     56
+    addi.d        sp,     sp,     64
+endfunc
+
+function ff_avg_h264_qpel16_mc00_lsx
+    slli.d        t0,     a2,     1
+    add.d         t1,     t0,     a2
+    slli.d        t2,     t0,     1
+    addi.d        t3,     a0,     0
+.rept 4
+    vld           vr0,    a1,     0
+    vldx          vr1,    a1,     a2
+    vldx          vr2,    a1,     t0
+    vldx          vr3,    a1,     t1
+    add.d         a1,     a1,     t2
+    vld           vr8,    t3,     0
+    vldx          vr9,    t3,     a2
+    vldx          vr10,   t3,     t0
+    vldx          vr11,   t3,     t1
+    add.d         t3,     t3,     t2
+    vavgr.bu      vr0,    vr8,    vr0
+    vavgr.bu      vr1,    vr9,    vr1
+    vavgr.bu      vr2,    vr10,   vr2
+    vavgr.bu      vr3,    vr11,   vr3
+    vst           vr0,    a0,     0
+    vstx          vr1,    a0,     a2
+    vstx          vr2,    a0,     t0
+    vstx          vr3,    a0,     t1
+    add.d         a0,     a0,     t2
+.endr
+endfunc
+
+.macro put_h264_qpel16_mc in0
+function ff_put_h264_qpel16_mc\in0\()_lsx
+    addi.d        sp,     sp,     -64
+    fst.d         f24,    sp,     0
+    fst.d         f25,    sp,     8
+    fst.d         f26,    sp,     16
+    fst.d         f27,    sp,     24
+    fst.d         f28,    sp,     32
+    fst.d         f29,    sp,     40
+    fst.d         f30,    sp,     48
+    fst.d         f31,    sp,     56
+    slli.d        t1,     a2,     1
+    add.d         t2,     t1,     a2
+    vldi          vr20,   0x414
+    vldi          vr21,   0x405
+    vldi          vr22,   0x410
+    addi.d        t0,     a1,     -2   // t0 = src - 2
+
+.ifc \in0, 33
+    add.d         t0,     t0,     a2
+.endif
+    add.d         t3,     a1,     zero // t3 = src
+    sub.d         t4,     a1,     t1   // t4 = src - 2 * stride
+    addi.d        t4,     t4,     1
+
+    VLD_DOUBLE_QPEL8_H_LSX vr12, vr13, vr14, vr15, t0
+    alsl.d        a1,     a2,     t0,    2
+    VLD_DOUBLE_QPEL8_H_LSX vr16, vr17, vr18, vr19, a1
+    addi.d        a1,     t0,     8
+    VLD_DOUBLE_QPEL8_H_SSRANI_LSX vr23, vr24, vr25, vr26, vr12, vr13, \
+                                  vr14, vr15, a1
+    alsl.d        a1,     a2,     a1,    2
+    VLD_DOUBLE_QPEL8_H_SSRANI_LSX vr27, vr28, vr29, vr30, vr16, vr17, \
+                                  vr18, vr19, a1
+    vld           vr0,    t4,     0      // t4 = src - 2 * stride + 1
+    vldx          vr1,    t4,     a2
+    vldx          vr2,    t4,     t1
+    vldx          vr3,    t4,     t2
+    alsl.d        t4,     a2,     t4,    2
+    vld           vr4,    t4,     0
+    vldx          vr5,    t4,     a2
+    vldx          vr6,    t4,     t1
+    VST_QPEL8_V_LOWPASS_LSX vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr23, vr24
+    vldx          vr0,    t4,     t2
+    alsl.d        t4,     a2,     t4,    2
+    vld           vr1,    t4,     0
+    VSTX_QPEL8_V_LOWPASS_LSX vr2, vr3, vr4, vr5, vr6, vr0, vr1, vr25, vr26
+    add.d         t6,     t4,     zero     // t6 = src + 6 * stride
+    alsl.d        a0,     a2,     a0,    2  // dst = dst + 4 * stride
+    vldx          vr2,    t4,     a2
+    vldx          vr3,    t4,     t1
+    VST_QPEL8_V_LOWPASS_LSX vr4, vr5, vr6, vr0, vr1, vr2, vr3, vr27, vr28
+    vldx          vr4,    t4,     t2
+    alsl.d        t4,     a2,     t4,    2
+    vld           vr5,    t4,     0
+    VSTX_QPEL8_V_LOWPASS_LSX vr6, vr0, vr1, vr2, vr3, vr4, vr5, vr29, vr30
+    alsl.d        a1,     a2,     t0,    3  // a1 = src + 8 * stride
+    addi.d        t5,     a1,     8         // a1 = src + 8 * stride + 8
+    VLD_DOUBLE_QPEL8_H_LSX vr12, vr13, vr14, vr15, a1
+    alsl.d        a1,     a2,     a1,    2
+    VLD_DOUBLE_QPEL8_H_LSX vr16, vr17, vr18, vr19, a1
+    VLD_DOUBLE_QPEL8_H_SSRANI_LSX vr23, vr24, vr25, vr26, vr12, vr13, \
+                                  vr14, vr15, t5
+    alsl.d        t5,     a2,     t5,    2
+    VLD_DOUBLE_QPEL8_H_SSRANI_LSX vr27, vr28, vr29, vr30, vr16, vr17, \
+                                  vr18, vr19, t5
+    alsl.d        a0,     a2,     a0,    2   // dst = dst + 4 * stride
+
+    // t6 = src + 6 * stride + 1
+    vld           vr0,    t6,     0
+    vldx          vr1,    t6,     a2
+    vldx          vr2,    t6,     t1
+    vldx          vr3,    t6,     t2
+    alsl.d        t6,     a2,     t6,    2
+    vld           vr4,    t6,     0
+    vldx          vr5,    t6,     a2
+    vldx          vr6,    t6,     t1
+    VST_QPEL8_V_LOWPASS_LSX vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr23, vr24
+    vldx          vr0,    t6,     t2
+    alsl.d        t6,     a2,     t6,    2
+    vld           vr1,    t6,     0
+    VSTX_QPEL8_V_LOWPASS_LSX vr2, vr3, vr4, vr5 ,vr6, vr0, vr1, vr25, vr26
+    alsl.d        a0,     a2,     a0,    2    // dst = dst + 4 *stride
+    vldx          vr2,    t6,     a2
+    vldx          vr3,    t6,     t1
+    VST_QPEL8_V_LOWPASS_LSX vr4, vr5, vr6, vr0, vr1, vr2, vr3, vr27, vr28
+    vldx          vr4,    t6,     t2
+    alsl.d        t6,     a2,     t6,    2
+    vld           vr5,    t6,     0
+    VSTX_QPEL8_V_LOWPASS_LSX vr6, vr0, vr1, vr2, vr3, vr4, vr5, vr29, vr30
+
+    fld.d         f24,    sp,     0
+    fld.d         f25,    sp,     8
+    fld.d         f26,    sp,     16
+    fld.d         f27,    sp,     24
+    fld.d         f28,    sp,     32
+    fld.d         f29,    sp,     40
+    fld.d         f30,    sp,     48
+    fld.d         f31,    sp,     56
+    addi.d        sp,     sp,     64
+endfunc
+.endm
+
+put_h264_qpel16_mc 33
+put_h264_qpel16_mc 31
+
+function ff_put_h264_qpel16_mc13_lsx
+    slli.d        t1,     a2,     1
+    add.d         t2,     t1,     a2
+    vldi          vr20,   0x414
+    vldi          vr21,   0x405
+    vldi          vr22,   0x410
+    addi.d        sp,     sp,     -64
+    fst.d         f24,    sp,     0
+    fst.d         f25,    sp,     8
+    fst.d         f26,    sp,     16
+    fst.d         f27,    sp,     24
+    fst.d         f28,    sp,     32
+    fst.d         f29,    sp,     40
+    fst.d         f30,    sp,     48
+    fst.d         f31,    sp,     56
+    addi.d        t0,     a1,     -2   // t0 = src - 2
+    add.d         t0,     t0,     a2
+    add.d         t3,     a1,     zero // t3 = src
+    sub.d         t4,     a1,     t1   // t4 = src - 2 * stride
+
+    VLD_DOUBLE_QPEL8_H_LSX vr12, vr13, vr14, vr15, t0
+    alsl.d        a1,     a2,     t0,    2
+    VLD_DOUBLE_QPEL8_H_LSX vr16, vr17, vr18, vr19, a1
+    addi.d        a1,     t0,     8
+    VLD_DOUBLE_QPEL8_H_SSRANI_LSX vr23, vr24, vr25, vr26, vr12, vr13, \
+                                  vr14, vr15, a1
+    alsl.d        a1,     a2,     a1,    2
+    VLD_DOUBLE_QPEL8_H_SSRANI_LSX vr27, vr28, vr29, vr30, vr16, vr17, \
+                                  vr18, vr19, a1
+    vld           vr0,    t4,     0      // t4 = src - 2 * stride + 1
+    vldx          vr1,    t4,     a2
+    vldx          vr2,    t4,     t1
+    vldx          vr3,    t4,     t2
+    alsl.d        t4,     a2,     t4,    2
+    vld           vr4,    t4,     0
+    vldx          vr5,    t4,     a2
+    vldx          vr6,    t4,     t1
+    VST_QPEL8_V_LOWPASS_LSX vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr23, vr24
+    vldx          vr0,    t4,     t2
+    alsl.d        t4,     a2,     t4,    2
+    vld           vr1,    t4,     0
+    VSTX_QPEL8_V_LOWPASS_LSX vr2, vr3, vr4, vr5, vr6, vr0, vr1, vr25, vr26
+    add.d         t6,     t4,     zero
+    alsl.d        a0,     a2,     a0,    2  // dst = dst + 4 * stride
+    vldx          vr2,    t4,     a2
+    vldx          vr3,    t4,     t1
+    VST_QPEL8_V_LOWPASS_LSX vr4, vr5, vr6, vr0, vr1, vr2, vr3, vr27, vr28
+    vldx          vr4,    t4,     t2
+    alsl.d        t4,     a2,     t4,    2
+    vld           vr5,    t4,     0
+    VSTX_QPEL8_V_LOWPASS_LSX vr6, vr0, vr1, vr2, vr3, vr4, vr5, vr29, vr30
+    alsl.d        a1,     a2,     t0,    3  // a1 = src + 8 * stride
+    addi.d        t5,     a1,     8         // a1 = src + 8 * stride + 8
+    VLD_DOUBLE_QPEL8_H_LSX vr12, vr13, vr14, vr15, a1
+    alsl.d        a1,     a2,     a1,    2
+    VLD_DOUBLE_QPEL8_H_LSX vr16, vr17, vr18, vr19, a1
+    VLD_DOUBLE_QPEL8_H_SSRANI_LSX vr23, vr24, vr25, vr26, vr12, vr13, \
+                                  vr14, vr15, t5
+    alsl.d        t5,     a2,     t5,    2
+    VLD_DOUBLE_QPEL8_H_SSRANI_LSX vr27, vr28, vr29, vr30, vr16, vr17, \
+                                  vr18, vr19, t5
+    alsl.d        a0,     a2,     a0,    2   // dst = dst + 4 * stride
+
+    vld           vr0,    t6,     0          // // t6 = src + 6 * stride + 1
+    vldx          vr1,    t6,     a2
+    vldx          vr2,    t6,     t1
+    vldx          vr3,    t6,     t2
+    alsl.d        t6,     a2,     t6,    2
+    vld           vr4,    t6,     0
+    vldx          vr5,    t6,     a2
+    vldx          vr6,    t6,     t1
+    VST_QPEL8_V_LOWPASS_LSX vr0, vr1, vr2, vr3, vr4, vr5, vr6, vr23, vr24
+    vldx          vr0,    t6,     t2
+    alsl.d        t6,     a2,     t6,    2
+    vld           vr1,    t6,     0
+    VSTX_QPEL8_V_LOWPASS_LSX vr2, vr3, vr4, vr5, vr6, vr0, vr1, vr25, vr26
+    alsl.d        a0,     a2,     a0,    2    // dst = dst + 4 *stride
+    vldx          vr2,    t6,     a2
+    vldx          vr3,    t6,     t1
+    VST_QPEL8_V_LOWPASS_LSX vr4, vr5, vr6, vr0, vr1, vr2, vr3, vr27, vr28
+    vldx          vr4,    t6,     t2
+    alsl.d        t6,     a2,     t6,    2
+    vld           vr5,    t6,     0
+    VSTX_QPEL8_V_LOWPASS_LSX vr6, vr0, vr1, vr2, vr3, vr4, vr5, vr29, vr30
+    fld.d         f24,    sp,     0
+    fld.d         f25,    sp,     8
+    fld.d         f26,    sp,     16
+    fld.d         f27,    sp,     24
+    fld.d         f28,    sp,     32
+    fld.d         f29,    sp,     40
+    fld.d         f30,    sp,     48
+    fld.d         f31,    sp,     56
+    addi.d        sp,     sp,     64
+endfunc
+
+function ff_avg_h264_qpel16_mc10_lsx
+    addi.d        t0,     a0,     0   // t0 = dst
+    addi.d        t4,     a1,     -2  // t1 = src - 2
+    addi.d        t5,     t4,     8
+    slli.d        t1,     a2,     1
+    add.d         t2,     a2,     t1
+    vldi          vr20,   0x414
+    vldi          vr21,   0x405
+    vldi          vr22,   0x410
+.rept 2
+    VLD_DOUBLE_QPEL8_H_LSX vr12, vr13, vr14, vr15, t4
+    alsl.d        t4,     a2,     t4,    2
+    VLD_DOUBLE_QPEL8_H_LSX vr16, vr17, vr18, vr19, t4
+    VLD_QPEL8_H_SSRANI_LSX vr2, vr3, vr12, vr13, t5
+    vld           vr0,    a1,     0
+    vldx          vr1,    a1,     a2
+    vld           vr12,   t0,     0
+    vldx          vr13,   t0,     a2
+    vavgr.bu      vr0,    vr0,    vr2
+    vavgr.bu      vr1,    vr1,    vr3
+    vavgr.bu      vr0,    vr0,    vr12
+    vavgr.bu      vr1,    vr1,    vr13
+    vst           vr0,    a0,     0
+    vstx          vr1,    a0,     a2
+    VLDX_QPEL8_H_SSRANI_LSX vr2, vr3, vr14, vr15, t5
+    vldx          vr0,    a1,     t1
+    vldx          vr1,    a1,     t2
+    vldx          vr12,   t0,     t1
+    vldx          vr13,   t0,     t2
+    vavgr.bu      vr0,    vr0,    vr2
+    vavgr.bu      vr1,    vr1,    vr3
+    vavgr.bu      vr0,    vr0,    vr12
+    vavgr.bu      vr1,    vr1,    vr13
+    vstx          vr0,    a0,     t1
+    vstx          vr1,    a0,     t2
+    alsl.d        t5,     a2,     t5,    2
+    alsl.d        a1,     a2,     a1,    2
+    alsl.d        t0,     a2,     t0,    2
+    alsl.d        a0,     a2,     a0,    2
+    VLD_QPEL8_H_SSRANI_LSX vr2, vr3, vr16, vr17, t5
+    vld           vr0,    a1,     0
+    vldx          vr1,    a1,     a2
+    vld           vr12,   t0,     0
+    vldx          vr13,   t0,     a2
+    vavgr.bu      vr0,    vr0,    vr2
+    vavgr.bu      vr1,    vr1,    vr3
+    vavgr.bu      vr0,    vr0,    vr12
+    vavgr.bu      vr1,    vr1,    vr13
+    vst           vr0,    a0,     0
+    vstx          vr1,    a0,     a2
+    VLDX_QPEL8_H_SSRANI_LSX vr2, vr3, vr18, vr19, t5
+    vldx          vr0,    a1,     t1
+    vldx          vr1,    a1,     t2
+    vldx          vr12,   t0,     t1
+    vldx          vr13,   t0,     t2
+    vavgr.bu      vr0,    vr0,    vr2
+    vavgr.bu      vr1,    vr1,    vr3
+    vavgr.bu      vr0,    vr0,    vr12
+    vavgr.bu      vr1,    vr1,    vr13
+    vstx          vr0,    a0,     t1
+    vstx          vr1,    a0,     t2
+    alsl.d        t5,     a2,     t5,    2
+    alsl.d        a1,     a2,     a1,    2
+    alsl.d        t0,     a2,     t0,    2
+    alsl.d        a0,     a2,     a0,    2
+    alsl.d        t4,     a2,     t4,    2   // src + 8 * stride -2
+.endr
+endfunc
+
+function ff_avg_h264_qpel16_mc30_lsx
+    addi.d        t0,     a0,     0   // t0 = dst
+    addi.d        t4,     a1,     -2  // t1 = src - 2
+    addi.d        t5,     t4,     8
+    addi.d        a1,     a1,     1   // a1 = a1 + 1
+    slli.d        t1,     a2,     1
+    add.d         t2,     a2,     t1
+    vldi          vr20,   0x414
+    vldi          vr21,   0x405
+    vldi          vr22,   0x410
+.rept 2
+    VLD_DOUBLE_QPEL8_H_LSX vr12, vr13, vr14, vr15, t4
+    alsl.d        t4,     a2,     t4,    2
+    VLD_DOUBLE_QPEL8_H_LSX vr16, vr17, vr18, vr19, t4
+    VLD_QPEL8_H_SSRANI_LSX vr2, vr3, vr12, vr13, t5
+    vld           vr0,    a1,     0
+    vldx          vr1,    a1,     a2
+    vld           vr12,   t0,     0
+    vldx          vr13,   t0,     a2
+    vavgr.bu      vr0,    vr0,    vr2
+    vavgr.bu      vr1,    vr1,    vr3
+    vavgr.bu      vr0,    vr0,    vr12
+    vavgr.bu      vr1,    vr1,    vr13
+    vst           vr0,    a0,     0
+    vstx          vr1,    a0,     a2
+    VLDX_QPEL8_H_SSRANI_LSX vr2, vr3, vr14, vr15, t5
+    vldx          vr0,    a1,     t1
+    vldx          vr1,    a1,     t2
+    vldx          vr12,   t0,     t1
+    vldx          vr13,   t0,     t2
+    vavgr.bu      vr0,    vr0,    vr2
+    vavgr.bu      vr1,    vr1,    vr3
+    vavgr.bu      vr0,    vr0,    vr12
+    vavgr.bu      vr1,    vr1,    vr13
+    vstx          vr0,    a0,     t1
+    vstx          vr1,    a0,     t2
+    alsl.d        t5,     a2,     t5,    2
+    alsl.d        a1,     a2,     a1,    2
+    alsl.d        t0,     a2,     t0,    2
+    alsl.d        a0,     a2,     a0,    2
+    VLD_QPEL8_H_SSRANI_LSX vr2, vr3, vr16, vr17, t5
+    vld           vr0,    a1,     0
+    vldx          vr1,    a1,     a2
+    vld           vr12,   t0,     0
+    vldx          vr13,   t0,     a2
+    vavgr.bu      vr0,    vr0,    vr2
+    vavgr.bu      vr1,    vr1,    vr3
+    vavgr.bu      vr0,    vr0,    vr12
+    vavgr.bu      vr1,    vr1,    vr13
+    vst           vr0,    a0,     0
+    vstx          vr1,    a0,     a2
+    VLDX_QPEL8_H_SSRANI_LSX vr2, vr3, vr18, vr19, t5
+    vldx          vr0,    a1,     t1
+    vldx          vr1,    a1,     t2
+    vldx          vr12,   t0,     t1
+    vldx          vr13,   t0,     t2
+    vavgr.bu      vr0,    vr0,    vr2
+    vavgr.bu      vr1,    vr1,    vr3
+    vavgr.bu      vr0,    vr0,    vr12
+    vavgr.bu      vr1,    vr1,    vr13
+    vstx          vr0,    a0,     t1
+    vstx          vr1,    a0,     t2
+    alsl.d        t5,     a2,     t5,    2
+    alsl.d        a1,     a2,     a1,    2
+    alsl.d        t0,     a2,     t0,    2
+    alsl.d        a0,     a2,     a0,    2
+    alsl.d        t4,     a2,     t4,    2   // t1 = src + 8 * stride -2
+.endr
+endfunc
+
+function ff_put_h264_qpel16_mc02_lsx
+    slli.d        t0,     a2,     1
+    add.d         t1,     t0,     a2
+    sub.d         t2,     a1,     t0  // t2 = src - 2 * stride
+    vldi          vr20,   0x414
+    vldi          vr21,   0x405
+    vldi          vr22,   0x410
+
+    vld           vr0,    t2,     0
+    vldx          vr1,    t2,     a2
+    vldx          vr2,    t2,     t0
+    vldx          vr3,    t2,     t1
+    alsl.d        t2,     a2,     t2,    2  // t2 = t2 + 4 * stride
+    vld           vr4,    t2,     0
+    vldx          vr5,    t2,     a2
+    vldx          vr6,    t2,     t0
+    QPEL8_V_LSX   vr0, vr1, vr2, vr3, vr4, vr5, vr6
+    vst           vr13,   a0,     0
+    vstx          vr14,   a0,     a2
+    vldx          vr0,    t2,     t1
+    alsl.d        t2,     a2,     t2,    2  // t2 = t2 + 4 *stride
+    vld           vr1,    t2,     0
+    QPEL8_V_LSX   vr2, vr3, vr4, vr5, vr6, vr0, vr1
+    vstx          vr13,   a0,     t0
+    vstx          vr14,   a0,     t1
+    alsl.d        a0,     a2,     a0,    2  // dst = dst + 4 * stride
+    vldx          vr2,    t2,     a2
+    vldx          vr3,    t2,     t0
+    QPEL8_V_LSX   vr4, vr5, vr6, vr0, vr1, vr2, vr3
+    vst           vr13,   a0,     0
+    vstx          vr14,   a0,     a2
+    vldx          vr4,    t2,     t1
+    alsl.d        t2,     a2,     t2,    2 // t2 = t2 + 4 * stride
+    vld           vr5,    t2,     0
+    QPEL8_V_LSX   vr6, vr0, vr1, vr2, vr3, vr4, vr5
+    vstx          vr13,   a0,     t0
+    vstx          vr14,   a0,     t1
+
+    alsl.d        a0,     a2,     a0,    2  // dst = dst + 4 * stride
+
+    vldx          vr6,    t2,     a2
+    vldx          vr0,    t2,     t0
+    QPEL8_V_LSX   vr1, vr2, vr3, vr4, vr5, vr6, vr0
+    vst           vr13,   a0,     0
+    vstx          vr14,   a0,     a2
+    vldx          vr1,    t2,     t1
+    alsl.d        t2,     a2,     t2,    2  // t2 = t2 + 4 * stride
+    vld           vr2,    t2,     0
+    QPEL8_V_LSX   vr3, vr4, vr5, vr6, vr0, vr1, vr2
+    vstx          vr13,   a0,     t0
+    vstx          vr14,   a0,     t1
+    alsl.d        a0,     a2,     a0,    2  // dst = dst + 4 * stride
+    vldx          vr3,    t2,     a2
+    vldx          vr4,    t2,     t0
+    QPEL8_V_LSX   vr5, vr6, vr0, vr1, vr2, vr3, vr4
+    vst           vr13,   a0,     0
+    vstx          vr14,   a0,     a2
+    vldx          vr5,    t2,     t1
+    alsl.d        t2,     a2,     t2,    2 // t2 = t2 + 4 * stride
+    vld           vr6,    t2,     0
+    QPEL8_V_LSX   vr0, vr1, vr2, vr3, vr4, vr5, vr6
+    vstx          vr13,   a0,     t0
+    vstx          vr14,   a0,     t1
+endfunc
+
+.macro avc_luma_hv_qrt_and_aver_dst_16x16_lsx
+    addi.d        sp,     sp,     -64
+    fst.d         f24,    sp,     0
+    fst.d         f25,    sp,     8
+    fst.d         f26,    sp,     16
+    fst.d         f27,    sp,     24
+    fst.d         f28,    sp,     32
+    fst.d         f29,    sp,     40
+    fst.d         f30,    sp,     48
+    fst.d         f31,    sp,     56
+    vldi          vr20,   0x414
+    vldi          vr21,   0x405
+    vldi          vr22,   0x410
+
+    VLD_DOUBLE_QPEL8_H_LSX vr12, vr13, vr14, vr15, t0
+    alsl.d        a1,     a2,     t0,    2
+    VLD_DOUBLE_QPEL8_H_LSX vr16, vr17, vr18, vr19, a1
+    addi.d        a1,     t0,     8
+    VLD_DOUBLE_QPEL8_H_SSRANI_LSX vr23, vr24, vr25, vr26, vr12, vr13, \
+                                  vr14, vr15, a1
+    alsl.d        a1,     a2,     a1,    2
+    VLD_DOUBLE_QPEL8_H_SSRANI_LSX vr27, vr28, vr29, vr30, vr16, vr17, \
+                                  vr18, vr19, a1
+    vld           vr0,    t4,     0      // t4 = src - 2 * stride + 1
+    vldx          vr1,    t4,     a2
+    vldx          vr2,    t4,     t1
+    vldx          vr3,    t4,     t2
+    alsl.d        t4,     a2,     t4,    2
+    vld           vr4,    t4,     0
+    vldx          vr5,    t4,     a2
+    vldx          vr6,    t4,     t1
+    QPEL8_V_LSX   vr0, vr1, vr2, vr3, vr4, vr5, vr6
+    vld           vr0,    t8,     0
+    vldx          vr1,    t8,     a2
+    vavgr.bu      vr13,   vr23,   vr13
+    vavgr.bu      vr14,   vr24,   vr14
+    vavgr.bu      vr13,   vr13,   vr0
+    vavgr.bu      vr14,   vr14,   vr1
+    vst           vr13,   a0,     0
+    vstx          vr14,   a0,     a2
+    vldx          vr0,    t4,     t2
+    alsl.d        t4,     a2,     t4,    2
+    vld           vr1,    t4,     0
+    QPEL8_V_LSX   vr2, vr3, vr4, vr5, vr6, vr0, vr1
+    vldx          vr2,    t8,     t1
+    vldx          vr3,    t8,     t2
+    vavgr.bu      vr13,   vr25,   vr13
+    vavgr.bu      vr14,   vr26,   vr14
+    vavgr.bu      vr13,   vr13,   vr2
+    vavgr.bu      vr14,   vr14,   vr3
+    add.d         t6,     t4,     zero     // t6 = src + 6 * stride
+    vstx          vr13,   a0,     t1
+    vstx          vr14,   a0,     t2
+    alsl.d        a0,     a2,     a0,    2  // dst = dst + 4 * stride
+    alsl.d        t8,     a2,     t8,    2
+    vldx          vr2,    t4,     a2
+    vldx          vr3,    t4,     t1
+    QPEL8_V_LSX   vr4, vr5, vr6, vr0, vr1, vr2, vr3
+    vld           vr4,    t8,     0
+    vldx          vr5,    t8,     a2
+    vavgr.bu      vr13,   vr27,   vr13
+    vavgr.bu      vr14,   vr28,   vr14
+    vavgr.bu      vr13,   vr13,   vr4
+    vavgr.bu      vr14,   vr14,   vr5
+    vst           vr13,   a0,     0
+    vstx          vr14,   a0,     a2
+    vldx          vr4,    t4,     t2
+    alsl.d        t4,     a2,     t4,    2
+    vld           vr5,    t4,     0
+    QPEL8_V_LSX   vr6, vr0, vr1, vr2, vr3, vr4, vr5
+    vldx          vr6,    t8,     t1
+    vldx          vr0,    t8,     t2
+    vavgr.bu      vr13,   vr29,   vr13
+    vavgr.bu      vr14,   vr30,   vr14
+    vavgr.bu      vr13,   vr13,   vr6
+    vavgr.bu      vr14,   vr14,   vr0
+    vstx          vr13,   a0,     t1
+    vstx          vr14,   a0,     t2
+    alsl.d        a1,     a2,     t0,    3  // a1 = src + 8 * stride
+    addi.d        t5,     a1,     8         // a1 = src + 8 * stride + 8
+    VLD_DOUBLE_QPEL8_H_LSX vr12, vr13, vr14, vr15, a1
+    alsl.d        a1,     a2,     a1,    2
+    VLD_DOUBLE_QPEL8_H_LSX vr16, vr17, vr18, vr19, a1
+    VLD_DOUBLE_QPEL8_H_SSRANI_LSX vr23, vr24, vr25, vr26, vr12, vr13, \
+                                  vr14, vr15, t5
+    alsl.d        t5,     a2,     t5,    2
+    VLD_DOUBLE_QPEL8_H_SSRANI_LSX vr27, vr28, vr29, vr30, vr16, vr17, \
+                                  vr18, vr19, t5
+    alsl.d        a0,     a2,     a0,    2   // dst = dst + 4 * stride
+    alsl.d        t8,     a2,     t8,    2
+    // t6 = src + 6 * stride + 1
+    vld           vr0,    t6,     0
+    vldx          vr1,    t6,     a2
+    vldx          vr2,    t6,     t1
+    vldx          vr3,    t6,     t2
+    alsl.d        t6,     a2,     t6,    2
+    vld           vr4,    t6,     0
+    vldx          vr5,    t6,     a2
+    vldx          vr6,    t6,     t1
+    QPEL8_V_LSX   vr0, vr1, vr2, vr3, vr4, vr5, vr6
+    vld           vr0,    t8,     0
+    vldx          vr1,    t8,     a2
+    vavgr.bu      vr13,   vr23,   vr13
+    vavgr.bu      vr14,   vr24,   vr14
+    vavgr.bu      vr13,   vr13,   vr0
+    vavgr.bu      vr14,   vr14,   vr1
+    vst           vr13,   a0,     0
+    vstx          vr14,   a0,     a2
+    vldx          vr0,    t6,     t2
+    alsl.d        t6,     a2,     t6,    2
+    vld           vr1,    t6,     0
+    QPEL8_V_LSX   vr2, vr3, vr4, vr5, vr6, vr0, vr1
+    vldx          vr2,    t8,     t1
+    vldx          vr3,    t8,     t2
+    vavgr.bu      vr13,   vr25,   vr13
+    vavgr.bu      vr14,   vr26,   vr14
+    vavgr.bu      vr13,   vr13,   vr2
+    vavgr.bu      vr14,   vr14,   vr3
+    vstx          vr13,   a0,     t1
+    vstx          vr14,   a0,     t2
+    alsl.d        a0,     a2,     a0,    2    // dst = dst + 4 *stride
+    alsl.d        t8,     a2,     t8,    2
+    vldx          vr2,    t6,     a2
+    vldx          vr3,    t6,     t1
+    QPEL8_V_LSX   vr4, vr5, vr6, vr0, vr1, vr2, vr3
+    vld           vr4,    t8,     0
+    vldx          vr5,    t8,     a2
+    vavgr.bu      vr13,   vr27,   vr13
+    vavgr.bu      vr14,   vr28,   vr14
+    vavgr.bu      vr13,   vr13,   vr4
+    vavgr.bu      vr14,   vr14,   vr5
+    vst           vr13,   a0,     0
+    vstx          vr14,   a0,     a2
+    vldx          vr4,    t6,     t2
+    alsl.d        t6,     a2,     t6,    2
+    vld           vr5,    t6,     0
+    QPEL8_V_LSX   vr6, vr0, vr1, vr2, vr3, vr4, vr5
+    vldx          vr6,    t8,     t1
+    vldx          vr0,    t8,     t2
+    vavgr.bu      vr13,   vr29,   vr13
+    vavgr.bu      vr14,   vr30,   vr14
+    vavgr.bu      vr13,   vr13,   vr6
+    vavgr.bu      vr14,   vr14,   vr0
+    vstx          vr13,   a0,     t1
+    vstx          vr14,   a0,     t2
+    fld.d         f24,    sp,     0
+    fld.d         f25,    sp,     8
+    fld.d         f26,    sp,     16
+    fld.d         f27,    sp,     24
+    fld.d         f28,    sp,     32
+    fld.d         f29,    sp,     40
+    fld.d         f30,    sp,     48
+    fld.d         f31,    sp,     56
+    addi.d        sp,     sp,     64
+.endm
+
+function ff_avg_h264_qpel16_mc33_lsx
+    slli.d        t1,     a2,     1
+    add.d         t2,     t1,     a2
+    addi.d        t0,     a1,     -2   // t0 = src - 2
+    add.d         t0,     t0,     a2   // t0 = src + stride - 2
+    add.d         t3,     a1,     zero // t3 = src
+    sub.d         t4,     a1,     t1   // t4 = src - 2 * stride
+    addi.d        t4,     t4,     1
+    addi.d        t8,     a0,     0
+    avc_luma_hv_qrt_and_aver_dst_16x16_lsx
+endfunc
+
+function ff_avg_h264_qpel16_mc11_lsx
+    slli.d        t1,     a2,     1
+    add.d         t2,     t1,     a2
+    addi.d        t0,     a1,     -2   // t0 = src - 2
+    add.d         t3,     a1,     zero // t3 = src
+    sub.d         t4,     a1,     t1   // t4 = src - 2 * stride
+    addi.d        t8,     a0,     0
+    avc_luma_hv_qrt_and_aver_dst_16x16_lsx
+endfunc
+
+function ff_avg_h264_qpel16_mc31_lsx
+    slli.d        t1,     a2,     1
+    add.d         t2,     t1,     a2
+    addi.d        t0,     a1,     -2   // t0 = src - 2
+    add.d         t3,     a1,     zero // t3 = src
+    sub.d         t4,     a1,     t1   // t4 = src - 2 * stride
+    addi.d        t4,     t4,     1
+    addi.d        t8,     a0,     0
+    avc_luma_hv_qrt_and_aver_dst_16x16_lsx
+endfunc
+
+function ff_avg_h264_qpel16_mc13_lsx
+    slli.d        t1,     a2,     1
+    add.d         t2,     t1,     a2
+    addi.d        t0,     a1,     -2   // t0 = src - 2
+    add.d         t0,     t0,     a2
+    add.d         t3,     a1,     zero // t3 = src
+    sub.d         t4,     a1,     t1   // t4 = src - 2 * stride
+    addi.d        t8,     a0,     0
+    avc_luma_hv_qrt_and_aver_dst_16x16_lsx
+endfunc
+
+function ff_avg_h264_qpel16_mc20_lsx
+    slli.d        t1,     a2,     1
+    add.d         t2,     t1,     a2
+    vldi          vr20,   0x414
+    vldi          vr21,   0x405
+    vldi          vr22,   0x410
+    addi.d        t0,     a1,     -2   // t0 = src - 2
+    addi.d        t5,     a0,     0
+    addi.d        a1,     t0,     8
+.rept 4
+    VLD_DOUBLE_QPEL8_H_LSX vr12, vr13, vr14, vr15, t0
+    VLD_QPEL8_H_SSRANI_LSX vr2, vr3, vr12, vr13, a1
+    vld           vr0,    t5,     0
+    vldx          vr1,    t5,     a2
+    vavgr.bu      vr0,    vr0,    vr2
+    vavgr.bu      vr1,    vr1,    vr3
+    vst           vr0,    a0,     0
+    vstx          vr1,    a0,     a2
+    add.d         a1,     a1,     t1
+    VLD_QPEL8_H_SSRANI_LSX vr2, vr3, vr14, vr15, a1
+    vldx          vr0,    t5,     t1
+    vldx          vr1,    t5,     t2
+    vavgr.bu      vr0,    vr0,    vr2
+    vavgr.bu      vr1,    vr1,    vr3
+    vstx          vr0,    a0,     t1
+    vstx          vr1,    a0,     t2
+    alsl.d        t0,     a2,     t0,    2
+    alsl.d        t5,     a2,     t5,    2
+    alsl.d        a0,     a2,     a0,    2
+    alsl.d        a1,     a2,     a1,    1
+.endr
+endfunc
+
+.macro QPEL8_HV_H_LSX out0, out1
+    vbsrl.v       vr2,    vr0,    1
+    vbsrl.v       vr3,    vr1,    1
+    vbsrl.v       vr4,    vr0,    2
+    vbsrl.v       vr5,    vr1,    2
+    vbsrl.v       vr6,    vr0,    3
+    vbsrl.v       vr7,    vr1,    3
+    vbsrl.v       vr8,    vr0,    4
+    vbsrl.v       vr9,    vr1,    4
+    vbsrl.v       vr10,   vr0,    5
+    vbsrl.v       vr11,   vr1,    5
+    vilvl.b       vr6,    vr4,    vr6
+    vilvl.b       vr7,    vr5,    vr7
+    vilvl.b       vr8,    vr2,    vr8
+    vilvl.b       vr9,    vr3,    vr9
+    vilvl.b       vr10,   vr0,    vr10
+    vilvl.b       vr11,   vr1,    vr11
+    vhaddw.hu.bu  vr6,    vr6,    vr6
+    vhaddw.hu.bu  vr7,    vr7,    vr7
+    vhaddw.hu.bu  vr8,    vr8,    vr8
+    vhaddw.hu.bu  vr9,    vr9,    vr9
+    vhaddw.hu.bu  vr10,   vr10,   vr10
+    vhaddw.hu.bu  vr11,   vr11,   vr11
+    vmul.h        vr2,    vr6,    vr20
+    vmul.h        vr3,    vr7,    vr20
+    vmul.h        vr4,    vr8,    vr21
+    vmul.h        vr5,    vr9,    vr21
+    vssub.h       vr2,    vr2,    vr4
+    vssub.h       vr3,    vr3,    vr5
+    vsadd.h       \out0,  vr2,    vr10
+    vsadd.h       \out1,  vr3,    vr11
+.endm
+
+.macro QPEL8_HV_V_LSX in0, in1, in2, in3, in4, in5, in6, out0, out1, out2, out3
+    vilvl.h       vr0,    \in2,   \in3
+    vilvl.h       vr1,    \in3,   \in4  // tmp0
+    vilvl.h       vr2,    \in1,   \in4
+    vilvl.h       vr3,    \in2,   \in5  // tmp2
+    vilvl.h       vr4,    \in0,   \in5
+    vilvl.h       vr5,    \in1,   \in6  // tmp4
+    vhaddw.w.h    vr0,    vr0,    vr0
+    vhaddw.w.h    vr1,    vr1,    vr1
+    vhaddw.w.h    vr2,    vr2,    vr2
+    vhaddw.w.h    vr3,    vr3,    vr3
+    vhaddw.w.h    vr4,    vr4,    vr4
+    vhaddw.w.h    vr5,    vr5,    vr5
+    vmul.w        vr0,    vr0,    vr22
+    vmul.w        vr1,    vr1,    vr22
+    vmul.w        vr2,    vr2,    vr23
+    vmul.w        vr3,    vr3,    vr23
+    vssub.w       vr0,    vr0,    vr2
+    vssub.w       vr1,    vr1,    vr3
+    vsadd.w       vr0,    vr0,    vr4
+    vsadd.w       vr1,    vr1,    vr5
+    vsadd.w       \out0,  vr0,    vr24
+    vsadd.w       \out1,  vr1,    vr24
+    vilvh.h       vr0,    \in2,   \in3
+    vilvh.h       vr1,    \in3,   \in4  // tmp0
+    vilvh.h       vr2,    \in1,   \in4
+    vilvh.h       vr3,    \in2,   \in5  // tmp2
+    vilvh.h       vr4,    \in0,   \in5
+    vilvh.h       vr5,    \in1,   \in6  // tmp4
+    vhaddw.w.h    vr0,    vr0,    vr0
+    vhaddw.w.h    vr1,    vr1,    vr1
+    vhaddw.w.h    vr2,    vr2,    vr2
+    vhaddw.w.h    vr3,    vr3,    vr3
+    vhaddw.w.h    vr4,    vr4,    vr4
+    vhaddw.w.h    vr5,    vr5,    vr5
+    vmul.w        vr0,    vr0,    vr22
+    vmul.w        vr1,    vr1,    vr22
+    vmul.w        vr2,    vr2,    vr23
+    vmul.w        vr3,    vr3,    vr23
+    vssub.w       vr0,    vr0,    vr2
+    vssub.w       vr1,    vr1,    vr3
+    vsadd.w       vr0,    vr0,    vr4
+    vsadd.w       vr1,    vr1,    vr5
+    vsadd.w       \out2,  vr0,    vr24
+    vsadd.w       \out3,  vr1,    vr24
+    vssrani.hu.w  \out2,  \out0,  10
+    vssrani.hu.w  \out3,  \out1,  10
+    vssrani.bu.h  \out3,  \out2,  0
+.endm
+
+.macro h264_qpel8_hv_lowpass_core_lsx in0, in1, type
+    vld           vr0,    \in0,  0
+    vldx          vr1,    \in0,  a3
+    QPEL8_HV_H_LSX vr12, vr13 // a b$
+    vldx          vr0,    \in0,  t1
+    vldx          vr1,    \in0,  t2
+    QPEL8_HV_H_LSX vr14, vr15 // c d$
+
+    alsl.d        \in0,   a3,    \in0,   2
+
+    vld           vr0,    \in0,  0
+    vldx          vr1,    \in0,  a3
+    QPEL8_HV_H_LSX vr16, vr17 // e f$
+    vldx          vr0,    \in0,  t1
+    vldx          vr1,    \in0,  t2
+    QPEL8_HV_H_LSX vr18, vr19 // g h$
+    QPEL8_HV_V_LSX vr12, vr13, vr14, vr15, vr16, vr17, vr18, vr6, vr7, vr0, vr1
+.ifc \type, avg
+    fld.d         f2,     t3,      0
+    fldx.d        f3,     t3,      a2
+    vilvl.d       vr2,    vr3,     vr2
+    vavgr.bu      vr1,    vr2,     vr1
+.endif
+    vstelm.d      vr1,    \in1,    0,     0
+    add.d         \in1,   \in1,    a2
+    vstelm.d      vr1,    \in1,    0,     1
+
+    alsl.d        \in0,    a3,    \in0,   2
+
+    // tmp8
+    vld           vr0,    \in0,   0
+    vldx          vr1,    \in0,   a3
+    QPEL8_HV_H_LSX vr12, vr13
+    QPEL8_HV_V_LSX vr14, vr15, vr16, vr17, vr18, vr19, vr12, vr6, vr7, vr0, vr1
+.ifc \type, avg
+    fldx.d        f2,     t3,      t5
+    fldx.d        f3,     t3,      t6
+    vilvl.d       vr2,    vr3,     vr2
+    vavgr.bu      vr1,    vr2,     vr1
+.endif
+    add.d         \in1,   \in1,   a2
+    vstelm.d      vr1,    \in1,   0,     0
+    add.d         \in1,   \in1,   a2
+    vstelm.d      vr1,    \in1,   0,     1
+
+    // tmp10
+    vldx          vr0,    \in0,   t1
+    vldx          vr1,    \in0,   t2
+    QPEL8_HV_H_LSX vr14, vr15
+    QPEL8_HV_V_LSX vr16, vr17, vr18, vr19, vr12, vr13, vr14, vr6, vr7, vr0, vr1
+.ifc \type, avg
+    alsl.d        t3,     a2,      t3,   2
+    fld.d         f2,     t3,      0
+    fldx.d        f3,     t3,      a2
+    vilvl.d       vr2,    vr3,     vr2
+    vavgr.bu      vr1,    vr2,     vr1
+.endif
+    add.d         \in1,   \in1,   a2
+    vstelm.d      vr1,    \in1,   0,     0
+    add.d         \in1,   \in1,   a2
+    vstelm.d      vr1,    \in1,   0,     1
+
+    // tmp12
+    alsl.d        \in0,   a3,     \in0,  2
+
+    vld           vr0,    \in0,   0
+    vldx          vr1,    \in0,   a3
+    QPEL8_HV_H_LSX vr16, vr17
+    QPEL8_HV_V_LSX vr18, vr19, vr12, vr13, vr14, vr15, vr16, vr6, vr7, vr0, vr1
+.ifc \type, avg
+    fldx.d        f2,     t3,     t5
+    fldx.d        f3,     t3,     t6
+    vilvl.d       vr2,    vr3,    vr2
+    vavgr.bu      vr1,    vr2,    vr1
+.endif
+    add.d         \in1,   \in1,   a2
+    vstelm.d      vr1,    \in1,   0,     0
+    add.d         \in1,   \in1,   a2
+    vstelm.d      vr1,    \in1,   0,     1
+.endm
+
+function put_h264_qpel8_hv_lowpass_lsx
+    slli.d        t1,     a3,     1
+    add.d         t2,     t1,     a3
+    addi.d        sp,     sp,     -8
+    fst.d         f24,    sp,     0
+    addi.d        t0,     a1,     -2   // t0 = src - 2
+    sub.d         t0,     t0,     t1   // t0 = t0 - 2 * stride
+    vldi          vr20,   0x414   // h_20
+    vldi          vr21,   0x405   // h_5
+    vldi          vr22,   0x814   // w_20
+    vldi          vr23,   0x805   // w_5
+    addi.d        t4,     zero,   512
+    vreplgr2vr.w  vr24,   t4      // w_512
+    h264_qpel8_hv_lowpass_core_lsx t0, a0, put
+    fld.d         f24,    sp,     0
+    addi.d        sp,     sp,     8
+endfunc
+
+function put_h264_qpel8_h_lowpass_lsx
+    slli.d        t1,     a3,     1
+    add.d         t2,     t1,     a3
+    vldi          vr20,   0x414
+    vldi          vr21,   0x405
+    vldi          vr22,   0x410
+    addi.d        t0,     a1,     -2   // t0 = src - 2
+    add.d         t3,     a1,     zero // t3 = src
+.rept 2
+    vld           vr0,    t0,     0
+    vldx          vr1,    t0,     a3
+    QPEL8_H_LSX   vr12, vr13
+    vssrani.bu.h  vr13,   vr12,   5
+    vstelm.d      vr13,   a0,     0,    0
+    add.d         a0,     a0,     a2
+    vstelm.d      vr13,   a0,     0,    1
+    add.d         a0,     a0,     a2
+    vldx          vr0,    t0,     t1
+    vldx          vr1,    t0,     t2
+    QPEL8_H_LSX   vr12, vr13
+    vssrani.bu.h  vr13,   vr12,   5
+    vstelm.d      vr13,   a0,     0,    0
+    add.d         a0,     a0,     a2
+    vstelm.d      vr13,   a0,     0,    1
+    add.d         a0,     a0,     a2
+    alsl.d        t0,     a3,     t0,    2
+.endr
+endfunc
+
+function put_pixels16_l2_8_lsx
+    slli.d        t0,     a4,     1
+    add.d         t1,     t0,     a4
+    slli.d        t2,     t0,     1
+    slli.d        t3,     a3,     1
+    add.d         t4,     t3,     a3
+    slli.d        t5,     t3,     1
+.rept 4
+    vld           vr0,    a1,     0
+    vldx          vr1,    a1,     a4
+    vldx          vr2,    a1,     t0
+    vldx          vr3,    a1,     t1
+    add.d         a1,     a1,     t2
+    vld           vr8,    a2,     0x00
+    vld           vr9,    a2,     0x10
+    vld           vr10,   a2,     0x20
+    vld           vr11,   a2,     0x30
+    addi.d        a2,     a2,     0x40
+    vavgr.bu      vr0,    vr8,    vr0
+    vavgr.bu      vr1,    vr9,    vr1
+    vavgr.bu      vr2,    vr10,   vr2
+    vavgr.bu      vr3,    vr11,   vr3
+    vst           vr0,    a0,     0
+    vstx          vr1,    a0,     a3
+    vstx          vr2,    a0,     t3
+    vstx          vr3,    a0,     t4
+    add.d         a0,     a0,     t5
+.endr
+endfunc
+
+.macro QPEL8_V1_LSX in0, in1, in2, in3, in4, in5, in6
+    vilvl.b       vr7,    \in3,   \in2
+    vilvl.b       vr8,    \in4,   \in3
+    vilvl.b       vr9,    \in4,   \in1
+    vilvl.b       vr10,   \in5,   \in2
+    vilvl.b       vr11,   \in5,   \in0
+    vilvl.b       vr12,   \in6,   \in1
+    vhaddw.hu.bu  vr7,    vr7,    vr7
+    vhaddw.hu.bu  vr8,    vr8,    vr8
+    vhaddw.hu.bu  vr9,    vr9,    vr9
+    vhaddw.hu.bu  vr10,   vr10,   vr10
+    vhaddw.hu.bu  vr11,   vr11,   vr11
+    vhaddw.hu.bu  vr12,   vr12,   vr12
+    vmul.h        vr7,    vr7,    vr20
+    vmul.h        vr8,    vr8,    vr20
+    vmul.h        vr9,    vr9,    vr21
+    vmul.h        vr10,   vr10,   vr21
+    vssub.h       vr7,    vr7,    vr9
+    vssub.h       vr8,    vr8,    vr10
+    vsadd.h       vr7,    vr7,    vr11
+    vsadd.h       vr8,    vr8,    vr12
+    vsadd.h       vr7,    vr7,    vr22
+    vsadd.h       vr8,    vr8,    vr22
+    vssrani.bu.h  vr8,    vr7,    5
+.endm
+
+.macro h264_qpel8_v_lowpass_lsx type
+function \type\()_h264_qpel8_v_lowpass_lsx
+    slli.d        t0,     a3,     1
+    add.d         t1,     t0,     a3
+    sub.d         t2,     a1,     t0  // t2 = src - 2 * stride
+.ifc \type, avg
+    addi.d        t3,     a0,     0
+    slli.d        t4,     a2,     1
+    add.d         t5,     t4,     a2
+.endif
+    vldi          vr20,   0x414
+    vldi          vr21,   0x405
+    vldi          vr22,   0x410
+
+    fld.d         f0,     t2,     0
+    fldx.d        f1,     t2,     a3
+    fldx.d        f2,     t2,     t0
+    fldx.d        f3,     t2,     t1
+    alsl.d        t2,     a3,     t2,    2  // t2 = t2 + 4 * stride
+    fld.d         f4,     t2,     0
+    fldx.d        f5,     t2,     a3
+    fldx.d        f6,     t2,     t0
+    QPEL8_V1_LSX  vr0, vr1, vr2, vr3, vr4, vr5, vr6
+.ifc \type, avg
+    fld.d         f0,     t3,     0
+    fldx.d        f1,     t3,     a2
+    vilvl.d       vr0,    vr1,    vr0
+    vavgr.bu      vr8,    vr8,    vr0
+.endif
+    vstelm.d      vr8,    a0,     0,    0
+    add.d         a0,     a0,     a2
+    vstelm.d      vr8,    a0,     0,    1
+    add.d         a0,     a0,     a2
+
+    fldx.d        f0,     t2,     t1
+    alsl.d        t2,     a3,     t2,   2  // t2 = t2 + 4 *stride
+    fld.d         f1,     t2,     0
+    QPEL8_V1_LSX  vr2, vr3, vr4, vr5, vr6, vr0, vr1
+.ifc \type, avg
+    fldx.d        f2,     t3,     t4
+    fldx.d        f3,     t3,     t5
+    vilvl.d       vr2,    vr3,    vr2
+    vavgr.bu      vr8,    vr8,    vr2
+.endif
+    vstelm.d      vr8,    a0,     0,    0
+    add.d         a0,     a0,     a2
+    vstelm.d      vr8,    a0,     0,    1
+    add.d         a0,     a0,     a2
+
+    alsl.d        t3,     a2,     t3,   2
+
+    fldx.d        f2,     t2,     a3
+    fldx.d        f3,     t2,     t0
+    QPEL8_V1_LSX  vr4, vr5, vr6, vr0, vr1, vr2, vr3
+.ifc \type, avg
+    fld.d         f4,     t3,     0
+    fldx.d        f5,     t3,     a2
+    vilvl.d       vr4,    vr5,    vr4
+    vavgr.bu      vr8,    vr8,    vr4
+.endif
+    vstelm.d      vr8,    a0,     0,    0
+    add.d         a0,     a0,     a2
+    vstelm.d      vr8,    a0,     0,    1
+    add.d         a0,     a0,     a2
+
+    fldx.d        f4,     t2,     t1
+    alsl.d        t2,     a3,     t2,   2 // t2 = t2 + 4 * stride
+    fld.d         f5,     t2,     0
+    QPEL8_V1_LSX  vr6, vr0, vr1, vr2, vr3, vr4, vr5
+.ifc \type, avg
+    fldx.d        f6,     t3,     t4
+    fldx.d        f0,     t3,     t5
+    vilvl.d       vr6,    vr0,    vr6
+    vavgr.bu      vr8,    vr8,    vr6
+.endif
+    vstelm.d      vr8,    a0,     0,    0
+    add.d         a0,     a0,     a2
+    vstelm.d      vr8,    a0,     0,    1
+endfunc
+.endm
+
+h264_qpel8_v_lowpass_lsx put
+h264_qpel8_v_lowpass_lsx avg
+
+function avg_pixels16_l2_8_lsx
+    slli.d        t0,     a4,     1
+    add.d         t1,     t0,     a4
+    slli.d        t2,     t0,     1
+    slli.d        t3,     a3,     1
+    add.d         t4,     t3,     a3
+    slli.d        t5,     t3,     1
+    addi.d        t6,     a0,     0
+.rept 4
+    vld           vr0,    a1,     0
+    vldx          vr1,    a1,     a4
+    vldx          vr2,    a1,     t0
+    vldx          vr3,    a1,     t1
+    add.d         a1,     a1,     t2
+    vld           vr8,    a2,     0x00
+    vld           vr9,    a2,     0x10
+    vld           vr10,   a2,     0x20
+    vld           vr11,   a2,     0x30
+    addi.d        a2,     a2,     0x40
+    vavgr.bu      vr0,    vr8,    vr0
+    vavgr.bu      vr1,    vr9,    vr1
+    vavgr.bu      vr2,    vr10,   vr2
+    vavgr.bu      vr3,    vr11,   vr3
+    vld           vr8,    t6,     0
+    vldx          vr9,    t6,     a3
+    vldx          vr10,   t6,     t3
+    vldx          vr11,   t6,     t4
+    add.d         t6,     t6,     t5
+    vavgr.bu      vr0,    vr8,    vr0
+    vavgr.bu      vr1,    vr9,    vr1
+    vavgr.bu      vr2,    vr10,   vr2
+    vavgr.bu      vr3,    vr11,   vr3
+    vst           vr0,    a0,     0
+    vstx          vr1,    a0,     a3
+    vstx          vr2,    a0,     t3
+    vstx          vr3,    a0,     t4
+    add.d         a0,     a0,     t5
+.endr
+endfunc
+
+function avg_h264_qpel8_hv_lowpass_lsx
+    slli.d        t1,     a3,     1
+    add.d         t2,     t1,     a3
+    slli.d        t5,     a2,     1
+    add.d         t6,     a2,     t5
+    addi.d        sp,     sp,     -8
+    fst.d         f24,    sp,     0
+    vldi          vr20,   0x414   // h_20
+    vldi          vr21,   0x405   // h_5
+    vldi          vr22,   0x814   // w_20
+    vldi          vr23,   0x805   // w_5
+    addi.d        t4,     zero,   512
+    vreplgr2vr.w  vr24,   t4      // w_512
+    addi.d        t0,     a1,     -2   // t0 = src - 2
+    sub.d         t0,     t0,     t1   // t0 = t0 - 2 * stride
+    addi.d        t3,     a0,     0    // t3 = dst
+    h264_qpel8_hv_lowpass_core_lsx t0, a0, avg
+    fld.d         f24,    sp,     0
+    addi.d        sp,     sp,     8
+endfunc
+
+function put_pixels8_l2_8_lsx
+    slli.d        t0,     a4,     1
+    add.d         t1,     t0,     a4
+    slli.d        t2,     t0,     1
+.rept 2
+    vld           vr0,    a1,     0
+    vldx          vr1,    a1,     a4
+    vldx          vr2,    a1,     t0
+    vldx          vr3,    a1,     t1
+    add.d         a1,     a1,     t2
+    vilvl.d       vr0,    vr1,    vr0
+    vilvl.d       vr2,    vr3,    vr2
+    vld           vr8,    a2,     0x00
+    vld           vr9,    a2,     0x08
+    vld           vr10,   a2,     0x10
+    vld           vr11,   a2,     0x18
+    vilvl.d       vr8,    vr9,    vr8
+    vilvl.d       vr10,   vr11,   vr10
+    addi.d        a2,     a2,     32
+    vavgr.bu      vr0,    vr8,    vr0
+    vavgr.bu      vr2,    vr10,   vr2
+    vstelm.d      vr0,    a0,     0,     0
+    add.d         a0,     a0,     a3
+    vstelm.d      vr0,    a0,     0,     1
+    add.d         a0,     a0,     a3
+    vstelm.d      vr2,    a0,     0,     0
+    add.d         a0,     a0,     a3
+    vstelm.d      vr2,    a0,     0,     1
+    add.d         a0,     a0,     a3
+.endr
+endfunc
+
+function ff_put_h264_qpel8_mc00_lsx
+    slli.d        t0,     a2,     1
+    add.d         t1,     t0,     a2
+    slli.d        t2,     t0,     1
+    ld.d          t3,     a1,     0x0
+    ldx.d         t4,     a1,     a2
+    ldx.d         t5,     a1,     t0
+    ldx.d         t6,     a1,     t1
+    st.d          t3,     a0,     0x0
+    stx.d         t4,     a0,     a2
+    stx.d         t5,     a0,     t0
+    stx.d         t6,     a0,     t1
+    add.d         a1,     a1,     t2
+    add.d         a0,     a0,     t2
+    ld.d          t3,     a1,     0x0
+    ldx.d         t4,     a1,     a2
+    ldx.d         t5,     a1,     t0
+    ldx.d         t6,     a1,     t1
+    st.d          t3,     a0,     0x0
+    stx.d         t4,     a0,     a2
+    stx.d         t5,     a0,     t0
+    stx.d         t6,     a0,     t1
+endfunc
+
+function ff_avg_h264_qpel8_mc00_lsx
+    slli.d        t0,     a2,     1
+    add.d         t1,     t0,     a2
+    slli.d        t2,     t0,     1
+    addi.d        t3,     a0,     0
+.rept 2
+    vld           vr0,    a1,     0
+    vldx          vr1,    a1,     a2
+    vldx          vr2,    a1,     t0
+    vldx          vr3,    a1,     t1
+    add.d         a1,     a1,     t2
+    vilvl.d       vr0,    vr1,    vr0
+    vilvl.d       vr2,    vr3,    vr2
+    vld           vr8,    t3,     0
+    vldx          vr9,    t3,     a2
+    vldx          vr10,   t3,     t0
+    vldx          vr11,   t3,     t1
+    add.d         t3,     t3,     t2
+    vilvl.d       vr8,    vr9,    vr8
+    vilvl.d       vr10,   vr11,   vr10
+    vavgr.bu      vr0,    vr8,    vr0
+    vavgr.bu      vr2,    vr10,   vr2
+    vstelm.d      vr0,    a0,     0,     0
+    add.d         a0,     a0,     a2
+    vstelm.d      vr0,    a0,     0,     1
+    add.d         a0,     a0,     a2
+    vstelm.d      vr2,    a0,     0,     0
+    add.d         a0,     a0,     a2
+    vstelm.d      vr2,    a0,     0,     1
+    add.d         a0,     a0,     a2
+.endr
+endfunc
+
+function avg_pixels8_l2_8_lsx
+    slli.d        t0,     a4,     1
+    add.d         t1,     t0,     a4
+    slli.d        t2,     t0,     1
+    addi.d        t3,     a0,     0
+    slli.d        t4,     a3,     1
+    add.d         t5,     t4,     a3
+    slli.d        t6,     t4,     1
+.rept 2
+    vld           vr0,    a1,     0
+    vldx          vr1,    a1,     a4
+    vldx          vr2,    a1,     t0
+    vldx          vr3,    a1,     t1
+    add.d         a1,     a1,     t2
+    vilvl.d       vr0,    vr1,    vr0
+    vilvl.d       vr2,    vr3,    vr2
+    vld           vr8,    a2,     0x00
+    vld           vr9,    a2,     0x08
+    vld           vr10,   a2,     0x10
+    vld           vr11,   a2,     0x18
+    addi.d        a2,     a2,     0x20
+    vilvl.d       vr8,    vr9,    vr8
+    vilvl.d       vr10,   vr11,   vr10
+    vavgr.bu      vr0,    vr8,    vr0
+    vavgr.bu      vr2,    vr10,   vr2
+    vld           vr8,    t3,     0
+    vldx          vr9,    t3,     a3
+    vldx          vr10,   t3,     t4
+    vldx          vr11,   t3,     t5
+    add.d         t3,     t3,     t6
+    vilvl.d       vr8,    vr9,    vr8
+    vilvl.d       vr10,   vr11,   vr10
+    vavgr.bu      vr0,    vr8,    vr0
+    vavgr.bu      vr2,    vr10,   vr2
+    vstelm.d      vr0,    a0,     0,     0
+    add.d         a0,     a0,     a3
+    vstelm.d      vr0,    a0,     0,     1
+    add.d         a0,     a0,     a3
+    vstelm.d      vr2,    a0,     0,     0
+    add.d         a0,     a0,     a3
+    vstelm.d      vr2,    a0,     0,     1
+    add.d         a0,     a0,     a3
+.endr
+endfunc
+
+function avg_h264_qpel8_h_lowpass_lsx
+    slli.d        t1,     a3,     1
+    add.d         t2,     t1,     a3
+    slli.d        t5,     a2,     1
+    add.d         t6,     t5,     a2
+    vldi          vr20,   0x414
+    vldi          vr21,   0x405
+    vldi          vr22,   0x410
+    addi.d        t0,     a1,     -2   // t0 = src - 2
+    add.d         t3,     a1,     zero // t3 = src
+    addi.d        t4,     a0,     0    // t4 = dst
+.rept 4
+    vld           vr0,    t0,     0
+    vldx          vr1,    t0,     a3
+    QPEL8_H_LSX   vr12, vr13
+    vssrani.bu.h  vr13,   vr12,   5
+    fld.d         f0,     t4,     0
+    fldx.d        f1,     t4,     a2
+    vilvl.d       vr0,    vr1,    vr0
+    vavgr.bu      vr13,   vr13,   vr0
+    vstelm.d      vr13,   a0,     0,    0
+    add.d         a0,     a0,     a2
+    vstelm.d      vr13,   a0,     0,    1
+    add.d         a0,     a0,     a2
+    add.d         t0,     t0,     t1
+    add.d         t4,     t4,     t1
+.endr
+endfunc
diff --git a/libavcodec/loongarch/h264qpel_init_loongarch.c b/libavcodec/loongarch/h264qpel_init_loongarch.c
new file mode 100644
index 0000000000..aac983ce40
--- /dev/null
+++ b/libavcodec/loongarch/h264qpel_init_loongarch.c
@@ -0,0 +1,169 @@
+/*
+ * Copyright (c) 2020 Loongson Technology Corporation Limited
+ * Contributed by Shiyou Yin <yinshiyou-hf@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "h264qpel_lsx.h"
+#include "h264qpel_lasx.h"
+#include "libavutil/attributes.h"
+#include "libavutil/loongarch/cpu.h"
+#include "libavcodec/h264qpel.h"
+
+av_cold void ff_h264qpel_init_loongarch(H264QpelContext *c, int bit_depth)
+{
+    int cpu_flags = av_get_cpu_flags();
+
+    if (have_lsx(cpu_flags)) {
+        if (8 == bit_depth) {
+            c->put_h264_qpel_pixels_tab[0][0]  = ff_put_h264_qpel16_mc00_lsx;
+            c->put_h264_qpel_pixels_tab[0][1]  = ff_put_h264_qpel16_mc10_lsx;
+            c->put_h264_qpel_pixels_tab[0][2]  = ff_put_h264_qpel16_mc20_lsx;
+            c->put_h264_qpel_pixels_tab[0][3]  = ff_put_h264_qpel16_mc30_lsx;
+            c->put_h264_qpel_pixels_tab[0][4]  = ff_put_h264_qpel16_mc01_lsx;
+            c->put_h264_qpel_pixels_tab[0][5]  = ff_put_h264_qpel16_mc11_lsx;
+            c->put_h264_qpel_pixels_tab[0][6]  = ff_put_h264_qpel16_mc21_lsx;
+            c->put_h264_qpel_pixels_tab[0][7]  = ff_put_h264_qpel16_mc31_lsx;
+            c->put_h264_qpel_pixels_tab[0][8]  = ff_put_h264_qpel16_mc02_lsx;
+            c->put_h264_qpel_pixels_tab[0][9]  = ff_put_h264_qpel16_mc12_lsx;
+            c->put_h264_qpel_pixels_tab[0][10] = ff_put_h264_qpel16_mc22_lsx;
+            c->put_h264_qpel_pixels_tab[0][11] = ff_put_h264_qpel16_mc32_lsx;
+            c->put_h264_qpel_pixels_tab[0][12] = ff_put_h264_qpel16_mc03_lsx;
+            c->put_h264_qpel_pixels_tab[0][13] = ff_put_h264_qpel16_mc13_lsx;
+            c->put_h264_qpel_pixels_tab[0][14] = ff_put_h264_qpel16_mc23_lsx;
+            c->put_h264_qpel_pixels_tab[0][15] = ff_put_h264_qpel16_mc33_lsx;
+
+            c->avg_h264_qpel_pixels_tab[0][0]  = ff_avg_h264_qpel16_mc00_lsx;
+            c->avg_h264_qpel_pixels_tab[0][1]  = ff_avg_h264_qpel16_mc10_lsx;
+            c->avg_h264_qpel_pixels_tab[0][2]  = ff_avg_h264_qpel16_mc20_lsx;
+            c->avg_h264_qpel_pixels_tab[0][3]  = ff_avg_h264_qpel16_mc30_lsx;
+            c->avg_h264_qpel_pixels_tab[0][4]  = ff_avg_h264_qpel16_mc01_lsx;
+            c->avg_h264_qpel_pixels_tab[0][5]  = ff_avg_h264_qpel16_mc11_lsx;
+            c->avg_h264_qpel_pixels_tab[0][6]  = ff_avg_h264_qpel16_mc21_lsx;
+            c->avg_h264_qpel_pixels_tab[0][7]  = ff_avg_h264_qpel16_mc31_lsx;
+            c->avg_h264_qpel_pixels_tab[0][8]  = ff_avg_h264_qpel16_mc02_lsx;
+            c->avg_h264_qpel_pixels_tab[0][9]  = ff_avg_h264_qpel16_mc12_lsx;
+            c->avg_h264_qpel_pixels_tab[0][10] = ff_avg_h264_qpel16_mc22_lsx;
+            c->avg_h264_qpel_pixels_tab[0][11] = ff_avg_h264_qpel16_mc32_lsx;
+            c->avg_h264_qpel_pixels_tab[0][12] = ff_avg_h264_qpel16_mc03_lsx;
+            c->avg_h264_qpel_pixels_tab[0][13] = ff_avg_h264_qpel16_mc13_lsx;
+            c->avg_h264_qpel_pixels_tab[0][14] = ff_avg_h264_qpel16_mc23_lsx;
+            c->avg_h264_qpel_pixels_tab[0][15] = ff_avg_h264_qpel16_mc33_lsx;
+
+            c->put_h264_qpel_pixels_tab[1][0]  = ff_put_h264_qpel8_mc00_lsx;
+            c->put_h264_qpel_pixels_tab[1][1]  = ff_put_h264_qpel8_mc10_lsx;
+            c->put_h264_qpel_pixels_tab[1][2]  = ff_put_h264_qpel8_mc20_lsx;
+            c->put_h264_qpel_pixels_tab[1][3]  = ff_put_h264_qpel8_mc30_lsx;
+            c->put_h264_qpel_pixels_tab[1][4]  = ff_put_h264_qpel8_mc01_lsx;
+            c->put_h264_qpel_pixels_tab[1][5]  = ff_put_h264_qpel8_mc11_lsx;
+            c->put_h264_qpel_pixels_tab[1][6]  = ff_put_h264_qpel8_mc21_lsx;
+            c->put_h264_qpel_pixels_tab[1][7]  = ff_put_h264_qpel8_mc31_lsx;
+            c->put_h264_qpel_pixels_tab[1][8]  = ff_put_h264_qpel8_mc02_lsx;
+            c->put_h264_qpel_pixels_tab[1][9]  = ff_put_h264_qpel8_mc12_lsx;
+            c->put_h264_qpel_pixels_tab[1][10] = ff_put_h264_qpel8_mc22_lsx;
+            c->put_h264_qpel_pixels_tab[1][11] = ff_put_h264_qpel8_mc32_lsx;
+            c->put_h264_qpel_pixels_tab[1][12] = ff_put_h264_qpel8_mc03_lsx;
+            c->put_h264_qpel_pixels_tab[1][13] = ff_put_h264_qpel8_mc13_lsx;
+            c->put_h264_qpel_pixels_tab[1][14] = ff_put_h264_qpel8_mc23_lsx;
+            c->put_h264_qpel_pixels_tab[1][15] = ff_put_h264_qpel8_mc33_lsx;
+
+            c->avg_h264_qpel_pixels_tab[1][0]  = ff_avg_h264_qpel8_mc00_lsx;
+            c->avg_h264_qpel_pixels_tab[1][1]  = ff_avg_h264_qpel8_mc10_lsx;
+            c->avg_h264_qpel_pixels_tab[1][2]  = ff_avg_h264_qpel8_mc20_lsx;
+            c->avg_h264_qpel_pixels_tab[1][3]  = ff_avg_h264_qpel8_mc30_lsx;
+            c->avg_h264_qpel_pixels_tab[1][5]  = ff_avg_h264_qpel8_mc11_lsx;
+            c->avg_h264_qpel_pixels_tab[1][6]  = ff_avg_h264_qpel8_mc21_lsx;
+            c->avg_h264_qpel_pixels_tab[1][7]  = ff_avg_h264_qpel8_mc31_lsx;
+            c->avg_h264_qpel_pixels_tab[1][8]  = ff_avg_h264_qpel8_mc02_lsx;
+            c->avg_h264_qpel_pixels_tab[1][9]  = ff_avg_h264_qpel8_mc12_lsx;
+            c->avg_h264_qpel_pixels_tab[1][10] = ff_avg_h264_qpel8_mc22_lsx;
+            c->avg_h264_qpel_pixels_tab[1][11] = ff_avg_h264_qpel8_mc32_lsx;
+            c->avg_h264_qpel_pixels_tab[1][13] = ff_avg_h264_qpel8_mc13_lsx;
+            c->avg_h264_qpel_pixels_tab[1][14] = ff_avg_h264_qpel8_mc23_lsx;
+            c->avg_h264_qpel_pixels_tab[1][15] = ff_avg_h264_qpel8_mc33_lsx;
+        }
+    }
+    if (have_lasx(cpu_flags)) {
+        if (8 == bit_depth) {
+            c->put_h264_qpel_pixels_tab[0][0]  = ff_put_h264_qpel16_mc00_lasx;
+            c->put_h264_qpel_pixels_tab[0][1]  = ff_put_h264_qpel16_mc10_lasx;
+            c->put_h264_qpel_pixels_tab[0][2]  = ff_put_h264_qpel16_mc20_lasx;
+            c->put_h264_qpel_pixels_tab[0][3]  = ff_put_h264_qpel16_mc30_lasx;
+            c->put_h264_qpel_pixels_tab[0][4]  = ff_put_h264_qpel16_mc01_lasx;
+            c->put_h264_qpel_pixels_tab[0][5]  = ff_put_h264_qpel16_mc11_lasx;
+
+            c->put_h264_qpel_pixels_tab[0][6]  = ff_put_h264_qpel16_mc21_lasx;
+            c->put_h264_qpel_pixels_tab[0][7]  = ff_put_h264_qpel16_mc31_lasx;
+            c->put_h264_qpel_pixels_tab[0][8]  = ff_put_h264_qpel16_mc02_lasx;
+            c->put_h264_qpel_pixels_tab[0][9]  = ff_put_h264_qpel16_mc12_lasx;
+            c->put_h264_qpel_pixels_tab[0][10] = ff_put_h264_qpel16_mc22_lasx;
+            c->put_h264_qpel_pixels_tab[0][11] = ff_put_h264_qpel16_mc32_lasx;
+            c->put_h264_qpel_pixels_tab[0][12] = ff_put_h264_qpel16_mc03_lasx;
+            c->put_h264_qpel_pixels_tab[0][13] = ff_put_h264_qpel16_mc13_lasx;
+            c->put_h264_qpel_pixels_tab[0][14] = ff_put_h264_qpel16_mc23_lasx;
+            c->put_h264_qpel_pixels_tab[0][15] = ff_put_h264_qpel16_mc33_lasx;
+            c->avg_h264_qpel_pixels_tab[0][0]  = ff_avg_h264_qpel16_mc00_lasx;
+            c->avg_h264_qpel_pixels_tab[0][1]  = ff_avg_h264_qpel16_mc10_lasx;
+            c->avg_h264_qpel_pixels_tab[0][2]  = ff_avg_h264_qpel16_mc20_lasx;
+            c->avg_h264_qpel_pixels_tab[0][3]  = ff_avg_h264_qpel16_mc30_lasx;
+            c->avg_h264_qpel_pixels_tab[0][4]  = ff_avg_h264_qpel16_mc01_lasx;
+            c->avg_h264_qpel_pixels_tab[0][5]  = ff_avg_h264_qpel16_mc11_lasx;
+            c->avg_h264_qpel_pixels_tab[0][6]  = ff_avg_h264_qpel16_mc21_lasx;
+            c->avg_h264_qpel_pixels_tab[0][7]  = ff_avg_h264_qpel16_mc31_lasx;
+            c->avg_h264_qpel_pixels_tab[0][8]  = ff_avg_h264_qpel16_mc02_lasx;
+            c->avg_h264_qpel_pixels_tab[0][9]  = ff_avg_h264_qpel16_mc12_lasx;
+            c->avg_h264_qpel_pixels_tab[0][10] = ff_avg_h264_qpel16_mc22_lasx;
+            c->avg_h264_qpel_pixels_tab[0][11] = ff_avg_h264_qpel16_mc32_lasx;
+            c->avg_h264_qpel_pixels_tab[0][12] = ff_avg_h264_qpel16_mc03_lasx;
+            c->avg_h264_qpel_pixels_tab[0][13] = ff_avg_h264_qpel16_mc13_lasx;
+            c->avg_h264_qpel_pixels_tab[0][14] = ff_avg_h264_qpel16_mc23_lasx;
+            c->avg_h264_qpel_pixels_tab[0][15] = ff_avg_h264_qpel16_mc33_lasx;
+
+            c->put_h264_qpel_pixels_tab[1][0]  = ff_put_h264_qpel8_mc00_lasx;
+            c->put_h264_qpel_pixels_tab[1][1]  = ff_put_h264_qpel8_mc10_lasx;
+            c->put_h264_qpel_pixels_tab[1][2]  = ff_put_h264_qpel8_mc20_lasx;
+            c->put_h264_qpel_pixels_tab[1][3]  = ff_put_h264_qpel8_mc30_lasx;
+            c->put_h264_qpel_pixels_tab[1][4]  = ff_put_h264_qpel8_mc01_lasx;
+            c->put_h264_qpel_pixels_tab[1][5]  = ff_put_h264_qpel8_mc11_lasx;
+            c->put_h264_qpel_pixels_tab[1][6]  = ff_put_h264_qpel8_mc21_lasx;
+            c->put_h264_qpel_pixels_tab[1][7]  = ff_put_h264_qpel8_mc31_lasx;
+            c->put_h264_qpel_pixels_tab[1][8]  = ff_put_h264_qpel8_mc02_lasx;
+            c->put_h264_qpel_pixels_tab[1][9]  = ff_put_h264_qpel8_mc12_lasx;
+            c->put_h264_qpel_pixels_tab[1][10] = ff_put_h264_qpel8_mc22_lasx;
+            c->put_h264_qpel_pixels_tab[1][11] = ff_put_h264_qpel8_mc32_lasx;
+            c->put_h264_qpel_pixels_tab[1][12] = ff_put_h264_qpel8_mc03_lasx;
+            c->put_h264_qpel_pixels_tab[1][13] = ff_put_h264_qpel8_mc13_lasx;
+            c->put_h264_qpel_pixels_tab[1][14] = ff_put_h264_qpel8_mc23_lasx;
+            c->put_h264_qpel_pixels_tab[1][15] = ff_put_h264_qpel8_mc33_lasx;
+            c->avg_h264_qpel_pixels_tab[1][0]  = ff_avg_h264_qpel8_mc00_lasx;
+            c->avg_h264_qpel_pixels_tab[1][1]  = ff_avg_h264_qpel8_mc10_lasx;
+            c->avg_h264_qpel_pixels_tab[1][2]  = ff_avg_h264_qpel8_mc20_lasx;
+            c->avg_h264_qpel_pixels_tab[1][3]  = ff_avg_h264_qpel8_mc30_lasx;
+            c->avg_h264_qpel_pixels_tab[1][5]  = ff_avg_h264_qpel8_mc11_lasx;
+            c->avg_h264_qpel_pixels_tab[1][6]  = ff_avg_h264_qpel8_mc21_lasx;
+            c->avg_h264_qpel_pixels_tab[1][7]  = ff_avg_h264_qpel8_mc31_lasx;
+            c->avg_h264_qpel_pixels_tab[1][8]  = ff_avg_h264_qpel8_mc02_lasx;
+            c->avg_h264_qpel_pixels_tab[1][9]  = ff_avg_h264_qpel8_mc12_lasx;
+            c->avg_h264_qpel_pixels_tab[1][10] = ff_avg_h264_qpel8_mc22_lasx;
+            c->avg_h264_qpel_pixels_tab[1][11] = ff_avg_h264_qpel8_mc32_lasx;
+            c->avg_h264_qpel_pixels_tab[1][13] = ff_avg_h264_qpel8_mc13_lasx;
+            c->avg_h264_qpel_pixels_tab[1][14] = ff_avg_h264_qpel8_mc23_lasx;
+            c->avg_h264_qpel_pixels_tab[1][15] = ff_avg_h264_qpel8_mc33_lasx;
+        }
+    }
+}
diff --git a/libavcodec/loongarch/h264qpel_lasx.c b/libavcodec/loongarch/h264qpel_lasx.c
new file mode 100644
index 0000000000..1c142e510e
--- /dev/null
+++ b/libavcodec/loongarch/h264qpel_lasx.c
@@ -0,0 +1,2038 @@
+/*
+ * Loongson LASX optimized h264qpel
+ *
+ * Copyright (c) 2020 Loongson Technology Corporation Limited
+ * Contributed by Shiyou Yin <yinshiyou-hf@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "h264qpel_lasx.h"
+#include "libavutil/loongarch/loongson_intrinsics.h"
+#include "libavutil/attributes.h"
+
+static const uint8_t luma_mask_arr[16 * 6] __attribute__((aligned(0x40))) = {
+    /* 8 width cases */
+    0, 5, 1, 6, 2, 7, 3, 8, 4, 9, 5, 10, 6, 11, 7, 12,
+    0, 5, 1, 6, 2, 7, 3, 8, 4, 9, 5, 10, 6, 11, 7, 12,
+    1, 4, 2, 5, 3, 6, 4, 7, 5, 8, 6, 9, 7, 10, 8, 11,
+    1, 4, 2, 5, 3, 6, 4, 7, 5, 8, 6, 9, 7, 10, 8, 11,
+    2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10,
+    2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10
+};
+
+#define AVC_HORZ_FILTER_SH(in0, in1, mask0, mask1, mask2)  \
+( {                                                        \
+    __m256i out0_m;                                        \
+    __m256i tmp0_m;                                        \
+                                                           \
+    tmp0_m = __lasx_xvshuf_b(in1, in0, mask0);             \
+    out0_m = __lasx_xvhaddw_h_b(tmp0_m, tmp0_m);           \
+    tmp0_m = __lasx_xvshuf_b(in1, in0, mask1);             \
+    out0_m = __lasx_xvdp2add_h_b(out0_m, minus5b, tmp0_m); \
+    tmp0_m = __lasx_xvshuf_b(in1, in0, mask2);             \
+    out0_m = __lasx_xvdp2add_h_b(out0_m, plus20b, tmp0_m); \
+                                                           \
+    out0_m;                                                \
+} )
+
+#define AVC_DOT_SH3_SH(in0, in1, in2, coeff0, coeff1, coeff2)  \
+( {                                                            \
+    __m256i out0_m;                                            \
+                                                               \
+    out0_m = __lasx_xvdp2_h_b(in0, coeff0);                    \
+    DUP2_ARG3(__lasx_xvdp2add_h_b, out0_m, in1, coeff1, out0_m,\
+              in2, coeff2, out0_m, out0_m);                    \
+                                                               \
+    out0_m;                                                    \
+} )
+
+static av_always_inline
+void avc_luma_hv_qrt_and_aver_dst_16x16_lasx(uint8_t *src_x,
+                                             uint8_t *src_y,
+                                             uint8_t *dst, ptrdiff_t stride)
+{
+    const int16_t filt_const0 = 0xfb01;
+    const int16_t filt_const1 = 0x1414;
+    const int16_t filt_const2 = 0x1fb;
+    uint32_t loop_cnt;
+    ptrdiff_t stride_2x = stride << 1;
+    ptrdiff_t stride_3x = stride_2x + stride;
+    ptrdiff_t stride_4x = stride << 2;
+    __m256i tmp0, tmp1;
+    __m256i src_hz0, src_hz1, src_hz2, src_hz3, mask0, mask1, mask2;
+    __m256i src_vt0, src_vt1, src_vt2, src_vt3, src_vt4, src_vt5, src_vt6;
+    __m256i src_vt7, src_vt8;
+    __m256i src_vt10_h, src_vt21_h, src_vt32_h, src_vt43_h, src_vt54_h;
+    __m256i src_vt65_h, src_vt76_h, src_vt87_h, filt0, filt1, filt2;
+    __m256i hz_out0, hz_out1, hz_out2, hz_out3, vt_out0, vt_out1, vt_out2;
+    __m256i vt_out3, out0, out1, out2, out3;
+    __m256i minus5b = __lasx_xvldi(0xFB);
+    __m256i plus20b = __lasx_xvldi(20);
+
+    filt0 = __lasx_xvreplgr2vr_h(filt_const0);
+    filt1 = __lasx_xvreplgr2vr_h(filt_const1);
+    filt2 = __lasx_xvreplgr2vr_h(filt_const2);
+
+    mask0 = __lasx_xvld(luma_mask_arr, 0);
+    DUP2_ARG2(__lasx_xvld, luma_mask_arr, 32, luma_mask_arr, 64, mask1, mask2);
+    src_vt0 = __lasx_xvld(src_y, 0);
+    DUP4_ARG2(__lasx_xvldx, src_y, stride, src_y, stride_2x, src_y, stride_3x,
+              src_y, stride_4x, src_vt1, src_vt2, src_vt3, src_vt4);
+    src_y += stride_4x;
+
+    src_vt0 = __lasx_xvxori_b(src_vt0, 128);
+    DUP4_ARG2(__lasx_xvxori_b, src_vt1, 128, src_vt2, 128, src_vt3, 128,
+              src_vt4, 128, src_vt1, src_vt2, src_vt3, src_vt4);
+
+    for (loop_cnt = 4; loop_cnt--;) {
+        src_hz0 = __lasx_xvld(src_x, 0);
+        DUP2_ARG2(__lasx_xvldx, src_x, stride, src_x, stride_2x,
+                  src_hz1, src_hz2);
+        src_hz3 = __lasx_xvldx(src_x, stride_3x);
+        src_x  += stride_4x;
+        src_hz0 = __lasx_xvpermi_d(src_hz0, 0x94);
+        src_hz1 = __lasx_xvpermi_d(src_hz1, 0x94);
+        src_hz2 = __lasx_xvpermi_d(src_hz2, 0x94);
+        src_hz3 = __lasx_xvpermi_d(src_hz3, 0x94);
+        DUP4_ARG2(__lasx_xvxori_b, src_hz0, 128, src_hz1, 128, src_hz2, 128,
+                  src_hz3, 128, src_hz0, src_hz1, src_hz2, src_hz3);
+
+        hz_out0 = AVC_HORZ_FILTER_SH(src_hz0, src_hz0, mask0, mask1, mask2);
+        hz_out1 = AVC_HORZ_FILTER_SH(src_hz1, src_hz1, mask0, mask1, mask2);
+        hz_out2 = AVC_HORZ_FILTER_SH(src_hz2, src_hz2, mask0, mask1, mask2);
+        hz_out3 = AVC_HORZ_FILTER_SH(src_hz3, src_hz3, mask0, mask1, mask2);
+        hz_out0 = __lasx_xvssrarni_b_h(hz_out1, hz_out0, 5);
+        hz_out2 = __lasx_xvssrarni_b_h(hz_out3, hz_out2, 5);
+
+        DUP4_ARG2(__lasx_xvldx, src_y, stride, src_y, stride_2x,
+                  src_y, stride_3x, src_y, stride_4x,
+                  src_vt5, src_vt6, src_vt7, src_vt8);
+        src_y += stride_4x;
+
+        DUP4_ARG2(__lasx_xvxori_b, src_vt5, 128, src_vt6, 128, src_vt7, 128,
+                  src_vt8, 128, src_vt5, src_vt6, src_vt7, src_vt8);
+
+        DUP4_ARG3(__lasx_xvpermi_q, src_vt0, src_vt4, 0x02, src_vt1, src_vt5,
+                  0x02, src_vt2, src_vt6, 0x02, src_vt3, src_vt7, 0x02,
+                  src_vt0, src_vt1, src_vt2, src_vt3);
+        src_vt87_h = __lasx_xvpermi_q(src_vt4, src_vt8, 0x02);
+        DUP4_ARG2(__lasx_xvilvh_b, src_vt1, src_vt0, src_vt2, src_vt1,
+                  src_vt3, src_vt2, src_vt87_h, src_vt3,
+                  src_hz0, src_hz1, src_hz2, src_hz3);
+        DUP4_ARG2(__lasx_xvilvl_b, src_vt1, src_vt0, src_vt2, src_vt1,
+                  src_vt3, src_vt2, src_vt87_h, src_vt3,
+                  src_vt0, src_vt1, src_vt2, src_vt3);
+        DUP4_ARG3(__lasx_xvpermi_q, src_vt0, src_hz0, 0x02, src_vt1, src_hz1,
+                  0x02, src_vt2, src_hz2, 0x02, src_vt3, src_hz3, 0x02,
+                  src_vt10_h, src_vt21_h, src_vt32_h, src_vt43_h);
+        DUP4_ARG3(__lasx_xvpermi_q, src_vt0, src_hz0, 0x13, src_vt1, src_hz1,
+                  0x13, src_vt2, src_hz2, 0x13, src_vt3, src_hz3, 0x13,
+                  src_vt54_h, src_vt65_h, src_vt76_h, src_vt87_h);
+        vt_out0 = AVC_DOT_SH3_SH(src_vt10_h, src_vt32_h, src_vt54_h, filt0,
+                                 filt1, filt2);
+        vt_out1 = AVC_DOT_SH3_SH(src_vt21_h, src_vt43_h, src_vt65_h, filt0,
+                                 filt1, filt2);
+        vt_out2 = AVC_DOT_SH3_SH(src_vt32_h, src_vt54_h, src_vt76_h, filt0,
+                                 filt1, filt2);
+        vt_out3 = AVC_DOT_SH3_SH(src_vt43_h, src_vt65_h, src_vt87_h, filt0,
+                                 filt1, filt2);
+        vt_out0 = __lasx_xvssrarni_b_h(vt_out1, vt_out0, 5);
+        vt_out2 = __lasx_xvssrarni_b_h(vt_out3, vt_out2, 5);
+
+        DUP2_ARG2(__lasx_xvaddwl_h_b, hz_out0, vt_out0, hz_out2, vt_out2,
+                  out0, out2);
+        DUP2_ARG2(__lasx_xvaddwh_h_b, hz_out0, vt_out0, hz_out2, vt_out2,
+                  out1, out3);
+        tmp0 = __lasx_xvssrarni_b_h(out1, out0, 1);
+        tmp1 = __lasx_xvssrarni_b_h(out3, out2, 1);
+
+        DUP2_ARG2(__lasx_xvxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
+        out0 = __lasx_xvld(dst, 0);
+        DUP2_ARG2(__lasx_xvldx, dst, stride, dst, stride_2x, out1, out2);
+        out3 = __lasx_xvldx(dst, stride_3x);
+        out0 = __lasx_xvpermi_q(out0, out2, 0x02);
+        out1 = __lasx_xvpermi_q(out1, out3, 0x02);
+        out2 = __lasx_xvilvl_d(out1, out0);
+        out3 = __lasx_xvilvh_d(out1, out0);
+        out0 = __lasx_xvpermi_q(out2, out3, 0x02);
+        out1 = __lasx_xvpermi_q(out2, out3, 0x13);
+        tmp0 = __lasx_xvavgr_bu(out0, tmp0);
+        tmp1 = __lasx_xvavgr_bu(out1, tmp1);
+
+        __lasx_xvstelm_d(tmp0, dst, 0, 0);
+        __lasx_xvstelm_d(tmp0, dst + stride, 0, 1);
+        __lasx_xvstelm_d(tmp1, dst + stride_2x, 0, 0);
+        __lasx_xvstelm_d(tmp1, dst + stride_3x, 0, 1);
+
+        __lasx_xvstelm_d(tmp0, dst, 8, 2);
+        __lasx_xvstelm_d(tmp0, dst + stride, 8, 3);
+        __lasx_xvstelm_d(tmp1, dst + stride_2x, 8, 2);
+        __lasx_xvstelm_d(tmp1, dst + stride_3x, 8, 3);
+
+        dst    += stride_4x;
+        src_vt0 = src_vt4;
+        src_vt1 = src_vt5;
+        src_vt2 = src_vt6;
+        src_vt3 = src_vt7;
+        src_vt4 = src_vt8;
+    }
+}
+
+static av_always_inline void
+avc_luma_hv_qrt_16x16_lasx(uint8_t *src_x, uint8_t *src_y,
+                           uint8_t *dst, ptrdiff_t stride)
+{
+    const int16_t filt_const0 = 0xfb01;
+    const int16_t filt_const1 = 0x1414;
+    const int16_t filt_const2 = 0x1fb;
+    uint32_t loop_cnt;
+    ptrdiff_t stride_2x = stride << 1;
+    ptrdiff_t stride_3x = stride_2x + stride;
+    ptrdiff_t stride_4x = stride << 2;
+    __m256i tmp0, tmp1;
+    __m256i src_hz0, src_hz1, src_hz2, src_hz3, mask0, mask1, mask2;
+    __m256i src_vt0, src_vt1, src_vt2, src_vt3, src_vt4, src_vt5, src_vt6;
+    __m256i src_vt7, src_vt8;
+    __m256i src_vt10_h, src_vt21_h, src_vt32_h, src_vt43_h, src_vt54_h;
+    __m256i src_vt65_h, src_vt76_h, src_vt87_h, filt0, filt1, filt2;
+    __m256i hz_out0, hz_out1, hz_out2, hz_out3, vt_out0, vt_out1, vt_out2;
+    __m256i vt_out3, out0, out1, out2, out3;
+    __m256i minus5b = __lasx_xvldi(0xFB);
+    __m256i plus20b = __lasx_xvldi(20);
+
+    filt0 = __lasx_xvreplgr2vr_h(filt_const0);
+    filt1 = __lasx_xvreplgr2vr_h(filt_const1);
+    filt2 = __lasx_xvreplgr2vr_h(filt_const2);
+
+    mask0 = __lasx_xvld(luma_mask_arr, 0);
+    DUP2_ARG2(__lasx_xvld, luma_mask_arr, 32, luma_mask_arr, 64, mask1, mask2);
+    src_vt0 = __lasx_xvld(src_y, 0);
+    DUP4_ARG2(__lasx_xvldx, src_y, stride, src_y, stride_2x, src_y, stride_3x,
+              src_y, stride_4x, src_vt1, src_vt2, src_vt3, src_vt4);
+    src_y += stride_4x;
+
+    src_vt0 = __lasx_xvxori_b(src_vt0, 128);
+    DUP4_ARG2(__lasx_xvxori_b, src_vt1, 128, src_vt2, 128, src_vt3, 128,
+              src_vt4, 128, src_vt1, src_vt2, src_vt3, src_vt4);
+
+    for (loop_cnt = 4; loop_cnt--;) {
+        src_hz0 = __lasx_xvld(src_x, 0);
+        DUP2_ARG2(__lasx_xvldx, src_x, stride, src_x, stride_2x,
+                  src_hz1, src_hz2);
+        src_hz3 = __lasx_xvldx(src_x, stride_3x);
+        src_x  += stride_4x;
+        src_hz0 = __lasx_xvpermi_d(src_hz0, 0x94);
+        src_hz1 = __lasx_xvpermi_d(src_hz1, 0x94);
+        src_hz2 = __lasx_xvpermi_d(src_hz2, 0x94);
+        src_hz3 = __lasx_xvpermi_d(src_hz3, 0x94);
+        DUP4_ARG2(__lasx_xvxori_b, src_hz0, 128, src_hz1, 128, src_hz2, 128,
+                  src_hz3, 128, src_hz0, src_hz1, src_hz2, src_hz3);
+
+        hz_out0 = AVC_HORZ_FILTER_SH(src_hz0, src_hz0, mask0, mask1, mask2);
+        hz_out1 = AVC_HORZ_FILTER_SH(src_hz1, src_hz1, mask0, mask1, mask2);
+        hz_out2 = AVC_HORZ_FILTER_SH(src_hz2, src_hz2, mask0, mask1, mask2);
+        hz_out3 = AVC_HORZ_FILTER_SH(src_hz3, src_hz3, mask0, mask1, mask2);
+        hz_out0 = __lasx_xvssrarni_b_h(hz_out1, hz_out0, 5);
+        hz_out2 = __lasx_xvssrarni_b_h(hz_out3, hz_out2, 5);
+
+        DUP4_ARG2(__lasx_xvldx, src_y, stride, src_y, stride_2x,
+                  src_y, stride_3x, src_y, stride_4x,
+                  src_vt5, src_vt6, src_vt7, src_vt8);
+        src_y += stride_4x;
+
+        DUP4_ARG2(__lasx_xvxori_b, src_vt5, 128, src_vt6, 128, src_vt7, 128,
+                  src_vt8, 128, src_vt5, src_vt6, src_vt7, src_vt8);
+        DUP4_ARG3(__lasx_xvpermi_q, src_vt0, src_vt4, 0x02, src_vt1, src_vt5,
+                  0x02, src_vt2, src_vt6, 0x02, src_vt3, src_vt7, 0x02,
+                  src_vt0, src_vt1, src_vt2, src_vt3);
+        src_vt87_h = __lasx_xvpermi_q(src_vt4, src_vt8, 0x02);
+        DUP4_ARG2(__lasx_xvilvh_b, src_vt1, src_vt0, src_vt2, src_vt1,
+                  src_vt3, src_vt2, src_vt87_h, src_vt3,
+                  src_hz0, src_hz1, src_hz2, src_hz3);
+        DUP4_ARG2(__lasx_xvilvl_b, src_vt1, src_vt0, src_vt2, src_vt1,
+                  src_vt3, src_vt2, src_vt87_h, src_vt3,
+                  src_vt0, src_vt1, src_vt2, src_vt3);
+        DUP4_ARG3(__lasx_xvpermi_q, src_vt0, src_hz0, 0x02, src_vt1,
+                  src_hz1, 0x02, src_vt2, src_hz2, 0x02, src_vt3, src_hz3,
+                  0x02, src_vt10_h, src_vt21_h, src_vt32_h, src_vt43_h);
+        DUP4_ARG3(__lasx_xvpermi_q, src_vt0, src_hz0, 0x13, src_vt1,
+                  src_hz1, 0x13, src_vt2, src_hz2, 0x13, src_vt3, src_hz3,
+                  0x13, src_vt54_h, src_vt65_h, src_vt76_h, src_vt87_h);
+
+        vt_out0 = AVC_DOT_SH3_SH(src_vt10_h, src_vt32_h, src_vt54_h,
+                                 filt0, filt1, filt2);
+        vt_out1 = AVC_DOT_SH3_SH(src_vt21_h, src_vt43_h, src_vt65_h,
+                                 filt0, filt1, filt2);
+        vt_out2 = AVC_DOT_SH3_SH(src_vt32_h, src_vt54_h, src_vt76_h,
+                                 filt0, filt1, filt2);
+        vt_out3 = AVC_DOT_SH3_SH(src_vt43_h, src_vt65_h, src_vt87_h,
+                                 filt0, filt1, filt2);
+        vt_out0 = __lasx_xvssrarni_b_h(vt_out1, vt_out0, 5);
+        vt_out2 = __lasx_xvssrarni_b_h(vt_out3, vt_out2, 5);
+
+        DUP2_ARG2(__lasx_xvaddwl_h_b, hz_out0, vt_out0, hz_out2, vt_out2,
+                  out0, out2);
+        DUP2_ARG2(__lasx_xvaddwh_h_b, hz_out0, vt_out0, hz_out2, vt_out2,
+                  out1, out3);
+        tmp0 = __lasx_xvssrarni_b_h(out1, out0, 1);
+        tmp1 = __lasx_xvssrarni_b_h(out3, out2, 1);
+
+        DUP2_ARG2(__lasx_xvxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
+        __lasx_xvstelm_d(tmp0, dst, 0, 0);
+        __lasx_xvstelm_d(tmp0, dst + stride, 0, 1);
+        __lasx_xvstelm_d(tmp1, dst + stride_2x, 0, 0);
+        __lasx_xvstelm_d(tmp1, dst + stride_3x, 0, 1);
+
+        __lasx_xvstelm_d(tmp0, dst, 8, 2);
+        __lasx_xvstelm_d(tmp0, dst + stride, 8, 3);
+        __lasx_xvstelm_d(tmp1, dst + stride_2x, 8, 2);
+        __lasx_xvstelm_d(tmp1, dst + stride_3x, 8, 3);
+
+        dst    += stride_4x;
+        src_vt0 = src_vt4;
+        src_vt1 = src_vt5;
+        src_vt2 = src_vt6;
+        src_vt3 = src_vt7;
+        src_vt4 = src_vt8;
+    }
+}
+
+/* put_pixels8_8_inline_asm: dst = src */
+static av_always_inline void
+put_pixels8_8_inline_asm(uint8_t *dst, const uint8_t *src, ptrdiff_t stride)
+{
+    uint64_t tmp[8];
+    ptrdiff_t stride_2, stride_3, stride_4;
+    __asm__ volatile (
+    "slli.d     %[stride_2],     %[stride],   1           \n\t"
+    "add.d      %[stride_3],     %[stride_2], %[stride]   \n\t"
+    "slli.d     %[stride_4],     %[stride_2], 1           \n\t"
+    "ld.d       %[tmp0],         %[src],      0x0         \n\t"
+    "ldx.d      %[tmp1],         %[src],      %[stride]   \n\t"
+    "ldx.d      %[tmp2],         %[src],      %[stride_2] \n\t"
+    "ldx.d      %[tmp3],         %[src],      %[stride_3] \n\t"
+    "add.d      %[src],          %[src],      %[stride_4] \n\t"
+    "ld.d       %[tmp4],         %[src],      0x0         \n\t"
+    "ldx.d      %[tmp5],         %[src],      %[stride]   \n\t"
+    "ldx.d      %[tmp6],         %[src],      %[stride_2] \n\t"
+    "ldx.d      %[tmp7],         %[src],      %[stride_3] \n\t"
+
+    "st.d       %[tmp0],         %[dst],      0x0         \n\t"
+    "stx.d      %[tmp1],         %[dst],      %[stride]   \n\t"
+    "stx.d      %[tmp2],         %[dst],      %[stride_2] \n\t"
+    "stx.d      %[tmp3],         %[dst],      %[stride_3] \n\t"
+    "add.d      %[dst],          %[dst],      %[stride_4] \n\t"
+    "st.d       %[tmp4],         %[dst],      0x0         \n\t"
+    "stx.d      %[tmp5],         %[dst],      %[stride]   \n\t"
+    "stx.d      %[tmp6],         %[dst],      %[stride_2] \n\t"
+    "stx.d      %[tmp7],         %[dst],      %[stride_3] \n\t"
+    : [tmp0]"=&r"(tmp[0]),        [tmp1]"=&r"(tmp[1]),
+      [tmp2]"=&r"(tmp[2]),        [tmp3]"=&r"(tmp[3]),
+      [tmp4]"=&r"(tmp[4]),        [tmp5]"=&r"(tmp[5]),
+      [tmp6]"=&r"(tmp[6]),        [tmp7]"=&r"(tmp[7]),
+      [stride_2]"=&r"(stride_2),  [stride_3]"=&r"(stride_3),
+      [stride_4]"=&r"(stride_4),
+      [dst]"+&r"(dst),            [src]"+&r"(src)
+    : [stride]"r"(stride)
+    : "memory"
+    );
+}
+
+/* avg_pixels8_8_lsx   : dst = avg(src, dst)
+ * put_pixels8_l2_8_lsx: dst = avg(src, half) , half stride is 8.
+ * avg_pixels8_l2_8_lsx: dst = avg(avg(src, half), dst) , half stride is 8.*/
+static av_always_inline void
+avg_pixels8_8_lsx(uint8_t *dst, const uint8_t *src, ptrdiff_t stride)
+{
+    uint8_t *tmp = dst;
+    ptrdiff_t stride_2, stride_3, stride_4;
+    __asm__ volatile (
+    /* h0~h7 */
+    "slli.d     %[stride_2],     %[stride],   1           \n\t"
+    "add.d      %[stride_3],     %[stride_2], %[stride]   \n\t"
+    "slli.d     %[stride_4],     %[stride_2], 1           \n\t"
+    "vld        $vr0,            %[src],      0           \n\t"
+    "vldx       $vr1,            %[src],      %[stride]   \n\t"
+    "vldx       $vr2,            %[src],      %[stride_2] \n\t"
+    "vldx       $vr3,            %[src],      %[stride_3] \n\t"
+    "add.d      %[src],          %[src],      %[stride_4] \n\t"
+    "vld        $vr4,            %[src],      0           \n\t"
+    "vldx       $vr5,            %[src],      %[stride]   \n\t"
+    "vldx       $vr6,            %[src],      %[stride_2] \n\t"
+    "vldx       $vr7,            %[src],      %[stride_3] \n\t"
+
+    "vld        $vr8,            %[tmp],      0           \n\t"
+    "vldx       $vr9,            %[tmp],      %[stride]   \n\t"
+    "vldx       $vr10,           %[tmp],      %[stride_2] \n\t"
+    "vldx       $vr11,           %[tmp],      %[stride_3] \n\t"
+    "add.d      %[tmp],          %[tmp],      %[stride_4] \n\t"
+    "vld        $vr12,           %[tmp],      0           \n\t"
+    "vldx       $vr13,           %[tmp],      %[stride]   \n\t"
+    "vldx       $vr14,           %[tmp],      %[stride_2] \n\t"
+    "vldx       $vr15,           %[tmp],      %[stride_3] \n\t"
+
+    "vavgr.bu    $vr0,           $vr8,        $vr0        \n\t"
+    "vavgr.bu    $vr1,           $vr9,        $vr1        \n\t"
+    "vavgr.bu    $vr2,           $vr10,       $vr2        \n\t"
+    "vavgr.bu    $vr3,           $vr11,       $vr3        \n\t"
+    "vavgr.bu    $vr4,           $vr12,       $vr4        \n\t"
+    "vavgr.bu    $vr5,           $vr13,       $vr5        \n\t"
+    "vavgr.bu    $vr6,           $vr14,       $vr6        \n\t"
+    "vavgr.bu    $vr7,           $vr15,       $vr7        \n\t"
+
+    "vstelm.d    $vr0,           %[dst],      0,  0       \n\t"
+    "add.d       %[dst],         %[dst],      %[stride]   \n\t"
+    "vstelm.d    $vr1,           %[dst],      0,  0       \n\t"
+    "add.d       %[dst],         %[dst],      %[stride]   \n\t"
+    "vstelm.d    $vr2,           %[dst],      0,  0       \n\t"
+    "add.d       %[dst],         %[dst],      %[stride]   \n\t"
+    "vstelm.d    $vr3,           %[dst],      0,  0       \n\t"
+    "add.d       %[dst],         %[dst],      %[stride]   \n\t"
+    "vstelm.d    $vr4,           %[dst],      0,  0       \n\t"
+    "add.d       %[dst],         %[dst],      %[stride]   \n\t"
+    "vstelm.d    $vr5,           %[dst],      0,  0       \n\t"
+    "add.d       %[dst],         %[dst],      %[stride]   \n\t"
+    "vstelm.d    $vr6,           %[dst],      0,  0       \n\t"
+    "add.d       %[dst],         %[dst],      %[stride]   \n\t"
+    "vstelm.d    $vr7,           %[dst],      0,  0       \n\t"
+    : [dst]"+&r"(dst), [tmp]"+&r"(tmp), [src]"+&r"(src),
+      [stride_2]"=&r"(stride_2),  [stride_3]"=&r"(stride_3),
+      [stride_4]"=&r"(stride_4)
+    : [stride]"r"(stride)
+    : "memory"
+    );
+}
+
+/* avg_pixels8_8_lsx   : dst = avg(src, dst)
+ * put_pixels8_l2_8_lsx: dst = avg(src, half) , half stride is 8.
+ * avg_pixels8_l2_8_lsx: dst = avg(avg(src, half), dst) , half stride is 8.*/
+static av_always_inline void
+put_pixels8_l2_8_lsx(uint8_t *dst, const uint8_t *src, const uint8_t *half,
+                     ptrdiff_t dstStride, ptrdiff_t srcStride)
+{
+    ptrdiff_t stride_2, stride_3, stride_4;
+    __asm__ volatile (
+    /* h0~h7 */
+    "slli.d     %[stride_2],     %[srcStride],   1            \n\t"
+    "add.d      %[stride_3],     %[stride_2],    %[srcStride] \n\t"
+    "slli.d     %[stride_4],     %[stride_2],    1            \n\t"
+    "vld        $vr0,            %[src],         0            \n\t"
+    "vldx       $vr1,            %[src],         %[srcStride] \n\t"
+    "vldx       $vr2,            %[src],         %[stride_2]  \n\t"
+    "vldx       $vr3,            %[src],         %[stride_3]  \n\t"
+    "add.d      %[src],          %[src],         %[stride_4]  \n\t"
+    "vld        $vr4,            %[src],         0            \n\t"
+    "vldx       $vr5,            %[src],         %[srcStride] \n\t"
+    "vldx       $vr6,            %[src],         %[stride_2]  \n\t"
+    "vldx       $vr7,            %[src],         %[stride_3]  \n\t"
+
+    "vld        $vr8,            %[half],        0x00         \n\t"
+    "vld        $vr9,            %[half],        0x08         \n\t"
+    "vld        $vr10,           %[half],        0x10         \n\t"
+    "vld        $vr11,           %[half],        0x18         \n\t"
+    "vld        $vr12,           %[half],        0x20         \n\t"
+    "vld        $vr13,           %[half],        0x28         \n\t"
+    "vld        $vr14,           %[half],        0x30         \n\t"
+    "vld        $vr15,           %[half],        0x38         \n\t"
+
+    "vavgr.bu   $vr0,            $vr8,           $vr0         \n\t"
+    "vavgr.bu   $vr1,            $vr9,           $vr1         \n\t"
+    "vavgr.bu   $vr2,            $vr10,          $vr2         \n\t"
+    "vavgr.bu   $vr3,            $vr11,          $vr3         \n\t"
+    "vavgr.bu   $vr4,            $vr12,          $vr4         \n\t"
+    "vavgr.bu   $vr5,            $vr13,          $vr5         \n\t"
+    "vavgr.bu   $vr6,            $vr14,          $vr6         \n\t"
+    "vavgr.bu   $vr7,            $vr15,          $vr7         \n\t"
+
+    "vstelm.d   $vr0,            %[dst],         0,  0        \n\t"
+    "add.d      %[dst],          %[dst],         %[dstStride] \n\t"
+    "vstelm.d   $vr1,            %[dst],         0,  0        \n\t"
+    "add.d      %[dst],          %[dst],         %[dstStride] \n\t"
+    "vstelm.d   $vr2,            %[dst],         0,  0        \n\t"
+    "add.d      %[dst],          %[dst],         %[dstStride] \n\t"
+    "vstelm.d   $vr3,            %[dst],         0,  0        \n\t"
+    "add.d      %[dst],          %[dst],         %[dstStride] \n\t"
+    "vstelm.d   $vr4,            %[dst],         0,  0        \n\t"
+    "add.d      %[dst],          %[dst],         %[dstStride] \n\t"
+    "vstelm.d   $vr5,            %[dst],         0,  0        \n\t"
+    "add.d      %[dst],          %[dst],         %[dstStride] \n\t"
+    "vstelm.d   $vr6,            %[dst],         0,  0        \n\t"
+    "add.d      %[dst],          %[dst],         %[dstStride] \n\t"
+    "vstelm.d   $vr7,            %[dst],         0,  0        \n\t"
+    : [dst]"+&r"(dst), [half]"+&r"(half), [src]"+&r"(src),
+      [stride_2]"=&r"(stride_2),  [stride_3]"=&r"(stride_3),
+      [stride_4]"=&r"(stride_4)
+    : [srcStride]"r"(srcStride), [dstStride]"r"(dstStride)
+    : "memory"
+    );
+}
+
+/* avg_pixels8_8_lsx   : dst = avg(src, dst)
+ * put_pixels8_l2_8_lsx: dst = avg(src, half) , half stride is 8.
+ * avg_pixels8_l2_8_lsx: dst = avg(avg(src, half), dst) , half stride is 8.*/
+static av_always_inline void
+avg_pixels8_l2_8_lsx(uint8_t *dst, const uint8_t *src, const uint8_t *half,
+                     ptrdiff_t dstStride, ptrdiff_t srcStride)
+{
+    uint8_t *tmp = dst;
+    ptrdiff_t stride_2, stride_3, stride_4;
+    __asm__ volatile (
+    /* h0~h7 */
+    "slli.d     %[stride_2],     %[srcStride],   1            \n\t"
+    "add.d      %[stride_3],     %[stride_2],    %[srcStride] \n\t"
+    "slli.d     %[stride_4],     %[stride_2],    1            \n\t"
+    "vld        $vr0,            %[src],         0            \n\t"
+    "vldx       $vr1,            %[src],         %[srcStride] \n\t"
+    "vldx       $vr2,            %[src],         %[stride_2]  \n\t"
+    "vldx       $vr3,            %[src],         %[stride_3]  \n\t"
+    "add.d      %[src],          %[src],         %[stride_4]  \n\t"
+    "vld        $vr4,            %[src],         0            \n\t"
+    "vldx       $vr5,            %[src],         %[srcStride] \n\t"
+    "vldx       $vr6,            %[src],         %[stride_2]  \n\t"
+    "vldx       $vr7,            %[src],         %[stride_3]  \n\t"
+
+    "vld        $vr8,            %[half],        0x00         \n\t"
+    "vld        $vr9,            %[half],        0x08         \n\t"
+    "vld        $vr10,           %[half],        0x10         \n\t"
+    "vld        $vr11,           %[half],        0x18         \n\t"
+    "vld        $vr12,           %[half],        0x20         \n\t"
+    "vld        $vr13,           %[half],        0x28         \n\t"
+    "vld        $vr14,           %[half],        0x30         \n\t"
+    "vld        $vr15,           %[half],        0x38         \n\t"
+
+    "vavgr.bu    $vr0,           $vr8,           $vr0         \n\t"
+    "vavgr.bu    $vr1,           $vr9,           $vr1         \n\t"
+    "vavgr.bu    $vr2,           $vr10,          $vr2         \n\t"
+    "vavgr.bu    $vr3,           $vr11,          $vr3         \n\t"
+    "vavgr.bu    $vr4,           $vr12,          $vr4         \n\t"
+    "vavgr.bu    $vr5,           $vr13,          $vr5         \n\t"
+    "vavgr.bu    $vr6,           $vr14,          $vr6         \n\t"
+    "vavgr.bu    $vr7,           $vr15,          $vr7         \n\t"
+
+    "slli.d     %[stride_2],     %[dstStride],   1            \n\t"
+    "add.d      %[stride_3],     %[stride_2],    %[dstStride] \n\t"
+    "slli.d     %[stride_4],     %[stride_2],    1            \n\t"
+    "vld        $vr8,            %[tmp],         0            \n\t"
+    "vldx       $vr9,            %[tmp],         %[dstStride] \n\t"
+    "vldx       $vr10,           %[tmp],         %[stride_2]  \n\t"
+    "vldx       $vr11,           %[tmp],         %[stride_3]  \n\t"
+    "add.d      %[tmp],          %[tmp],         %[stride_4]  \n\t"
+    "vld        $vr12,           %[tmp],         0            \n\t"
+    "vldx       $vr13,           %[tmp],         %[dstStride] \n\t"
+    "vldx       $vr14,           %[tmp],         %[stride_2]  \n\t"
+    "vldx       $vr15,           %[tmp],         %[stride_3]  \n\t"
+
+    "vavgr.bu    $vr0,           $vr8,           $vr0         \n\t"
+    "vavgr.bu    $vr1,           $vr9,           $vr1         \n\t"
+    "vavgr.bu    $vr2,           $vr10,          $vr2         \n\t"
+    "vavgr.bu    $vr3,           $vr11,          $vr3         \n\t"
+    "vavgr.bu    $vr4,           $vr12,          $vr4         \n\t"
+    "vavgr.bu    $vr5,           $vr13,          $vr5         \n\t"
+    "vavgr.bu    $vr6,           $vr14,          $vr6         \n\t"
+    "vavgr.bu    $vr7,           $vr15,          $vr7         \n\t"
+
+    "vstelm.d    $vr0,           %[dst],         0,  0        \n\t"
+    "add.d       %[dst],         %[dst],         %[dstStride] \n\t"
+    "vstelm.d    $vr1,           %[dst],         0,  0        \n\t"
+    "add.d       %[dst],         %[dst],         %[dstStride] \n\t"
+    "vstelm.d    $vr2,           %[dst],         0,  0        \n\t"
+    "add.d       %[dst],         %[dst],         %[dstStride] \n\t"
+    "vstelm.d    $vr3,           %[dst],         0,  0        \n\t"
+    "add.d       %[dst],         %[dst],         %[dstStride] \n\t"
+    "vstelm.d    $vr4,           %[dst],         0,  0        \n\t"
+    "add.d       %[dst],         %[dst],         %[dstStride] \n\t"
+    "vstelm.d    $vr5,           %[dst],         0,  0        \n\t"
+    "add.d       %[dst],         %[dst],         %[dstStride] \n\t"
+    "vstelm.d    $vr6,           %[dst],         0,  0        \n\t"
+    "add.d       %[dst],         %[dst],         %[dstStride] \n\t"
+    "vstelm.d    $vr7,           %[dst],         0,  0        \n\t"
+    : [dst]"+&r"(dst), [tmp]"+&r"(tmp), [half]"+&r"(half),
+      [src]"+&r"(src), [stride_2]"=&r"(stride_2),
+      [stride_3]"=&r"(stride_3), [stride_4]"=&r"(stride_4)
+    : [dstStride]"r"(dstStride), [srcStride]"r"(srcStride)
+    : "memory"
+    );
+}
+
+/* put_pixels16_8_lsx: dst = src */
+static av_always_inline void
+put_pixels16_8_lsx(uint8_t *dst, const uint8_t *src, ptrdiff_t stride)
+{
+    ptrdiff_t stride_2, stride_3, stride_4;
+    __asm__ volatile (
+    "slli.d     %[stride_2],     %[stride],      1            \n\t"
+    "add.d      %[stride_3],     %[stride_2],    %[stride]    \n\t"
+    "slli.d     %[stride_4],     %[stride_2],    1            \n\t"
+    "vld        $vr0,            %[src],         0            \n\t"
+    "vldx       $vr1,            %[src],         %[stride]    \n\t"
+    "vldx       $vr2,            %[src],         %[stride_2]  \n\t"
+    "vldx       $vr3,            %[src],         %[stride_3]  \n\t"
+    "add.d      %[src],          %[src],         %[stride_4]  \n\t"
+    "vld        $vr4,            %[src],         0            \n\t"
+    "vldx       $vr5,            %[src],         %[stride]    \n\t"
+    "vldx       $vr6,            %[src],         %[stride_2]  \n\t"
+    "vldx       $vr7,            %[src],         %[stride_3]  \n\t"
+    "add.d      %[src],          %[src],         %[stride_4]  \n\t"
+
+    "vst        $vr0,            %[dst],         0            \n\t"
+    "vstx       $vr1,            %[dst],         %[stride]    \n\t"
+    "vstx       $vr2,            %[dst],         %[stride_2]  \n\t"
+    "vstx       $vr3,            %[dst],         %[stride_3]  \n\t"
+    "add.d      %[dst],          %[dst],         %[stride_4]  \n\t"
+    "vst        $vr4,            %[dst],         0            \n\t"
+    "vstx       $vr5,            %[dst],         %[stride]    \n\t"
+    "vstx       $vr6,            %[dst],         %[stride_2]  \n\t"
+    "vstx       $vr7,            %[dst],         %[stride_3]  \n\t"
+    "add.d      %[dst],          %[dst],         %[stride_4]  \n\t"
+
+    "vld        $vr0,            %[src],         0            \n\t"
+    "vldx       $vr1,            %[src],         %[stride]    \n\t"
+    "vldx       $vr2,            %[src],         %[stride_2]  \n\t"
+    "vldx       $vr3,            %[src],         %[stride_3]  \n\t"
+    "add.d      %[src],          %[src],         %[stride_4]  \n\t"
+    "vld        $vr4,            %[src],         0            \n\t"
+    "vldx       $vr5,            %[src],         %[stride]    \n\t"
+    "vldx       $vr6,            %[src],         %[stride_2]  \n\t"
+    "vldx       $vr7,            %[src],         %[stride_3]  \n\t"
+
+    "vst        $vr0,            %[dst],         0            \n\t"
+    "vstx       $vr1,            %[dst],         %[stride]    \n\t"
+    "vstx       $vr2,            %[dst],         %[stride_2]  \n\t"
+    "vstx       $vr3,            %[dst],         %[stride_3]  \n\t"
+    "add.d      %[dst],          %[dst],         %[stride_4]  \n\t"
+    "vst        $vr4,            %[dst],         0            \n\t"
+    "vstx       $vr5,            %[dst],         %[stride]    \n\t"
+    "vstx       $vr6,            %[dst],         %[stride_2]  \n\t"
+    "vstx       $vr7,            %[dst],         %[stride_3]  \n\t"
+    : [dst]"+&r"(dst),            [src]"+&r"(src),
+      [stride_2]"=&r"(stride_2),  [stride_3]"=&r"(stride_3),
+      [stride_4]"=&r"(stride_4)
+    : [stride]"r"(stride)
+    : "memory"
+    );
+}
+
+/* avg_pixels16_8_lsx    : dst = avg(src, dst)
+ * put_pixels16_l2_8_lsx: dst = avg(src, half) , half stride is 8.
+ * avg_pixels16_l2_8_lsx: dst = avg(avg(src, half), dst) , half stride is 8.*/
+static av_always_inline void
+avg_pixels16_8_lsx(uint8_t *dst, const uint8_t *src, ptrdiff_t stride)
+{
+    uint8_t *tmp = dst;
+    ptrdiff_t stride_2, stride_3, stride_4;
+    __asm__ volatile (
+    /* h0~h7 */
+    "slli.d     %[stride_2],     %[stride],      1            \n\t"
+    "add.d      %[stride_3],     %[stride_2],    %[stride]    \n\t"
+    "slli.d     %[stride_4],     %[stride_2],    1            \n\t"
+    "vld        $vr0,            %[src],         0            \n\t"
+    "vldx       $vr1,            %[src],         %[stride]    \n\t"
+    "vldx       $vr2,            %[src],         %[stride_2]  \n\t"
+    "vldx       $vr3,            %[src],         %[stride_3]  \n\t"
+    "add.d      %[src],          %[src],         %[stride_4]  \n\t"
+    "vld        $vr4,            %[src],         0            \n\t"
+    "vldx       $vr5,            %[src],         %[stride]    \n\t"
+    "vldx       $vr6,            %[src],         %[stride_2]  \n\t"
+    "vldx       $vr7,            %[src],         %[stride_3]  \n\t"
+    "add.d      %[src],          %[src],         %[stride_4]  \n\t"
+
+    "vld        $vr8,            %[tmp],         0            \n\t"
+    "vldx       $vr9,            %[tmp],         %[stride]    \n\t"
+    "vldx       $vr10,           %[tmp],         %[stride_2]  \n\t"
+    "vldx       $vr11,           %[tmp],         %[stride_3]  \n\t"
+    "add.d      %[tmp],          %[tmp],         %[stride_4]  \n\t"
+    "vld        $vr12,           %[tmp],         0            \n\t"
+    "vldx       $vr13,           %[tmp],         %[stride]    \n\t"
+    "vldx       $vr14,           %[tmp],         %[stride_2]  \n\t"
+    "vldx       $vr15,           %[tmp],         %[stride_3]  \n\t"
+    "add.d      %[tmp],          %[tmp],         %[stride_4]  \n\t"
+
+    "vavgr.bu   $vr0,            $vr8,           $vr0         \n\t"
+    "vavgr.bu   $vr1,            $vr9,           $vr1         \n\t"
+    "vavgr.bu   $vr2,            $vr10,          $vr2         \n\t"
+    "vavgr.bu   $vr3,            $vr11,          $vr3         \n\t"
+    "vavgr.bu   $vr4,            $vr12,          $vr4         \n\t"
+    "vavgr.bu   $vr5,            $vr13,          $vr5         \n\t"
+    "vavgr.bu   $vr6,            $vr14,          $vr6         \n\t"
+    "vavgr.bu   $vr7,            $vr15,          $vr7         \n\t"
+
+    "vst        $vr0,            %[dst],         0            \n\t"
+    "vstx       $vr1,            %[dst],         %[stride]    \n\t"
+    "vstx       $vr2,            %[dst],         %[stride_2]  \n\t"
+    "vstx       $vr3,            %[dst],         %[stride_3]  \n\t"
+    "add.d      %[dst],          %[dst],         %[stride_4]  \n\t"
+    "vst        $vr4,            %[dst],         0            \n\t"
+    "vstx       $vr5,            %[dst],         %[stride]    \n\t"
+    "vstx       $vr6,            %[dst],         %[stride_2]  \n\t"
+    "vstx       $vr7,            %[dst],         %[stride_3]  \n\t"
+    "add.d      %[dst],          %[dst],         %[stride_4]  \n\t"
+
+    /* h8~h15 */
+    "vld        $vr0,            %[src],         0            \n\t"
+    "vldx       $vr1,            %[src],         %[stride]    \n\t"
+    "vldx       $vr2,            %[src],         %[stride_2]  \n\t"
+    "vldx       $vr3,            %[src],         %[stride_3]  \n\t"
+    "add.d      %[src],          %[src],         %[stride_4]  \n\t"
+    "vld        $vr4,            %[src],         0            \n\t"
+    "vldx       $vr5,            %[src],         %[stride]    \n\t"
+    "vldx       $vr6,            %[src],         %[stride_2]  \n\t"
+    "vldx       $vr7,            %[src],         %[stride_3]  \n\t"
+
+    "vld        $vr8,            %[tmp],         0            \n\t"
+    "vldx       $vr9,            %[tmp],         %[stride]    \n\t"
+    "vldx       $vr10,           %[tmp],         %[stride_2]  \n\t"
+    "vldx       $vr11,           %[tmp],         %[stride_3]  \n\t"
+    "add.d      %[tmp],          %[tmp],         %[stride_4]  \n\t"
+    "vld        $vr12,           %[tmp],         0            \n\t"
+    "vldx       $vr13,           %[tmp],         %[stride]    \n\t"
+    "vldx       $vr14,           %[tmp],         %[stride_2]  \n\t"
+    "vldx       $vr15,           %[tmp],         %[stride_3]  \n\t"
+
+    "vavgr.bu    $vr0,           $vr8,           $vr0         \n\t"
+    "vavgr.bu    $vr1,           $vr9,           $vr1         \n\t"
+    "vavgr.bu    $vr2,           $vr10,          $vr2         \n\t"
+    "vavgr.bu    $vr3,           $vr11,          $vr3         \n\t"
+    "vavgr.bu    $vr4,           $vr12,          $vr4         \n\t"
+    "vavgr.bu    $vr5,           $vr13,          $vr5         \n\t"
+    "vavgr.bu    $vr6,           $vr14,          $vr6         \n\t"
+    "vavgr.bu    $vr7,           $vr15,          $vr7         \n\t"
+
+    "vst        $vr0,            %[dst],         0            \n\t"
+    "vstx       $vr1,            %[dst],         %[stride]    \n\t"
+    "vstx       $vr2,            %[dst],         %[stride_2]  \n\t"
+    "vstx       $vr3,            %[dst],         %[stride_3]  \n\t"
+    "add.d      %[dst],          %[dst],         %[stride_4]  \n\t"
+    "vst        $vr4,            %[dst],         0            \n\t"
+    "vstx       $vr5,            %[dst],         %[stride]    \n\t"
+    "vstx       $vr6,            %[dst],         %[stride_2]  \n\t"
+    "vstx       $vr7,            %[dst],         %[stride_3]  \n\t"
+    : [dst]"+&r"(dst), [tmp]"+&r"(tmp), [src]"+&r"(src),
+      [stride_2]"=&r"(stride_2),  [stride_3]"=&r"(stride_3),
+      [stride_4]"=&r"(stride_4)
+    : [stride]"r"(stride)
+    : "memory"
+    );
+}
+
+/* avg_pixels16_8_lsx   : dst = avg(src, dst)
+ * put_pixels16_l2_8_lsx: dst = avg(src, half) , half stride is 8.
+ * avg_pixels16_l2_8_lsx: dst = avg(avg(src, half), dst) , half stride is 8.*/
+static av_always_inline void
+put_pixels16_l2_8_lsx(uint8_t *dst, const uint8_t *src, uint8_t *half,
+                      ptrdiff_t dstStride, ptrdiff_t srcStride)
+{
+    ptrdiff_t stride_2, stride_3, stride_4;
+    ptrdiff_t dstride_2, dstride_3, dstride_4;
+    __asm__ volatile (
+    "slli.d     %[stride_2],     %[srcStride],   1            \n\t"
+    "add.d      %[stride_3],     %[stride_2],    %[srcStride] \n\t"
+    "slli.d     %[stride_4],     %[stride_2],    1            \n\t"
+    "slli.d     %[dstride_2],    %[dstStride],   1            \n\t"
+    "add.d      %[dstride_3],    %[dstride_2],   %[dstStride] \n\t"
+    "slli.d     %[dstride_4],    %[dstride_2],   1            \n\t"
+    /* h0~h7 */
+    "vld        $vr0,            %[src],         0            \n\t"
+    "vldx       $vr1,            %[src],         %[srcStride] \n\t"
+    "vldx       $vr2,            %[src],         %[stride_2]  \n\t"
+    "vldx       $vr3,            %[src],         %[stride_3]  \n\t"
+    "add.d      %[src],          %[src],         %[stride_4]  \n\t"
+    "vld        $vr4,            %[src],         0            \n\t"
+    "vldx       $vr5,            %[src],         %[srcStride] \n\t"
+    "vldx       $vr6,            %[src],         %[stride_2]  \n\t"
+    "vldx       $vr7,            %[src],         %[stride_3]  \n\t"
+    "add.d      %[src],          %[src],         %[stride_4]  \n\t"
+
+    "vld        $vr8,            %[half],        0x00         \n\t"
+    "vld        $vr9,            %[half],        0x10         \n\t"
+    "vld        $vr10,           %[half],        0x20         \n\t"
+    "vld        $vr11,           %[half],        0x30         \n\t"
+    "vld        $vr12,           %[half],        0x40         \n\t"
+    "vld        $vr13,           %[half],        0x50         \n\t"
+    "vld        $vr14,           %[half],        0x60         \n\t"
+    "vld        $vr15,           %[half],        0x70         \n\t"
+
+    "vavgr.bu   $vr0,            $vr8,           $vr0         \n\t"
+    "vavgr.bu   $vr1,            $vr9,           $vr1         \n\t"
+    "vavgr.bu   $vr2,            $vr10,          $vr2         \n\t"
+    "vavgr.bu   $vr3,            $vr11,          $vr3         \n\t"
+    "vavgr.bu   $vr4,            $vr12,          $vr4         \n\t"
+    "vavgr.bu   $vr5,            $vr13,          $vr5         \n\t"
+    "vavgr.bu   $vr6,            $vr14,          $vr6         \n\t"
+    "vavgr.bu   $vr7,            $vr15,          $vr7         \n\t"
+
+    "vst        $vr0,            %[dst],         0            \n\t"
+    "vstx       $vr1,            %[dst],         %[dstStride] \n\t"
+    "vstx       $vr2,            %[dst],         %[dstride_2] \n\t"
+    "vstx       $vr3,            %[dst],         %[dstride_3] \n\t"
+    "add.d      %[dst],          %[dst],         %[dstride_4] \n\t"
+    "vst        $vr4,            %[dst],         0            \n\t"
+    "vstx       $vr5,            %[dst],         %[dstStride] \n\t"
+    "vstx       $vr6,            %[dst],         %[dstride_2] \n\t"
+    "vstx       $vr7,            %[dst],         %[dstride_3] \n\t"
+    "add.d      %[dst],          %[dst],         %[dstride_4] \n\t"
+
+    /* h8~h15 */
+    "vld        $vr0,            %[src],         0            \n\t"
+    "vldx       $vr1,            %[src],         %[srcStride] \n\t"
+    "vldx       $vr2,            %[src],         %[stride_2]  \n\t"
+    "vldx       $vr3,            %[src],         %[stride_3]  \n\t"
+    "add.d      %[src],          %[src],         %[stride_4]  \n\t"
+    "vld        $vr4,            %[src],         0            \n\t"
+    "vldx       $vr5,            %[src],         %[srcStride] \n\t"
+    "vldx       $vr6,            %[src],         %[stride_2]  \n\t"
+    "vldx       $vr7,            %[src],         %[stride_3]  \n\t"
+
+    "vld        $vr8,            %[half],        0x80         \n\t"
+    "vld        $vr9,            %[half],        0x90         \n\t"
+    "vld        $vr10,           %[half],        0xa0         \n\t"
+    "vld        $vr11,           %[half],        0xb0         \n\t"
+    "vld        $vr12,           %[half],        0xc0         \n\t"
+    "vld        $vr13,           %[half],        0xd0         \n\t"
+    "vld        $vr14,           %[half],        0xe0         \n\t"
+    "vld        $vr15,           %[half],        0xf0         \n\t"
+
+    "vavgr.bu   $vr0,            $vr8,           $vr0         \n\t"
+    "vavgr.bu   $vr1,            $vr9,           $vr1         \n\t"
+    "vavgr.bu   $vr2,            $vr10,          $vr2         \n\t"
+    "vavgr.bu   $vr3,            $vr11,          $vr3         \n\t"
+    "vavgr.bu   $vr4,            $vr12,          $vr4         \n\t"
+    "vavgr.bu   $vr5,            $vr13,          $vr5         \n\t"
+    "vavgr.bu   $vr6,            $vr14,          $vr6         \n\t"
+    "vavgr.bu   $vr7,            $vr15,          $vr7         \n\t"
+
+    "vst        $vr0,            %[dst],         0            \n\t"
+    "vstx       $vr1,            %[dst],         %[dstStride] \n\t"
+    "vstx       $vr2,            %[dst],         %[dstride_2] \n\t"
+    "vstx       $vr3,            %[dst],         %[dstride_3] \n\t"
+    "add.d      %[dst],          %[dst],         %[dstride_4] \n\t"
+    "vst        $vr4,            %[dst],         0            \n\t"
+    "vstx       $vr5,            %[dst],         %[dstStride] \n\t"
+    "vstx       $vr6,            %[dst],         %[dstride_2] \n\t"
+    "vstx       $vr7,            %[dst],         %[dstride_3] \n\t"
+    : [dst]"+&r"(dst), [half]"+&r"(half), [src]"+&r"(src),
+      [stride_2]"=&r"(stride_2),  [stride_3]"=&r"(stride_3),
+      [stride_4]"=&r"(stride_4),  [dstride_2]"=&r"(dstride_2),
+      [dstride_3]"=&r"(dstride_3), [dstride_4]"=&r"(dstride_4)
+    : [dstStride]"r"(dstStride), [srcStride]"r"(srcStride)
+    : "memory"
+    );
+}
+
+/* avg_pixels16_8_lsx    : dst = avg(src, dst)
+ * put_pixels16_l2_8_lsx: dst = avg(src, half) , half stride is 8.
+ * avg_pixels16_l2_8_lsx: dst = avg(avg(src, half), dst) , half stride is 8.*/
+static av_always_inline void
+avg_pixels16_l2_8_lsx(uint8_t *dst, const uint8_t *src, uint8_t *half,
+                      ptrdiff_t dstStride, ptrdiff_t srcStride)
+{
+    uint8_t *tmp = dst;
+    ptrdiff_t stride_2, stride_3, stride_4;
+    ptrdiff_t dstride_2, dstride_3, dstride_4;
+    __asm__ volatile (
+    "slli.d     %[stride_2],     %[srcStride],   1            \n\t"
+    "add.d      %[stride_3],     %[stride_2],    %[srcStride] \n\t"
+    "slli.d     %[stride_4],     %[stride_2],    1            \n\t"
+    "slli.d     %[dstride_2],    %[dstStride],   1            \n\t"
+    "add.d      %[dstride_3],    %[dstride_2],   %[dstStride] \n\t"
+    "slli.d     %[dstride_4],    %[dstride_2],   1            \n\t"
+    /* h0~h7 */
+    "vld        $vr0,            %[src],         0            \n\t"
+    "vldx       $vr1,            %[src],         %[srcStride] \n\t"
+    "vldx       $vr2,            %[src],         %[stride_2]  \n\t"
+    "vldx       $vr3,            %[src],         %[stride_3]  \n\t"
+    "add.d      %[src],          %[src],         %[stride_4]  \n\t"
+    "vld        $vr4,            %[src],         0            \n\t"
+    "vldx       $vr5,            %[src],         %[srcStride] \n\t"
+    "vldx       $vr6,            %[src],         %[stride_2]  \n\t"
+    "vldx       $vr7,            %[src],         %[stride_3]  \n\t"
+    "add.d      %[src],          %[src],         %[stride_4]  \n\t"
+
+    "vld        $vr8,            %[half],        0x00         \n\t"
+    "vld        $vr9,            %[half],        0x10         \n\t"
+    "vld        $vr10,           %[half],        0x20         \n\t"
+    "vld        $vr11,           %[half],        0x30         \n\t"
+    "vld        $vr12,           %[half],        0x40         \n\t"
+    "vld        $vr13,           %[half],        0x50         \n\t"
+    "vld        $vr14,           %[half],        0x60         \n\t"
+    "vld        $vr15,           %[half],        0x70         \n\t"
+
+    "vavgr.bu   $vr0,            $vr8,           $vr0         \n\t"
+    "vavgr.bu   $vr1,            $vr9,           $vr1         \n\t"
+    "vavgr.bu   $vr2,            $vr10,          $vr2         \n\t"
+    "vavgr.bu   $vr3,            $vr11,          $vr3         \n\t"
+    "vavgr.bu   $vr4,            $vr12,          $vr4         \n\t"
+    "vavgr.bu   $vr5,            $vr13,          $vr5         \n\t"
+    "vavgr.bu   $vr6,            $vr14,          $vr6         \n\t"
+    "vavgr.bu   $vr7,            $vr15,          $vr7         \n\t"
+
+    "vld        $vr8,            %[tmp],         0            \n\t"
+    "vldx       $vr9,            %[tmp],         %[dstStride] \n\t"
+    "vldx       $vr10,           %[tmp],         %[dstride_2] \n\t"
+    "vldx       $vr11,           %[tmp],         %[dstride_3] \n\t"
+    "add.d      %[tmp],          %[tmp],         %[dstride_4] \n\t"
+    "vld        $vr12,           %[tmp],         0            \n\t"
+    "vldx       $vr13,           %[tmp],         %[dstStride] \n\t"
+    "vldx       $vr14,           %[tmp],         %[dstride_2] \n\t"
+    "vldx       $vr15,           %[tmp],         %[dstride_3] \n\t"
+    "add.d      %[tmp],          %[tmp],         %[dstride_4] \n\t"
+
+    "vavgr.bu    $vr0,           $vr8,           $vr0         \n\t"
+    "vavgr.bu    $vr1,           $vr9,           $vr1         \n\t"
+    "vavgr.bu    $vr2,           $vr10,          $vr2         \n\t"
+    "vavgr.bu    $vr3,           $vr11,          $vr3         \n\t"
+    "vavgr.bu    $vr4,           $vr12,          $vr4         \n\t"
+    "vavgr.bu    $vr5,           $vr13,          $vr5         \n\t"
+    "vavgr.bu    $vr6,           $vr14,          $vr6         \n\t"
+    "vavgr.bu    $vr7,           $vr15,          $vr7         \n\t"
+
+    "vst        $vr0,            %[dst],         0            \n\t"
+    "vstx       $vr1,            %[dst],         %[dstStride] \n\t"
+    "vstx       $vr2,            %[dst],         %[dstride_2] \n\t"
+    "vstx       $vr3,            %[dst],         %[dstride_3] \n\t"
+    "add.d      %[dst],          %[dst],         %[dstride_4] \n\t"
+    "vst        $vr4,            %[dst],         0            \n\t"
+    "vstx       $vr5,            %[dst],         %[dstStride] \n\t"
+    "vstx       $vr6,            %[dst],         %[dstride_2] \n\t"
+    "vstx       $vr7,            %[dst],         %[dstride_3] \n\t"
+    "add.d      %[dst],          %[dst],         %[dstride_4] \n\t"
+
+    /* h8~h15    */
+    "vld        $vr0,            %[src],         0            \n\t"
+    "vldx       $vr1,            %[src],         %[srcStride] \n\t"
+    "vldx       $vr2,            %[src],         %[stride_2]  \n\t"
+    "vldx       $vr3,            %[src],         %[stride_3]  \n\t"
+    "add.d      %[src],          %[src],         %[stride_4]  \n\t"
+    "vld        $vr4,            %[src],         0            \n\t"
+    "vldx       $vr5,            %[src],         %[srcStride] \n\t"
+    "vldx       $vr6,            %[src],         %[stride_2]  \n\t"
+    "vldx       $vr7,            %[src],         %[stride_3]  \n\t"
+
+    "vld        $vr8,            %[half],        0x80         \n\t"
+    "vld        $vr9,            %[half],        0x90         \n\t"
+    "vld        $vr10,           %[half],        0xa0         \n\t"
+    "vld        $vr11,           %[half],        0xb0         \n\t"
+    "vld        $vr12,           %[half],        0xc0         \n\t"
+    "vld        $vr13,           %[half],        0xd0         \n\t"
+    "vld        $vr14,           %[half],        0xe0         \n\t"
+    "vld        $vr15,           %[half],        0xf0         \n\t"
+
+    "vavgr.bu    $vr0,           $vr8,           $vr0         \n\t"
+    "vavgr.bu    $vr1,           $vr9,           $vr1         \n\t"
+    "vavgr.bu    $vr2,           $vr10,          $vr2         \n\t"
+    "vavgr.bu    $vr3,           $vr11,          $vr3         \n\t"
+    "vavgr.bu    $vr4,           $vr12,          $vr4         \n\t"
+    "vavgr.bu    $vr5,           $vr13,          $vr5         \n\t"
+    "vavgr.bu    $vr6,           $vr14,          $vr6         \n\t"
+    "vavgr.bu    $vr7,           $vr15,          $vr7         \n\t"
+
+    "vld        $vr8,            %[tmp],         0            \n\t"
+    "vldx       $vr9,            %[tmp],         %[dstStride] \n\t"
+    "vldx       $vr10,           %[tmp],         %[dstride_2] \n\t"
+    "vldx       $vr11,           %[tmp],         %[dstride_3] \n\t"
+    "add.d      %[tmp],          %[tmp],         %[dstride_4] \n\t"
+    "vld        $vr12,           %[tmp],         0            \n\t"
+    "vldx       $vr13,           %[tmp],         %[dstStride] \n\t"
+    "vldx       $vr14,           %[tmp],         %[dstride_2] \n\t"
+    "vldx       $vr15,           %[tmp],         %[dstride_3] \n\t"
+
+    "vavgr.bu    $vr0,           $vr8,           $vr0         \n\t"
+    "vavgr.bu    $vr1,           $vr9,           $vr1         \n\t"
+    "vavgr.bu    $vr2,           $vr10,          $vr2         \n\t"
+    "vavgr.bu    $vr3,           $vr11,          $vr3         \n\t"
+    "vavgr.bu    $vr4,           $vr12,          $vr4         \n\t"
+    "vavgr.bu    $vr5,           $vr13,          $vr5         \n\t"
+    "vavgr.bu    $vr6,           $vr14,          $vr6         \n\t"
+    "vavgr.bu    $vr7,           $vr15,          $vr7         \n\t"
+
+    "vst        $vr0,            %[dst],         0            \n\t"
+    "vstx       $vr1,            %[dst],         %[dstStride] \n\t"
+    "vstx       $vr2,            %[dst],         %[dstride_2] \n\t"
+    "vstx       $vr3,            %[dst],         %[dstride_3] \n\t"
+    "add.d      %[dst],          %[dst],         %[dstride_4] \n\t"
+    "vst        $vr4,            %[dst],         0            \n\t"
+    "vstx       $vr5,            %[dst],         %[dstStride] \n\t"
+    "vstx       $vr6,            %[dst],         %[dstride_2] \n\t"
+    "vstx       $vr7,            %[dst],         %[dstride_3] \n\t"
+    : [dst]"+&r"(dst), [tmp]"+&r"(tmp), [half]"+&r"(half), [src]"+&r"(src),
+      [stride_2]"=&r"(stride_2),  [stride_3]"=&r"(stride_3),
+      [stride_4]"=&r"(stride_4),  [dstride_2]"=&r"(dstride_2),
+      [dstride_3]"=&r"(dstride_3), [dstride_4]"=&r"(dstride_4)
+    : [dstStride]"r"(dstStride), [srcStride]"r"(srcStride)
+    : "memory"
+    );
+}
+
+#define QPEL8_H_LOWPASS(out_v)                                               \
+    src00 = __lasx_xvld(src, - 2);                                           \
+    src += srcStride;                                                        \
+    src10 = __lasx_xvld(src, - 2);                                           \
+    src += srcStride;                                                        \
+    src00 = __lasx_xvpermi_q(src00, src10, 0x02);                            \
+    src01 = __lasx_xvshuf_b(src00, src00, (__m256i)mask1);                   \
+    src02 = __lasx_xvshuf_b(src00, src00, (__m256i)mask2);                   \
+    src03 = __lasx_xvshuf_b(src00, src00, (__m256i)mask3);                   \
+    src04 = __lasx_xvshuf_b(src00, src00, (__m256i)mask4);                   \
+    src05 = __lasx_xvshuf_b(src00, src00, (__m256i)mask5);                   \
+    DUP2_ARG2(__lasx_xvaddwl_h_bu, src02, src03, src01, src04, src02, src01);\
+    src00 = __lasx_xvaddwl_h_bu(src00, src05);                               \
+    src02 = __lasx_xvmul_h(src02, h_20);                                     \
+    src01 = __lasx_xvmul_h(src01, h_5);                                      \
+    src02 = __lasx_xvssub_h(src02, src01);                                   \
+    src02 = __lasx_xvsadd_h(src02, src00);                                   \
+    src02 = __lasx_xvsadd_h(src02, h_16);                                    \
+    out_v = __lasx_xvssrani_bu_h(src02, src02, 5);                           \
+
+static av_always_inline void
+put_h264_qpel8_h_lowpass_lasx(uint8_t *dst, const uint8_t *src, int dstStride,
+                              int srcStride)
+{
+    int dstStride_2x = dstStride << 1;
+    __m256i src00, src01, src02, src03, src04, src05, src10;
+    __m256i out0, out1, out2, out3;
+    __m256i h_20 = __lasx_xvldi(0x414);
+    __m256i h_5  = __lasx_xvldi(0x405);
+    __m256i h_16 = __lasx_xvldi(0x410);
+    __m256i mask1 = {0x0807060504030201, 0x0, 0x0807060504030201, 0x0};
+    __m256i mask2 = {0x0908070605040302, 0x0, 0x0908070605040302, 0x0};
+    __m256i mask3 = {0x0a09080706050403, 0x0, 0x0a09080706050403, 0x0};
+    __m256i mask4 = {0x0b0a090807060504, 0x0, 0x0b0a090807060504, 0x0};
+    __m256i mask5 = {0x0c0b0a0908070605, 0x0, 0x0c0b0a0908070605, 0x0};
+
+    QPEL8_H_LOWPASS(out0)
+    QPEL8_H_LOWPASS(out1)
+    QPEL8_H_LOWPASS(out2)
+    QPEL8_H_LOWPASS(out3)
+    __lasx_xvstelm_d(out0, dst, 0, 0);
+    __lasx_xvstelm_d(out0, dst + dstStride, 0, 2);
+    dst += dstStride_2x;
+    __lasx_xvstelm_d(out1, dst, 0, 0);
+    __lasx_xvstelm_d(out1, dst + dstStride, 0, 2);
+    dst += dstStride_2x;
+    __lasx_xvstelm_d(out2, dst, 0, 0);
+    __lasx_xvstelm_d(out2, dst + dstStride, 0, 2);
+    dst += dstStride_2x;
+    __lasx_xvstelm_d(out3, dst, 0, 0);
+    __lasx_xvstelm_d(out3, dst + dstStride, 0, 2);
+}
+
+#define QPEL8_V_LOWPASS(src0, src1, src2, src3, src4, src5, src6,       \
+                        tmp0, tmp1, tmp2, tmp3, tmp4, tmp5)             \
+{                                                                       \
+    tmp0 = __lasx_xvpermi_q(src0, src1, 0x02);                          \
+    tmp1 = __lasx_xvpermi_q(src1, src2, 0x02);                          \
+    tmp2 = __lasx_xvpermi_q(src2, src3, 0x02);                          \
+    tmp3 = __lasx_xvpermi_q(src3, src4, 0x02);                          \
+    tmp4 = __lasx_xvpermi_q(src4, src5, 0x02);                          \
+    tmp5 = __lasx_xvpermi_q(src5, src6, 0x02);                          \
+    DUP2_ARG2(__lasx_xvaddwl_h_bu, tmp2, tmp3, tmp1, tmp4, tmp2, tmp1); \
+    tmp0 = __lasx_xvaddwl_h_bu(tmp0, tmp5);                             \
+    tmp2 = __lasx_xvmul_h(tmp2, h_20);                                  \
+    tmp1 = __lasx_xvmul_h(tmp1, h_5);                                   \
+    tmp2 = __lasx_xvssub_h(tmp2, tmp1);                                 \
+    tmp2 = __lasx_xvsadd_h(tmp2, tmp0);                                 \
+    tmp2 = __lasx_xvsadd_h(tmp2, h_16);                                 \
+    tmp2 = __lasx_xvssrani_bu_h(tmp2, tmp2, 5);                         \
+}
+
+static av_always_inline void
+put_h264_qpel8_v_lowpass_lasx(uint8_t *dst, uint8_t *src, int dstStride,
+                              int srcStride)
+{
+    int srcStride_2x = srcStride << 1;
+    int dstStride_2x = dstStride << 1;
+    int srcStride_4x = srcStride << 2;
+    int srcStride_3x = srcStride_2x + srcStride;
+    __m256i src00, src01, src02, src03, src04, src05, src06;
+    __m256i src07, src08, src09, src10, src11, src12;
+    __m256i tmp00, tmp01, tmp02, tmp03, tmp04, tmp05;
+    __m256i h_20 = __lasx_xvldi(0x414);
+    __m256i h_5  = __lasx_xvldi(0x405);
+    __m256i h_16 = __lasx_xvldi(0x410);
+
+    DUP2_ARG2(__lasx_xvld, src - srcStride_2x, 0, src - srcStride, 0,
+              src00, src01);
+    src02 = __lasx_xvld(src, 0);
+    DUP4_ARG2(__lasx_xvldx, src, srcStride, src, srcStride_2x, src,
+              srcStride_3x, src, srcStride_4x, src03, src04, src05, src06);
+    src += srcStride_4x;
+    DUP4_ARG2(__lasx_xvldx, src, srcStride, src, srcStride_2x, src,
+              srcStride_3x, src, srcStride_4x, src07, src08, src09, src10);
+    src += srcStride_4x;
+    DUP2_ARG2(__lasx_xvldx, src, srcStride, src, srcStride_2x, src11, src12);
+
+    QPEL8_V_LOWPASS(src00, src01, src02, src03, src04, src05, src06,
+                    tmp00, tmp01, tmp02, tmp03, tmp04, tmp05);
+    __lasx_xvstelm_d(tmp02, dst, 0, 0);
+    __lasx_xvstelm_d(tmp02, dst + dstStride, 0, 2);
+    dst += dstStride_2x;
+    QPEL8_V_LOWPASS(src02, src03, src04, src05, src06, src07, src08,
+                    tmp00, tmp01, tmp02, tmp03, tmp04, tmp05);
+    __lasx_xvstelm_d(tmp02, dst, 0, 0);
+    __lasx_xvstelm_d(tmp02, dst + dstStride, 0, 2);
+    dst += dstStride_2x;
+    QPEL8_V_LOWPASS(src04, src05, src06, src07, src08, src09, src10,
+                    tmp00, tmp01, tmp02, tmp03, tmp04, tmp05);
+    __lasx_xvstelm_d(tmp02, dst, 0, 0);
+    __lasx_xvstelm_d(tmp02, dst + dstStride, 0, 2);
+    dst += dstStride_2x;
+    QPEL8_V_LOWPASS(src06, src07, src08, src09, src10, src11, src12,
+                    tmp00, tmp01, tmp02, tmp03, tmp04, tmp05);
+    __lasx_xvstelm_d(tmp02, dst, 0, 0);
+    __lasx_xvstelm_d(tmp02, dst + dstStride, 0, 2);
+}
+
+static av_always_inline void
+avg_h264_qpel8_v_lowpass_lasx(uint8_t *dst, uint8_t *src, int dstStride,
+                              int srcStride)
+{
+    int srcStride_2x = srcStride << 1;
+    int srcStride_4x = srcStride << 2;
+    int dstStride_2x = dstStride << 1;
+    int dstStride_4x = dstStride << 2;
+    int srcStride_3x = srcStride_2x + srcStride;
+    int dstStride_3x = dstStride_2x + dstStride;
+    __m256i src00, src01, src02, src03, src04, src05, src06;
+    __m256i src07, src08, src09, src10, src11, src12, tmp00;
+    __m256i tmp01, tmp02, tmp03, tmp04, tmp05, tmp06, tmp07, tmp08, tmp09;
+    __m256i h_20 = __lasx_xvldi(0x414);
+    __m256i h_5  = __lasx_xvldi(0x405);
+    __m256i h_16 = __lasx_xvldi(0x410);
+
+
+    DUP2_ARG2(__lasx_xvld, src - srcStride_2x, 0, src - srcStride, 0,
+              src00, src01);
+    src02 = __lasx_xvld(src, 0);
+    DUP4_ARG2(__lasx_xvldx, src, srcStride, src, srcStride_2x, src,
+              srcStride_3x, src, srcStride_4x, src03, src04, src05, src06);
+    src += srcStride_4x;
+    DUP4_ARG2(__lasx_xvldx, src, srcStride, src, srcStride_2x, src,
+              srcStride_3x, src, srcStride_4x, src07, src08, src09, src10);
+    src += srcStride_4x;
+    DUP2_ARG2(__lasx_xvldx, src, srcStride, src, srcStride_2x, src11, src12);
+
+    tmp06 = __lasx_xvld(dst, 0);
+    DUP4_ARG2(__lasx_xvldx, dst, dstStride, dst, dstStride_2x,
+              dst, dstStride_3x, dst, dstStride_4x,
+              tmp07, tmp02, tmp03, tmp04);
+    dst += dstStride_4x;
+    DUP2_ARG2(__lasx_xvldx, dst, dstStride, dst, dstStride_2x,
+              tmp05, tmp00);
+    tmp01 = __lasx_xvldx(dst, dstStride_3x);
+    dst -= dstStride_4x;
+
+    tmp06 = __lasx_xvpermi_q(tmp06, tmp07, 0x02);
+    tmp07 = __lasx_xvpermi_q(tmp02, tmp03, 0x02);
+    tmp08 = __lasx_xvpermi_q(tmp04, tmp05, 0x02);
+    tmp09 = __lasx_xvpermi_q(tmp00, tmp01, 0x02);
+
+    QPEL8_V_LOWPASS(src00, src01, src02, src03, src04, src05, src06,
+                    tmp00, tmp01, tmp02, tmp03, tmp04, tmp05);
+    tmp06 = __lasx_xvavgr_bu(tmp06, tmp02);
+    __lasx_xvstelm_d(tmp06, dst, 0, 0);
+    __lasx_xvstelm_d(tmp06, dst + dstStride, 0, 2);
+    dst += dstStride_2x;
+    QPEL8_V_LOWPASS(src02, src03, src04, src05, src06, src07, src08,
+                    tmp00, tmp01, tmp02, tmp03, tmp04, tmp05);
+    tmp07 = __lasx_xvavgr_bu(tmp07, tmp02);
+    __lasx_xvstelm_d(tmp07, dst, 0, 0);
+    __lasx_xvstelm_d(tmp07, dst + dstStride, 0, 2);
+    dst += dstStride_2x;
+    QPEL8_V_LOWPASS(src04, src05, src06, src07, src08, src09, src10,
+                    tmp00, tmp01, tmp02, tmp03, tmp04, tmp05);
+    tmp08 = __lasx_xvavgr_bu(tmp08, tmp02);
+    __lasx_xvstelm_d(tmp08, dst, 0, 0);
+    __lasx_xvstelm_d(tmp08, dst + dstStride, 0, 2);
+    dst += dstStride_2x;
+    QPEL8_V_LOWPASS(src06, src07, src08, src09, src10, src11, src12,
+                    tmp00, tmp01, tmp02, tmp03, tmp04, tmp05);
+    tmp09 = __lasx_xvavgr_bu(tmp09, tmp02);
+    __lasx_xvstelm_d(tmp09, dst, 0, 0);
+    __lasx_xvstelm_d(tmp09, dst + dstStride, 0, 2);
+}
+
+#define QPEL8_HV_LOWPASS_H(tmp)                                              \
+{                                                                            \
+    src00 = __lasx_xvld(src, -2);                                            \
+    src += srcStride;                                                        \
+    src10 = __lasx_xvld(src, -2);                                            \
+    src += srcStride;                                                        \
+    src00 = __lasx_xvpermi_q(src00, src10, 0x02);                            \
+    src01 = __lasx_xvshuf_b(src00, src00, (__m256i)mask1);                   \
+    src02 = __lasx_xvshuf_b(src00, src00, (__m256i)mask2);                   \
+    src03 = __lasx_xvshuf_b(src00, src00, (__m256i)mask3);                   \
+    src04 = __lasx_xvshuf_b(src00, src00, (__m256i)mask4);                   \
+    src05 = __lasx_xvshuf_b(src00, src00, (__m256i)mask5);                   \
+    DUP2_ARG2(__lasx_xvaddwl_h_bu, src02, src03, src01, src04, src02, src01);\
+    src00 = __lasx_xvaddwl_h_bu(src00, src05);                               \
+    src02 = __lasx_xvmul_h(src02, h_20);                                     \
+    src01 = __lasx_xvmul_h(src01, h_5);                                      \
+    src02 = __lasx_xvssub_h(src02, src01);                                   \
+    tmp  = __lasx_xvsadd_h(src02, src00);                                    \
+}
+
+#define QPEL8_HV_LOWPASS_V(src0, src1, src2, src3,                       \
+                           src4, src5, temp0, temp1,                     \
+                           temp2, temp3, temp4, temp5,                   \
+                           out)                                          \
+{                                                                        \
+    DUP2_ARG2(__lasx_xvaddwl_w_h, src2, src3, src1, src4, temp0, temp2); \
+    DUP2_ARG2(__lasx_xvaddwh_w_h, src2, src3, src1, src4, temp1, temp3); \
+    temp4 = __lasx_xvaddwl_w_h(src0, src5);                              \
+    temp5 = __lasx_xvaddwh_w_h(src0, src5);                              \
+    temp0 = __lasx_xvmul_w(temp0, w_20);                                 \
+    temp1 = __lasx_xvmul_w(temp1, w_20);                                 \
+    temp2 = __lasx_xvmul_w(temp2, w_5);                                  \
+    temp3 = __lasx_xvmul_w(temp3, w_5);                                  \
+    temp0 = __lasx_xvssub_w(temp0, temp2);                               \
+    temp1 = __lasx_xvssub_w(temp1, temp3);                               \
+    temp0 = __lasx_xvsadd_w(temp0, temp4);                               \
+    temp1 = __lasx_xvsadd_w(temp1, temp5);                               \
+    temp0 = __lasx_xvsadd_w(temp0, w_512);                               \
+    temp1 = __lasx_xvsadd_w(temp1, w_512);                               \
+    temp0 = __lasx_xvssrani_hu_w(temp0, temp0, 10);                      \
+    temp1 = __lasx_xvssrani_hu_w(temp1, temp1, 10);                      \
+    temp0 = __lasx_xvpackev_d(temp1, temp0);                             \
+    out   = __lasx_xvssrani_bu_h(temp0, temp0, 0);                       \
+}
+
+static av_always_inline void
+put_h264_qpel8_hv_lowpass_lasx(uint8_t *dst, const uint8_t *src,
+                               ptrdiff_t dstStride, ptrdiff_t srcStride)
+{
+    __m256i src00, src01, src02, src03, src04, src05, src10;
+    __m256i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6;
+    __m256i tmp7, tmp8, tmp9, tmp10, tmp11, tmp12;
+    __m256i h_20 = __lasx_xvldi(0x414);
+    __m256i h_5  = __lasx_xvldi(0x405);
+    __m256i w_20 = __lasx_xvldi(0x814);
+    __m256i w_5  = __lasx_xvldi(0x805);
+    __m256i w_512 = {512};
+    __m256i mask1 = {0x0807060504030201, 0x0, 0x0807060504030201, 0x0};
+    __m256i mask2 = {0x0908070605040302, 0x0, 0x0908070605040302, 0x0};
+    __m256i mask3 = {0x0a09080706050403, 0x0, 0x0a09080706050403, 0x0};
+    __m256i mask4 = {0x0b0a090807060504, 0x0, 0x0b0a090807060504, 0x0};
+    __m256i mask5 = {0x0c0b0a0908070605, 0x0, 0x0c0b0a0908070605, 0x0};
+
+    w_512 = __lasx_xvreplve0_w(w_512);
+
+    src -= srcStride << 1;
+    QPEL8_HV_LOWPASS_H(tmp0)
+    QPEL8_HV_LOWPASS_H(tmp2)
+    QPEL8_HV_LOWPASS_H(tmp4)
+    QPEL8_HV_LOWPASS_H(tmp6)
+    QPEL8_HV_LOWPASS_H(tmp8)
+    QPEL8_HV_LOWPASS_H(tmp10)
+    QPEL8_HV_LOWPASS_H(tmp12)
+    tmp11 = __lasx_xvpermi_q(tmp12, tmp10, 0x21);
+    tmp9  = __lasx_xvpermi_q(tmp10, tmp8,  0x21);
+    tmp7  = __lasx_xvpermi_q(tmp8,  tmp6,  0x21);
+    tmp5  = __lasx_xvpermi_q(tmp6,  tmp4,  0x21);
+    tmp3  = __lasx_xvpermi_q(tmp4,  tmp2,  0x21);
+    tmp1  = __lasx_xvpermi_q(tmp2,  tmp0,  0x21);
+
+    QPEL8_HV_LOWPASS_V(tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, src00, src01,
+                       src02, src03, src04, src05, tmp0)
+    QPEL8_HV_LOWPASS_V(tmp2, tmp3, tmp4, tmp5, tmp6, tmp7, src00, src01,
+                       src02, src03, src04, src05, tmp2)
+    QPEL8_HV_LOWPASS_V(tmp4, tmp5, tmp6, tmp7, tmp8, tmp9, src00, src01,
+                       src02, src03, src04, src05, tmp4)
+    QPEL8_HV_LOWPASS_V(tmp6, tmp7, tmp8, tmp9, tmp10, tmp11, src00, src01,
+                       src02, src03, src04, src05, tmp6)
+    __lasx_xvstelm_d(tmp0, dst, 0, 0);
+    dst += dstStride;
+    __lasx_xvstelm_d(tmp0, dst, 0, 2);
+    dst += dstStride;
+    __lasx_xvstelm_d(tmp2, dst, 0, 0);
+    dst += dstStride;
+    __lasx_xvstelm_d(tmp2, dst, 0, 2);
+    dst += dstStride;
+    __lasx_xvstelm_d(tmp4, dst, 0, 0);
+    dst += dstStride;
+    __lasx_xvstelm_d(tmp4, dst, 0, 2);
+    dst += dstStride;
+    __lasx_xvstelm_d(tmp6, dst, 0, 0);
+    dst += dstStride;
+    __lasx_xvstelm_d(tmp6, dst, 0, 2);
+}
+
+static av_always_inline void
+avg_h264_qpel8_h_lowpass_lasx(uint8_t *dst, const uint8_t *src, int dstStride,
+                              int srcStride)
+{
+    int dstStride_2x = dstStride << 1;
+    int dstStride_4x = dstStride << 2;
+    int dstStride_3x = dstStride_2x + dstStride;
+    __m256i src00, src01, src02, src03, src04, src05, src10;
+    __m256i dst00, dst01, dst0, dst1, dst2, dst3;
+    __m256i out0, out1, out2, out3;
+    __m256i h_20 = __lasx_xvldi(0x414);
+    __m256i h_5  = __lasx_xvldi(0x405);
+    __m256i h_16 = __lasx_xvldi(0x410);
+    __m256i mask1 = {0x0807060504030201, 0x0, 0x0807060504030201, 0x0};
+    __m256i mask2 = {0x0908070605040302, 0x0, 0x0908070605040302, 0x0};
+    __m256i mask3 = {0x0a09080706050403, 0x0, 0x0a09080706050403, 0x0};
+    __m256i mask4 = {0x0b0a090807060504, 0x0, 0x0b0a090807060504, 0x0};
+    __m256i mask5 = {0x0c0b0a0908070605, 0x0, 0x0c0b0a0908070605, 0x0};
+
+    QPEL8_H_LOWPASS(out0)
+    QPEL8_H_LOWPASS(out1)
+    QPEL8_H_LOWPASS(out2)
+    QPEL8_H_LOWPASS(out3)
+    src00 = __lasx_xvld(dst, 0);
+    DUP4_ARG2(__lasx_xvldx, dst, dstStride, dst, dstStride_2x, dst,
+              dstStride_3x, dst, dstStride_4x, src01, src02, src03, src04);
+    dst += dstStride_4x;
+    DUP2_ARG2(__lasx_xvldx, dst, dstStride, dst, dstStride_2x, src05, dst00);
+    dst01 = __lasx_xvldx(dst, dstStride_3x);
+    dst -= dstStride_4x;
+    dst0 = __lasx_xvpermi_q(src00, src01, 0x02);
+    dst1 = __lasx_xvpermi_q(src02, src03, 0x02);
+    dst2 = __lasx_xvpermi_q(src04, src05, 0x02);
+    dst3 = __lasx_xvpermi_q(dst00, dst01, 0x02);
+    dst0 = __lasx_xvavgr_bu(dst0, out0);
+    dst1 = __lasx_xvavgr_bu(dst1, out1);
+    dst2 = __lasx_xvavgr_bu(dst2, out2);
+    dst3 = __lasx_xvavgr_bu(dst3, out3);
+    __lasx_xvstelm_d(dst0, dst, 0, 0);
+    __lasx_xvstelm_d(dst0, dst + dstStride, 0, 2);
+    __lasx_xvstelm_d(dst1, dst + dstStride_2x, 0, 0);
+    __lasx_xvstelm_d(dst1, dst + dstStride_3x, 0, 2);
+    dst += dstStride_4x;
+    __lasx_xvstelm_d(dst2, dst, 0, 0);
+    __lasx_xvstelm_d(dst2, dst + dstStride, 0, 2);
+    __lasx_xvstelm_d(dst3, dst + dstStride_2x, 0, 0);
+    __lasx_xvstelm_d(dst3, dst + dstStride_3x, 0, 2);
+}
+
+static av_always_inline void
+avg_h264_qpel8_hv_lowpass_lasx(uint8_t *dst, const uint8_t *src,
+                               ptrdiff_t dstStride, ptrdiff_t srcStride)
+{
+    __m256i src00, src01, src02, src03, src04, src05, src10;
+    __m256i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6;
+    __m256i tmp7, tmp8, tmp9, tmp10, tmp11, tmp12;
+    __m256i h_20 = __lasx_xvldi(0x414);
+    __m256i h_5  = __lasx_xvldi(0x405);
+    __m256i w_20 = __lasx_xvldi(0x814);
+    __m256i w_5  = __lasx_xvldi(0x805);
+    __m256i w_512 = {512};
+    __m256i mask1 = {0x0807060504030201, 0x0, 0x0807060504030201, 0x0};
+    __m256i mask2 = {0x0908070605040302, 0x0, 0x0908070605040302, 0x0};
+    __m256i mask3 = {0x0a09080706050403, 0x0, 0x0a09080706050403, 0x0};
+    __m256i mask4 = {0x0b0a090807060504, 0x0, 0x0b0a090807060504, 0x0};
+    __m256i mask5 = {0x0c0b0a0908070605, 0x0, 0x0c0b0a0908070605, 0x0};
+    ptrdiff_t dstStride_2x = dstStride << 1;
+    ptrdiff_t dstStride_4x = dstStride << 2;
+    ptrdiff_t dstStride_3x = dstStride_2x + dstStride;
+
+    w_512 = __lasx_xvreplve0_w(w_512);
+
+    src -= srcStride << 1;
+    QPEL8_HV_LOWPASS_H(tmp0)
+    QPEL8_HV_LOWPASS_H(tmp2)
+    QPEL8_HV_LOWPASS_H(tmp4)
+    QPEL8_HV_LOWPASS_H(tmp6)
+    QPEL8_HV_LOWPASS_H(tmp8)
+    QPEL8_HV_LOWPASS_H(tmp10)
+    QPEL8_HV_LOWPASS_H(tmp12)
+    tmp11 = __lasx_xvpermi_q(tmp12, tmp10, 0x21);
+    tmp9  = __lasx_xvpermi_q(tmp10, tmp8,  0x21);
+    tmp7  = __lasx_xvpermi_q(tmp8,  tmp6,  0x21);
+    tmp5  = __lasx_xvpermi_q(tmp6,  tmp4,  0x21);
+    tmp3  = __lasx_xvpermi_q(tmp4,  tmp2,  0x21);
+    tmp1  = __lasx_xvpermi_q(tmp2,  tmp0,  0x21);
+
+    QPEL8_HV_LOWPASS_V(tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, src00, src01,
+                       src02, src03, src04, src05, tmp0)
+    QPEL8_HV_LOWPASS_V(tmp2, tmp3, tmp4, tmp5, tmp6, tmp7, src00, src01,
+                       src02, src03, src04, src05, tmp2)
+    QPEL8_HV_LOWPASS_V(tmp4, tmp5, tmp6, tmp7, tmp8, tmp9, src00, src01,
+                       src02, src03, src04, src05, tmp4)
+    QPEL8_HV_LOWPASS_V(tmp6, tmp7, tmp8, tmp9, tmp10, tmp11, src00, src01,
+                       src02, src03, src04, src05, tmp6)
+
+    src00 = __lasx_xvld(dst, 0);
+    DUP4_ARG2(__lasx_xvldx, dst, dstStride, dst, dstStride_2x, dst,
+              dstStride_3x, dst, dstStride_4x, src01, src02, src03, src04);
+    dst += dstStride_4x;
+    DUP2_ARG2(__lasx_xvldx, dst, dstStride, dst, dstStride_2x, src05, tmp8);
+    tmp9 = __lasx_xvldx(dst, dstStride_3x);
+    dst -= dstStride_4x;
+    tmp1 = __lasx_xvpermi_q(src00, src01, 0x02);
+    tmp3 = __lasx_xvpermi_q(src02, src03, 0x02);
+    tmp5 = __lasx_xvpermi_q(src04, src05, 0x02);
+    tmp7 = __lasx_xvpermi_q(tmp8,  tmp9,  0x02);
+    tmp0 = __lasx_xvavgr_bu(tmp0, tmp1);
+    tmp2 = __lasx_xvavgr_bu(tmp2, tmp3);
+    tmp4 = __lasx_xvavgr_bu(tmp4, tmp5);
+    tmp6 = __lasx_xvavgr_bu(tmp6, tmp7);
+    __lasx_xvstelm_d(tmp0, dst, 0, 0);
+    dst += dstStride;
+    __lasx_xvstelm_d(tmp0, dst, 0, 2);
+    dst += dstStride;
+    __lasx_xvstelm_d(tmp2, dst, 0, 0);
+    dst += dstStride;
+    __lasx_xvstelm_d(tmp2, dst, 0, 2);
+    dst += dstStride;
+    __lasx_xvstelm_d(tmp4, dst, 0, 0);
+    dst += dstStride;
+    __lasx_xvstelm_d(tmp4, dst, 0, 2);
+    dst += dstStride;
+    __lasx_xvstelm_d(tmp6, dst, 0, 0);
+    dst += dstStride;
+    __lasx_xvstelm_d(tmp6, dst, 0, 2);
+}
+
+static av_always_inline void
+put_h264_qpel16_h_lowpass_lasx(uint8_t *dst, const uint8_t *src,
+                               int dstStride, int srcStride)
+{
+    put_h264_qpel8_h_lowpass_lasx(dst, src, dstStride, srcStride);
+    put_h264_qpel8_h_lowpass_lasx(dst+8, src+8, dstStride, srcStride);
+    src += srcStride << 3;
+    dst += dstStride << 3;
+    put_h264_qpel8_h_lowpass_lasx(dst, src, dstStride, srcStride);
+    put_h264_qpel8_h_lowpass_lasx(dst+8, src+8, dstStride, srcStride);
+}
+
+static av_always_inline void
+avg_h264_qpel16_h_lowpass_lasx(uint8_t *dst, const uint8_t *src,
+                               int dstStride, int srcStride)
+{
+    avg_h264_qpel8_h_lowpass_lasx(dst, src, dstStride, srcStride);
+    avg_h264_qpel8_h_lowpass_lasx(dst+8, src+8, dstStride, srcStride);
+    src += srcStride << 3;
+    dst += dstStride << 3;
+    avg_h264_qpel8_h_lowpass_lasx(dst, src, dstStride, srcStride);
+    avg_h264_qpel8_h_lowpass_lasx(dst+8, src+8, dstStride, srcStride);
+}
+
+static void put_h264_qpel16_v_lowpass_lasx(uint8_t *dst, const uint8_t *src,
+                                           int dstStride, int srcStride)
+{
+    put_h264_qpel8_v_lowpass_lasx(dst, (uint8_t*)src, dstStride, srcStride);
+    put_h264_qpel8_v_lowpass_lasx(dst+8, (uint8_t*)src+8, dstStride, srcStride);
+    src += 8*srcStride;
+    dst += 8*dstStride;
+    put_h264_qpel8_v_lowpass_lasx(dst, (uint8_t*)src, dstStride, srcStride);
+    put_h264_qpel8_v_lowpass_lasx(dst+8, (uint8_t*)src+8, dstStride, srcStride);
+}
+
+static void avg_h264_qpel16_v_lowpass_lasx(uint8_t *dst, const uint8_t *src,
+                                           int dstStride, int srcStride)
+{
+    avg_h264_qpel8_v_lowpass_lasx(dst, (uint8_t*)src, dstStride, srcStride);
+    avg_h264_qpel8_v_lowpass_lasx(dst+8, (uint8_t*)src+8, dstStride, srcStride);
+    src += 8*srcStride;
+    dst += 8*dstStride;
+    avg_h264_qpel8_v_lowpass_lasx(dst, (uint8_t*)src, dstStride, srcStride);
+    avg_h264_qpel8_v_lowpass_lasx(dst+8, (uint8_t*)src+8, dstStride, srcStride);
+}
+
+static void put_h264_qpel16_hv_lowpass_lasx(uint8_t *dst, const uint8_t *src,
+                                     ptrdiff_t dstStride, ptrdiff_t srcStride)
+{
+    put_h264_qpel8_hv_lowpass_lasx(dst, src, dstStride, srcStride);
+    put_h264_qpel8_hv_lowpass_lasx(dst + 8, src + 8, dstStride, srcStride);
+    src += srcStride << 3;
+    dst += dstStride << 3;
+    put_h264_qpel8_hv_lowpass_lasx(dst, src, dstStride, srcStride);
+    put_h264_qpel8_hv_lowpass_lasx(dst + 8, src + 8, dstStride, srcStride);
+}
+
+static void avg_h264_qpel16_hv_lowpass_lasx(uint8_t *dst, const uint8_t *src,
+                                     ptrdiff_t dstStride, ptrdiff_t srcStride)
+{
+    avg_h264_qpel8_hv_lowpass_lasx(dst, src, dstStride, srcStride);
+    avg_h264_qpel8_hv_lowpass_lasx(dst + 8, src + 8, dstStride, srcStride);
+    src += srcStride << 3;
+    dst += dstStride << 3;
+    avg_h264_qpel8_hv_lowpass_lasx(dst, src, dstStride, srcStride);
+    avg_h264_qpel8_hv_lowpass_lasx(dst + 8, src + 8, dstStride, srcStride);
+}
+
+void ff_put_h264_qpel8_mc00_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride)
+{
+    /* In mmi optimization, it used function ff_put_pixels8_8_mmi
+     * which implemented in hpeldsp_mmi.c */
+    put_pixels8_8_inline_asm(dst, src, stride);
+}
+
+void ff_put_h264_qpel8_mc10_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride)
+{
+    uint8_t half[64];
+
+    put_h264_qpel8_h_lowpass_lasx(half, src, 8, stride);
+    /* in qpel8, the stride of half and height of block is 8 */
+    put_pixels8_l2_8_lsx(dst, src, half, stride, stride);
+}
+
+void ff_put_h264_qpel8_mc20_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride)
+{
+    put_h264_qpel8_h_lowpass_lasx(dst, src, stride, stride);
+}
+
+void ff_put_h264_qpel8_mc30_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride)
+{
+    uint8_t half[64];
+
+    put_h264_qpel8_h_lowpass_lasx(half, src, 8, stride);
+    put_pixels8_l2_8_lsx(dst, src+1, half, stride, stride);
+}
+
+void ff_put_h264_qpel8_mc01_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride)
+{
+    uint8_t half[64];
+
+    put_h264_qpel8_v_lowpass_lasx(half, (uint8_t*)src, 8, stride);
+    put_pixels8_l2_8_lsx(dst, src, half, stride, stride);
+}
+
+void ff_put_h264_qpel8_mc11_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride)
+{
+    uint8_t halfH[64];
+    uint8_t halfV[64];
+
+    put_h264_qpel8_h_lowpass_lasx(halfH, src, 8, stride);
+    put_h264_qpel8_v_lowpass_lasx(halfV, (uint8_t*)src, 8, stride);
+    put_pixels8_l2_8_lsx(dst, halfH, halfV, stride, 8);
+}
+
+void ff_put_h264_qpel8_mc21_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride)
+{
+    uint8_t temp[128];
+    uint8_t *const halfH  = temp;
+    uint8_t *const halfHV = temp + 64;
+
+    put_h264_qpel8_h_lowpass_lasx(halfH, src, 8, stride);
+    put_h264_qpel8_hv_lowpass_lasx(halfHV, src, 8, stride);
+    put_pixels8_l2_8_lsx(dst, halfH, halfHV, stride, 8);
+}
+
+void ff_put_h264_qpel8_mc31_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride)
+{
+    uint8_t halfH[64];
+    uint8_t halfV[64];
+
+    put_h264_qpel8_h_lowpass_lasx(halfH, src, 8, stride);
+    put_h264_qpel8_v_lowpass_lasx(halfV, (uint8_t*)src + 1, 8, stride);
+    put_pixels8_l2_8_lsx(dst, halfH, halfV, stride, 8);
+}
+
+void ff_put_h264_qpel8_mc02_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride)
+{
+    put_h264_qpel8_v_lowpass_lasx(dst, (uint8_t*)src, stride, stride);
+}
+
+void ff_put_h264_qpel8_mc12_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride)
+{
+    uint8_t temp[128];
+    uint8_t *const halfHV = temp;
+    uint8_t *const halfH  = temp + 64;
+
+    put_h264_qpel8_hv_lowpass_lasx(halfHV, src, 8, stride);
+    put_h264_qpel8_v_lowpass_lasx(halfH, (uint8_t*)src, 8, stride);
+    put_pixels8_l2_8_lsx(dst, halfH, halfHV, stride, 8);
+}
+
+void ff_put_h264_qpel8_mc22_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride)
+{
+    put_h264_qpel8_hv_lowpass_lasx(dst, src, stride, stride);
+}
+
+void ff_put_h264_qpel8_mc32_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride)
+{
+    uint8_t temp[128];
+    uint8_t *const halfHV = temp;
+    uint8_t *const halfH  = temp + 64;
+
+    put_h264_qpel8_hv_lowpass_lasx(halfHV, src, 8, stride);
+    put_h264_qpel8_v_lowpass_lasx(halfH, (uint8_t*)src + 1, 8, stride);
+    put_pixels8_l2_8_lsx(dst, halfH, halfHV, stride, 8);
+}
+
+void ff_put_h264_qpel8_mc03_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride)
+{
+    uint8_t half[64];
+
+    put_h264_qpel8_v_lowpass_lasx(half, (uint8_t*)src, 8, stride);
+    put_pixels8_l2_8_lsx(dst, src + stride, half, stride, stride);
+}
+
+void ff_put_h264_qpel8_mc13_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride)
+{
+    uint8_t halfH[64];
+    uint8_t halfV[64];
+
+    put_h264_qpel8_h_lowpass_lasx(halfH, src + stride, 8, stride);
+    put_h264_qpel8_v_lowpass_lasx(halfV, (uint8_t*)src, 8, stride);
+    put_pixels8_l2_8_lsx(dst, halfH, halfV, stride, 8);
+}
+
+void ff_put_h264_qpel8_mc23_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride)
+{
+    uint8_t temp[128];
+    uint8_t *const halfH  = temp;
+    uint8_t *const halfHV = temp + 64;
+
+    put_h264_qpel8_h_lowpass_lasx(halfH, src + stride, 8, stride);
+    put_h264_qpel8_hv_lowpass_lasx(halfHV, src, 8, stride);
+    put_pixels8_l2_8_lsx(dst, halfH, halfHV, stride, 8);
+}
+
+void ff_put_h264_qpel8_mc33_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride)
+{
+    uint8_t halfH[64];
+    uint8_t halfV[64];
+
+    put_h264_qpel8_h_lowpass_lasx(halfH, src + stride, 8, stride);
+    put_h264_qpel8_v_lowpass_lasx(halfV, (uint8_t*)src + 1, 8, stride);
+    put_pixels8_l2_8_lsx(dst, halfH, halfV, stride, 8);
+}
+
+void ff_avg_h264_qpel8_mc00_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride)
+{
+    /* In mmi optimization, it used function ff_avg_pixels8_8_mmi
+     * which implemented in hpeldsp_mmi.c */
+    avg_pixels8_8_lsx(dst, src, stride);
+}
+
+void ff_avg_h264_qpel8_mc10_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride)
+{
+    uint8_t half[64];
+
+    put_h264_qpel8_h_lowpass_lasx(half, src, 8, stride);
+    avg_pixels8_l2_8_lsx(dst, src, half, stride, stride);
+}
+
+void ff_avg_h264_qpel8_mc20_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride)
+{
+    avg_h264_qpel8_h_lowpass_lasx(dst, src, stride, stride);
+}
+
+void ff_avg_h264_qpel8_mc30_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride)
+{
+    uint8_t half[64];
+
+    put_h264_qpel8_h_lowpass_lasx(half, src, 8, stride);
+    avg_pixels8_l2_8_lsx(dst, src+1, half, stride, stride);
+}
+
+void ff_avg_h264_qpel8_mc11_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride)
+{
+    uint8_t halfH[64];
+    uint8_t halfV[64];
+
+    put_h264_qpel8_h_lowpass_lasx(halfH, src, 8, stride);
+    put_h264_qpel8_v_lowpass_lasx(halfV, (uint8_t*)src, 8, stride);
+    avg_pixels8_l2_8_lsx(dst, halfH, halfV, stride, 8);
+}
+
+void ff_avg_h264_qpel8_mc21_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride)
+{
+    uint8_t temp[128];
+    uint8_t *const halfH  = temp;
+    uint8_t *const halfHV = temp + 64;
+
+    put_h264_qpel8_h_lowpass_lasx(halfH, src, 8, stride);
+    put_h264_qpel8_hv_lowpass_lasx(halfHV, src, 8, stride);
+    avg_pixels8_l2_8_lsx(dst, halfH, halfHV, stride, 8);
+}
+
+void ff_avg_h264_qpel8_mc31_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride)
+{
+    uint8_t halfH[64];
+    uint8_t halfV[64];
+
+    put_h264_qpel8_h_lowpass_lasx(halfH, src, 8, stride);
+    put_h264_qpel8_v_lowpass_lasx(halfV, (uint8_t*)src + 1, 8, stride);
+    avg_pixels8_l2_8_lsx(dst, halfH, halfV, stride, 8);
+}
+
+void ff_avg_h264_qpel8_mc02_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride)
+{
+    avg_h264_qpel8_v_lowpass_lasx(dst, (uint8_t*)src, stride, stride);
+}
+
+void ff_avg_h264_qpel8_mc12_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride)
+{
+    uint8_t temp[128];
+    uint8_t *const halfHV = temp;
+    uint8_t *const halfH  = temp + 64;
+
+    put_h264_qpel8_hv_lowpass_lasx(halfHV, src, 8, stride);
+    put_h264_qpel8_v_lowpass_lasx(halfH, (uint8_t*)src, 8, stride);
+    avg_pixels8_l2_8_lsx(dst, halfH, halfHV, stride, 8);
+}
+
+void ff_avg_h264_qpel8_mc22_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride)
+{
+    avg_h264_qpel8_hv_lowpass_lasx(dst, src, stride, stride);
+}
+
+void ff_avg_h264_qpel8_mc32_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride)
+{
+    uint8_t temp[128];
+    uint8_t *const halfHV = temp;
+    uint8_t *const halfH  = temp + 64;
+
+    put_h264_qpel8_hv_lowpass_lasx(halfHV, src, 8, stride);
+    put_h264_qpel8_v_lowpass_lasx(halfH, (uint8_t*)src + 1, 8, stride);
+    avg_pixels8_l2_8_lsx(dst, halfH, halfHV, stride, 8);
+}
+
+void ff_avg_h264_qpel8_mc13_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride)
+{
+    uint8_t halfH[64];
+    uint8_t halfV[64];
+
+    put_h264_qpel8_h_lowpass_lasx(halfH, src + stride, 8, stride);
+    put_h264_qpel8_v_lowpass_lasx(halfV, (uint8_t*)src, 8, stride);
+    avg_pixels8_l2_8_lsx(dst, halfH, halfV, stride, 8);
+}
+
+void ff_avg_h264_qpel8_mc23_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride)
+{
+    uint8_t temp[128];
+    uint8_t *const halfH  = temp;
+    uint8_t *const halfHV = temp + 64;
+
+    put_h264_qpel8_h_lowpass_lasx(halfH, src + stride, 8, stride);
+    put_h264_qpel8_hv_lowpass_lasx(halfHV, src, 8, stride);
+    avg_pixels8_l2_8_lsx(dst, halfH, halfHV, stride, 8);
+}
+
+void ff_avg_h264_qpel8_mc33_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride)
+{
+    uint8_t halfH[64];
+    uint8_t halfV[64];
+
+    put_h264_qpel8_h_lowpass_lasx(halfH, src + stride, 8, stride);
+    put_h264_qpel8_v_lowpass_lasx(halfV, (uint8_t*)src + 1, 8, stride);
+    avg_pixels8_l2_8_lsx(dst, halfH, halfV, stride, 8);
+}
+
+void ff_put_h264_qpel16_mc00_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t stride)
+{
+    /* In mmi optimization, it used function ff_put_pixels16_8_mmi
+     * which implemented in hpeldsp_mmi.c */
+    put_pixels16_8_lsx(dst, src, stride);
+}
+
+void ff_put_h264_qpel16_mc10_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t stride)
+{
+    uint8_t half[256];
+
+    put_h264_qpel16_h_lowpass_lasx(half, src, 16, stride);
+    put_pixels16_l2_8_lsx(dst, src, half, stride, stride);
+}
+
+void ff_put_h264_qpel16_mc20_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t stride)
+{
+    put_h264_qpel16_h_lowpass_lasx(dst, src, stride, stride);
+}
+
+void ff_put_h264_qpel16_mc30_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t stride)
+{
+    uint8_t half[256];
+
+    put_h264_qpel16_h_lowpass_lasx(half, src, 16, stride);
+    put_pixels16_l2_8_lsx(dst, src+1, half, stride, stride);
+}
+
+void ff_put_h264_qpel16_mc01_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t stride)
+{
+    uint8_t half[256];
+
+    put_h264_qpel16_v_lowpass_lasx(half, src, 16, stride);
+    put_pixels16_l2_8_lsx(dst, src, half, stride, stride);
+}
+
+void ff_put_h264_qpel16_mc11_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t stride)
+{
+    avc_luma_hv_qrt_16x16_lasx((uint8_t*)src - 2, (uint8_t*)src - (stride * 2),
+                               dst, stride);
+}
+
+void ff_put_h264_qpel16_mc21_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t stride)
+{
+    uint8_t temp[512];
+    uint8_t *const halfH  = temp;
+    uint8_t *const halfHV = temp + 256;
+
+    put_h264_qpel16_h_lowpass_lasx(halfH, src, 16, stride);
+    put_h264_qpel16_hv_lowpass_lasx(halfHV, src, 16, stride);
+    put_pixels16_l2_8_lsx(dst, halfH, halfHV, stride, 16);
+}
+
+void ff_put_h264_qpel16_mc31_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t stride)
+{
+    avc_luma_hv_qrt_16x16_lasx((uint8_t*)src - 2, (uint8_t*)src - (stride * 2) + 1,
+                               dst, stride);
+}
+
+void ff_put_h264_qpel16_mc02_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t stride)
+{
+    put_h264_qpel16_v_lowpass_lasx(dst, src, stride, stride);
+}
+
+void ff_put_h264_qpel16_mc12_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t stride)
+{
+    uint8_t temp[512];
+    uint8_t *const halfHV = temp;
+    uint8_t *const halfH  = temp + 256;
+
+    put_h264_qpel16_hv_lowpass_lasx(halfHV, src, 16, stride);
+    put_h264_qpel16_v_lowpass_lasx(halfH, src, 16, stride);
+    put_pixels16_l2_8_lsx(dst, halfH, halfHV, stride, 16);
+}
+
+void ff_put_h264_qpel16_mc22_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t stride)
+{
+    put_h264_qpel16_hv_lowpass_lasx(dst, src, stride, stride);
+}
+
+void ff_put_h264_qpel16_mc32_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t stride)
+{
+    uint8_t temp[512];
+    uint8_t *const halfHV = temp;
+    uint8_t *const halfH  = temp + 256;
+
+    put_h264_qpel16_hv_lowpass_lasx(halfHV, src, 16, stride);
+    put_h264_qpel16_v_lowpass_lasx(halfH, src + 1, 16, stride);
+    put_pixels16_l2_8_lsx(dst, halfH, halfHV, stride, 16);
+}
+
+void ff_put_h264_qpel16_mc03_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t stride)
+{
+    uint8_t half[256];
+
+    put_h264_qpel16_v_lowpass_lasx(half, src, 16, stride);
+    put_pixels16_l2_8_lsx(dst, src+stride, half, stride, stride);
+}
+
+void ff_put_h264_qpel16_mc13_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t stride)
+{
+    avc_luma_hv_qrt_16x16_lasx((uint8_t*)src + stride - 2, (uint8_t*)src - (stride * 2),
+                               dst, stride);
+}
+
+void ff_put_h264_qpel16_mc23_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t stride)
+{
+    uint8_t temp[512];
+    uint8_t *const halfH  = temp;
+    uint8_t *const halfHV = temp + 256;
+
+    put_h264_qpel16_h_lowpass_lasx(halfH, src + stride, 16, stride);
+    put_h264_qpel16_hv_lowpass_lasx(halfHV, src, 16, stride);
+    put_pixels16_l2_8_lsx(dst, halfH, halfHV, stride, 16);
+}
+
+void ff_put_h264_qpel16_mc33_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t stride)
+{
+    avc_luma_hv_qrt_16x16_lasx((uint8_t*)src + stride - 2,
+                               (uint8_t*)src - (stride * 2) + 1, dst, stride);
+}
+
+void ff_avg_h264_qpel16_mc00_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t stride)
+{
+    /* In mmi optimization, it used function ff_avg_pixels16_8_mmi
+     * which implemented in hpeldsp_mmi.c */
+    avg_pixels16_8_lsx(dst, src, stride);
+}
+
+void ff_avg_h264_qpel16_mc10_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t stride)
+{
+    uint8_t half[256];
+
+    put_h264_qpel16_h_lowpass_lasx(half, src, 16, stride);
+    avg_pixels16_l2_8_lsx(dst, src, half, stride, stride);
+}
+
+void ff_avg_h264_qpel16_mc20_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t stride)
+{
+    avg_h264_qpel16_h_lowpass_lasx(dst, src, stride, stride);
+}
+
+void ff_avg_h264_qpel16_mc30_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t stride)
+{
+    uint8_t half[256];
+
+    put_h264_qpel16_h_lowpass_lasx(half, src, 16, stride);
+    avg_pixels16_l2_8_lsx(dst, src+1, half, stride, stride);
+}
+
+void ff_avg_h264_qpel16_mc01_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t stride)
+{
+    uint8_t half[256];
+
+    put_h264_qpel16_v_lowpass_lasx(half, src, 16, stride);
+    avg_pixels16_l2_8_lsx(dst, src, half, stride, stride);
+}
+
+void ff_avg_h264_qpel16_mc11_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t stride)
+{
+    avc_luma_hv_qrt_and_aver_dst_16x16_lasx((uint8_t*)src - 2,
+                                           (uint8_t*)src - (stride * 2),
+                                           dst, stride);
+}
+
+void ff_avg_h264_qpel16_mc21_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t stride)
+{
+    uint8_t temp[512];
+    uint8_t *const halfH  = temp;
+    uint8_t *const halfHV = temp + 256;
+
+    put_h264_qpel16_h_lowpass_lasx(halfH, src, 16, stride);
+    put_h264_qpel16_hv_lowpass_lasx(halfHV, src, 16, stride);
+    avg_pixels16_l2_8_lsx(dst, halfH, halfHV, stride, 16);
+}
+
+void ff_avg_h264_qpel16_mc31_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t stride)
+{
+    avc_luma_hv_qrt_and_aver_dst_16x16_lasx((uint8_t*)src - 2,
+                                            (uint8_t*)src - (stride * 2) + 1,
+                                            dst, stride);
+}
+
+void ff_avg_h264_qpel16_mc02_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t stride)
+{
+    avg_h264_qpel16_v_lowpass_lasx(dst, src, stride, stride);
+}
+
+void ff_avg_h264_qpel16_mc12_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t stride)
+{
+    uint8_t temp[512];
+    uint8_t *const halfHV = temp;
+    uint8_t *const halfH  = temp + 256;
+
+    put_h264_qpel16_hv_lowpass_lasx(halfHV, src, 16, stride);
+    put_h264_qpel16_v_lowpass_lasx(halfH, src, 16, stride);
+    avg_pixels16_l2_8_lsx(dst, halfH, halfHV, stride, 16);
+}
+
+void ff_avg_h264_qpel16_mc22_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t stride)
+{
+    avg_h264_qpel16_hv_lowpass_lasx(dst, src, stride, stride);
+}
+
+void ff_avg_h264_qpel16_mc32_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t stride)
+{
+    uint8_t temp[512];
+    uint8_t *const halfHV = temp;
+    uint8_t *const halfH  = temp + 256;
+
+    put_h264_qpel16_hv_lowpass_lasx(halfHV, src, 16, stride);
+    put_h264_qpel16_v_lowpass_lasx(halfH, src + 1, 16, stride);
+    avg_pixels16_l2_8_lsx(dst, halfH, halfHV, stride, 16);
+}
+
+void ff_avg_h264_qpel16_mc03_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t stride)
+{
+    uint8_t half[256];
+
+    put_h264_qpel16_v_lowpass_lasx(half, src, 16, stride);
+    avg_pixels16_l2_8_lsx(dst, src + stride, half, stride, stride);
+}
+
+void ff_avg_h264_qpel16_mc13_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t stride)
+{
+    avc_luma_hv_qrt_and_aver_dst_16x16_lasx((uint8_t*)src + stride - 2,
+                                            (uint8_t*)src - (stride * 2),
+                                            dst, stride);
+}
+
+void ff_avg_h264_qpel16_mc23_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t stride)
+{
+    uint8_t temp[512];
+    uint8_t *const halfH  = temp;
+    uint8_t *const halfHV = temp + 256;
+
+    put_h264_qpel16_h_lowpass_lasx(halfH, src + stride, 16, stride);
+    put_h264_qpel16_hv_lowpass_lasx(halfHV, src, 16, stride);
+    avg_pixels16_l2_8_lsx(dst, halfH, halfHV, stride, 16);
+}
+
+void ff_avg_h264_qpel16_mc33_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t stride)
+{
+    avc_luma_hv_qrt_and_aver_dst_16x16_lasx((uint8_t*)src + stride - 2,
+                                            (uint8_t*)src - (stride * 2) + 1,
+                                            dst, stride);
+}
diff --git a/libavcodec/loongarch/h264qpel_lasx.h b/libavcodec/loongarch/h264qpel_lasx.h
new file mode 100644
index 0000000000..c5f23eb649
--- /dev/null
+++ b/libavcodec/loongarch/h264qpel_lasx.h
@@ -0,0 +1,159 @@
+/*
+ * Copyright (c) 2020 Loongson Technology Corporation Limited
+ * Contributed by Shiyou Yin <yinshiyou-hf@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef AVCODEC_LOONGARCH_H264QPEL_LASX_H
+#define AVCODEC_LOONGARCH_H264QPEL_LASX_H
+
+#include <stdint.h>
+#include <stddef.h>
+#include "libavcodec/h264.h"
+
+void ff_h264_h_lpf_luma_inter_lasx(uint8_t *src, int stride,
+                                   int alpha, int beta, int8_t *tc0);
+void ff_h264_v_lpf_luma_inter_lasx(uint8_t *src, int stride,
+                                   int alpha, int beta, int8_t *tc0);
+void ff_put_h264_qpel16_mc00_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t dst_stride);
+void ff_put_h264_qpel16_mc10_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t dst_stride);
+void ff_put_h264_qpel16_mc20_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t dst_stride);
+void ff_put_h264_qpel16_mc30_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t dst_stride);
+void ff_put_h264_qpel16_mc01_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t dst_stride);
+void ff_put_h264_qpel16_mc11_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t dst_stride);
+void ff_put_h264_qpel16_mc21_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t dst_stride);
+void ff_put_h264_qpel16_mc31_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t dst_stride);
+void ff_put_h264_qpel16_mc02_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t dst_stride);
+void ff_put_h264_qpel16_mc12_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t dst_stride);
+void ff_put_h264_qpel16_mc32_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t dst_stride);
+void ff_put_h264_qpel16_mc22_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t dst_stride);
+void ff_put_h264_qpel16_mc03_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t dst_stride);
+void ff_put_h264_qpel16_mc13_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t dst_stride);
+void ff_put_h264_qpel16_mc23_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t dst_stride);
+void ff_put_h264_qpel16_mc33_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t dst_stride);
+void ff_avg_h264_qpel16_mc00_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t dst_stride);
+void ff_avg_h264_qpel16_mc10_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t dst_stride);
+void ff_avg_h264_qpel16_mc20_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t dst_stride);
+void ff_avg_h264_qpel16_mc30_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t dst_stride);
+void ff_avg_h264_qpel16_mc01_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t dst_stride);
+void ff_avg_h264_qpel16_mc11_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t dst_stride);
+void ff_avg_h264_qpel16_mc21_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t dst_stride);
+void ff_avg_h264_qpel16_mc31_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t dst_stride);
+void ff_avg_h264_qpel16_mc02_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t dst_stride);
+void ff_avg_h264_qpel16_mc12_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t dst_stride);
+void ff_avg_h264_qpel16_mc22_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t dst_stride);
+void ff_avg_h264_qpel16_mc32_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t dst_stride);
+void ff_avg_h264_qpel16_mc03_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t dst_stride);
+void ff_avg_h264_qpel16_mc13_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t dst_stride);
+void ff_avg_h264_qpel16_mc23_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t dst_stride);
+void ff_avg_h264_qpel16_mc33_lasx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t dst_stride);
+
+void ff_put_h264_qpel8_mc00_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride);
+void ff_put_h264_qpel8_mc10_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride);
+void ff_put_h264_qpel8_mc20_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride);
+void ff_put_h264_qpel8_mc30_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride);
+void ff_put_h264_qpel8_mc01_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride);
+void ff_put_h264_qpel8_mc11_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride);
+void ff_put_h264_qpel8_mc21_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride);
+void ff_put_h264_qpel8_mc31_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride);
+void ff_put_h264_qpel8_mc02_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride);
+void ff_put_h264_qpel8_mc12_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride);
+void ff_put_h264_qpel8_mc22_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride);
+void ff_put_h264_qpel8_mc32_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride);
+void ff_put_h264_qpel8_mc03_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride);
+void ff_put_h264_qpel8_mc13_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride);
+void ff_put_h264_qpel8_mc23_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride);
+void ff_put_h264_qpel8_mc33_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride);
+void ff_avg_h264_qpel8_mc00_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t dst_stride);
+void ff_avg_h264_qpel8_mc10_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t dst_stride);
+void ff_avg_h264_qpel8_mc20_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t dst_stride);
+void ff_avg_h264_qpel8_mc30_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t dst_stride);
+void ff_avg_h264_qpel8_mc11_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t dst_stride);
+void ff_avg_h264_qpel8_mc21_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t dst_stride);
+void ff_avg_h264_qpel8_mc31_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t dst_stride);
+void ff_avg_h264_qpel8_mc02_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t dst_stride);
+void ff_avg_h264_qpel8_mc12_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t dst_stride);
+void ff_avg_h264_qpel8_mc22_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t dst_stride);
+void ff_avg_h264_qpel8_mc32_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t dst_stride);
+void ff_avg_h264_qpel8_mc13_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t dst_stride);
+void ff_avg_h264_qpel8_mc23_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t dst_stride);
+void ff_avg_h264_qpel8_mc33_lasx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t dst_stride);
+
+#endif  // #ifndef AVCODEC_LOONGARCH_H264QPEL_LASX_H
diff --git a/libavcodec/loongarch/h264qpel_lsx.c b/libavcodec/loongarch/h264qpel_lsx.c
new file mode 100644
index 0000000000..c44b09e044
--- /dev/null
+++ b/libavcodec/loongarch/h264qpel_lsx.c
@@ -0,0 +1,488 @@
+/*
+ * Loongson LSX optimized h264qpel
+ *
+ * Copyright (c) 2022 Loongson Technology Corporation Limited
+ * Contributed by Hecai Yuan <yuanhecai@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "h264qpel_lsx.h"
+#include "libavutil/loongarch/loongson_intrinsics.h"
+#include "libavutil/attributes.h"
+
+static void put_h264_qpel16_hv_lowpass_lsx(uint8_t *dst, const uint8_t *src,
+                                           ptrdiff_t dstStride, ptrdiff_t srcStride)
+{
+    put_h264_qpel8_hv_lowpass_lsx(dst, src, dstStride, srcStride);
+    put_h264_qpel8_hv_lowpass_lsx(dst + 8, src + 8, dstStride, srcStride);
+    src += srcStride << 3;
+    dst += dstStride << 3;
+    put_h264_qpel8_hv_lowpass_lsx(dst, src, dstStride, srcStride);
+    put_h264_qpel8_hv_lowpass_lsx(dst + 8, src + 8, dstStride, srcStride);
+}
+
+void ff_put_h264_qpel16_mc22_lsx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride)
+{
+    put_h264_qpel16_hv_lowpass_lsx(dst, src, stride, stride);
+}
+
+static void put_h264_qpel16_h_lowpass_lsx(uint8_t *dst, const uint8_t *src,
+                                          int dstStride, int srcStride)
+{
+    put_h264_qpel8_h_lowpass_lsx(dst, src, dstStride, srcStride);
+    put_h264_qpel8_h_lowpass_lsx(dst+8, src+8, dstStride, srcStride);
+    src += srcStride << 3;
+    dst += dstStride << 3;
+    put_h264_qpel8_h_lowpass_lsx(dst, src, dstStride, srcStride);
+    put_h264_qpel8_h_lowpass_lsx(dst+8, src+8, dstStride, srcStride);
+}
+
+static void put_h264_qpel16_v_lowpass_lsx(uint8_t *dst, const uint8_t *src,
+                                           int dstStride, int srcStride)
+{
+    put_h264_qpel8_v_lowpass_lsx(dst, (uint8_t*)src, dstStride, srcStride);
+    put_h264_qpel8_v_lowpass_lsx(dst+8, (uint8_t*)src+8, dstStride, srcStride);
+    src += 8*srcStride;
+    dst += 8*dstStride;
+    put_h264_qpel8_v_lowpass_lsx(dst, (uint8_t*)src, dstStride, srcStride);
+    put_h264_qpel8_v_lowpass_lsx(dst+8, (uint8_t*)src+8, dstStride, srcStride);
+}
+
+void ff_put_h264_qpel16_mc21_lsx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride)
+{
+    uint8_t temp[512];
+    uint8_t *const halfH  = temp;
+    uint8_t *const halfHV = temp + 256;
+
+    put_h264_qpel16_h_lowpass_lsx(halfH, src, 16, stride);
+    put_h264_qpel16_hv_lowpass_lsx(halfHV, src, 16, stride);
+    put_pixels16_l2_8_lsx(dst, halfH, halfHV, stride, 16);
+}
+
+void ff_put_h264_qpel16_mc12_lsx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride)
+{
+    uint8_t temp[512];
+    uint8_t *const halfHV = temp;
+    uint8_t *const halfH  = temp + 256;
+
+    put_h264_qpel16_hv_lowpass_lsx(halfHV, src, 16, stride);
+    put_h264_qpel16_v_lowpass_lsx(halfH, src, 16, stride);
+    put_pixels16_l2_8_lsx(dst, halfH, halfHV, stride, 16);
+}
+
+void ff_put_h264_qpel16_mc32_lsx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride)
+{
+    uint8_t temp[512];
+    uint8_t *const halfHV = temp;
+    uint8_t *const halfH  = temp + 256;
+
+    put_h264_qpel16_hv_lowpass_lsx(halfHV, src, 16, stride);
+    put_h264_qpel16_v_lowpass_lsx(halfH, src + 1, 16, stride);
+    put_pixels16_l2_8_lsx(dst, halfH, halfHV, stride, 16);
+}
+
+void ff_put_h264_qpel16_mc23_lsx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride)
+{
+    uint8_t temp[512];
+    uint8_t *const halfH  = temp;
+    uint8_t *const halfHV = temp + 256;
+
+    put_h264_qpel16_h_lowpass_lsx(halfH, src + stride, 16, stride);
+    put_h264_qpel16_hv_lowpass_lsx(halfHV, src, 16, stride);
+    put_pixels16_l2_8_lsx(dst, halfH, halfHV, stride, 16);
+}
+
+static void avg_h264_qpel16_v_lowpass_lsx(uint8_t *dst, const uint8_t *src,
+                                          int dstStride, int srcStride)
+{
+    avg_h264_qpel8_v_lowpass_lsx(dst, (uint8_t*)src, dstStride, srcStride);
+    avg_h264_qpel8_v_lowpass_lsx(dst+8, (uint8_t*)src+8, dstStride, srcStride);
+    src += 8*srcStride;
+    dst += 8*dstStride;
+    avg_h264_qpel8_v_lowpass_lsx(dst, (uint8_t*)src, dstStride, srcStride);
+    avg_h264_qpel8_v_lowpass_lsx(dst+8, (uint8_t*)src+8, dstStride, srcStride);
+}
+
+void ff_avg_h264_qpel16_mc02_lsx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride)
+{
+    avg_h264_qpel16_v_lowpass_lsx(dst, src, stride, stride);
+}
+
+void ff_avg_h264_qpel16_mc03_lsx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride)
+{
+    uint8_t half[256];
+
+    put_h264_qpel16_v_lowpass_lsx(half, src, 16, stride);
+    avg_pixels16_l2_8_lsx(dst, src + stride, half, stride, stride);
+}
+
+void ff_avg_h264_qpel16_mc23_lsx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride)
+{
+    uint8_t temp[512];
+    uint8_t *const halfH  = temp;
+    uint8_t *const halfHV = temp + 256;
+
+    put_h264_qpel16_h_lowpass_lsx(halfH, src + stride, 16, stride);
+    put_h264_qpel16_hv_lowpass_lsx(halfHV, src, 16, stride);
+    avg_pixels16_l2_8_lsx(dst, halfH, halfHV, stride, 16);
+}
+
+void ff_avg_h264_qpel16_mc21_lsx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride)
+{
+    uint8_t temp[512];
+    uint8_t *const halfH  = temp;
+    uint8_t *const halfHV = temp + 256;
+
+    put_h264_qpel16_h_lowpass_lsx(halfH, src, 16, stride);
+    put_h264_qpel16_hv_lowpass_lsx(halfHV, src, 16, stride);
+    avg_pixels16_l2_8_lsx(dst, halfH, halfHV, stride, 16);
+}
+
+void ff_avg_h264_qpel16_mc01_lsx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride)
+{
+    uint8_t half[256];
+
+    put_h264_qpel16_v_lowpass_lsx(half, src, 16, stride);
+    avg_pixels16_l2_8_lsx(dst, src, half, stride, stride);
+}
+
+void ff_avg_h264_qpel16_mc32_lsx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride)
+{
+    uint8_t temp[512];
+    uint8_t *const halfHV = temp;
+    uint8_t *const halfH  = temp + 256;
+
+    put_h264_qpel16_hv_lowpass_lsx(halfHV, src, 16, stride);
+    put_h264_qpel16_v_lowpass_lsx(halfH, src + 1, 16, stride);
+    avg_pixels16_l2_8_lsx(dst, halfH, halfHV, stride, 16);
+}
+
+void ff_avg_h264_qpel16_mc12_lsx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride)
+{
+    uint8_t temp[512];
+    uint8_t *const halfHV = temp;
+    uint8_t *const halfH  = temp + 256;
+
+    put_h264_qpel16_hv_lowpass_lsx(halfHV, src, 16, stride);
+    put_h264_qpel16_v_lowpass_lsx(halfH, src, 16, stride);
+    avg_pixels16_l2_8_lsx(dst, halfH, halfHV, stride, 16);
+}
+
+static void avg_h264_qpel16_hv_lowpass_lsx(uint8_t *dst, const uint8_t *src,
+                                           ptrdiff_t dstStride, ptrdiff_t srcStride)
+{
+    avg_h264_qpel8_hv_lowpass_lsx(dst, src, dstStride, srcStride);
+    avg_h264_qpel8_hv_lowpass_lsx(dst + 8, src + 8, dstStride, srcStride);
+    src += srcStride << 3;
+    dst += dstStride << 3;
+    avg_h264_qpel8_hv_lowpass_lsx(dst, src, dstStride, srcStride);
+    avg_h264_qpel8_hv_lowpass_lsx(dst + 8, src + 8, dstStride, srcStride);
+}
+
+void ff_avg_h264_qpel16_mc22_lsx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride)
+{
+    avg_h264_qpel16_hv_lowpass_lsx(dst, src, stride, stride);
+}
+
+void ff_put_h264_qpel8_mc03_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride)
+{
+    uint8_t half[64];
+
+    put_h264_qpel8_v_lowpass_lsx(half, (uint8_t*)src, 8, stride);
+    put_pixels8_l2_8_lsx(dst, src + stride, half, stride, stride);
+}
+
+void ff_put_h264_qpel8_mc01_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride)
+{
+    uint8_t half[64];
+
+    put_h264_qpel8_v_lowpass_lsx(half, (uint8_t*)src, 8, stride);
+    put_pixels8_l2_8_lsx(dst, src, half, stride, stride);
+}
+
+void ff_put_h264_qpel8_mc30_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride)
+{
+    uint8_t half[64];
+
+    put_h264_qpel8_h_lowpass_lsx(half, src, 8, stride);
+    put_pixels8_l2_8_lsx(dst, src+1, half, stride, stride);
+}
+
+void ff_put_h264_qpel8_mc10_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride)
+{
+    uint8_t half[64];
+
+    put_h264_qpel8_h_lowpass_lsx(half, src, 8, stride);
+    put_pixels8_l2_8_lsx(dst, src, half, stride, stride);
+}
+
+void ff_put_h264_qpel8_mc33_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride)
+{
+    uint8_t halfH[64];
+    uint8_t halfV[64];
+
+    put_h264_qpel8_h_lowpass_lsx(halfH, src + stride, 8, stride);
+    put_h264_qpel8_v_lowpass_lsx(halfV, (uint8_t*)src + 1, 8, stride);
+    put_pixels8_l2_8_lsx(dst, halfH, halfV, stride, 8);
+}
+
+void ff_put_h264_qpel8_mc13_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride)
+{
+    uint8_t halfH[64];
+    uint8_t halfV[64];
+
+    put_h264_qpel8_h_lowpass_lsx(halfH, src + stride, 8, stride);
+    put_h264_qpel8_v_lowpass_lsx(halfV, (uint8_t*)src, 8, stride);
+    put_pixels8_l2_8_lsx(dst, halfH, halfV, stride, 8);
+}
+
+void ff_put_h264_qpel8_mc31_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride)
+{
+    uint8_t halfH[64];
+    uint8_t halfV[64];
+
+    put_h264_qpel8_h_lowpass_lsx(halfH, src, 8, stride);
+    put_h264_qpel8_v_lowpass_lsx(halfV, (uint8_t*)src + 1, 8, stride);
+    put_pixels8_l2_8_lsx(dst, halfH, halfV, stride, 8);
+}
+
+void ff_put_h264_qpel8_mc11_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride)
+{
+    uint8_t halfH[64];
+    uint8_t halfV[64];
+
+    put_h264_qpel8_h_lowpass_lsx(halfH, src, 8, stride);
+    put_h264_qpel8_v_lowpass_lsx(halfV, (uint8_t*)src, 8, stride);
+    put_pixels8_l2_8_lsx(dst, halfH, halfV, stride, 8);
+}
+
+void ff_put_h264_qpel8_mc32_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride)
+{
+    uint8_t temp[128];
+    uint8_t *const halfHV = temp;
+    uint8_t *const halfH  = temp + 64;
+
+    put_h264_qpel8_hv_lowpass_lsx(halfHV, src, 8, stride);
+    put_h264_qpel8_v_lowpass_lsx(halfH, (uint8_t*)src + 1, 8, stride);
+    put_pixels8_l2_8_lsx(dst, halfH, halfHV, stride, 8);
+}
+
+void ff_put_h264_qpel8_mc21_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride)
+{
+    uint8_t temp[128];
+    uint8_t *const halfH  = temp;
+    uint8_t *const halfHV = temp + 64;
+
+    put_h264_qpel8_h_lowpass_lsx(halfH, src, 8, stride);
+    put_h264_qpel8_hv_lowpass_lsx(halfHV, src, 8, stride);
+    put_pixels8_l2_8_lsx(dst, halfH, halfHV, stride, 8);
+}
+
+void ff_put_h264_qpel8_mc23_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride)
+{
+    uint8_t temp[128];
+    uint8_t *const halfH  = temp;
+    uint8_t *const halfHV = temp + 64;
+
+    put_h264_qpel8_h_lowpass_lsx(halfH, src + stride, 8, stride);
+    put_h264_qpel8_hv_lowpass_lsx(halfHV, src, 8, stride);
+    put_pixels8_l2_8_lsx(dst, halfH, halfHV, stride, 8);
+}
+
+void ff_put_h264_qpel8_mc12_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride)
+{
+    uint8_t temp[128];
+    uint8_t *const halfHV = temp;
+    uint8_t *const halfH  = temp + 64;
+
+    put_h264_qpel8_hv_lowpass_lsx(halfHV, src, 8, stride);
+    put_h264_qpel8_v_lowpass_lsx(halfH, (uint8_t*)src, 8, stride);
+    put_pixels8_l2_8_lsx(dst, halfH, halfHV, stride, 8);
+}
+
+void ff_put_h264_qpel8_mc02_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride)
+{
+    put_h264_qpel8_v_lowpass_lsx(dst, (uint8_t*)src, stride, stride);
+}
+
+void ff_put_h264_qpel8_mc22_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride)
+{
+    put_h264_qpel8_hv_lowpass_lsx(dst, src, stride, stride);
+}
+
+void ff_put_h264_qpel8_mc20_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride)
+{
+    put_h264_qpel8_h_lowpass_lsx(dst, src, stride, stride);
+}
+
+void ff_avg_h264_qpel8_mc10_lsx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride)
+{
+    uint8_t half[64];
+
+    put_h264_qpel8_h_lowpass_lsx(half, src, 8, stride);
+    avg_pixels8_l2_8_lsx(dst, src, half, stride, stride);
+}
+
+void ff_avg_h264_qpel8_mc20_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride)
+{
+    avg_h264_qpel8_h_lowpass_lsx(dst, src, stride, stride);
+}
+
+void ff_avg_h264_qpel8_mc30_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride)
+{
+    uint8_t half[64];
+
+    put_h264_qpel8_h_lowpass_lsx(half, src, 8, stride);
+    avg_pixels8_l2_8_lsx(dst, src+1, half, stride, stride);
+}
+
+void ff_avg_h264_qpel8_mc11_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride)
+{
+    uint8_t halfH[64];
+    uint8_t halfV[64];
+
+    put_h264_qpel8_h_lowpass_lsx(halfH, src, 8, stride);
+    put_h264_qpel8_v_lowpass_lsx(halfV, (uint8_t*)src, 8, stride);
+    avg_pixels8_l2_8_lsx(dst, halfH, halfV, stride, 8);
+}
+
+void ff_avg_h264_qpel8_mc21_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride)
+{
+    uint8_t temp[128];
+    uint8_t *const halfH  = temp;
+    uint8_t *const halfHV = temp + 64;
+
+    put_h264_qpel8_h_lowpass_lsx(halfH, src, 8, stride);
+    put_h264_qpel8_hv_lowpass_lsx(halfHV, src, 8, stride);
+    avg_pixels8_l2_8_lsx(dst, halfH, halfHV, stride, 8);
+}
+
+void ff_avg_h264_qpel8_mc31_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride)
+{
+    uint8_t halfH[64];
+    uint8_t halfV[64];
+
+    put_h264_qpel8_h_lowpass_lsx(halfH, src, 8, stride);
+    put_h264_qpel8_v_lowpass_lsx(halfV, (uint8_t*)src + 1, 8, stride);
+    avg_pixels8_l2_8_lsx(dst, halfH, halfV, stride, 8);
+}
+
+void ff_avg_h264_qpel8_mc02_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride)
+{
+    avg_h264_qpel8_v_lowpass_lsx(dst, (uint8_t*)src, stride, stride);
+}
+
+void ff_avg_h264_qpel8_mc12_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride)
+{
+    uint8_t temp[128];
+    uint8_t *const halfHV = temp;
+    uint8_t *const halfH  = temp + 64;
+
+    put_h264_qpel8_hv_lowpass_lsx(halfHV, src, 8, stride);
+    put_h264_qpel8_v_lowpass_lsx(halfH, (uint8_t*)src, 8, stride);
+    avg_pixels8_l2_8_lsx(dst, halfH, halfHV, stride, 8);
+}
+
+void ff_avg_h264_qpel8_mc22_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride)
+{
+    avg_h264_qpel8_hv_lowpass_lsx(dst, src, stride, stride);
+}
+
+void ff_avg_h264_qpel8_mc32_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride)
+{
+    uint8_t temp[128];
+    uint8_t *const halfHV = temp;
+    uint8_t *const halfH  = temp + 64;
+
+    put_h264_qpel8_hv_lowpass_lsx(halfHV, src, 8, stride);
+    put_h264_qpel8_v_lowpass_lsx(halfH, (uint8_t*)src + 1, 8, stride);
+    avg_pixels8_l2_8_lsx(dst, halfH, halfHV, stride, 8);
+}
+
+void ff_avg_h264_qpel8_mc13_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride)
+{
+    uint8_t halfH[64];
+    uint8_t halfV[64];
+
+    put_h264_qpel8_h_lowpass_lsx(halfH, src + stride, 8, stride);
+    put_h264_qpel8_v_lowpass_lsx(halfV, (uint8_t*)src, 8, stride);
+    avg_pixels8_l2_8_lsx(dst, halfH, halfV, stride, 8);
+}
+
+void ff_avg_h264_qpel8_mc23_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride)
+{
+    uint8_t temp[128];
+    uint8_t *const halfH  = temp;
+    uint8_t *const halfHV = temp + 64;
+
+    put_h264_qpel8_h_lowpass_lsx(halfH, src + stride, 8, stride);
+    put_h264_qpel8_hv_lowpass_lsx(halfHV, src, 8, stride);
+    avg_pixels8_l2_8_lsx(dst, halfH, halfHV, stride, 8);
+}
+
+void ff_avg_h264_qpel8_mc33_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride)
+{
+    uint8_t halfH[64];
+    uint8_t halfV[64];
+
+    put_h264_qpel8_h_lowpass_lsx(halfH, src + stride, 8, stride);
+    put_h264_qpel8_v_lowpass_lsx(halfV, (uint8_t*)src + 1, 8, stride);
+    avg_pixels8_l2_8_lsx(dst, halfH, halfV, stride, 8);
+}
+
diff --git a/libavcodec/loongarch/h264qpel_lsx.h b/libavcodec/loongarch/h264qpel_lsx.h
new file mode 100644
index 0000000000..34056f0e76
--- /dev/null
+++ b/libavcodec/loongarch/h264qpel_lsx.h
@@ -0,0 +1,178 @@
+/*
+ * Copyright (c) 2022 Loongson Technology Corporation Limited
+ * Contributed by Hecai Yuan <yuanhecai@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef AVCODEC_LOONGARCH_H264QPEL_LSX_H
+#define AVCODEC_LOONGARCH_H264QPEL_LSX_H
+
+#include <stdint.h>
+#include <stddef.h>
+#include "libavcodec/h264.h"
+
+void put_h264_qpel8_hv_lowpass_lsx(uint8_t *dst, const uint8_t *src,
+                                   ptrdiff_t dstStride, ptrdiff_t srcStride);
+void put_h264_qpel8_h_lowpass_lsx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t dstStride, ptrdiff_t srcStride);
+void put_h264_qpel8_v_lowpass_lsx(uint8_t *dst, const uint8_t *src,
+                                  ptrdiff_t dstStride, ptrdiff_t srcStride);
+void put_pixels16_l2_8_lsx(uint8_t *dst, const uint8_t *src, uint8_t *half,
+                           ptrdiff_t dstStride, ptrdiff_t srcStride);
+void put_pixels8_l2_8_lsx(uint8_t *dst, const uint8_t *src, const uint8_t *half,
+                          ptrdiff_t dstStride, ptrdiff_t srcStride);
+
+void avg_h264_qpel8_h_lowpass_lsx(uint8_t *dst, const uint8_t *src, int dstStride,
+                                  int srcStride);
+void avg_h264_qpel8_v_lowpass_lsx(uint8_t *dst, uint8_t *src, int dstStride,
+                                  int srcStride);
+void avg_pixels16_l2_8_lsx(uint8_t *dst, const uint8_t *src, uint8_t *half,
+                           ptrdiff_t dstStride, ptrdiff_t srcStride);
+void avg_h264_qpel8_hv_lowpass_lsx(uint8_t *dst, const uint8_t *src,
+                                   ptrdiff_t dstStride, ptrdiff_t srcStride);
+void avg_pixels8_l2_8_lsx(uint8_t *dst, const uint8_t *src, const uint8_t *half,
+                          ptrdiff_t dstStride, ptrdiff_t srcStride);
+
+void ff_put_h264_qpel16_mc00_lsx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t dst_stride);
+void ff_put_h264_qpel16_mc10_lsx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t dst_stride);
+void ff_put_h264_qpel16_mc20_lsx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t dst_stride);
+void ff_put_h264_qpel16_mc30_lsx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t dst_stride);
+void ff_put_h264_qpel16_mc01_lsx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t dst_stride);
+void ff_put_h264_qpel16_mc11_lsx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t dst_stride);
+void ff_put_h264_qpel16_mc13_lsx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t dst_stride);
+void ff_put_h264_qpel16_mc31_lsx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t dst_stride);
+void ff_put_h264_qpel16_mc33_lsx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t dst_stride);
+void ff_put_h264_qpel16_mc03_lsx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t dst_stride);
+void ff_put_h264_qpel16_mc02_lsx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t dst_stride);
+void ff_put_h264_qpel16_mc22_lsx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t dst_stride);
+void ff_put_h264_qpel16_mc21_lsx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride);
+void ff_put_h264_qpel16_mc12_lsx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride);
+void ff_put_h264_qpel16_mc32_lsx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride);
+void ff_put_h264_qpel16_mc23_lsx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride);
+
+void ff_avg_h264_qpel16_mc00_lsx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t dst_stride);
+void ff_avg_h264_qpel16_mc10_lsx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t dst_stride);
+void ff_avg_h264_qpel16_mc30_lsx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t dst_stride);
+void ff_avg_h264_qpel16_mc33_lsx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t dst_stride);
+void ff_avg_h264_qpel16_mc11_lsx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t dst_stride);
+void ff_avg_h264_qpel16_mc31_lsx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t dst_stride);
+void ff_avg_h264_qpel16_mc13_lsx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t dst_stride);
+void ff_avg_h264_qpel16_mc20_lsx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t dst_stride);
+void ff_avg_h264_qpel16_mc02_lsx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride);
+void ff_avg_h264_qpel16_mc03_lsx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride);
+void ff_avg_h264_qpel16_mc23_lsx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride);
+void ff_avg_h264_qpel16_mc21_lsx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride);
+void ff_avg_h264_qpel16_mc01_lsx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride);
+void ff_avg_h264_qpel16_mc32_lsx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride);
+void ff_avg_h264_qpel16_mc12_lsx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride);
+void ff_avg_h264_qpel16_mc22_lsx(uint8_t *dst, const uint8_t *src,
+                                 ptrdiff_t stride);
+
+void ff_put_h264_qpel8_mc03_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride);
+void ff_put_h264_qpel8_mc00_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride);
+void ff_put_h264_qpel8_mc01_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride);
+void ff_put_h264_qpel8_mc30_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride);
+void ff_put_h264_qpel8_mc10_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride);
+void ff_put_h264_qpel8_mc33_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride);
+void ff_put_h264_qpel8_mc13_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride);
+void ff_put_h264_qpel8_mc31_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride);
+void ff_put_h264_qpel8_mc11_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride);
+void ff_put_h264_qpel8_mc32_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride);
+void ff_put_h264_qpel8_mc21_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride);
+void ff_put_h264_qpel8_mc23_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride);
+void ff_put_h264_qpel8_mc12_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride);
+void ff_put_h264_qpel8_mc02_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride);
+void ff_put_h264_qpel8_mc22_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride);
+void ff_put_h264_qpel8_mc20_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride);
+
+void ff_avg_h264_qpel8_mc00_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride);
+void ff_avg_h264_qpel8_mc10_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride);
+void ff_avg_h264_qpel8_mc20_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride);
+void ff_avg_h264_qpel8_mc30_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride);
+void ff_avg_h264_qpel8_mc11_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride);
+void ff_avg_h264_qpel8_mc21_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride);
+void ff_avg_h264_qpel8_mc31_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride);
+void ff_avg_h264_qpel8_mc02_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride);
+void ff_avg_h264_qpel8_mc12_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride);
+void ff_avg_h264_qpel8_mc22_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride);
+void ff_avg_h264_qpel8_mc32_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride);
+void ff_avg_h264_qpel8_mc13_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride);
+void ff_avg_h264_qpel8_mc23_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride);
+void ff_avg_h264_qpel8_mc33_lsx(uint8_t *dst, const uint8_t *src,
+                                ptrdiff_t stride);
+#endif  // #ifndef AVCODEC_LOONGARCH_H264QPEL_LSX_H
diff --git a/libavcodec/loongarch/hevc_idct_lsx.c b/libavcodec/loongarch/hevc_idct_lsx.c
new file mode 100644
index 0000000000..2193b27546
--- /dev/null
+++ b/libavcodec/loongarch/hevc_idct_lsx.c
@@ -0,0 +1,842 @@
+/*
+ * Copyright (c) 2022 Loongson Technology Corporation Limited
+ * Contributed by Shiyou Yin <yinshiyou-hf@loongson.cn>
+ *                Hao Chen <chenhao@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "libavutil/loongarch/loongson_intrinsics.h"
+#include "hevcdsp_lsx.h"
+
+static const int16_t gt8x8_cnst[16] __attribute__ ((aligned (64))) = {
+    64, 64, 83, 36, 89, 50, 18, 75, 64, -64, 36, -83, 75, -89, -50, -18
+};
+
+static const int16_t gt16x16_cnst[64] __attribute__ ((aligned (64))) = {
+    64, 83, 64, 36, 89, 75, 50, 18, 90, 80, 57, 25, 70, 87, 9, 43,
+    64, 36, -64, -83, 75, -18, -89, -50, 87, 9, -80, -70, -43, 57, -25, -90,
+    64, -36, -64, 83, 50, -89, 18, 75, 80, -70, -25, 90, -87, 9, 43, 57,
+    64, -83, 64, -36, 18, -50, 75, -89, 70, -87, 90, -80, 9, -43, -57, 25
+};
+
+static const int16_t gt32x32_cnst0[256] __attribute__ ((aligned (64))) = {
+    90, 90, 88, 85, 82, 78, 73, 67, 61, 54, 46, 38, 31, 22, 13, 4,
+    90, 82, 67, 46, 22, -4, -31, -54, -73, -85, -90, -88, -78, -61, -38, -13,
+    88, 67, 31, -13, -54, -82, -90, -78, -46, -4, 38, 73, 90, 85, 61, 22,
+    85, 46, -13, -67, -90, -73, -22, 38, 82, 88, 54, -4, -61, -90, -78, -31,
+    82, 22, -54, -90, -61, 13, 78, 85, 31, -46, -90, -67, 4, 73, 88, 38,
+    78, -4, -82, -73, 13, 85, 67, -22, -88, -61, 31, 90, 54, -38, -90, -46,
+    73, -31, -90, -22, 78, 67, -38, -90, -13, 82, 61, -46, -88, -4, 85, 54,
+    67, -54, -78, 38, 85, -22, -90, 4, 90, 13, -88, -31, 82, 46, -73, -61,
+    61, -73, -46, 82, 31, -88, -13, 90, -4, -90, 22, 85, -38, -78, 54, 67,
+    54, -85, -4, 88, -46, -61, 82, 13, -90, 38, 67, -78, -22, 90, -31, -73,
+    46, -90, 38, 54, -90, 31, 61, -88, 22, 67, -85, 13, 73, -82, 4, 78,
+    38, -88, 73, -4, -67, 90, -46, -31, 85, -78, 13, 61, -90, 54, 22, -82,
+    31, -78, 90, -61, 4, 54, -88, 82, -38, -22, 73, -90, 67, -13, -46, 85,
+    22, -61, 85, -90, 73, -38, -4, 46, -78, 90, -82, 54, -13, -31, 67, -88,
+    13, -38, 61, -78, 88, -90, 85, -73, 54, -31, 4, 22, -46, 67, -82, 90,
+    4, -13, 22, -31, 38, -46, 54, -61, 67, -73, 78, -82, 85, -88, 90, -90
+};
+
+static const int16_t gt32x32_cnst1[64] __attribute__ ((aligned (64))) = {
+    90, 87, 80, 70, 57, 43, 25, 9, 87, 57, 9, -43, -80, -90, -70, -25,
+    80, 9, -70, -87, -25, 57, 90, 43, 70, -43, -87, 9, 90, 25, -80, -57,
+    57, -80, -25, 90, -9, -87, 43, 70, 43, -90, 57, 25, -87, 70, 9, -80,
+    25, -70, 90, -80, 43, 9, -57, 87, 9, -25, 43, -57, 70, -80, 87, -90
+};
+
+static const int16_t gt32x32_cnst2[16] __attribute__ ((aligned (64))) = {
+    89, 75, 50, 18, 75, -18, -89, -50, 50, -89, 18, 75, 18, -50, 75, -89
+};
+
+#define HEVC_IDCT4x4_COL(in_r0, in_l0, in_r1, in_l1,          \
+                         sum0, sum1, sum2, sum3, shift)       \
+{                                                             \
+    __m128i vec0, vec1, vec2, vec3, vec4, vec5;               \
+    __m128i cnst64 = __lsx_vldi(0x0840);                      \
+    __m128i cnst83 = __lsx_vldi(0x0853);                      \
+    __m128i cnst36 = __lsx_vldi(0x0824);                      \
+                                                              \
+    vec0 = __lsx_vdp2_w_h(in_r0, cnst64);                     \
+    vec1 = __lsx_vdp2_w_h(in_l0, cnst83);                     \
+    vec2 = __lsx_vdp2_w_h(in_r1, cnst64);                     \
+    vec3 = __lsx_vdp2_w_h(in_l1, cnst36);                     \
+    vec4 = __lsx_vdp2_w_h(in_l0, cnst36);                     \
+    vec5 = __lsx_vdp2_w_h(in_l1, cnst83);                     \
+                                                              \
+    sum0 = __lsx_vadd_w(vec0, vec2);                          \
+    sum1 = __lsx_vsub_w(vec0, vec2);                          \
+    vec1 = __lsx_vadd_w(vec1, vec3);                          \
+    vec4 = __lsx_vsub_w(vec4, vec5);                          \
+    sum2 = __lsx_vsub_w(sum1, vec4);                          \
+    sum3 = __lsx_vsub_w(sum0, vec1);                          \
+    sum0 = __lsx_vadd_w(sum0, vec1);                          \
+    sum1 = __lsx_vadd_w(sum1, vec4);                          \
+                                                              \
+    sum0 = __lsx_vsrari_w(sum0, shift);                       \
+    sum1 = __lsx_vsrari_w(sum1, shift);                       \
+    sum2 = __lsx_vsrari_w(sum2, shift);                       \
+    sum3 = __lsx_vsrari_w(sum3, shift);                       \
+    sum0 = __lsx_vsat_w(sum0, 15);                            \
+    sum1 = __lsx_vsat_w(sum1, 15);                            \
+    sum2 = __lsx_vsat_w(sum2, 15);                            \
+    sum3 = __lsx_vsat_w(sum3, 15);                            \
+}
+
+#define HEVC_IDCT8x8_COL(in0, in1, in2, in3, in4, in5, in6, in7, shift)  \
+{                                                                        \
+    __m128i src0_r, src1_r, src2_r, src3_r;                              \
+    __m128i src0_l, src1_l, src2_l, src3_l;                              \
+    __m128i filter0, filter1, filter2, filter3;                          \
+    __m128i temp0_r, temp1_r, temp2_r, temp3_r, temp4_r, temp5_r;        \
+    __m128i temp0_l, temp1_l, temp2_l, temp3_l, temp4_l, temp5_l;        \
+    __m128i sum0_r, sum1_r, sum2_r, sum3_r;                              \
+    __m128i sum0_l, sum1_l, sum2_l, sum3_l;                              \
+                                                                         \
+    DUP4_ARG2(__lsx_vilvl_h, in4, in0, in6, in2, in5, in1, in3, in7,     \
+              src0_r, src1_r, src2_r, src3_r);                           \
+    DUP4_ARG2(__lsx_vilvh_h, in4, in0, in6, in2, in5, in1, in3, in7,     \
+              src0_l, src1_l, src2_l, src3_l);                           \
+                                                                         \
+    DUP4_ARG2(__lsx_vldrepl_w, filter, 0, filter, 4, filter, 8,          \
+              filter, 12, filter0, filter1, filter2, filter3);           \
+    DUP4_ARG2(__lsx_vdp2_w_h, src0_r, filter0, src0_l, filter0,          \
+              src1_r, filter1, src1_l, filter1,  temp0_r, temp0_l,       \
+              temp1_r, temp1_l);                                         \
+                                                                         \
+    LSX_BUTTERFLY_4_W(temp0_r, temp0_l, temp1_l, temp1_r, sum0_r, sum0_l,\
+                      sum1_l, sum1_r);                                   \
+    sum2_r = sum1_r;                                                     \
+    sum2_l = sum1_l;                                                     \
+    sum3_r = sum0_r;                                                     \
+    sum3_l = sum0_l;                                                     \
+                                                                         \
+    DUP4_ARG2(__lsx_vdp2_w_h, src2_r, filter2, src2_l, filter2,          \
+              src3_r, filter3, src3_l, filter3,  temp2_r, temp2_l,       \
+              temp3_r, temp3_l);                                         \
+    temp2_r = __lsx_vadd_w(temp2_r, temp3_r);                            \
+    temp2_l = __lsx_vadd_w(temp2_l, temp3_l);                            \
+    sum0_r  = __lsx_vadd_w(sum0_r, temp2_r);                             \
+    sum0_l  = __lsx_vadd_w(sum0_l, temp2_l);                             \
+    sum3_r  = __lsx_vsub_w(sum3_r, temp2_r);                             \
+    sum3_l  = __lsx_vsub_w(sum3_l, temp2_l);                             \
+                                                                         \
+    in0 = __lsx_vssrarni_h_w(sum0_l, sum0_r, shift);                     \
+    in7 = __lsx_vssrarni_h_w(sum3_l, sum3_r, shift);                     \
+                                                                         \
+    DUP4_ARG2(__lsx_vdp2_w_h, src2_r, filter3, src2_l, filter3,          \
+              src3_r, filter2, src3_l, filter2,  temp4_r, temp4_l,       \
+              temp5_r, temp5_l);                                         \
+    temp4_r = __lsx_vsub_w(temp4_r, temp5_r);                            \
+    temp4_l = __lsx_vsub_w(temp4_l, temp5_l);                            \
+    sum1_r  = __lsx_vadd_w(sum1_r, temp4_r);                             \
+    sum1_l  = __lsx_vadd_w(sum1_l, temp4_l);                             \
+    sum2_r  = __lsx_vsub_w(sum2_r, temp4_r);                             \
+    sum2_l  = __lsx_vsub_w(sum2_l, temp4_l);                             \
+                                                                         \
+    in3 = __lsx_vssrarni_h_w(sum1_l, sum1_r, shift);                     \
+    in4 = __lsx_vssrarni_h_w(sum2_l, sum2_r, shift);                     \
+                                                                         \
+    DUP4_ARG2(__lsx_vldrepl_w, filter, 16, filter, 20, filter, 24,       \
+              filter, 28, filter0, filter1, filter2, filter3);           \
+    DUP4_ARG2(__lsx_vdp2_w_h, src0_r, filter0, src0_l, filter0,          \
+              src1_r, filter1, src1_l, filter1,  temp0_r, temp0_l,       \
+              temp1_r, temp1_l);                                         \
+                                                                         \
+    LSX_BUTTERFLY_4_W(temp0_r, temp0_l, temp1_l, temp1_r, sum0_r, sum0_l,\
+                      sum1_l, sum1_r);                                   \
+    sum2_r = sum1_r;                                                     \
+    sum2_l = sum1_l;                                                     \
+    sum3_r = sum0_r;                                                     \
+    sum3_l = sum0_l;                                                     \
+                                                                         \
+    DUP4_ARG2(__lsx_vdp2_w_h, src2_r, filter2, src2_l, filter2,          \
+              src3_r, filter3, src3_l, filter3,  temp2_r, temp2_l,       \
+              temp3_r, temp3_l);                                         \
+    temp2_r = __lsx_vadd_w(temp2_r, temp3_r);                            \
+    temp2_l = __lsx_vadd_w(temp2_l, temp3_l);                            \
+    sum0_r  = __lsx_vadd_w(sum0_r, temp2_r);                             \
+    sum0_l  = __lsx_vadd_w(sum0_l, temp2_l);                             \
+    sum3_r  = __lsx_vsub_w(sum3_r, temp2_r);                             \
+    sum3_l  = __lsx_vsub_w(sum3_l, temp2_l);                             \
+                                                                         \
+    in1 = __lsx_vssrarni_h_w(sum0_l, sum0_r, shift);                     \
+    in6 = __lsx_vssrarni_h_w(sum3_l, sum3_r, shift);                     \
+                                                                         \
+    DUP4_ARG2(__lsx_vdp2_w_h, src2_r, filter3, src2_l, filter3,          \
+              src3_r, filter2, src3_l, filter2,  temp4_r, temp4_l,       \
+              temp5_r, temp5_l);                                         \
+    temp4_r = __lsx_vsub_w(temp4_r, temp5_r);                            \
+    temp4_l = __lsx_vsub_w(temp4_l, temp5_l);                            \
+    sum1_r  = __lsx_vsub_w(sum1_r, temp4_r);                             \
+    sum1_l  = __lsx_vsub_w(sum1_l, temp4_l);                             \
+    sum2_r  = __lsx_vadd_w(sum2_r, temp4_r);                             \
+    sum2_l  = __lsx_vadd_w(sum2_l, temp4_l);                             \
+                                                                         \
+    in2 = __lsx_vssrarni_h_w(sum1_l, sum1_r, shift);                     \
+    in5 = __lsx_vssrarni_h_w(sum2_l, sum2_r, shift);                     \
+}
+
+#define HEVC_IDCT16x16_COL(src0_r, src1_r, src2_r, src3_r,                   \
+                           src4_r, src5_r, src6_r, src7_r,                   \
+                           src0_l, src1_l, src2_l, src3_l,                   \
+                           src4_l, src5_l, src6_l, src7_l, shift)            \
+{                                                                            \
+    int16_t *ptr0, *ptr1;                                                    \
+    __m128i dst0, dst1;                                                      \
+    __m128i filter0, filter1, filter2, filter3;                              \
+    __m128i temp0_r, temp1_r, temp0_l, temp1_l;                              \
+    __m128i sum0_r, sum1_r, sum2_r, sum3_r, sum0_l, sum1_l, sum2_l;          \
+    __m128i sum3_l, res0_r, res1_r, res0_l, res1_l;                          \
+                                                                             \
+    ptr0 = (buf_ptr + 112);                                                  \
+    ptr1 = (buf_ptr + 128);                                                  \
+    k = -1;                                                                  \
+                                                                             \
+    for (j = 0; j < 4; j++)                                                  \
+    {                                                                        \
+        DUP4_ARG2(__lsx_vldrepl_w, filter, 0, filter, 4, filter, 16,         \
+                  filter, 20, filter0, filter1, filter2, filter3);           \
+        DUP4_ARG2(__lsx_vdp2_w_h, src0_r, filter0, src0_l, filter0,          \
+                  src4_r, filter2, src4_l, filter2,  sum0_r, sum0_l,         \
+                  sum2_r, sum2_l);                                           \
+        DUP2_ARG2(__lsx_vdp2_w_h, src7_r, filter2, src7_l, filter2,          \
+                  sum3_r, sum3_l);                                           \
+        DUP4_ARG3(__lsx_vdp2add_w_h, sum0_r, src1_r, filter1, sum0_l,        \
+                  src1_l, filter1, sum2_r, src5_r, filter3, sum2_l,          \
+                  src5_l, filter3, sum0_r, sum0_l, sum2_r, sum2_l);          \
+        DUP2_ARG3(__lsx_vdp2add_w_h, sum3_r, src6_r, filter3, sum3_l,        \
+                  src6_l, filter3, sum3_r, sum3_l);                          \
+                                                                             \
+        sum1_r = sum0_r;                                                     \
+        sum1_l = sum0_l;                                                     \
+                                                                             \
+        DUP4_ARG2(__lsx_vldrepl_w, filter, 8, filter, 12, filter, 24,        \
+                  filter, 28, filter0, filter1, filter2, filter3);           \
+        filter += 16;                                                        \
+        DUP2_ARG2(__lsx_vdp2_w_h, src2_r, filter0, src2_l, filter0,          \
+                  temp0_r, temp0_l);                                         \
+        DUP2_ARG3(__lsx_vdp2add_w_h, sum2_r, src6_r, filter2, sum2_l,        \
+                  src6_l, filter2, sum2_r, sum2_l);                          \
+        DUP2_ARG2(__lsx_vdp2_w_h, src5_r, filter2, src5_l, filter2,          \
+                  temp1_r, temp1_l);                                         \
+                                                                             \
+        sum0_r = __lsx_vadd_w(sum0_r, temp0_r);                              \
+        sum0_l = __lsx_vadd_w(sum0_l, temp0_l);                              \
+        sum1_r = __lsx_vsub_w(sum1_r, temp0_r);                              \
+        sum1_l = __lsx_vsub_w(sum1_l, temp0_l);                              \
+        sum3_r = __lsx_vsub_w(temp1_r, sum3_r);                              \
+        sum3_l = __lsx_vsub_w(temp1_l, sum3_l);                              \
+                                                                             \
+        DUP2_ARG2(__lsx_vdp2_w_h, src3_r, filter1, src3_l, filter1,          \
+                  temp0_r, temp0_l);                                         \
+        DUP4_ARG3(__lsx_vdp2add_w_h, sum2_r, src7_r, filter3, sum2_l,        \
+                  src7_l, filter3, sum3_r, src4_r, filter3, sum3_l,          \
+                  src4_l, filter3, sum2_r, sum2_l, sum3_r, sum3_l);          \
+                                                                             \
+        sum0_r = __lsx_vadd_w(sum0_r, temp0_r);                              \
+        sum0_l = __lsx_vadd_w(sum0_l, temp0_l);                              \
+        sum1_r = __lsx_vsub_w(sum1_r, temp0_r);                              \
+        sum1_l = __lsx_vsub_w(sum1_l, temp0_l);                              \
+                                                                             \
+        LSX_BUTTERFLY_4_W(sum0_r, sum0_l, sum2_l, sum2_r, res0_r, res0_l,    \
+                          res1_l, res1_r);                                   \
+        dst0 = __lsx_vssrarni_h_w(res0_l, res0_r, shift);                    \
+        dst1 = __lsx_vssrarni_h_w(res1_l, res1_r, shift);                    \
+        __lsx_vst(dst0, buf_ptr, 0);                                         \
+        __lsx_vst(dst1, (buf_ptr + ((15 - (j * 2)) << 4)), 0);               \
+                                                                             \
+        LSX_BUTTERFLY_4_W(sum1_r, sum1_l, sum3_l, sum3_r, res0_r, res0_l,    \
+                          res1_l, res1_r);                                   \
+                                                                             \
+        dst0 = __lsx_vssrarni_h_w(res0_l, res0_r, shift);                    \
+        dst1 = __lsx_vssrarni_h_w(res1_l, res1_r, shift);                    \
+        __lsx_vst(dst0, (ptr0 + ((((j + 1) >> 1) * 2 * k) << 4)), 0);        \
+        __lsx_vst(dst1, (ptr1 - ((((j + 1) >> 1) * 2 * k) << 4)), 0);        \
+                                                                             \
+        k *= -1;                                                             \
+        buf_ptr += 16;                                                       \
+    }                                                                        \
+}
+
+#define HEVC_EVEN16_CALC(input, sum0_r, sum0_l, load_idx, store_idx)  \
+{                                                                     \
+    tmp0_r = __lsx_vld(input + load_idx * 8, 0);                      \
+    tmp0_l = __lsx_vld(input + load_idx * 8, 16);                     \
+    tmp1_r = sum0_r;                                                  \
+    tmp1_l = sum0_l;                                                  \
+    sum0_r = __lsx_vadd_w(sum0_r, tmp0_r);                            \
+    sum0_l = __lsx_vadd_w(sum0_l, tmp0_l);                            \
+    __lsx_vst(sum0_r, (input + load_idx * 8), 0);                     \
+    __lsx_vst(sum0_l, (input + load_idx * 8), 16);                    \
+    tmp1_r = __lsx_vsub_w(tmp1_r, tmp0_r);                            \
+    tmp1_l = __lsx_vsub_w(tmp1_l, tmp0_l);                            \
+    __lsx_vst(tmp1_r, (input + store_idx * 8), 0);                    \
+    __lsx_vst(tmp1_l, (input + store_idx * 8), 16);                   \
+}
+
+#define HEVC_IDCT_LUMA4x4_COL(in_r0, in_l0, in_r1, in_l1,     \
+                              res0, res1, res2, res3, shift)  \
+{                                                             \
+    __m128i vec0, vec1, vec2, vec3;                           \
+    __m128i cnst74 = __lsx_vldi(0x84a);                       \
+    __m128i cnst55 = __lsx_vldi(0x837);                       \
+    __m128i cnst29 = __lsx_vldi(0x81d);                       \
+                                                              \
+    vec0 = __lsx_vadd_w(in_r0, in_r1);                        \
+    vec2 = __lsx_vsub_w(in_r0, in_l1);                        \
+    res0 = __lsx_vmul_w(vec0, cnst29);                        \
+    res1 = __lsx_vmul_w(vec2, cnst55);                        \
+    res2 = __lsx_vsub_w(in_r0, in_r1);                        \
+    vec1 = __lsx_vadd_w(in_r1, in_l1);                        \
+    res2 = __lsx_vadd_w(res2, in_l1);                         \
+    vec3 = __lsx_vmul_w(in_l0, cnst74);                       \
+    res3 = __lsx_vmul_w(vec0, cnst55);                        \
+                                                              \
+    res0 = __lsx_vadd_w(res0, __lsx_vmul_w(vec1, cnst55));    \
+    res1 = __lsx_vsub_w(res1, __lsx_vmul_w(vec1, cnst29));    \
+    res2 = __lsx_vmul_w(res2, cnst74);                        \
+    res3 = __lsx_vadd_w(res3, __lsx_vmul_w(vec2, cnst29));    \
+                                                              \
+    res0 = __lsx_vadd_w(res0, vec3);                          \
+    res1 = __lsx_vadd_w(res1, vec3);                          \
+    res3 = __lsx_vsub_w(res3, vec3);                          \
+                                                              \
+    res0 = __lsx_vsrari_w(res0, shift);                       \
+    res1 = __lsx_vsrari_w(res1, shift);                       \
+    res2 = __lsx_vsrari_w(res2, shift);                       \
+    res3 = __lsx_vsrari_w(res3, shift);                       \
+    res0 = __lsx_vsat_w(res0, 15);                            \
+    res1 = __lsx_vsat_w(res1, 15);                            \
+    res2 = __lsx_vsat_w(res2, 15);                            \
+    res3 = __lsx_vsat_w(res3, 15);                            \
+}
+
+void ff_hevc_idct_4x4_lsx(int16_t *coeffs, int col_limit)
+{
+    __m128i in0, in1;
+    __m128i in_r0, in_l0, in_r1, in_l1;
+    __m128i sum0, sum1, sum2, sum3;
+    __m128i zero = __lsx_vldi(0x00);
+
+    in0   = __lsx_vld(coeffs, 0);
+    in1   = __lsx_vld(coeffs, 16);
+    in_r0 = __lsx_vilvl_h(zero, in0);
+    in_l0 = __lsx_vilvh_h(zero, in0);
+    in_r1 = __lsx_vilvl_h(zero, in1);
+    in_l1 = __lsx_vilvh_h(zero, in1);
+
+    HEVC_IDCT4x4_COL(in_r0, in_l0, in_r1, in_l1, sum0, sum1, sum2, sum3, 7);
+    LSX_TRANSPOSE4x4_W(sum0, sum1, sum2, sum3, in_r0, in_l0, in_r1, in_l1);
+    HEVC_IDCT4x4_COL(in_r0, in_l0, in_r1, in_l1, sum0, sum1, sum2, sum3, 12);
+
+    /* Pack and transpose */
+    in0  = __lsx_vpickev_h(sum2, sum0);
+    in1  = __lsx_vpickev_h(sum3, sum1);
+    sum0 = __lsx_vilvl_h(in1, in0);
+    sum1 = __lsx_vilvh_h(in1, in0);
+    in0  = __lsx_vilvl_w(sum1, sum0);
+    in1  = __lsx_vilvh_w(sum1, sum0);
+
+    __lsx_vst(in0, coeffs, 0);
+    __lsx_vst(in1, coeffs, 16);
+}
+
+void ff_hevc_idct_8x8_lsx(int16_t *coeffs, int col_limit)
+{
+    const int16_t *filter = &gt8x8_cnst[0];
+    __m128i in0, in1, in2, in3, in4, in5, in6, in7;
+
+    DUP4_ARG2(__lsx_vld, coeffs, 0, coeffs, 16, coeffs, 32,
+              coeffs, 48, in0, in1, in2, in3);
+    DUP4_ARG2(__lsx_vld, coeffs, 64, coeffs, 80, coeffs, 96,
+              coeffs, 112, in4, in5, in6, in7);
+    HEVC_IDCT8x8_COL(in0, in1, in2, in3, in4, in5, in6, in7, 7);
+    LSX_TRANSPOSE8x8_H(in0, in1, in2, in3, in4, in5, in6, in7,
+                       in0, in1, in2, in3, in4, in5, in6, in7);
+    HEVC_IDCT8x8_COL(in0, in1, in2, in3, in4, in5, in6, in7, 12);
+    LSX_TRANSPOSE8x8_H(in0, in1, in2, in3, in4, in5, in6, in7,
+                       in0, in1, in2, in3, in4, in5, in6, in7);
+
+    __lsx_vst(in0, coeffs, 0);
+    __lsx_vst(in1, coeffs, 16);
+    __lsx_vst(in2, coeffs, 32);
+    __lsx_vst(in3, coeffs, 48);
+    __lsx_vst(in4, coeffs, 64);
+    __lsx_vst(in5, coeffs, 80);
+    __lsx_vst(in6, coeffs, 96);
+    __lsx_vst(in7, coeffs, 112);
+}
+
+void ff_hevc_idct_16x16_lsx(int16_t *coeffs, int col_limit)
+{
+    int16_t i, j, k;
+    int16_t buf[256];
+    int16_t *buf_ptr = &buf[0];
+    int16_t *src = coeffs;
+    const int16_t *filter = &gt16x16_cnst[0];
+    __m128i in0, in1, in2, in3, in4, in5, in6, in7;
+    __m128i in8, in9, in10, in11, in12, in13, in14, in15;
+    __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
+    __m128i src0_r, src1_r, src2_r, src3_r, src4_r, src5_r, src6_r, src7_r;
+    __m128i src0_l, src1_l, src2_l, src3_l, src4_l, src5_l, src6_l, src7_l;
+
+    for (i = 2; i--;) {
+        DUP4_ARG2(__lsx_vld, src, 0, src, 32, src, 64, src, 96,
+                  in0, in1, in2, in3);
+        DUP4_ARG2(__lsx_vld, src, 128, src, 160, src, 192, src, 224,
+                  in4, in5, in6, in7);
+        DUP4_ARG2(__lsx_vld, src, 256, src, 288, src, 320, src, 352,
+                  in8, in9, in10, in11);
+        DUP4_ARG2(__lsx_vld, src, 384, src, 416, src, 448, src, 480,
+                  in12, in13, in14, in15);
+
+        DUP4_ARG2(__lsx_vilvl_h, in4, in0, in12, in8, in6, in2, in14, in10,
+                  src0_r, src1_r, src2_r, src3_r);
+        DUP4_ARG2(__lsx_vilvl_h, in5, in1, in13, in9, in3, in7, in11, in15,
+                  src4_r, src5_r, src6_r, src7_r);
+        DUP4_ARG2(__lsx_vilvh_h, in4, in0, in12, in8, in6, in2, in14, in10,
+                  src0_l, src1_l, src2_l, src3_l);
+        DUP4_ARG2(__lsx_vilvh_h, in5, in1, in13, in9, in3, in7, in11, in15,
+                  src4_l, src5_l, src6_l, src7_l);
+
+        HEVC_IDCT16x16_COL(src0_r, src1_r, src2_r, src3_r, src4_r, src5_r,
+                           src6_r, src7_r, src0_l, src1_l, src2_l, src3_l,
+                           src4_l, src5_l, src6_l, src7_l, 7);
+
+        src += 8;
+        buf_ptr = (&buf[0] + 8);
+        filter = &gt16x16_cnst[0];
+    }
+
+    src = &buf[0];
+    buf_ptr = coeffs;
+    filter = &gt16x16_cnst[0];
+
+    for (i = 2; i--;) {
+        DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48,
+                  in0, in8, in1, in9);
+        DUP4_ARG2(__lsx_vld, src, 64, src, 80, src, 96, src, 112,
+                  in2, in10, in3, in11);
+        DUP4_ARG2(__lsx_vld, src, 128, src, 144, src, 160, src, 176,
+                  in4, in12, in5, in13);
+        DUP4_ARG2(__lsx_vld, src, 192, src, 208, src, 224, src, 240,
+                  in6, in14, in7, in15);
+        LSX_TRANSPOSE8x8_H(in0, in1, in2, in3, in4, in5, in6, in7,
+                           in0, in1, in2, in3, in4, in5, in6, in7);
+        LSX_TRANSPOSE8x8_H(in8, in9, in10, in11, in12, in13, in14, in15,
+                           in8, in9, in10, in11, in12, in13, in14, in15);
+        DUP4_ARG2(__lsx_vilvl_h, in4, in0, in12, in8, in6, in2, in14, in10,
+                  src0_r, src1_r, src2_r, src3_r);
+        DUP4_ARG2(__lsx_vilvl_h, in5, in1, in13, in9, in3, in7, in11, in15,
+                  src4_r, src5_r, src6_r, src7_r);
+        DUP4_ARG2(__lsx_vilvh_h, in4, in0, in12, in8, in6, in2, in14, in10,
+                  src0_l, src1_l, src2_l, src3_l);
+        DUP4_ARG2(__lsx_vilvh_h, in5, in1, in13, in9, in3, in7, in11, in15,
+                  src4_l, src5_l, src6_l, src7_l);
+        HEVC_IDCT16x16_COL(src0_r, src1_r, src2_r, src3_r, src4_r, src5_r,
+                           src6_r, src7_r, src0_l, src1_l, src2_l, src3_l,
+                           src4_l, src5_l, src6_l, src7_l, 12);
+
+        src += 128;
+        buf_ptr = coeffs + 8;
+        filter = &gt16x16_cnst[0];
+    }
+
+    DUP4_ARG2(__lsx_vld, coeffs, 0, coeffs, 32, coeffs, 64, coeffs, 96,
+              in0, in1, in2, in3);
+    DUP4_ARG2(__lsx_vld, coeffs, 128, coeffs, 160, coeffs, 192, coeffs, 224,
+              in4, in5, in6, in7);
+    LSX_TRANSPOSE8x8_H(in0, in1, in2, in3, in4, in5, in6, in7,
+                       vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7);
+    __lsx_vst(vec0, coeffs, 0);
+    __lsx_vst(vec1, coeffs, 32);
+    __lsx_vst(vec2, coeffs, 64);
+    __lsx_vst(vec3, coeffs, 96);
+    __lsx_vst(vec4, coeffs, 128);
+    __lsx_vst(vec5, coeffs, 160);
+    __lsx_vst(vec6, coeffs, 192);
+    __lsx_vst(vec7, coeffs, 224);
+
+    src = coeffs + 8;
+    DUP4_ARG2(__lsx_vld, src, 0, src, 32, src, 64, src, 96, in0, in1, in2, in3);
+    DUP4_ARG2(__lsx_vld, src, 128, src, 160, src, 192, src, 224,
+              in4, in5, in6, in7);
+    LSX_TRANSPOSE8x8_H(in0, in1, in2, in3, in4, in5, in6, in7,
+                       vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7);
+    src = coeffs + 128;
+    DUP4_ARG2(__lsx_vld, src, 0, src, 32, src, 64, src, 96,
+              in8, in9, in10, in11);
+    DUP4_ARG2(__lsx_vld, src, 128, src, 160, src, 192, src, 224,
+              in12, in13, in14, in15);
+
+    __lsx_vst(vec0, src, 0);
+    __lsx_vst(vec1, src, 32);
+    __lsx_vst(vec2, src, 64);
+    __lsx_vst(vec3, src, 96);
+    __lsx_vst(vec4, src, 128);
+    __lsx_vst(vec5, src, 160);
+    __lsx_vst(vec6, src, 192);
+    __lsx_vst(vec7, src, 224);
+    LSX_TRANSPOSE8x8_H(in8, in9, in10, in11, in12, in13, in14, in15,
+                       vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7);
+    src = coeffs + 8;
+    __lsx_vst(vec0, src, 0);
+    __lsx_vst(vec1, src, 32);
+    __lsx_vst(vec2, src, 64);
+    __lsx_vst(vec3, src, 96);
+    __lsx_vst(vec4, src, 128);
+    __lsx_vst(vec5, src, 160);
+    __lsx_vst(vec6, src, 192);
+    __lsx_vst(vec7, src, 224);
+
+    src = coeffs + 136;
+    DUP4_ARG2(__lsx_vld, src, 0, src, 32, src, 64, src, 96,
+              in0, in1, in2, in3);
+    DUP4_ARG2(__lsx_vld, src, 128, src, 160, src, 192, src, 224,
+              in4, in5, in6, in7);
+    LSX_TRANSPOSE8x8_H(in0, in1, in2, in3, in4, in5, in6, in7,
+                       vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7);
+    __lsx_vst(vec0, src, 0);
+    __lsx_vst(vec1, src, 32);
+    __lsx_vst(vec2, src, 64);
+    __lsx_vst(vec3, src, 96);
+    __lsx_vst(vec4, src, 128);
+    __lsx_vst(vec5, src, 160);
+    __lsx_vst(vec6, src, 192);
+    __lsx_vst(vec7, src, 224);
+}
+
+static void hevc_idct_8x32_column_lsx(int16_t *coeffs, int32_t buf_pitch,
+                                      uint8_t round)
+{
+    uint8_t i;
+    int32_t buf_pitch_2  = buf_pitch << 1;
+    int32_t buf_pitch_4  = buf_pitch << 2;
+    int32_t buf_pitch_8  = buf_pitch << 3;
+    int32_t buf_pitch_16 = buf_pitch << 4;
+
+    const int16_t *filter_ptr0 = &gt32x32_cnst0[0];
+    const int16_t *filter_ptr1 = &gt32x32_cnst1[0];
+    const int16_t *filter_ptr2 = &gt32x32_cnst2[0];
+    const int16_t *filter_ptr3 = &gt8x8_cnst[0];
+    int16_t *src0 = (coeffs + buf_pitch);
+    int16_t *src1 = (coeffs + buf_pitch_2);
+    int16_t *src2 = (coeffs + buf_pitch_4);
+    int16_t *src3 = (coeffs);
+    int32_t tmp_buf[8 * 32 + 15];
+    int32_t *tmp_buf_ptr = tmp_buf + 15;
+    __m128i in0, in1, in2, in3, in4, in5, in6, in7;
+    __m128i src0_r, src1_r, src2_r, src3_r, src4_r, src5_r, src6_r, src7_r;
+    __m128i src0_l, src1_l, src2_l, src3_l, src4_l, src5_l, src6_l, src7_l;
+    __m128i filter0, filter1, filter2, filter3;
+    __m128i sum0_r, sum0_l, sum1_r, sum1_l, tmp0_r, tmp0_l, tmp1_r, tmp1_l;
+
+    /* Align pointer to 64 byte boundary */
+    tmp_buf_ptr = (int32_t *)(((uintptr_t) tmp_buf_ptr) & ~(uintptr_t) 63);
+
+    /* process coeff 4, 12, 20, 28 */
+    in0 = __lsx_vld(src2, 0);
+    in1 = __lsx_vld(src2 + buf_pitch_8, 0);
+    in2 = __lsx_vld(src2 + buf_pitch_16, 0);
+    in3 = __lsx_vld(src2 + buf_pitch_16 + buf_pitch_8, 0);
+    in4 = __lsx_vld(src3, 0);
+    in5 = __lsx_vld(src3 + buf_pitch_8, 0);
+    in6 = __lsx_vld(src3 + buf_pitch_16, 0);
+    in7 = __lsx_vld(src3 + buf_pitch_16 + buf_pitch_8, 0);
+    DUP4_ARG2(__lsx_vilvl_h, in1, in0, in3, in2, in6, in4, in7, in5,
+              src0_r, src1_r, src2_r, src3_r);
+    DUP4_ARG2(__lsx_vilvh_h, in1, in0, in3, in2, in6, in4, in7, in5,
+              src0_l, src1_l, src2_l, src3_l);
+
+    filter0 = __lsx_vldrepl_w(filter_ptr2, 0);
+    filter1 = __lsx_vldrepl_w(filter_ptr2, 4);
+    sum0_r = __lsx_vdp2_w_h(src0_r, filter0);
+    sum0_l = __lsx_vdp2_w_h(src0_l, filter0);
+    sum0_r = __lsx_vdp2add_w_h(sum0_r, src1_r, filter1);
+    sum0_l = __lsx_vdp2add_w_h(sum0_l, src1_l, filter1);
+    __lsx_vst(sum0_r, tmp_buf_ptr, 0);
+    __lsx_vst(sum0_l, tmp_buf_ptr, 16);
+
+    filter0 = __lsx_vldrepl_w(filter_ptr2, 8);
+    filter1 = __lsx_vldrepl_w(filter_ptr2, 12);
+    sum0_r = __lsx_vdp2_w_h(src0_r, filter0);
+    sum0_l = __lsx_vdp2_w_h(src0_l, filter0);
+    sum0_r = __lsx_vdp2add_w_h(sum0_r, src1_r, filter1);
+    sum0_l = __lsx_vdp2add_w_h(sum0_l, src1_l, filter1);
+    __lsx_vst(sum0_r, tmp_buf_ptr, 32);
+    __lsx_vst(sum0_l, tmp_buf_ptr, 48);
+
+    filter0 = __lsx_vldrepl_w(filter_ptr2, 16);
+    filter1 = __lsx_vldrepl_w(filter_ptr2, 20);
+    sum0_r = __lsx_vdp2_w_h(src0_r, filter0);
+    sum0_l = __lsx_vdp2_w_h(src0_l, filter0);
+    sum0_r = __lsx_vdp2add_w_h(sum0_r, src1_r, filter1);
+    sum0_l = __lsx_vdp2add_w_h(sum0_l, src1_l, filter1);
+    __lsx_vst(sum0_r, tmp_buf_ptr, 64);
+    __lsx_vst(sum0_l, tmp_buf_ptr, 80);
+
+    filter0 = __lsx_vldrepl_w(filter_ptr2, 24);
+    filter1 = __lsx_vldrepl_w(filter_ptr2, 28);
+    sum0_r = __lsx_vdp2_w_h(src0_r, filter0);
+    sum0_l = __lsx_vdp2_w_h(src0_l, filter0);
+    sum0_r = __lsx_vdp2add_w_h(sum0_r, src1_r, filter1);
+    sum0_l = __lsx_vdp2add_w_h(sum0_l, src1_l, filter1);
+    __lsx_vst(sum0_r, tmp_buf_ptr, 96);
+    __lsx_vst(sum0_l, tmp_buf_ptr, 112);
+
+    /* process coeff 0, 8, 16, 24 */
+    filter0 = __lsx_vldrepl_w(filter_ptr3, 0);
+    filter1 = __lsx_vldrepl_w(filter_ptr3, 4);
+
+    DUP4_ARG2(__lsx_vdp2_w_h, src2_r, filter0, src2_l, filter0,
+              src3_r, filter1, src3_l, filter1, sum0_r, sum0_l, tmp1_r, tmp1_l);
+    sum1_r = __lsx_vsub_w(sum0_r, tmp1_r);
+    sum1_l = __lsx_vsub_w(sum0_l, tmp1_l);
+    sum0_r = __lsx_vadd_w(sum0_r, tmp1_r);
+    sum0_l = __lsx_vadd_w(sum0_l, tmp1_l);
+
+    HEVC_EVEN16_CALC(tmp_buf_ptr, sum0_r, sum0_l, 0, 7);
+    HEVC_EVEN16_CALC(tmp_buf_ptr, sum1_r, sum1_l, 3, 4);
+
+    filter0 = __lsx_vldrepl_w(filter_ptr3, 16);
+    filter1 = __lsx_vldrepl_w(filter_ptr3, 20);
+
+    DUP4_ARG2(__lsx_vdp2_w_h, src2_r, filter0, src2_l, filter0,
+              src3_r, filter1, src3_l, filter1, sum0_r, sum0_l, tmp1_r, tmp1_l);
+    sum1_r = __lsx_vsub_w(sum0_r, tmp1_r);
+    sum1_l = __lsx_vsub_w(sum0_l, tmp1_l);
+    sum0_r = __lsx_vadd_w(sum0_r, tmp1_r);
+    sum0_l = __lsx_vadd_w(sum0_l, tmp1_l);
+
+    HEVC_EVEN16_CALC(tmp_buf_ptr, sum0_r, sum0_l, 1, 6);
+    HEVC_EVEN16_CALC(tmp_buf_ptr, sum1_r, sum1_l, 2, 5);
+
+    /* process coeff 2 6 10 14 18 22 26 30 */
+    in0 = __lsx_vld(src1, 0);
+    in1 = __lsx_vld(src1 + buf_pitch_4, 0);
+    in2 = __lsx_vld(src1 + buf_pitch_8, 0);
+    in3 = __lsx_vld(src1 + buf_pitch_8 + buf_pitch_4, 0);
+    in4 = __lsx_vld(src1 + buf_pitch_16, 0);
+    in5 = __lsx_vld(src1 + buf_pitch_16 + buf_pitch_4, 0);
+    in6 = __lsx_vld(src1 + buf_pitch_16 + buf_pitch_8, 0);
+    in7 = __lsx_vld(src1 + buf_pitch_16 + buf_pitch_8 + buf_pitch_4, 0);
+
+    DUP4_ARG2(__lsx_vilvl_h, in1, in0, in3, in2, in5, in4, in7, in6,
+              src0_r, src1_r, src2_r, src3_r);
+    DUP4_ARG2(__lsx_vilvh_h, in1, in0, in3, in2, in5, in4, in7, in6,
+              src0_l, src1_l, src2_l, src3_l);
+
+    /* loop for all columns of constants */
+    for (i = 0; i < 8; i++) {
+        /* processing single column of constants */
+        filter0 = __lsx_vldrepl_w(filter_ptr1, 0);
+        filter1 = __lsx_vldrepl_w(filter_ptr1, 4);
+        filter2 = __lsx_vldrepl_w(filter_ptr1, 8);
+        filter3 = __lsx_vldrepl_w(filter_ptr1, 12);
+        sum0_r = __lsx_vdp2_w_h(src0_r, filter0);
+        sum0_l = __lsx_vdp2_w_h(src0_l, filter0);
+        sum0_r = __lsx_vdp2add_w_h(sum0_r, src1_r, filter1);
+        sum0_l = __lsx_vdp2add_w_h(sum0_l, src1_l, filter1);
+        sum0_r = __lsx_vdp2add_w_h(sum0_r, src2_r, filter2);
+        sum0_l = __lsx_vdp2add_w_h(sum0_l, src2_l, filter2);
+        sum0_r = __lsx_vdp2add_w_h(sum0_r, src3_r, filter3);
+        sum0_l = __lsx_vdp2add_w_h(sum0_l, src3_l, filter3);
+
+        tmp0_r = __lsx_vld(tmp_buf_ptr + (i << 3), 0);
+        tmp0_l = __lsx_vld(tmp_buf_ptr + (i << 3), 16);
+        tmp1_r = tmp0_r;
+        tmp1_l = tmp0_l;
+        tmp0_r = __lsx_vadd_w(tmp0_r, sum0_r);
+        tmp0_l = __lsx_vadd_w(tmp0_l, sum0_l);
+        tmp1_r = __lsx_vsub_w(tmp1_r, sum0_r);
+        tmp1_l = __lsx_vsub_w(tmp1_l, sum0_l);
+        __lsx_vst(tmp0_r, tmp_buf_ptr + (i << 3), 0);
+        __lsx_vst(tmp0_l, tmp_buf_ptr + (i << 3), 16);
+        __lsx_vst(tmp1_r, tmp_buf_ptr + ((15 - i) * 8), 0);
+        __lsx_vst(tmp1_l, tmp_buf_ptr + ((15 - i) * 8), 16);
+
+        filter_ptr1 += 8;
+    }
+
+    /* process coeff 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 */
+    in0 = __lsx_vld(src0, 0);
+    in1 = __lsx_vld(src0 + buf_pitch_2, 0);
+    in2 = __lsx_vld(src0 + buf_pitch_4, 0);
+    in3 = __lsx_vld(src0 + buf_pitch_4 + buf_pitch_2, 0);
+    in4 = __lsx_vld(src0 + buf_pitch_8, 0);
+    in5 = __lsx_vld(src0 + buf_pitch_8 + buf_pitch_2, 0);
+    in6 = __lsx_vld(src0 + buf_pitch_8 + buf_pitch_4, 0);
+    in7 = __lsx_vld(src0 + buf_pitch_8 + buf_pitch_4 + buf_pitch_2, 0);
+
+    src0 += 16 * buf_pitch;
+    DUP4_ARG2(__lsx_vilvl_h, in1, in0, in3, in2, in5, in4, in7, in6,
+              src0_r, src1_r, src2_r, src3_r);
+    DUP4_ARG2(__lsx_vilvh_h, in1, in0, in3, in2, in5, in4, in7, in6,
+              src0_l, src1_l, src2_l, src3_l);
+    in0 = __lsx_vld(src0, 0);
+    in1 = __lsx_vld(src0 + buf_pitch_2, 0);
+    in2 = __lsx_vld(src0 + buf_pitch_4, 0);
+    in3 = __lsx_vld(src0 + buf_pitch_4 + buf_pitch_2, 0);
+    in4 = __lsx_vld(src0 + buf_pitch_8, 0);
+    in5 = __lsx_vld(src0 + buf_pitch_8 + buf_pitch_2, 0);
+    in6 = __lsx_vld(src0 + buf_pitch_8 + buf_pitch_4, 0);
+    in7 = __lsx_vld(src0 + buf_pitch_8 + buf_pitch_4 + buf_pitch_2, 0);
+
+    DUP4_ARG2(__lsx_vilvl_h, in1, in0, in3, in2, in5, in4, in7, in6,
+              src4_r, src5_r, src6_r, src7_r);
+    DUP4_ARG2(__lsx_vilvh_h, in1, in0, in3, in2, in5, in4, in7, in6,
+              src4_l, src5_l, src6_l, src7_l);
+
+    /* loop for all columns of filter constants */
+    for (i = 0; i < 16; i++) {
+        /* processing single column of constants */
+        filter0 = __lsx_vldrepl_w(filter_ptr0, 0);
+        filter1 = __lsx_vldrepl_w(filter_ptr0, 4);
+        filter2 = __lsx_vldrepl_w(filter_ptr0, 8);
+        filter3 = __lsx_vldrepl_w(filter_ptr0, 12);
+        sum0_r = __lsx_vdp2_w_h(src0_r, filter0);
+        sum0_l = __lsx_vdp2_w_h(src0_l, filter0);
+        sum0_r = __lsx_vdp2add_w_h(sum0_r, src1_r, filter1);
+        sum0_l = __lsx_vdp2add_w_h(sum0_l, src1_l, filter1);
+        sum0_r = __lsx_vdp2add_w_h(sum0_r, src2_r, filter2);
+        sum0_l = __lsx_vdp2add_w_h(sum0_l, src2_l, filter2);
+        sum0_r = __lsx_vdp2add_w_h(sum0_r, src3_r, filter3);
+        sum0_l = __lsx_vdp2add_w_h(sum0_l, src3_l, filter3);
+        tmp1_r = sum0_r;
+        tmp1_l = sum0_l;
+
+        filter0 = __lsx_vldrepl_w(filter_ptr0, 16);
+        filter1 = __lsx_vldrepl_w(filter_ptr0, 20);
+        filter2 = __lsx_vldrepl_w(filter_ptr0, 24);
+        filter3 = __lsx_vldrepl_w(filter_ptr0, 28);
+        sum0_r = __lsx_vdp2_w_h(src4_r, filter0);
+        sum0_l = __lsx_vdp2_w_h(src4_l, filter0);
+        sum0_r = __lsx_vdp2add_w_h(sum0_r, src5_r, filter1);
+        sum0_l = __lsx_vdp2add_w_h(sum0_l, src5_l, filter1);
+        sum0_r = __lsx_vdp2add_w_h(sum0_r, src6_r, filter2);
+        sum0_l = __lsx_vdp2add_w_h(sum0_l, src6_l, filter2);
+        sum0_r = __lsx_vdp2add_w_h(sum0_r, src7_r, filter3);
+        sum0_l = __lsx_vdp2add_w_h(sum0_l, src7_l, filter3);
+        sum0_r = __lsx_vadd_w(sum0_r, tmp1_r);
+        sum0_l = __lsx_vadd_w(sum0_l, tmp1_l);
+
+        tmp0_r = __lsx_vld(tmp_buf_ptr + i * 8, 0);
+        tmp0_l = __lsx_vld(tmp_buf_ptr + i * 8, 16);
+        tmp1_r = tmp0_r;
+        tmp1_l = tmp0_l;
+        tmp0_r = __lsx_vadd_w(tmp0_r, sum0_r);
+        tmp0_l = __lsx_vadd_w(tmp0_l, sum0_l);
+        sum1_r = __lsx_vreplgr2vr_w(round);
+        tmp0_r = __lsx_vssrarn_h_w(tmp0_r, sum1_r);
+        tmp0_l = __lsx_vssrarn_h_w(tmp0_l, sum1_r);
+        in0    = __lsx_vpackev_d(tmp0_l, tmp0_r);
+        __lsx_vst(in0, (coeffs + i * buf_pitch), 0);
+        tmp1_r = __lsx_vsub_w(tmp1_r, sum0_r);
+        tmp1_l = __lsx_vsub_w(tmp1_l, sum0_l);
+        tmp1_r = __lsx_vssrarn_h_w(tmp1_r, sum1_r);
+        tmp1_l = __lsx_vssrarn_h_w(tmp1_l, sum1_r);
+        in0    = __lsx_vpackev_d(tmp1_l, tmp1_r);
+        __lsx_vst(in0, (coeffs + (31 - i) * buf_pitch), 0);
+
+        filter_ptr0 += 16;
+    }
+}
+
+static void hevc_idct_transpose_32x8_to_8x32(int16_t *coeffs, int16_t *tmp_buf)
+{
+    uint8_t i;
+    __m128i in0, in1, in2, in3, in4, in5, in6, in7;
+
+    for (i = 0; i < 4; i++) {
+        DUP4_ARG2(__lsx_vld, coeffs, 0, coeffs, 64, coeffs, 128,
+                  coeffs, 192, in0, in1, in2, in3);
+        DUP4_ARG2(__lsx_vld, coeffs, 256, coeffs, 320, coeffs, 384,
+                  coeffs, 448, in4, in5, in6, in7);
+        coeffs += 8;
+        LSX_TRANSPOSE8x8_H(in0, in1, in2, in3, in4, in5, in6, in7,
+                           in0, in1, in2, in3, in4, in5, in6, in7);
+        __lsx_vst(in0, tmp_buf, 0);
+        __lsx_vst(in1, tmp_buf, 16);
+        __lsx_vst(in2, tmp_buf, 32);
+        __lsx_vst(in3, tmp_buf, 48);
+        __lsx_vst(in4, tmp_buf, 64);
+        __lsx_vst(in5, tmp_buf, 80);
+        __lsx_vst(in6, tmp_buf, 96);
+        __lsx_vst(in7, tmp_buf, 112);
+        tmp_buf += 64;
+    }
+}
+
+static void hevc_idct_transpose_8x32_to_32x8(int16_t *tmp_buf, int16_t *coeffs)
+{
+    uint8_t i;
+    __m128i in0, in1, in2, in3, in4, in5, in6, in7;
+
+    for (i = 0; i < 4; i++) {
+        DUP4_ARG2(__lsx_vld, tmp_buf, 0, tmp_buf, 16, tmp_buf, 32,
+                  tmp_buf, 48, in0, in1, in2, in3);
+        DUP4_ARG2(__lsx_vld, tmp_buf, 64, tmp_buf, 80, tmp_buf, 96,
+                  tmp_buf, 112, in4, in5, in6, in7);
+        tmp_buf += 64;
+        LSX_TRANSPOSE8x8_H(in0, in1, in2, in3, in4, in5, in6, in7,
+                           in0, in1, in2, in3, in4, in5, in6, in7);
+        __lsx_vst(in0, coeffs, 0);
+        __lsx_vst(in1, coeffs, 64);
+        __lsx_vst(in2, coeffs, 128);
+        __lsx_vst(in3, coeffs, 192);
+        __lsx_vst(in4, coeffs, 256);
+        __lsx_vst(in5, coeffs, 320);
+        __lsx_vst(in6, coeffs, 384);
+        __lsx_vst(in7, coeffs, 448);
+        coeffs += 8;
+    }
+}
+
+void ff_hevc_idct_32x32_lsx(int16_t *coeffs, int col_limit)
+{
+    uint8_t row_cnt, col_cnt;
+    int16_t *src = coeffs;
+    int16_t tmp_buf[8 * 32 + 31];
+    int16_t *tmp_buf_ptr = tmp_buf + 31;
+    uint8_t round;
+    int32_t buf_pitch;
+
+    /* Align pointer to 64 byte boundary */
+    tmp_buf_ptr = (int16_t *)(((uintptr_t) tmp_buf_ptr) & ~(uintptr_t) 63);
+
+    /* column transform */
+    round = 7;
+    buf_pitch = 32;
+    for (col_cnt = 0; col_cnt < 4; col_cnt++) {
+        /* process 8x32 blocks */
+        hevc_idct_8x32_column_lsx((coeffs + col_cnt * 8), buf_pitch, round);
+    }
+
+    /* row transform */
+    round = 12;
+    buf_pitch = 8;
+    for (row_cnt = 0; row_cnt < 4; row_cnt++) {
+        /* process 32x8 blocks */
+        src = (coeffs + 32 * 8 * row_cnt);
+
+        hevc_idct_transpose_32x8_to_8x32(src, tmp_buf_ptr);
+        hevc_idct_8x32_column_lsx(tmp_buf_ptr, buf_pitch, round);
+        hevc_idct_transpose_8x32_to_32x8(tmp_buf_ptr, src);
+    }
+}
diff --git a/libavcodec/loongarch/hevc_lpf_sao_lsx.c b/libavcodec/loongarch/hevc_lpf_sao_lsx.c
new file mode 100644
index 0000000000..fc10e8eda8
--- /dev/null
+++ b/libavcodec/loongarch/hevc_lpf_sao_lsx.c
@@ -0,0 +1,2485 @@
+/*
+ * Copyright (c) 2022 Loongson Technology Corporation Limited
+ * Contributed by Lu Wang <wanglu@loongson.cn>
+ *                Hao Chen <chenhao@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "libavutil/loongarch/loongson_intrinsics.h"
+#include "hevcdsp_lsx.h"
+
+void ff_hevc_loop_filter_luma_h_8_lsx(uint8_t *src, ptrdiff_t stride,
+                                      int32_t beta, int32_t *tc,
+                                      uint8_t *p_is_pcm, uint8_t *q_is_pcm)
+{
+    ptrdiff_t stride_2x = (stride << 1);
+    ptrdiff_t stride_4x = (stride << 2);
+    ptrdiff_t stride_3x = stride_2x + stride;
+    uint8_t *p3 = src - stride_4x;
+    uint8_t *p2 = src - stride_3x;
+    uint8_t *p1 = src - stride_2x;
+    uint8_t *p0 = src - stride;
+    uint8_t *q0 = src;
+    uint8_t *q1 = src + stride;
+    uint8_t *q2 = src + stride_2x;
+    uint8_t *q3 = src + stride_3x;
+    uint8_t flag0, flag1;
+    int32_t dp00, dq00, dp30, dq30, d00, d30, d0030, d0434;
+    int32_t dp04, dq04, dp34, dq34, d04, d34;
+    int32_t tc0, p_is_pcm0, q_is_pcm0, beta30, beta20, tc250;
+    int32_t tc4, p_is_pcm4, q_is_pcm4, tc254, tmp;
+
+    __m128i dst0, dst1, dst2, dst3, dst4, dst5;
+    __m128i cmp0, cmp1, cmp2, cmp3, p_is_pcm_vec, q_is_pcm_vec;
+    __m128i temp0, temp1;
+    __m128i temp2, tc_pos, tc_neg;
+    __m128i diff0, diff1, delta0, delta1, delta2, abs_delta0;
+    __m128i zero = {0};
+    __m128i p3_src, p2_src, p1_src, p0_src, q0_src, q1_src, q2_src, q3_src;
+
+    dp00 = abs(p2[0] - (p1[0] << 1) + p0[0]);
+    dq00 = abs(q2[0] - (q1[0] << 1) + q0[0]);
+    dp30 = abs(p2[3] - (p1[3] << 1) + p0[3]);
+    dq30 = abs(q2[3] - (q1[3] << 1) + q0[3]);
+    d00 = dp00 + dq00;
+    d30 = dp30 + dq30;
+    dp04 = abs(p2[4] - (p1[4] << 1) + p0[4]);
+    dq04 = abs(q2[4] - (q1[4] << 1) + q0[4]);
+    dp34 = abs(p2[7] - (p1[7] << 1) + p0[7]);
+    dq34 = abs(q2[7] - (q1[7] << 1) + q0[7]);
+    d04 = dp04 + dq04;
+    d34 = dp34 + dq34;
+
+    p_is_pcm0 = p_is_pcm[0];
+    p_is_pcm4 = p_is_pcm[1];
+    q_is_pcm0 = q_is_pcm[0];
+    q_is_pcm4 = q_is_pcm[1];
+
+    DUP2_ARG1(__lsx_vreplgr2vr_d, p_is_pcm0, p_is_pcm4, cmp0, cmp1);
+    p_is_pcm_vec = __lsx_vpackev_d(cmp1, cmp0);
+    p_is_pcm_vec = __lsx_vseqi_d(p_is_pcm_vec, 0);
+    d0030 = (d00 + d30) >= beta;
+    d0434 = (d04 + d34) >= beta;
+    DUP2_ARG1(__lsx_vreplgr2vr_w, d0030, d0434, cmp0, cmp1);
+    cmp3 = __lsx_vpackev_w(cmp1, cmp0);
+    cmp3 = __lsx_vseqi_w(cmp3, 0);
+
+    if ((!p_is_pcm0 || !p_is_pcm4 || !q_is_pcm0 || !q_is_pcm4) &&
+        (!d0030 || !d0434)) {
+        DUP4_ARG2(__lsx_vld, p3, 0, p2, 0, p1, 0, p0, 0,
+                  p3_src, p2_src, p1_src, p0_src);
+        DUP2_ARG1(__lsx_vreplgr2vr_d, q_is_pcm0, q_is_pcm4, cmp0, cmp1);
+        q_is_pcm_vec = __lsx_vpackev_d(cmp1, cmp0);
+        q_is_pcm_vec = __lsx_vseqi_d(q_is_pcm_vec, 0);
+
+        tc0 = tc[0];
+        beta30 = beta >> 3;
+        beta20 = beta >> 2;
+        tc250 = (((tc0 << 2) + tc0 + 1) >> 1);
+        tc4 = tc[1];
+        tc254 = (((tc4 << 2) + tc4 + 1) >> 1);
+
+        DUP2_ARG1(__lsx_vreplgr2vr_h, tc0, tc4, cmp0, cmp1);
+        DUP4_ARG2(__lsx_vilvl_b, zero, p3_src, zero, p2_src, zero, p1_src, zero,
+                  p0_src, p3_src, p2_src, p1_src, p0_src);
+        DUP4_ARG2(__lsx_vld, q0, 0, q1, 0, q2, 0, q3, 0,
+                  q0_src, q1_src, q2_src, q3_src);
+        flag0 = abs(p3[0] - p0[0]) + abs(q3[0] - q0[0]) < beta30 &&
+                abs(p0[0] - q0[0]) < tc250;
+        flag0 = flag0 && (abs(p3[3] - p0[3]) + abs(q3[3] - q0[3]) < beta30 &&
+                abs(p0[3] - q0[3]) < tc250 && (d00 << 1) < beta20 &&
+                (d30 << 1) < beta20);
+        tc_pos = __lsx_vpackev_d(cmp1, cmp0);
+        DUP4_ARG2(__lsx_vilvl_b, zero, q0_src, zero, q1_src, zero, q2_src,
+                  zero, q3_src, q0_src, q1_src, q2_src, q3_src);
+
+        flag1 = abs(p3[4] - p0[4]) + abs(q3[4] - q0[4]) < beta30 &&
+                abs(p0[4] - q0[4]) < tc254;
+        flag1 = flag1 && (abs(p3[7] - p0[7]) + abs(q3[7] - q0[7]) < beta30 &&
+                abs(p0[7] - q0[7]) < tc254 && (d04 << 1) < beta20 &&
+                (d34 << 1) < beta20);
+        DUP2_ARG1(__lsx_vreplgr2vr_w, flag0, flag1, cmp0, cmp1);
+        cmp2 = __lsx_vpackev_w(cmp1, cmp0);
+        cmp2 = __lsx_vseqi_w(cmp2, 0);
+
+        if (flag0 && flag1) { /* strong only */
+            /* strong filter */
+            tc_pos = __lsx_vslli_h(tc_pos, 1);
+            tc_neg = __lsx_vneg_h(tc_pos);
+
+            /* p part */
+            DUP2_ARG2(__lsx_vadd_h, p1_src, p0_src, temp0, q0_src,
+                      temp0, temp0);
+            temp1 = __lsx_vadd_h(p3_src, p2_src);
+            temp1 = __lsx_vslli_h(temp1, 1);
+            DUP2_ARG2(__lsx_vadd_h, temp1, p2_src, temp1, temp0, temp1, temp1);
+            temp1 = __lsx_vsrari_h(temp1, 3);
+            temp2 = __lsx_vsub_h(temp1, p2_src);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
+            dst0 = __lsx_vadd_h(temp2, p2_src);
+
+            temp1 = __lsx_vadd_h(temp0, p2_src);
+            temp1 = __lsx_vsrari_h(temp1, 2);
+            temp2 = __lsx_vsub_h(temp1, p1_src);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
+            dst1 = __lsx_vadd_h(temp2, p1_src);
+
+            temp1 = __lsx_vslli_h(temp0, 1);
+            DUP2_ARG2(__lsx_vadd_h, temp1, p2_src, temp1, q1_src,
+                      temp1, temp1);
+            temp1 = __lsx_vsrari_h(temp1, 3);
+            temp2 = __lsx_vsub_h(temp1, p0_src);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
+            dst2 = __lsx_vadd_h(temp2, p0_src);
+
+            p_is_pcm_vec = __lsx_vnor_v(p_is_pcm_vec, p_is_pcm_vec);
+            DUP2_ARG3(__lsx_vbitsel_v, dst0, p2_src, p_is_pcm_vec, dst1,
+                      p1_src, p_is_pcm_vec, dst0, dst1);
+            dst2 = __lsx_vbitsel_v(dst2, p0_src, p_is_pcm_vec);
+
+            /* q part */
+            DUP2_ARG2(__lsx_vadd_h, q1_src, p0_src, temp0, q0_src,
+                      temp0, temp0);
+            temp1 = __lsx_vadd_h(q3_src, q2_src);
+            temp1 = __lsx_vslli_h(temp1, 1);
+            DUP2_ARG2(__lsx_vadd_h, temp1, q2_src, temp1, temp0, temp1, temp1);
+            temp1 = __lsx_vsrari_h(temp1, 3);
+            temp2 = __lsx_vsub_h(temp1, q2_src);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
+            dst5 = __lsx_vadd_h(temp2, q2_src);
+
+            temp1 = __lsx_vadd_h(temp0, q2_src);
+            temp1 = __lsx_vsrari_h(temp1, 2);
+            temp2 = __lsx_vsub_h(temp1, q1_src);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
+            dst4 = __lsx_vadd_h(temp2, q1_src);
+
+            temp0 = __lsx_vslli_h(temp0, 1);
+            DUP2_ARG2(__lsx_vadd_h, temp0, p1_src, temp1, q2_src,
+                      temp1, temp1);
+            temp1 = __lsx_vsrari_h(temp1, 3);
+            temp2 = __lsx_vsub_h(temp1, q0_src);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
+            dst3 = __lsx_vadd_h(temp2, q0_src);
+
+            q_is_pcm_vec = __lsx_vnor_v(q_is_pcm_vec, q_is_pcm_vec);
+            DUP2_ARG3(__lsx_vbitsel_v, dst3, q0_src, q_is_pcm_vec, dst4,
+                      q1_src, q_is_pcm_vec, dst3, dst4);
+            dst5 = __lsx_vbitsel_v(dst5, q2_src, q_is_pcm_vec);
+
+            /* pack results to 8 bit */
+            DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, dst0, dst1);
+            dst2 = __lsx_vpickev_b(dst5, dst4);
+
+            /* pack src to 8 bit */
+            DUP2_ARG2(__lsx_vpickev_b, p1_src, p2_src, q0_src, p0_src,
+                      dst3, dst4);
+            dst5 = __lsx_vpickev_b(q2_src, q1_src);
+
+            cmp3 = __lsx_vnor_v(cmp3, cmp3);
+            DUP2_ARG3(__lsx_vbitsel_v, dst0, dst3, cmp3, dst1, dst4, cmp3,
+                      dst0, dst1);
+            dst2 = __lsx_vbitsel_v(dst2, dst5, cmp3);
+
+            __lsx_vstelm_d(dst0, p2, 0, 0);
+            __lsx_vstelm_d(dst0, p2 + stride, 0, 1);
+            __lsx_vstelm_d(dst1, p2 + stride_2x, 0, 0);
+            __lsx_vstelm_d(dst1, p2 + stride_3x, 0, 1);
+            __lsx_vstelm_d(dst2, p2 + stride_4x, 0, 0);
+            __lsx_vstelm_d(dst2, p2 + stride_4x + stride, 0, 1);
+            /* strong filter ends */
+        } else if (flag0 == flag1) { /* weak only */
+            /* weak filter */
+            tc_neg = __lsx_vneg_h(tc_pos);
+            DUP2_ARG2(__lsx_vsub_h, q0_src, p0_src, q1_src, p1_src,
+                      diff0, diff1);
+            DUP2_ARG2(__lsx_vadd_h, __lsx_vslli_h(diff0, 3), diff0,
+                      __lsx_vslli_h(diff1, 1), diff1, diff0, diff1);
+            delta0 = __lsx_vsub_h(diff0, diff1);
+            delta0 = __lsx_vsrari_h(delta0, 4);
+            temp1 = __lsx_vadd_h(__lsx_vslli_h(tc_pos, 3),
+                                 __lsx_vslli_h(tc_pos, 1));
+            abs_delta0 = __lsx_vadda_h(delta0, zero);
+            abs_delta0 = __lsx_vsle_hu(temp1, abs_delta0);
+            abs_delta0 = __lsx_vnor_v(abs_delta0, abs_delta0);
+
+            delta0 = __lsx_vclip_h(delta0, tc_neg, tc_pos);
+            temp2 = __lsx_vadd_h(delta0, p0_src);
+            temp2 = __lsx_vclip255_h(temp2);
+            temp0 = __lsx_vbitsel_v(temp2, p0_src,
+                                    __lsx_vnor_v(p_is_pcm_vec, p_is_pcm_vec));
+            temp2 = __lsx_vsub_h(q0_src, delta0);
+            temp2 = __lsx_vclip255_h(temp2);
+            temp2 = __lsx_vbitsel_v(temp2, q0_src, __lsx_vnor_v(q_is_pcm_vec,
+                                    q_is_pcm_vec));
+            DUP2_ARG2(__lsx_vnor_v, p_is_pcm_vec, p_is_pcm_vec, q_is_pcm_vec,
+                      q_is_pcm_vec, p_is_pcm_vec, q_is_pcm_vec);
+
+            tmp = (beta + (beta >> 1)) >> 3;
+            DUP2_ARG1(__lsx_vreplgr2vr_d, dp00 + dp30 < tmp, dp04 + dp34 < tmp,
+                      cmp0, cmp1);
+            cmp0 = __lsx_vpackev_d(cmp1, cmp0);
+            cmp0 = __lsx_vseqi_d(cmp0, 0);
+            p_is_pcm_vec = __lsx_vor_v(p_is_pcm_vec, cmp0);
+
+            DUP2_ARG1(__lsx_vreplgr2vr_d, dq00 + dq30 < tmp, dq04 + dq34 < tmp,
+                      cmp0, cmp1);
+            cmp0 = __lsx_vpackev_d(cmp1, cmp0);
+            cmp0 = __lsx_vseqi_d(cmp0, 0);
+            q_is_pcm_vec = __lsx_vor_v(q_is_pcm_vec, cmp0);
+            tc_pos = __lsx_vsrai_h(tc_pos, 1);
+            tc_neg = __lsx_vneg_h(tc_pos);
+
+            DUP2_ARG2(__lsx_vavgr_hu, p2_src, p0_src, q0_src, q2_src,
+                      delta1, delta2);
+            DUP2_ARG2(__lsx_vsub_h, delta1, p1_src, delta2, q1_src,
+                      delta1, delta2);
+            delta1 = __lsx_vadd_h(delta1, delta0);
+            delta2 = __lsx_vsub_h(delta2, delta0);
+            DUP2_ARG2(__lsx_vsrai_h, delta1, 1, delta2, 1, delta1, delta2);
+            DUP2_ARG3(__lsx_vclip_h, delta1, tc_neg, tc_pos, delta2,
+                      tc_neg, tc_pos, delta1, delta2);
+            DUP2_ARG2(__lsx_vadd_h, p1_src, delta1, q1_src, delta2,
+                      delta1, delta2);
+            DUP2_ARG1(__lsx_vclip255_h, delta1, delta2, delta1, delta2);
+            DUP2_ARG3(__lsx_vbitsel_v, delta1, p1_src, p_is_pcm_vec, delta2,
+                      q1_src, q_is_pcm_vec, delta1, delta2);
+
+            abs_delta0 = __lsx_vnor_v(abs_delta0, abs_delta0);
+            DUP4_ARG3(__lsx_vbitsel_v, delta1, p1_src, abs_delta0, temp0,
+                      p0_src,  abs_delta0, temp2, q0_src, abs_delta0, delta2,
+                      q1_src, abs_delta0, dst1, dst2, dst3, dst4);
+            /* pack results to 8 bit */
+            DUP2_ARG2(__lsx_vpickev_b, dst2, dst1, dst4, dst3, dst0, dst1);
+            /* pack src to 8 bit */
+            DUP2_ARG2(__lsx_vpickev_b, p0_src, p1_src, q1_src, q0_src,
+                      dst2, dst3);
+            cmp3 = __lsx_vnor_v(cmp3, cmp3);
+            DUP2_ARG3(__lsx_vbitsel_v, dst0, dst2, cmp3, dst1, dst3, cmp3,
+                      dst0, dst1);
+
+            p2 += stride;
+            __lsx_vstelm_d(dst0, p2, 0, 0);
+            __lsx_vstelm_d(dst0, p2 + stride, 0, 1);
+            __lsx_vstelm_d(dst1, p2 + stride_2x, 0, 0);
+            __lsx_vstelm_d(dst1, p2 + stride_3x, 0, 1);
+            /* weak filter ends */
+        } else { /* strong + weak */
+            /* strong filter */
+            tc_pos = __lsx_vslli_h(tc_pos, 1);
+            tc_neg = __lsx_vneg_h(tc_pos);
+
+            /* p part */
+            DUP2_ARG2(__lsx_vadd_h, p1_src, p0_src, temp0, q0_src,
+                      temp0, temp0);
+            temp1 = __lsx_vadd_h(p3_src, p2_src);
+            temp1 = __lsx_vslli_h(temp1, 1);
+            DUP2_ARG2(__lsx_vadd_h, temp1, p2_src, temp1, temp0, temp1, temp1);
+            temp1 = __lsx_vsrari_h(temp1, 3);
+            temp2 = __lsx_vsub_h(temp1, p2_src);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
+            dst0 = __lsx_vadd_h(temp2, p2_src);
+
+            temp1 = __lsx_vadd_h(temp0, p2_src);
+            temp1 = __lsx_vsrari_h(temp1, 2);
+            temp2 = __lsx_vsub_h(temp1, p1_src);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
+            dst1 = __lsx_vadd_h(temp2, p1_src);
+
+            temp1 = __lsx_vslli_h(temp0, 1);
+            DUP2_ARG2(__lsx_vadd_h, temp1, p2_src, temp1, q1_src, temp1, temp1);
+            temp1 = __lsx_vsrari_h(temp1, 3);
+            temp2 = __lsx_vsub_h(temp1, p0_src);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
+            dst2 = __lsx_vadd_h(temp2, p0_src);
+
+            p_is_pcm_vec = __lsx_vnor_v(p_is_pcm_vec, p_is_pcm_vec);
+            DUP2_ARG3(__lsx_vbitsel_v, dst0, p2_src, p_is_pcm_vec, dst1,
+                      p1_src, p_is_pcm_vec, dst0, dst1);
+            dst2 = __lsx_vbitsel_v(dst2, p0_src, p_is_pcm_vec);
+
+            /* q part */
+            DUP2_ARG2(__lsx_vadd_h, q1_src, p0_src, temp0, q0_src,
+                      temp0, temp0);
+            temp1 = __lsx_vadd_h(q3_src, q2_src);
+            temp1 = __lsx_vslli_h(temp1, 1);
+            DUP2_ARG2(__lsx_vadd_h, temp1,  q2_src, temp1, temp0, temp1, temp1);
+            temp1 = __lsx_vsrari_h(temp1, 3);
+            temp2 = __lsx_vsub_h(temp1, q2_src);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
+            dst5 = __lsx_vadd_h(temp2, q2_src);
+
+            temp1 = __lsx_vadd_h(temp0, q2_src);
+            temp1 = __lsx_vsrari_h(temp1, 2);
+            temp2 = __lsx_vsub_h(temp1, q1_src);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
+            dst4 = __lsx_vadd_h(temp2, q1_src);
+
+            temp1 = __lsx_vslli_h(temp0, 1);
+            DUP2_ARG2(__lsx_vadd_h, temp1, p1_src, temp1, q2_src, temp1, temp1);
+            temp1 = __lsx_vsrari_h(temp1, 3);
+            temp2 = __lsx_vsub_h(temp1, q0_src);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
+            dst3 = __lsx_vadd_h(temp2, q0_src);
+
+            q_is_pcm_vec = __lsx_vnor_v(q_is_pcm_vec, q_is_pcm_vec);
+            DUP2_ARG3(__lsx_vbitsel_v, dst3, q0_src, q_is_pcm_vec, dst4,
+                      q1_src, q_is_pcm_vec, dst3, dst4);
+            dst5 = __lsx_vbitsel_v(dst5, q2_src, q_is_pcm_vec);
+
+            /* pack strong results to 8 bit */
+            DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, dst0, dst1);
+            dst2 = __lsx_vpickev_b(dst5, dst4);
+            /* strong filter ends */
+
+            /* weak filter */
+            tc_pos = __lsx_vsrai_h(tc_pos, 1);
+            tc_neg = __lsx_vneg_h(tc_pos);
+
+            DUP2_ARG2(__lsx_vsub_h, q0_src, p0_src, q1_src, p1_src,
+                      diff0, diff1);
+            DUP2_ARG2(__lsx_vadd_h, __lsx_vslli_h(diff0, 3), diff0,
+                      __lsx_vslli_h(diff1, 1), diff1, diff0, diff1);
+            delta0 = __lsx_vsub_h(diff0, diff1);
+            delta0 = __lsx_vsrari_h(delta0, 4);
+            temp1 = __lsx_vadd_h(__lsx_vslli_h(tc_pos, 3),
+                                 __lsx_vslli_h(tc_pos, 1));
+            abs_delta0 = __lsx_vadda_h(delta0, zero);
+            abs_delta0 = __lsx_vsle_hu(temp1, abs_delta0);
+            abs_delta0 = __lsx_vnor_v(abs_delta0, abs_delta0);
+
+            delta0 = __lsx_vclip_h(delta0, tc_neg, tc_pos);
+            temp2 = __lsx_vadd_h(delta0, p0_src);
+            temp2 = __lsx_vclip255_h(temp2);
+            temp0 = __lsx_vbitsel_v(temp2, p0_src, p_is_pcm_vec);
+
+            temp2 = __lsx_vsub_h(q0_src, delta0);
+            temp2 = __lsx_vclip255_h(temp2);
+            temp2 = __lsx_vbitsel_v(temp2, q0_src, q_is_pcm_vec);
+
+            tmp = (beta + (beta >> 1)) >> 3;
+            DUP2_ARG1(__lsx_vreplgr2vr_d, dp00 + dp30 < tmp, dp04 + dp34 < tmp,
+                      cmp0, cmp1);
+            cmp0 = __lsx_vpackev_d(cmp1, cmp0);
+            p_is_pcm_vec = __lsx_vor_v(p_is_pcm_vec, __lsx_vseqi_d(cmp0, 0));
+            DUP2_ARG1(__lsx_vreplgr2vr_d, dq00 + dq30 < tmp, dq04 + dq34 < tmp,
+                      cmp0, cmp1);
+            cmp0 = __lsx_vpackev_d(cmp1, cmp0);
+            q_is_pcm_vec = __lsx_vor_v(q_is_pcm_vec, __lsx_vseqi_d(cmp0, 0));
+
+            tc_pos = __lsx_vsrai_h(tc_pos, 1);
+            tc_neg = __lsx_vneg_h(tc_pos);
+
+            DUP2_ARG2(__lsx_vavgr_hu, p2_src, p0_src, q0_src, q2_src,
+                      delta1, delta2);
+            DUP2_ARG2(__lsx_vsub_h, delta1, p1_src, delta2, q1_src,
+                      delta1, delta2);
+            delta1 = __lsx_vadd_h(delta1, delta0);
+            delta2 = __lsx_vsub_h(delta2, delta0);
+            DUP2_ARG2(__lsx_vsrai_h, delta1, 1, delta2, 1, delta1, delta2);
+            DUP2_ARG3(__lsx_vclip_h, delta1, tc_neg, tc_pos, delta2, tc_neg,
+                      tc_pos, delta1, delta2);
+            DUP2_ARG2(__lsx_vadd_h, p1_src, delta1, q1_src, delta2,
+                      delta1, delta2);
+            DUP2_ARG1(__lsx_vclip255_h, delta1, delta2, delta1, delta2);
+            DUP2_ARG3(__lsx_vbitsel_v, delta1, p1_src, p_is_pcm_vec, delta2,
+                      q1_src, q_is_pcm_vec, delta1, delta2);
+            abs_delta0 = __lsx_vnor_v(abs_delta0, abs_delta0);
+            DUP4_ARG3(__lsx_vbitsel_v, delta1, p1_src, abs_delta0, delta2,
+                      q1_src, abs_delta0, temp0, p0_src, abs_delta0, temp2,
+                      q0_src, abs_delta0, delta1, delta2, temp0, temp2);
+            /* weak filter ends */
+
+            /* pack weak results to 8 bit */
+            DUP2_ARG2(__lsx_vpickev_b, delta1, p2_src, temp2, temp0,
+                      dst3, dst4);
+            dst5 = __lsx_vpickev_b(q2_src, delta2);
+
+            /* select between weak or strong */
+            DUP2_ARG3(__lsx_vbitsel_v, dst0, dst3, cmp2, dst1, dst4, cmp2,
+                      dst0, dst1);
+            dst2 = __lsx_vbitsel_v(dst2, dst5, cmp2);
+
+            /* pack src to 8 bit */
+            DUP2_ARG2(__lsx_vpickev_b, p1_src, p2_src, q0_src, p0_src,
+                      dst3, dst4);
+            dst5 = __lsx_vpickev_b(q2_src, q1_src);
+
+            cmp3 = __lsx_vnor_v(cmp3, cmp3);
+            DUP2_ARG3(__lsx_vbitsel_v, dst0, dst3, cmp3, dst1, dst4, cmp3,
+                      dst0, dst1);
+            dst2 = __lsx_vbitsel_v(dst2, dst5, cmp3);
+
+            __lsx_vstelm_d(dst0, p2, 0, 0);
+            __lsx_vstelm_d(dst0, p2 + stride, 0, 1);
+            __lsx_vstelm_d(dst1, p2 + stride_2x, 0, 0);
+            __lsx_vstelm_d(dst1, p2 + stride_3x, 0, 1);
+            __lsx_vstelm_d(dst2, p2 + stride_4x, 0, 0);
+            __lsx_vstelm_d(dst2, p2 + stride_4x + stride, 0, 1);
+        }
+    }
+}
+
+void ff_hevc_loop_filter_luma_v_8_lsx(uint8_t *src, ptrdiff_t stride,
+                                      int32_t beta, int32_t *tc,
+                                      uint8_t *p_is_pcm, uint8_t *q_is_pcm)
+{
+    ptrdiff_t stride_2x = (stride << 1);
+    ptrdiff_t stride_4x = (stride << 2);
+    ptrdiff_t stride_3x = stride_2x + stride;
+    uint8_t *p3 = src;
+    uint8_t *p2 = src + stride_3x;
+    uint8_t *p1 = src + stride_4x;
+    uint8_t *p0 = src + stride_4x + stride_3x;
+    uint8_t flag0, flag1;
+    int32_t dp00, dq00, dp30, dq30, d00, d30;
+    int32_t d0030, d0434;
+    int32_t dp04, dq04, dp34, dq34, d04, d34;
+    int32_t tc0, p_is_pcm0, q_is_pcm0, beta30, beta20, tc250;
+    int32_t tc4, p_is_pcm4, q_is_pcm4, tc254, tmp;
+
+    __m128i dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    __m128i cmp0, cmp1, cmp2, p_is_pcm_vec, q_is_pcm_vec;
+    __m128i cmp3;
+    __m128i temp0, temp1;
+    __m128i temp2;
+    __m128i tc_pos, tc_neg;
+    __m128i diff0, diff1, delta0, delta1, delta2, abs_delta0;
+    __m128i zero = {0};
+    __m128i p3_src, p2_src, p1_src, p0_src, q0_src, q1_src, q2_src, q3_src;
+
+    dp00 = abs(p3[-3] - (p3[-2] << 1) + p3[-1]);
+    dq00 = abs(p3[2] - (p3[1] << 1) + p3[0]);
+    dp30 = abs(p2[-3] - (p2[-2] << 1) + p2[-1]);
+    dq30 = abs(p2[2] - (p2[1] << 1) + p2[0]);
+    d00 = dp00 + dq00;
+    d30 = dp30 + dq30;
+    p_is_pcm0 = p_is_pcm[0];
+    q_is_pcm0 = q_is_pcm[0];
+
+    dp04 = abs(p1[-3] - (p1[-2] << 1) + p1[-1]);
+    dq04 = abs(p1[2] - (p1[1] << 1) + p1[0]);
+    dp34 = abs(p0[-3] - (p0[-2] << 1) + p0[-1]);
+    dq34 = abs(p0[2] - (p0[1] << 1) + p0[0]);
+    d04 = dp04 + dq04;
+    d34 = dp34 + dq34;
+    p_is_pcm4 = p_is_pcm[1];
+    q_is_pcm4 = q_is_pcm[1];
+
+    DUP2_ARG1(__lsx_vreplgr2vr_d, p_is_pcm0, p_is_pcm4, cmp0, cmp1);
+    p_is_pcm_vec = __lsx_vpackev_d(cmp1, cmp0);
+    p_is_pcm_vec = __lsx_vseqi_d(p_is_pcm_vec, 0);
+
+    d0030 = (d00 + d30) >= beta;
+    d0434 = (d04 + d34) >= beta;
+
+    DUP2_ARG1(__lsx_vreplgr2vr_d, d0030, d0434, cmp0, cmp1);
+    cmp3 = __lsx_vpackev_d(cmp1, cmp0);
+    cmp3 = __lsx_vseqi_d(cmp3, 0);
+
+    if ((!p_is_pcm0 || !p_is_pcm4 || !q_is_pcm0 || !q_is_pcm4) &&
+        (!d0030 || !d0434)) {
+        src -= 4;
+        DUP4_ARG2(__lsx_vld, src, 0, src + stride, 0, src + stride_2x, 0,
+                  src + stride_3x, 0, p3_src, p2_src, p1_src, p0_src);
+        src += stride_4x;
+        DUP4_ARG2(__lsx_vld, src, 0, src + stride, 0, src + stride_2x, 0,
+                  src + stride_3x, 0, q0_src, q1_src, q2_src, q3_src);
+        src -= stride_4x;
+
+        DUP2_ARG1(__lsx_vreplgr2vr_d, q_is_pcm0, q_is_pcm4, cmp0, cmp1);
+        q_is_pcm_vec = __lsx_vpackev_d(cmp1, cmp0);
+        q_is_pcm_vec = __lsx_vseqi_d(q_is_pcm_vec, 0);
+
+        tc0 = tc[0];
+        beta30 = beta >> 3;
+        beta20 = beta >> 2;
+        tc250 = (((tc0 << 2) + tc0 + 1) >> 1);
+        tc4 = tc[1];
+        tc254 = (((tc4 << 2) + tc4 + 1) >> 1);
+        DUP2_ARG1( __lsx_vreplgr2vr_h, tc0 << 1, tc4 << 1, cmp0, cmp1);
+        tc_pos = __lsx_vpackev_d(cmp1, cmp0);
+        LSX_TRANSPOSE8x8_B(p3_src, p2_src, p1_src, p0_src, q0_src, q1_src,
+                           q2_src, q3_src, p3_src, p2_src, p1_src, p0_src,
+                           q0_src, q1_src, q2_src, q3_src);
+
+        flag0 = abs(p3[-4] - p3[-1]) + abs(p3[3] - p3[0]) < beta30 &&
+                abs(p3[-1] - p3[0]) < tc250;
+        flag0 = flag0 && (abs(p2[-4] - p2[-1]) + abs(p2[3] - p2[0]) < beta30 &&
+                abs(p2[-1] - p2[0]) < tc250 && (d00 << 1) < beta20 &&
+                (d30 << 1) < beta20);
+        cmp0 = __lsx_vreplgr2vr_d(flag0);
+        DUP4_ARG2(__lsx_vilvl_b, zero, p3_src, zero, p2_src, zero, p1_src, zero,
+                  p0_src, p3_src, p2_src, p1_src, p0_src);
+
+        flag1 = abs(p1[-4] - p1[-1]) + abs(p1[3] - p1[0]) < beta30 &&
+                abs(p1[-1] - p1[0]) < tc254;
+        flag1 = flag1 && (abs(p0[-4] - p0[-1]) + abs(p0[3] - p0[0]) < beta30 &&
+                abs(p0[-1] - p0[0]) < tc254 && (d04 << 1) < beta20 &&
+                (d34 << 1) < beta20);
+        DUP4_ARG2(__lsx_vilvl_b, zero, q0_src, zero, q1_src, zero, q2_src, zero,
+                  q3_src, q0_src, q1_src, q2_src, q3_src);
+
+        cmp1 = __lsx_vreplgr2vr_d(flag1);
+        cmp2 = __lsx_vpackev_d(cmp1, cmp0);
+        cmp2 = __lsx_vseqi_d(cmp2, 0);
+
+        if (flag0 && flag1) { /* strong only */
+            /* strong filter */
+            tc_neg = __lsx_vneg_h(tc_pos);
+            /* p part */
+            DUP2_ARG2(__lsx_vadd_h, p1_src, p0_src, temp0, q0_src,
+                      temp0, temp0);
+            temp1 = __lsx_vadd_h(p3_src, p2_src);
+            temp1 = __lsx_vslli_h(temp1, 1);
+            DUP2_ARG2(__lsx_vadd_h, temp1, p2_src, temp1, temp0, temp1, temp1);
+            temp1 = __lsx_vsrari_h(temp1, 3);
+            temp2 = __lsx_vsub_h(temp1, p2_src);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
+            dst0 = __lsx_vadd_h(temp2, p2_src);
+
+            temp1 = __lsx_vadd_h(temp0, p2_src);
+            temp1 = __lsx_vsrari_h(temp1, 2);
+            temp2 = __lsx_vsub_h(temp1, p1_src);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
+            dst1 = __lsx_vadd_h(temp2, p1_src);
+
+            temp1 = __lsx_vslli_h(temp0, 1);
+            DUP2_ARG2(__lsx_vadd_h, temp1, p2_src, temp1, q1_src, temp1, temp1);
+            temp1 = __lsx_vsrari_h(temp1, 3);
+            temp2 = __lsx_vsub_h(temp1, p0_src);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
+            dst2 = __lsx_vadd_h(temp2, p0_src);
+
+            p_is_pcm_vec = __lsx_vnor_v(p_is_pcm_vec, p_is_pcm_vec);
+            DUP2_ARG3(__lsx_vbitsel_v, dst0, p2_src, p_is_pcm_vec, dst1, p1_src,
+                      p_is_pcm_vec, dst0, dst1);
+            dst2 = __lsx_vbitsel_v(dst2, p0_src, p_is_pcm_vec);
+
+            /* q part */
+            DUP2_ARG2(__lsx_vadd_h, q1_src, p0_src, temp0, q0_src,
+                      temp0, temp0);
+            temp1 = __lsx_vadd_h(q3_src, q2_src);
+            temp1 = __lsx_vslli_h(temp1, 1);
+            DUP2_ARG2(__lsx_vadd_h, temp1, q2_src, temp1, temp0, temp1, temp1);
+            temp1 = __lsx_vsrari_h(temp1, 3);
+            temp2 = __lsx_vsub_h(temp1, q2_src);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
+            dst5 = __lsx_vadd_h(temp2, q2_src);
+
+            temp1 = __lsx_vadd_h(temp0, q2_src);
+            temp1 = __lsx_vsrari_h(temp1, 2);
+            temp2 = __lsx_vsub_h(temp1, q1_src);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
+            dst4 = __lsx_vadd_h(temp2, q1_src);
+
+            temp1 = __lsx_vslli_h(temp0, 1);
+            DUP2_ARG2(__lsx_vadd_h, temp1, p1_src, temp1, q2_src, temp1, temp1);
+            temp1 = __lsx_vsrari_h(temp1, 3);
+            temp2 = __lsx_vsub_h(temp1, q0_src);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
+            dst3 = __lsx_vadd_h(temp2, q0_src);
+
+            q_is_pcm_vec = __lsx_vnor_v(q_is_pcm_vec, q_is_pcm_vec);
+            DUP2_ARG3(__lsx_vbitsel_v, dst3, q0_src, q_is_pcm_vec, dst4, q1_src,
+                      q_is_pcm_vec, dst3, dst4);
+            dst5 = __lsx_vbitsel_v(dst5, q2_src, q_is_pcm_vec);
+            /* strong filter ends */
+        } else if (flag0 == flag1) { /* weak only */
+            /* weak filter */
+            tc_pos = __lsx_vsrai_h(tc_pos, 1);
+            tc_neg = __lsx_vneg_h(tc_pos);
+
+            DUP2_ARG2(__lsx_vsub_h, q0_src, p0_src, q1_src, p1_src,
+                      diff0, diff1);
+            DUP2_ARG2(__lsx_vadd_h, __lsx_vslli_h(diff0, 3), diff0,
+                      __lsx_vslli_h(diff1, 1), diff1, diff0, diff1);
+            delta0 = __lsx_vsub_h(diff0, diff1);
+            delta0 = __lsx_vsrari_h(delta0, 4);
+            temp1 = __lsx_vadd_h(__lsx_vslli_h(tc_pos, 3),
+                                 __lsx_vslli_h(tc_pos, 1));
+            abs_delta0 = __lsx_vadda_h(delta0, zero);
+            abs_delta0 = __lsx_vsle_hu(temp1, abs_delta0);
+            abs_delta0 = __lsx_vnor_v(abs_delta0, abs_delta0);
+
+            delta0 = __lsx_vclip_h(delta0, tc_neg, tc_pos);
+            temp2 = __lsx_vadd_h(delta0, p0_src);
+            temp2 = __lsx_vclip255_h(temp2);
+            p_is_pcm_vec = __lsx_vnor_v(p_is_pcm_vec, p_is_pcm_vec);
+            temp0 = __lsx_vbitsel_v(temp2, p0_src, p_is_pcm_vec);
+
+            temp2 = __lsx_vsub_h(q0_src, delta0);
+            temp2 = __lsx_vclip255_h(temp2);
+            q_is_pcm_vec = __lsx_vnor_v(q_is_pcm_vec, q_is_pcm_vec);
+            temp2 = __lsx_vbitsel_v(temp2, q0_src, q_is_pcm_vec);
+
+            tmp = ((beta + (beta >> 1)) >> 3);
+            DUP2_ARG1(__lsx_vreplgr2vr_d, !p_is_pcm0 && ((dp00 + dp30) < tmp),
+                      !p_is_pcm4 && ((dp04 + dp34) < tmp), cmp0, cmp1);
+            p_is_pcm_vec = __lsx_vpackev_d(cmp1, cmp0);
+            p_is_pcm_vec = __lsx_vseqi_d(p_is_pcm_vec, 0);
+
+            DUP2_ARG1(__lsx_vreplgr2vr_h, (!q_is_pcm0) && (dq00 + dq30 < tmp),
+                      (!q_is_pcm4) && (dq04 + dq34 < tmp), cmp0, cmp1);
+            q_is_pcm_vec = __lsx_vpackev_d(cmp1, cmp0);
+            q_is_pcm_vec = __lsx_vseqi_d(q_is_pcm_vec, 0);
+            tc_pos = __lsx_vsrai_h(tc_pos, 1);
+            tc_neg = __lsx_vneg_h(tc_pos);
+
+            DUP2_ARG2(__lsx_vavgr_hu, p2_src, p0_src, q0_src, q2_src,
+                      delta1, delta2);
+            DUP2_ARG2(__lsx_vsub_h, delta1, p1_src, delta2, q1_src,
+                      delta1, delta2);
+            delta1 = __lsx_vadd_h(delta1, delta0);
+            delta2 = __lsx_vsub_h(delta2, delta0);
+            DUP2_ARG2(__lsx_vsrai_h, delta1, 1, delta2, 1, delta1, delta2);
+            DUP2_ARG3(__lsx_vclip_h, delta1, tc_neg, tc_pos, delta2, tc_neg,
+                      tc_pos, delta1, delta2);
+            DUP2_ARG2(__lsx_vadd_h, p1_src, delta1, q1_src, delta2,
+                      delta1, delta2);
+            DUP2_ARG1(__lsx_vclip255_h, delta1, delta2, delta1, delta2);
+            DUP2_ARG3(__lsx_vbitsel_v, delta1, p1_src, p_is_pcm_vec, delta2,
+                      q1_src, q_is_pcm_vec, delta1, delta2);
+
+            abs_delta0 = __lsx_vnor_v(abs_delta0, abs_delta0);
+            DUP4_ARG3(__lsx_vbitsel_v, delta1, p1_src, abs_delta0, temp0,
+                      p0_src, abs_delta0, temp2, q0_src, abs_delta0, delta2,
+                      q1_src, abs_delta0, dst0, dst1, dst2, dst3);
+            /* weak filter ends */
+
+            cmp3 = __lsx_vnor_v(cmp3, cmp3);
+            DUP4_ARG3(__lsx_vbitsel_v, dst0, p1_src, cmp3, dst1, p0_src,
+                      cmp3, dst2, q0_src, cmp3, dst3, q1_src, cmp3,
+                      dst0, dst1, dst2, dst3);
+            DUP2_ARG2(__lsx_vpickev_b, dst2, dst0, dst3, dst1, dst0, dst1);
+
+            /* transpose */
+            dst4 = __lsx_vilvl_b(dst1, dst0);
+            dst5 = __lsx_vilvh_b(dst1, dst0);
+            dst0 = __lsx_vilvl_h(dst5, dst4);
+            dst1 = __lsx_vilvh_h(dst5, dst4);
+
+            src += 2;
+            __lsx_vstelm_w(dst0, src, 0, 0);
+            __lsx_vstelm_w(dst0, src + stride, 0, 1);
+            __lsx_vstelm_w(dst0, src + stride_2x, 0, 2);
+            __lsx_vstelm_w(dst0, src + stride_3x, 0, 3);
+            src += stride_4x;
+            __lsx_vstelm_w(dst1, src, 0, 0);
+            __lsx_vstelm_w(dst1, src + stride, 0, 1);
+            __lsx_vstelm_w(dst1, src + stride_2x, 0, 2);
+            __lsx_vstelm_w(dst1, src + stride_3x, 0, 3);
+            return;
+        } else { /* strong + weak */
+            /* strong filter */
+            tc_neg = __lsx_vneg_h(tc_pos);
+
+            /* p part */
+            DUP2_ARG2(__lsx_vadd_h, p1_src, p0_src, temp0, q0_src,
+                      temp0, temp0);
+
+            temp1 = __lsx_vadd_h(p3_src, p2_src);
+            temp1 = __lsx_vslli_h(temp1, 1);
+            DUP2_ARG2(__lsx_vadd_h, temp1, p2_src, temp1, temp0, temp1, temp1);
+            temp1 = __lsx_vsrari_h(temp1, 3);
+            temp2 = __lsx_vsub_h(temp1, p2_src);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
+            dst0 = __lsx_vadd_h(temp2, p2_src);
+
+            temp1 = __lsx_vadd_h(temp0, p2_src);
+            temp1 = __lsx_vsrari_h(temp1, 2);
+            temp2 = __lsx_vsub_h(temp1, p1_src);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
+            dst1 = __lsx_vadd_h(temp2, p1_src);
+
+            temp1 = __lsx_vslli_h(temp0, 1);
+            DUP2_ARG2(__lsx_vadd_h, temp1, p2_src, temp1, q1_src, temp1, temp1);
+            temp1 = __lsx_vsrari_h(temp1, 3);
+            temp2 = __lsx_vsub_h(temp1, p0_src);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
+            dst2 = __lsx_vadd_h(temp2, p0_src);
+
+            p_is_pcm_vec = __lsx_vnor_v(p_is_pcm_vec, p_is_pcm_vec);
+            DUP2_ARG3(__lsx_vbitsel_v, dst0, p2_src, p_is_pcm_vec, dst1, p1_src,
+                      p_is_pcm_vec, dst0, dst1);
+            dst2 = __lsx_vbitsel_v(dst2, p0_src, p_is_pcm_vec);
+
+            /* q part */
+            DUP2_ARG2(__lsx_vadd_h, q1_src, p0_src, temp0, q0_src, temp0, temp0);
+            temp1 = __lsx_vadd_h(q3_src, q2_src);
+            temp1 = __lsx_vslli_h(temp1, 1);
+            DUP2_ARG2(__lsx_vadd_h, temp1, q2_src, temp1, temp0, temp1, temp1);
+            temp1 = __lsx_vsrari_h(temp1, 3);
+            temp2 = __lsx_vsub_h(temp1, q2_src);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
+            dst5 = __lsx_vadd_h(temp2, q2_src);
+
+            temp1 = __lsx_vadd_h(temp0, q2_src);
+            temp1 = __lsx_vsrari_h(temp1, 2);
+            temp2 = __lsx_vsub_h(temp1, q1_src);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
+            dst4 = __lsx_vadd_h(temp2, q1_src);
+
+            temp1 = __lsx_vslli_h(temp0, 1);
+            DUP2_ARG2(__lsx_vadd_h, temp1, p1_src, temp1, q2_src, temp1, temp1);
+            temp1 = __lsx_vsrari_h(temp1, 3);
+            temp2 = __lsx_vsub_h(temp1, q0_src);
+            temp2 = __lsx_vclip_h(temp2, tc_neg, tc_pos);
+            dst3 = __lsx_vadd_h(temp2, q0_src);
+
+            q_is_pcm_vec = __lsx_vnor_v(q_is_pcm_vec, q_is_pcm_vec);
+            DUP2_ARG3(__lsx_vbitsel_v, dst3, q0_src, q_is_pcm_vec, dst4, q1_src,
+                      q_is_pcm_vec, dst3, dst4);
+            dst5 = __lsx_vbitsel_v(dst5, q2_src, q_is_pcm_vec);
+            /* strong filter ends */
+
+            /* weak filter */
+            tc_pos = __lsx_vsrai_h(tc_pos, 1);
+            tc_neg = __lsx_vneg_h(tc_pos);
+
+            DUP2_ARG2(__lsx_vsub_h, q0_src, p0_src, q1_src, p1_src,
+                      diff0, diff1);
+            DUP2_ARG2(__lsx_vadd_h, __lsx_vslli_h(diff0, 3), diff0,
+                      __lsx_vslli_h(diff1, 1), diff1, diff0, diff1);
+            delta0 = __lsx_vsub_h(diff0, diff1);
+            delta0 = __lsx_vsrari_h(delta0, 4);
+
+            temp1 = __lsx_vadd_h(__lsx_vslli_h(tc_pos, 3),
+                    __lsx_vslli_h(tc_pos, 1));
+            abs_delta0 = __lsx_vadda_h(delta0, zero);
+            abs_delta0 = __lsx_vsle_hu(temp1, abs_delta0);
+            abs_delta0 = __lsx_vnor_v(abs_delta0, abs_delta0);
+            delta0 = __lsx_vclip_h(delta0, tc_neg, tc_pos);
+            temp2 = __lsx_vadd_h(delta0, p0_src);
+            temp2 = __lsx_vclip255_h(temp2);
+            temp0 = __lsx_vbitsel_v(temp2, p0_src, p_is_pcm_vec);
+            temp2 = __lsx_vsub_h(q0_src, delta0);
+            temp2 = __lsx_vclip255_h(temp2);
+            temp2 = __lsx_vbitsel_v(temp2, q0_src, q_is_pcm_vec);
+
+            tmp = (beta + (beta >> 1)) >> 3;
+            DUP2_ARG1(__lsx_vreplgr2vr_d, !p_is_pcm0 && ((dp00 + dp30) < tmp),
+                      !p_is_pcm4 && ((dp04 + dp34) < tmp), cmp0, cmp1);
+            p_is_pcm_vec = __lsx_vpackev_d(cmp1, cmp0);
+            p_is_pcm_vec = __lsx_vseqi_d(p_is_pcm_vec, 0);
+
+            DUP2_ARG1(__lsx_vreplgr2vr_h, (!q_is_pcm0) && (dq00 + dq30 < tmp),
+                      (!q_is_pcm4) && (dq04 + dq34 < tmp), cmp0, cmp1);
+            q_is_pcm_vec = __lsx_vpackev_d(cmp1, cmp0);
+            q_is_pcm_vec = __lsx_vseqi_d(q_is_pcm_vec, 0);
+            tc_pos = __lsx_vsrai_h(tc_pos, 1);
+            tc_neg = __lsx_vneg_h(tc_pos);
+
+            DUP2_ARG2(__lsx_vavgr_hu, p2_src, p0_src, q0_src, q2_src,
+                      delta1, delta2);
+            DUP2_ARG2(__lsx_vsub_h, delta1, p1_src, delta2, q1_src,
+                      delta1, delta2);
+            delta1 = __lsx_vadd_h(delta1, delta0);
+            delta2 = __lsx_vsub_h(delta2, delta0);
+            DUP2_ARG2(__lsx_vsrai_h, delta1, 1, delta2, 1, delta1, delta2);
+            DUP2_ARG3(__lsx_vclip_h, delta1, tc_neg, tc_pos, delta2, tc_neg,
+                      tc_pos, delta1, delta2);
+            DUP2_ARG2(__lsx_vadd_h, p1_src, delta1, q1_src, delta2,
+                      delta1, delta2);
+            DUP2_ARG1(__lsx_vclip255_h, delta1, delta2, delta1, delta2);
+            DUP2_ARG3(__lsx_vbitsel_v, delta1, p1_src, p_is_pcm_vec, delta2,
+                      q1_src, q_is_pcm_vec, delta1, delta2);
+
+            abs_delta0 = __lsx_vnor_v(abs_delta0, abs_delta0);
+            DUP4_ARG3(__lsx_vbitsel_v, delta1, p1_src, abs_delta0, delta2,
+                      q1_src, abs_delta0, temp0, p0_src, abs_delta0, temp2,
+                      q0_src, abs_delta0, delta1, delta2, temp0, temp2);
+            /* weak filter ends*/
+
+            /* select between weak or strong */
+            DUP4_ARG3(__lsx_vbitsel_v, dst0, p2_src, cmp2, dst1, delta1,
+                      cmp2, dst2, temp0, cmp2, dst3, temp2, cmp2,
+                      dst0, dst1, dst2, dst3);
+            DUP2_ARG3(__lsx_vbitsel_v, dst4, delta2, cmp2, dst5, q2_src, cmp2,
+                      dst4, dst5);
+        }
+
+        cmp3 = __lsx_vnor_v(cmp3, cmp3);
+        DUP4_ARG3(__lsx_vbitsel_v, dst0, p2_src, cmp3, dst1, p1_src, cmp3, dst2,
+                  p0_src, cmp3, dst3, q0_src, cmp3, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vbitsel_v, dst4, q1_src, cmp3, dst5, q2_src, cmp3,
+                  dst4, dst5);
+
+        /* pack results to 8 bit */
+        DUP4_ARG2(__lsx_vpickev_b, dst2, dst0, dst3, dst1, dst4, dst4, dst5,
+                  dst5, dst0, dst1, dst2, dst3);
+
+        /* transpose */
+        DUP2_ARG2(__lsx_vilvl_b, dst1, dst0, dst3, dst2, dst4, dst6);
+        DUP2_ARG2(__lsx_vilvh_b, dst1, dst0, dst3, dst2, dst5, dst7);
+        DUP2_ARG2(__lsx_vilvl_h, dst5, dst4, dst7, dst6, dst0, dst2);
+        DUP2_ARG2(__lsx_vilvh_h, dst5, dst4, dst7, dst6, dst1, dst3);
+
+        src += 1;
+        __lsx_vstelm_w(dst0, src, 0, 0);
+        __lsx_vstelm_h(dst2, src, 4, 0);
+        src += stride;
+        __lsx_vstelm_w(dst0, src, 0, 1);
+        __lsx_vstelm_h(dst2, src, 4, 2);
+        src += stride;
+
+        __lsx_vstelm_w(dst0, src, 0, 2);
+        __lsx_vstelm_h(dst2, src, 4, 4);
+        src += stride;
+        __lsx_vstelm_w(dst0, src, 0, 3);
+        __lsx_vstelm_h(dst2, src, 4, 6);
+        src += stride;
+
+        __lsx_vstelm_w(dst1, src, 0, 0);
+        __lsx_vstelm_h(dst3, src, 4, 0);
+        src += stride;
+        __lsx_vstelm_w(dst1, src, 0, 1);
+        __lsx_vstelm_h(dst3, src, 4, 2);
+        src += stride;
+
+        __lsx_vstelm_w(dst1, src, 0, 2);
+        __lsx_vstelm_h(dst3, src, 4, 4);
+        src += stride;
+        __lsx_vstelm_w(dst1, src, 0, 3);
+        __lsx_vstelm_h(dst3, src, 4, 6);
+    }
+}
+
+void ff_hevc_loop_filter_chroma_h_8_lsx(uint8_t *src, ptrdiff_t stride,
+                                        int32_t *tc, uint8_t *p_is_pcm,
+                                        uint8_t *q_is_pcm)
+{
+    uint8_t *p1_ptr = src - (stride << 1);
+    uint8_t *p0_ptr = src - stride;
+    uint8_t *q0_ptr = src;
+    uint8_t *q1_ptr = src + stride;
+    __m128i cmp0, cmp1, p_is_pcm_vec, q_is_pcm_vec;
+    __m128i p1, p0, q0, q1;
+    __m128i tc_pos, tc_neg;
+    __m128i zero = {0};
+    __m128i temp0, temp1, delta;
+
+    if (!(tc[0] <= 0) || !(tc[1] <= 0)) {
+        DUP2_ARG1(__lsx_vreplgr2vr_h, tc[0], tc[1], cmp0, cmp1);
+        tc_pos = __lsx_vpackev_d(cmp1, cmp0);
+        tc_neg = __lsx_vneg_h(tc_pos);
+        DUP2_ARG1(__lsx_vreplgr2vr_d, p_is_pcm[0], p_is_pcm[1], cmp0, cmp1);
+        p_is_pcm_vec = __lsx_vpackev_d(cmp1, cmp0);
+        p_is_pcm_vec = __lsx_vseqi_d(p_is_pcm_vec, 0);
+
+        DUP2_ARG1(__lsx_vreplgr2vr_d, q_is_pcm[0], q_is_pcm[1], cmp0, cmp1);
+        q_is_pcm_vec = __lsx_vpackev_d(cmp1, cmp0);
+        q_is_pcm_vec = __lsx_vseqi_d(q_is_pcm_vec, 0);
+
+        DUP4_ARG2(__lsx_vld, p1_ptr, 0, p0_ptr, 0, q0_ptr, 0, q1_ptr, 0,
+                  p1, p0, q0, q1);
+        DUP4_ARG2(__lsx_vilvl_b, zero, p1, zero, p0, zero, q0, zero, q1,
+                  p1, p0, q0, q1);
+        DUP2_ARG2(__lsx_vsub_h, q0, p0, p1, q1, temp0, temp1);
+        temp0 = __lsx_vslli_h(temp0, 2);
+        temp0 = __lsx_vadd_h(temp0, temp1);
+        delta = __lsx_vsrari_h(temp0, 3);
+        delta = __lsx_vclip_h(delta, tc_neg, tc_pos);
+        temp0 = __lsx_vadd_h(p0, delta);
+        temp0 = __lsx_vclip255_h(temp0);
+        p_is_pcm_vec = __lsx_vnor_v(p_is_pcm_vec, p_is_pcm_vec);
+        temp0 = __lsx_vbitsel_v(temp0, p0, p_is_pcm_vec);
+
+        temp1 = __lsx_vsub_h(q0, delta);
+        temp1 = __lsx_vclip255_h(temp1);
+        q_is_pcm_vec = __lsx_vnor_v(q_is_pcm_vec, q_is_pcm_vec);
+        temp1 = __lsx_vbitsel_v(temp1, q0, q_is_pcm_vec);
+
+        tc_pos = __lsx_vslei_d(tc_pos, 0);
+        DUP2_ARG3(__lsx_vbitsel_v, temp0, p0, tc_pos, temp1, q0, tc_pos,
+                  temp0, temp1);
+        temp0 = __lsx_vpickev_b(temp1, temp0);
+        __lsx_vstelm_d(temp0, p0_ptr, 0, 0);
+        __lsx_vstelm_d(temp0, p0_ptr + stride, 0, 1);
+    }
+}
+
+void ff_hevc_loop_filter_chroma_v_8_lsx(uint8_t *src, ptrdiff_t stride,
+                                        int32_t *tc, uint8_t *p_is_pcm,
+                                        uint8_t *q_is_pcm)
+{
+    ptrdiff_t stride_2x = (stride << 1);
+    ptrdiff_t stride_4x = (stride << 2);
+    ptrdiff_t stride_3x = stride_2x + stride;
+    __m128i cmp0, cmp1, p_is_pcm_vec, q_is_pcm_vec;
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m128i p1, p0, q0, q1;
+    __m128i tc_pos, tc_neg;
+    __m128i zero = {0};
+    __m128i temp0, temp1, delta;
+
+    if (!(tc[0] <= 0) || !(tc[1] <= 0)) {
+        DUP2_ARG1(__lsx_vreplgr2vr_h, tc[0], tc[1], cmp0, cmp1);
+        tc_pos = __lsx_vpackev_d(cmp1, cmp0);
+        tc_neg = __lsx_vneg_h(tc_pos);
+
+        DUP2_ARG1(__lsx_vreplgr2vr_d, p_is_pcm[0], p_is_pcm[1], cmp0, cmp1);
+        p_is_pcm_vec = __lsx_vpackev_d(cmp1, cmp0);
+        p_is_pcm_vec = __lsx_vseqi_d(p_is_pcm_vec, 0);
+        DUP2_ARG1(__lsx_vreplgr2vr_d, q_is_pcm[0], q_is_pcm[1], cmp0, cmp1);
+        q_is_pcm_vec = __lsx_vpackev_d(cmp1, cmp0);
+        q_is_pcm_vec = __lsx_vseqi_d(q_is_pcm_vec, 0);
+
+        src -= 2;
+        DUP4_ARG2(__lsx_vld, src, 0, src + stride, 0, src + stride_2x, 0,
+                  src + stride_3x, 0, src0, src1, src2, src3);
+        src += stride_4x;
+        DUP4_ARG2(__lsx_vld, src, 0, src + stride, 0, src + stride_2x, 0,
+                  src + stride_3x, 0, src4, src5, src6, src7);
+        src -= stride_4x;
+        LSX_TRANSPOSE8x4_B(src0, src1, src2, src3, src4, src5, src6, src7,
+                           p1, p0, q0, q1);
+        DUP4_ARG2(__lsx_vilvl_b, zero, p1, zero, p0, zero, q0, zero, q1,
+                  p1, p0, q0, q1);
+
+        DUP2_ARG2(__lsx_vsub_h, q0, p0, p1, q1, temp0, temp1);
+        temp0 = __lsx_vslli_h(temp0, 2);
+        temp0 = __lsx_vadd_h(temp0, temp1);
+        delta = __lsx_vsrari_h(temp0, 3);
+        delta = __lsx_vclip_h(delta, tc_neg, tc_pos);
+
+        temp0 = __lsx_vadd_h(p0, delta);
+        temp1 = __lsx_vsub_h(q0, delta);
+        DUP2_ARG1(__lsx_vclip255_h, temp0, temp1, temp0, temp1);
+        DUP2_ARG2(__lsx_vnor_v, p_is_pcm_vec, p_is_pcm_vec, q_is_pcm_vec,
+                  q_is_pcm_vec, p_is_pcm_vec, q_is_pcm_vec);
+        DUP2_ARG3(__lsx_vbitsel_v, temp0, p0, p_is_pcm_vec, temp1, q0,
+                  q_is_pcm_vec, temp0, temp1);
+
+        tc_pos = __lsx_vslei_d(tc_pos, 0);
+        DUP2_ARG3(__lsx_vbitsel_v, temp0, p0, tc_pos, temp1, q0, tc_pos,
+                  temp0, temp1);
+        temp0 = __lsx_vpackev_b(temp1, temp0);
+
+        src += 1;
+        __lsx_vstelm_h(temp0, src, 0, 0);
+        __lsx_vstelm_h(temp0, src + stride, 0, 1);
+        __lsx_vstelm_h(temp0, src + stride_2x, 0, 2);
+        __lsx_vstelm_h(temp0, src + stride_3x, 0, 3);
+        src += stride_4x;
+        __lsx_vstelm_h(temp0, src, 0, 4);
+        __lsx_vstelm_h(temp0, src + stride, 0, 5);
+        __lsx_vstelm_h(temp0, src + stride_2x, 0, 6);
+        __lsx_vstelm_h(temp0, src + stride_3x, 0, 7);
+        src -= stride_4x;
+    }
+}
+
+static void hevc_sao_edge_filter_0degree_4width_lsx(uint8_t *dst,
+                                                    int32_t dst_stride,
+                                                    uint8_t *src,
+                                                    int32_t src_stride,
+                                                    int16_t *sao_offset_val,
+                                                    int32_t height)
+{
+    const int32_t src_stride_2x = (src_stride << 1);
+    const int32_t dst_stride_2x = (dst_stride << 1);
+    __m128i shuf1 = {0x807060504030201, 0x100F0E0D0C0B0A09};
+    __m128i shuf2 = {0x908070605040302, 0x11100F0E0D0C0B0A};
+    __m128i edge_idx = {0x403000201, 0x0};
+    __m128i cmp_minus10, cmp_minus11, diff_minus10, diff_minus11;
+    __m128i sao_offset = __lsx_vld(sao_offset_val, 0);
+    __m128i src_minus10, src_minus11, src_plus10, offset, src0, dst0;
+    __m128i const1 = __lsx_vldi(1);
+    __m128i zero = {0};
+
+    sao_offset = __lsx_vpickev_b(sao_offset, sao_offset);
+    src -= 1;
+
+    /* load in advance */
+    DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src_minus10, src_minus11);
+
+    for (height -= 2; height; height -= 2) {
+        src += src_stride_2x;
+        src_minus10 = __lsx_vpickev_d(src_minus11, src_minus10);
+        src0 = __lsx_vshuf_b(zero, src_minus10, shuf1);
+        src_plus10 = __lsx_vshuf_b(zero, src_minus10, shuf2);
+
+        DUP2_ARG2(__lsx_vseq_b, src0, src_minus10, src0, src_plus10,
+                  cmp_minus10, cmp_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11,
+                  cmp_minus11, diff_minus10, diff_minus11);
+        DUP2_ARG2(__lsx_vsle_bu, src0, src_minus10, src0, src_plus10,
+                  cmp_minus10, cmp_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11,
+                  cmp_minus11, cmp_minus10, cmp_minus11);
+        DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10,
+        diff_minus11, const1, cmp_minus11, diff_minus10, diff_minus11);
+
+        offset = __lsx_vadd_b(diff_minus10, diff_minus11);
+        offset = __lsx_vaddi_bu(offset, 2);
+
+        /* load in advance */
+        DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0,
+                  src_minus10, src_minus11);
+        DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset,
+                  sao_offset, sao_offset, offset, offset, offset);
+        src0 = __lsx_vxori_b(src0, 128);
+        dst0 = __lsx_vsadd_b(src0, offset);
+        dst0 = __lsx_vxori_b(dst0, 128);
+
+        __lsx_vstelm_w(dst0, dst, 0, 0);
+        __lsx_vstelm_w(dst0, dst + dst_stride, 0, 2);
+        dst += dst_stride_2x;
+    }
+
+    src_minus10 = __lsx_vpickev_d(src_minus11, src_minus10);
+    src0 = __lsx_vshuf_b(zero, src_minus10, shuf1);
+    src_plus10 = __lsx_vshuf_b(zero, src_minus10, shuf2);
+
+    DUP2_ARG2(__lsx_vseq_b, src0, src_minus10, src0, src_plus10, cmp_minus10,
+              cmp_minus11);
+    DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+              diff_minus10, diff_minus11);
+    DUP2_ARG2(__lsx_vsle_bu, src0, src_minus10, src0, src_plus10, cmp_minus10,
+              cmp_minus11);
+    DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+              cmp_minus10, cmp_minus11);
+    DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
+              const1, cmp_minus11, diff_minus10, diff_minus11);
+
+    offset = __lsx_vadd_b(diff_minus10, diff_minus11);
+    offset = __lsx_vaddi_bu(offset, 2);
+    DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset, sao_offset,
+              offset, offset, offset);
+    src0 = __lsx_vxori_b(src0, 128);
+    dst0 = __lsx_vsadd_b(src0, offset);
+    dst0 = __lsx_vxori_b(dst0, 128);
+
+    __lsx_vstelm_w(dst0, dst, 0, 0);
+    __lsx_vstelm_w(dst0, dst + dst_stride, 0, 2);
+}
+
+static void hevc_sao_edge_filter_0degree_8width_lsx(uint8_t *dst,
+                                                    int32_t dst_stride,
+                                                    uint8_t *src,
+                                                    int32_t src_stride,
+                                                    int16_t *sao_offset_val,
+                                                    int32_t height)
+{
+    const int32_t src_stride_2x = (src_stride << 1);
+    const int32_t dst_stride_2x = (dst_stride << 1);
+    __m128i shuf1 = {0x807060504030201, 0x100F0E0D0C0B0A09};
+    __m128i shuf2 = {0x908070605040302, 0x11100F0E0D0C0B0A};
+    __m128i edge_idx = {0x403000201, 0x0};
+    __m128i const1 = __lsx_vldi(1);
+    __m128i cmp_minus10, cmp_minus11, diff_minus10, diff_minus11;
+    __m128i src0, src1, dst0, src_minus10, src_minus11, src_plus10, src_plus11;
+    __m128i offset, sao_offset = __lsx_vld(sao_offset_val, 0);
+    __m128i zeros = {0};
+
+    sao_offset = __lsx_vpickev_b(sao_offset, sao_offset);
+    src -= 1;
+
+    /* load in advance */
+    DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src_minus10, src_minus11);
+
+    for (height -= 2; height; height -= 2) {
+        src += src_stride_2x;
+        DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus10, shuf1, zeros,
+                  src_minus11, shuf1, src0, src1);
+        DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus10, shuf2, zeros,
+                  src_minus11, shuf2, src_plus10, src_plus11);
+        DUP2_ARG2(__lsx_vpickev_d, src_minus11, src_minus10, src_plus11,
+                  src_plus10, src_minus10, src_plus10);
+        src0 = __lsx_vpickev_d(src1, src0);
+
+        DUP2_ARG2(__lsx_vseq_b, src0, src_minus10, src0, src_plus10,
+                  cmp_minus10, cmp_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11,
+                  cmp_minus11, diff_minus10, diff_minus11);
+        DUP2_ARG2(__lsx_vsle_bu, src0, src_minus10, src0, src_plus10,
+                  cmp_minus10, cmp_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11,
+                  cmp_minus11, cmp_minus10, cmp_minus11);
+        DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10,
+        diff_minus11, const1, cmp_minus11, diff_minus10, diff_minus11);
+
+        offset = __lsx_vadd_b(diff_minus10, diff_minus11);
+        offset = __lsx_vaddi_bu(offset, 2);
+
+        /* load in advance */
+        DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0,
+                  src_minus10, src_minus11);
+        DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset,
+                  sao_offset, offset, offset, offset);
+        src0 = __lsx_vxori_b(src0, 128);
+        dst0 = __lsx_vsadd_b(src0, offset);
+        dst0 = __lsx_vxori_b(dst0, 128);
+
+        __lsx_vstelm_d(dst0, dst, 0, 0);
+        __lsx_vstelm_d(dst0, dst + dst_stride, 0, 1);
+        dst += dst_stride_2x;
+    }
+
+    DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus10, shuf1, zeros, src_minus11,
+              shuf1, src0, src1);
+    DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus10, shuf2, zeros, src_minus11,
+              shuf2, src_plus10, src_plus11);
+    DUP2_ARG2(__lsx_vpickev_d, src_minus11, src_minus10, src_plus11,
+              src_plus10, src_minus10, src_plus10);
+    src0 =  __lsx_vpickev_d(src1, src0);
+
+    DUP2_ARG2(__lsx_vseq_b, src0, src_minus10, src0, src_plus10, cmp_minus10,
+              cmp_minus11);
+    DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+              diff_minus10, diff_minus11);
+    DUP2_ARG2(__lsx_vsle_bu, src0, src_minus10, src0, src_plus10, cmp_minus10,
+              cmp_minus11);
+    DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+              cmp_minus10, cmp_minus11);
+    DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
+              const1, cmp_minus11, diff_minus10, diff_minus11);
+
+    offset = __lsx_vadd_b(diff_minus10, diff_minus11);
+    offset = __lsx_vaddi_bu(offset, 2);
+    DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset,
+              sao_offset, offset, offset, offset);
+    src0 = __lsx_vxori_b(src0, 128);
+    dst0 = __lsx_vsadd_b(src0, offset);
+    dst0 = __lsx_vxori_b(dst0, 128);
+
+    __lsx_vstelm_d(dst0, dst, 0, 0);
+    __lsx_vstelm_d(dst0, dst + dst_stride, 0, 1);
+}
+
+static void hevc_sao_edge_filter_0degree_16multiple_lsx(uint8_t *dst,
+                                                        int32_t dst_stride,
+                                                        uint8_t *src,
+                                                        int32_t src_stride,
+                                                        int16_t *sao_offset_val,
+                                                        int32_t width,
+                                                        int32_t height)
+{
+    uint8_t *dst_ptr, *src_minus1;
+    int32_t v_cnt;
+    const int32_t src_stride_2x = (src_stride << 1);
+    const int32_t dst_stride_2x = (dst_stride << 1);
+    const int32_t src_stride_4x = (src_stride << 2);
+    const int32_t dst_stride_4x = (dst_stride << 2);
+    const int32_t src_stride_3x = src_stride_2x + src_stride;
+    const int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+
+    __m128i shuf1 = {0x807060504030201, 0x100F0E0D0C0B0A09};
+    __m128i shuf2 = {0x908070605040302, 0x11100F0E0D0C0B0A};
+    __m128i edge_idx = {0x403000201, 0x0};
+    __m128i const1 = __lsx_vldi(1);
+    __m128i sao_offset;
+    __m128i cmp_minus10, cmp_plus10, diff_minus10, diff_plus10, cmp_minus11;
+    __m128i cmp_plus11, diff_minus11, diff_plus11, cmp_minus12, cmp_plus12;
+    __m128i diff_minus12, diff_plus12, cmp_minus13, cmp_plus13, diff_minus13;
+    __m128i diff_plus13;
+    __m128i src10, src11, src12, src13, dst0, dst1, dst2, dst3;
+    __m128i src_minus10, src_minus11, src_minus12, src_minus13;
+    __m128i offset_mask0, offset_mask1, offset_mask2, offset_mask3;
+    __m128i src_zero0, src_zero1, src_zero2, src_zero3;
+    __m128i src_plus10, src_plus11, src_plus12, src_plus13;
+
+    sao_offset = __lsx_vld(sao_offset_val, 0);
+    sao_offset = __lsx_vpickev_b(sao_offset, sao_offset);
+
+    for (; height; height -= 4) {
+        src_minus1 = src - 1;
+        src_minus10 = __lsx_vld(src_minus1, 0);
+        DUP2_ARG2(__lsx_vldx, src_minus1, src_stride, src_minus1,
+                  src_stride_2x, src_minus11, src_minus12);
+        src_minus13 = __lsx_vldx(src_minus1, src_stride_3x);
+
+        for (v_cnt = 0; v_cnt < width; v_cnt += 16) {
+            src_minus1 += 16;
+            dst_ptr = dst + v_cnt;
+            src10 = __lsx_vld(src_minus1, 0);
+            DUP2_ARG2(__lsx_vldx, src_minus1, src_stride, src_minus1,
+                      src_stride_2x, src11, src12);
+            src13 = __lsx_vldx(src_minus1, src_stride_3x);
+            DUP4_ARG3(__lsx_vshuf_b, src10, src_minus10, shuf1, src11,
+                      src_minus11, shuf1, src12, src_minus12, shuf1, src13,
+                      src_minus13, shuf1, src_zero0, src_zero1,
+                      src_zero2, src_zero3);
+            DUP4_ARG3(__lsx_vshuf_b, src10, src_minus10, shuf2, src11,
+                      src_minus11, shuf2, src12, src_minus12, shuf2, src13,
+                      src_minus13, shuf2, src_plus10, src_plus11,
+                      src_plus12, src_plus13);
+            DUP4_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero0,
+                      src_plus10, src_zero1, src_minus11, src_zero1, src_plus11,
+                      cmp_minus10, cmp_plus10, cmp_minus11, cmp_plus11);
+            DUP4_ARG2(__lsx_vseq_b, src_zero2, src_minus12, src_zero2,
+                      src_plus12, src_zero3, src_minus13, src_zero3, src_plus13,
+                      cmp_minus12, cmp_plus12, cmp_minus13, cmp_plus13);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_plus10,
+                      cmp_plus10, cmp_minus11, cmp_minus11, cmp_plus11,
+                      cmp_plus11, diff_minus10, diff_plus10, diff_minus11,
+                      diff_plus11);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus12, cmp_minus12, cmp_plus12,
+                      cmp_plus12, cmp_minus13, cmp_minus13, cmp_plus13,
+                      cmp_plus13, diff_minus12, diff_plus12, diff_minus13,
+                      diff_plus13);
+            DUP4_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero0,
+                      src_plus10, src_zero1, src_minus11, src_zero1, src_plus11,
+                      cmp_minus10, cmp_plus10, cmp_minus11, cmp_plus11);
+            DUP4_ARG2(__lsx_vsle_bu, src_zero2, src_minus12, src_zero2,
+                      src_plus12, src_zero3, src_minus13, src_zero3, src_plus13,
+                      cmp_minus12, cmp_plus12, cmp_minus13, cmp_plus13);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_plus10,
+                      cmp_plus10, cmp_minus11, cmp_minus11, cmp_plus11,
+                      cmp_plus11, cmp_minus10, cmp_plus10, cmp_minus11,
+                      cmp_plus11);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus12, cmp_minus12, cmp_plus12,
+                      cmp_plus12, cmp_minus13, cmp_minus13, cmp_plus13,
+                      cmp_plus13, cmp_minus12, cmp_plus12, cmp_minus13,
+                      cmp_plus13);
+            DUP4_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10,
+                      diff_plus10, const1, cmp_plus10, diff_minus11, const1,
+                      cmp_minus11, diff_plus11, const1, cmp_plus11,
+                      diff_minus10, diff_plus10, diff_minus11, diff_plus11);
+            DUP4_ARG3(__lsx_vbitsel_v, diff_minus12, const1, cmp_minus12,
+                      diff_plus12, const1, cmp_plus12, diff_minus13, const1,
+                      cmp_minus13, diff_plus13, const1, cmp_plus13,
+                      diff_minus12, diff_plus12, diff_minus13, diff_plus13);
+
+            DUP4_ARG2(__lsx_vadd_b, diff_minus10, diff_plus10, diff_minus11,
+                      diff_plus11, diff_minus12, diff_plus12, diff_minus13,
+                      diff_plus13, offset_mask0, offset_mask1, offset_mask2,
+                      offset_mask3);
+            DUP4_ARG2(__lsx_vaddi_bu, offset_mask0, 2, offset_mask1, 2,
+                      offset_mask2, 2, offset_mask3, 2, offset_mask0,
+                      offset_mask1, offset_mask2, offset_mask3);
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask0,
+                      sao_offset, sao_offset, offset_mask0, offset_mask0,
+                      offset_mask0);
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask1,
+                      sao_offset, sao_offset, offset_mask1, offset_mask1,
+                      offset_mask1);
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask2,
+                      sao_offset, sao_offset, offset_mask2, offset_mask2,
+                      offset_mask2);
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask3,
+                      sao_offset, sao_offset, offset_mask3, offset_mask3,
+                      offset_mask3);
+
+            DUP4_ARG2(__lsx_vxori_b, src_zero0, 128, src_zero1, 128,
+                      src_zero2, 128, src_zero3, 128, src_zero0, src_zero1,
+                      src_zero2, src_zero3);
+            DUP4_ARG2(__lsx_vsadd_b, src_zero0, offset_mask0, src_zero1,
+                      offset_mask1, src_zero2, offset_mask2, src_zero3,
+                      offset_mask3, dst0, dst1, dst2, dst3);
+            DUP4_ARG2(__lsx_vxori_b, dst0, 128, dst1, 128, dst2, 128, dst3,
+                      128, dst0, dst1, dst2, dst3);
+
+            src_minus10 = src10;
+            src_minus11 = src11;
+            src_minus12 = src12;
+            src_minus13 = src13;
+
+            __lsx_vst(dst0, dst_ptr, 0);
+            __lsx_vst(dst1, dst_ptr + dst_stride, 0);
+            __lsx_vst(dst2, dst_ptr + dst_stride_2x, 0);
+            __lsx_vst(dst3, dst_ptr + dst_stride_3x, 0);
+        }
+        src += src_stride_4x;
+        dst += dst_stride_4x;
+    }
+}
+
+static void hevc_sao_edge_filter_90degree_4width_lsx(uint8_t *dst,
+                                                     int32_t dst_stride,
+                                                     uint8_t *src,
+                                                     int32_t src_stride,
+                                                     int16_t *sao_offset_val,
+                                                     int32_t height)
+{
+    const int32_t src_stride_2x = (src_stride << 1);
+    const int32_t dst_stride_2x = (dst_stride << 1);
+    __m128i edge_idx = {0x403000201, 0x0};
+    __m128i const1 = __lsx_vldi(1);
+    __m128i dst0;
+    __m128i sao_offset = __lsx_vld(sao_offset_val, 0);
+    __m128i cmp_minus10, diff_minus10, cmp_minus11, diff_minus11;
+    __m128i src_minus10, src_minus11, src10, src11;
+    __m128i src_zero0, src_zero1;
+    __m128i offset;
+    __m128i offset_mask0, offset_mask1;
+
+    sao_offset = __lsx_vpickev_b(sao_offset, sao_offset);
+
+    /* load in advance */
+    DUP4_ARG2(__lsx_vld, src - src_stride, 0, src, 0, src + src_stride, 0,
+              src + src_stride_2x, 0, src_minus10, src_minus11, src10, src11);
+
+    for (height -= 2; height; height -= 2) {
+        src += src_stride_2x;
+        DUP4_ARG2(__lsx_vilvl_b, src10, src_minus10, src_minus11, src_minus11,
+                  src11, src_minus11, src10, src10, src_minus10, src_zero0,
+                  src_minus11, src_zero1);
+        DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
+                  cmp_minus10, cmp_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11,
+                  cmp_minus11, diff_minus10, diff_minus11);
+        DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1,
+                  src_minus11, cmp_minus10, cmp_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11,
+                  cmp_minus11, cmp_minus10, cmp_minus11);
+        DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10,
+                 diff_minus11, const1, cmp_minus11, diff_minus10, diff_minus11);
+
+        DUP2_ARG2(__lsx_vhaddw_hu_bu, diff_minus10, diff_minus10, diff_minus11,
+                  diff_minus11, offset_mask0, offset_mask1);
+        DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2,
+                  offset_mask0, offset_mask1);
+        DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1,
+                  src_zero0, offset, dst0);
+        DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset,
+                  sao_offset, offset, offset, offset);
+
+        dst0 = __lsx_vxori_b(dst0, 128);
+        dst0 = __lsx_vsadd_b(dst0, offset);
+        dst0 = __lsx_vxori_b(dst0, 128);
+        src_minus10 = src10;
+        src_minus11 = src11;
+
+        /* load in advance */
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x,
+                  src10, src11);
+
+        __lsx_vstelm_w(dst0, dst, 0, 0);
+        __lsx_vstelm_w(dst0, dst + dst_stride, 0, 2);
+        dst += dst_stride_2x;
+    }
+
+    DUP4_ARG2(__lsx_vilvl_b, src10, src_minus10, src_minus11, src_minus11,
+              src11,  src_minus11, src10, src10, src_minus10, src_zero0,
+              src_minus11, src_zero1);
+    DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
+              cmp_minus10, cmp_minus11);
+    DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+              diff_minus10, diff_minus11);
+    DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1, src_minus11,
+              cmp_minus10, cmp_minus11);
+    DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+              cmp_minus10, cmp_minus11);
+    DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
+              const1, cmp_minus11, diff_minus10, diff_minus11);
+
+    DUP2_ARG2(__lsx_vhaddw_hu_bu, diff_minus10, diff_minus10, diff_minus11,
+              diff_minus11, offset_mask0, offset_mask1);
+    DUP2_ARG2(__lsx_vaddi_bu, offset_mask0, 2, offset_mask1, 2,
+              offset_mask0, offset_mask1);
+    DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1,
+              src_zero0, offset, dst0);
+    DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset,
+              sao_offset, offset, offset, offset);
+    dst0 = __lsx_vxori_b(dst0, 128);
+    dst0 = __lsx_vsadd_b(dst0, offset);
+    dst0 = __lsx_vxori_b(dst0, 128);
+
+    __lsx_vstelm_w(dst0, dst, 0, 0);
+    __lsx_vstelm_w(dst0, dst + dst_stride, 0, 2);
+}
+
+static void hevc_sao_edge_filter_90degree_8width_lsx(uint8_t *dst,
+                                                     int32_t dst_stride,
+                                                     uint8_t *src,
+                                                     int32_t src_stride,
+                                                     int16_t *sao_offset_val,
+                                                     int32_t height)
+{
+    const int32_t src_stride_2x = (src_stride << 1);
+    const int32_t dst_stride_2x = (dst_stride << 1);
+    __m128i edge_idx = {0x403000201, 0x0};
+    __m128i const1 = __lsx_vldi(1);
+    __m128i offset, sao_offset = __lsx_vld(sao_offset_val, 0);
+    __m128i src_zero0, src_zero1, dst0;
+    __m128i cmp_minus10, diff_minus10, cmp_minus11, diff_minus11;
+    __m128i src_minus10, src_minus11, src10, src11;
+    __m128i offset_mask0, offset_mask1;
+
+    sao_offset = __lsx_vpickev_b(sao_offset, sao_offset);
+
+    /* load in advance */
+    DUP2_ARG2(__lsx_vld, src - src_stride, 0, src, 0, src_minus10, src_minus11);
+    DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src10, src11);
+
+    for (height -= 2; height; height -= 2) {
+        src += src_stride_2x;
+        DUP4_ARG2(__lsx_vilvl_b, src10, src_minus10, src_minus11, src_minus11,
+                  src11, src_minus11, src10, src10, src_minus10, src_zero0,
+                  src_minus11, src_zero1);
+        DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
+                  cmp_minus10, cmp_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11,
+                  cmp_minus11, diff_minus10, diff_minus11);
+        DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1,
+                  src_minus11, cmp_minus10, cmp_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11,
+                  cmp_minus11, cmp_minus10, cmp_minus11);
+        DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10,
+                diff_minus11, const1, cmp_minus11, diff_minus10, diff_minus11);
+
+        DUP2_ARG2(__lsx_vhaddw_hu_bu, diff_minus10, diff_minus10, diff_minus11,
+                  diff_minus11, offset_mask0, offset_mask1);
+        DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2,
+                  offset_mask0, offset_mask1);
+        DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1,
+                  src_zero0, offset, dst0);
+        DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset,
+                  sao_offset, offset, offset, offset);
+
+        dst0 = __lsx_vxori_b(dst0, 128);
+        dst0 = __lsx_vsadd_b(dst0, offset);
+        dst0 = __lsx_vxori_b(dst0, 128);
+        src_minus10 = src10;
+        src_minus11 = src11;
+
+        /* load in advance */
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x,
+                  src10, src11);
+
+        __lsx_vstelm_d(dst0, dst, 0, 0);
+        __lsx_vstelm_d(dst0, dst + dst_stride, 0, 1);
+        dst += dst_stride_2x;
+    }
+
+    DUP4_ARG2(__lsx_vilvl_b, src10, src_minus10, src_minus11, src_minus11,
+              src11, src_minus11, src10, src10, src_minus10, src_zero0,
+              src_minus11, src_zero1);
+    DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
+              cmp_minus10, cmp_minus11);
+    DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+              diff_minus10, diff_minus11);
+    DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1, src_minus11,
+              cmp_minus10, cmp_minus11);
+    DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+              cmp_minus10, cmp_minus11);
+    DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
+              const1, cmp_minus11, diff_minus10, diff_minus11);
+
+    DUP2_ARG2(__lsx_vhaddw_hu_bu, diff_minus10, diff_minus10, diff_minus11,
+              diff_minus11, offset_mask0, offset_mask1);
+    DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2,
+              offset_mask0, offset_mask1);
+    DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1,
+              src_zero0, offset, dst0);
+    DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset,
+              sao_offset, offset, offset, offset);
+    dst0 =  __lsx_vxori_b(dst0, 128);
+    dst0 = __lsx_vsadd_b(dst0, offset);
+    dst0 = __lsx_vxori_b(dst0, 128);
+
+    __lsx_vstelm_d(dst0, dst, 0, 0);
+    __lsx_vstelm_d(dst0, dst + dst_stride, 0, 1);
+}
+
+static void hevc_sao_edge_filter_90degree_16multiple_lsx(uint8_t *dst,
+                                                         int32_t dst_stride,
+                                                         uint8_t *src,
+                                                         int32_t src_stride,
+                                                         int16_t *
+                                                         sao_offset_val,
+                                                         int32_t width,
+                                                         int32_t height)
+{
+    uint8_t *src_orig = src;
+    uint8_t *dst_orig = dst;
+    int32_t h_cnt, v_cnt;
+    const int32_t src_stride_2x = (src_stride << 1);
+    const int32_t dst_stride_2x = (dst_stride << 1);
+    const int32_t src_stride_4x = (src_stride << 2);
+    const int32_t dst_stride_4x = (dst_stride << 2);
+    const int32_t src_stride_3x = src_stride_2x + src_stride;
+    const int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    __m128i edge_idx = {0x403000201, 0x0};
+    __m128i const1 = __lsx_vldi(1);
+    __m128i cmp_minus10, cmp_plus10, diff_minus10, diff_plus10, cmp_minus11;
+    __m128i cmp_plus11, diff_minus11, diff_plus11, cmp_minus12, cmp_plus12;
+    __m128i diff_minus12, diff_plus12, cmp_minus13, cmp_plus13, diff_minus13;
+    __m128i diff_plus13;
+    __m128i src10, src_minus10, dst0, src11, src_minus11, dst1;
+    __m128i src12, dst2, src13, dst3;
+    __m128i offset_mask0, offset_mask1, offset_mask2, offset_mask3, sao_offset;
+
+    sao_offset = __lsx_vld(sao_offset_val, 0);
+    sao_offset = __lsx_vpickev_b(sao_offset, sao_offset);
+
+    for (v_cnt = 0; v_cnt < width; v_cnt += 16) {
+        src = src_orig + v_cnt;
+        dst = dst_orig + v_cnt;
+
+        DUP2_ARG2(__lsx_vld, src - src_stride, 0, src, 0,
+                  src_minus10, src_minus11);
+
+        for (h_cnt = (height >> 2); h_cnt--;) {
+            DUP4_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x,
+                      src, src_stride_3x, src, src_stride_4x,
+                      src10, src11, src12, src13);
+            DUP4_ARG2(__lsx_vseq_b, src_minus11, src_minus10, src_minus11,
+                      src10, src10, src_minus11, src10, src11, cmp_minus10,
+                      cmp_plus10, cmp_minus11, cmp_plus11);
+            DUP4_ARG2(__lsx_vseq_b, src11, src10, src11, src12, src12, src11,
+                      src12, src13, cmp_minus12, cmp_plus12,
+                      cmp_minus13, cmp_plus13);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_plus10,
+                      cmp_plus10, cmp_minus11, cmp_minus11, cmp_plus11,
+                      cmp_plus11, diff_minus10, diff_plus10, diff_minus11,
+                      diff_plus11);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus12, cmp_minus12, cmp_plus12,
+                      cmp_plus12, cmp_minus13, cmp_minus13, cmp_plus13,
+                      cmp_plus13, diff_minus12, diff_plus12, diff_minus13,
+                      diff_plus13);
+            DUP4_ARG2(__lsx_vsle_bu, src_minus11, src_minus10, src_minus11,
+                      src10, src10, src_minus11, src10, src11, cmp_minus10,
+                      cmp_plus10, cmp_minus11, cmp_plus11);
+            DUP4_ARG2(__lsx_vsle_bu, src11, src10, src11, src12, src12, src11,
+                      src12, src13, cmp_minus12, cmp_plus12, cmp_minus13,
+                      cmp_plus13);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_plus10,
+                      cmp_plus10, cmp_minus11, cmp_minus11, cmp_plus11,
+                      cmp_plus11, cmp_minus10, cmp_plus10, cmp_minus11,
+                      cmp_plus11);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus12, cmp_minus12, cmp_plus12,
+                      cmp_plus12, cmp_minus13, cmp_minus13, cmp_plus13,
+                      cmp_plus13, cmp_minus12, cmp_plus12, cmp_minus13,
+                      cmp_plus13);
+            DUP4_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10,
+                      diff_plus10, const1, cmp_plus10, diff_minus11, const1,
+                      cmp_minus11, diff_plus11, const1, cmp_plus11,
+                      diff_minus10, diff_plus10, diff_minus11, diff_plus11);
+            DUP4_ARG3(__lsx_vbitsel_v, diff_minus12, const1, cmp_minus12,
+                      diff_plus12, const1, cmp_plus12, diff_minus13, const1,
+                      cmp_minus13, diff_plus13, const1, cmp_plus13,
+                      diff_minus12, diff_plus12, diff_minus13, diff_plus13);
+
+            DUP4_ARG2(__lsx_vadd_b, diff_minus10, diff_plus10, diff_minus11,
+                      diff_plus11, diff_minus12, diff_plus12, diff_minus13,
+                      diff_plus13, offset_mask0, offset_mask1, offset_mask2,
+                      offset_mask3);
+            DUP4_ARG2(__lsx_vaddi_bu, offset_mask0, 2, offset_mask1, 2,
+                      offset_mask2, 2, offset_mask3, 2, offset_mask0,
+                      offset_mask1, offset_mask2, offset_mask3);
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask0,
+                      sao_offset, sao_offset, offset_mask0,\
+                      offset_mask0, offset_mask0);
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask1,
+                      sao_offset, sao_offset, offset_mask1, offset_mask1,
+                      offset_mask1);
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask2,
+                      sao_offset, sao_offset, offset_mask2, offset_mask2,
+                      offset_mask2);
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask3,
+                      sao_offset, sao_offset, offset_mask3, offset_mask3,
+                      offset_mask3);
+
+            src_minus10 = src12;
+            DUP4_ARG2(__lsx_vxori_b, src_minus11, 128, src10, 128, src11, 128,
+                      src12, 128, src_minus11, src10, src11, src12);
+            DUP4_ARG2(__lsx_vsadd_b, src_minus11, offset_mask0, src10,
+                      offset_mask1, src11, offset_mask2, src12,
+                      offset_mask3, dst0, dst1, dst2, dst3);
+            DUP4_ARG2(__lsx_vxori_b, dst0, 128, dst1, 128, dst2, 128, dst3,
+                      128, dst0, dst1, dst2, dst3);
+            src_minus11 = src13;
+
+            __lsx_vst(dst0, dst, 0);
+            __lsx_vstx(dst1, dst, dst_stride);
+            __lsx_vstx(dst2, dst, dst_stride_2x);
+            __lsx_vstx(dst3, dst, dst_stride_3x);
+            src += src_stride_4x;
+            dst += dst_stride_4x;
+        }
+    }
+}
+
+static void hevc_sao_edge_filter_45degree_4width_lsx(uint8_t *dst,
+                                                     int32_t dst_stride,
+                                                     uint8_t *src,
+                                                     int32_t src_stride,
+                                                     int16_t *sao_offset_val,
+                                                     int32_t height)
+{
+    uint8_t *src_orig;
+    const int32_t src_stride_2x = (src_stride << 1);
+    const int32_t dst_stride_2x = (dst_stride << 1);
+    __m128i shuf1 = {0x807060504030201, 0x100F0E0D0C0B0A09};
+    __m128i shuf2 = {0x908070605040302, 0x11100F0E0D0C0B0A};
+    __m128i edge_idx = {0x403000201, 0x0};
+    __m128i const1 = __lsx_vldi(1);
+    __m128i offset, sao_offset = __lsx_vld(sao_offset_val, 0);
+    __m128i cmp_minus10, diff_minus10, src_minus10, cmp_minus11, diff_minus11;
+    __m128i src_minus11, src10, src11;
+    __m128i src_plus0, src_zero0, src_plus1, src_zero1, dst0;
+    __m128i offset_mask0, offset_mask1;
+    __m128i zeros = {0};
+
+    sao_offset = __lsx_vpickev_b(sao_offset, sao_offset);
+    src_orig = src - 1;
+
+    /* load in advance */
+    DUP2_ARG2(__lsx_vld, src_orig - src_stride, 0, src_orig, 0,
+              src_minus10, src_minus11);
+    DUP2_ARG2(__lsx_vldx, src_orig, src_stride, src_orig, src_stride_2x,
+              src10, src11);
+
+    for (height -= 2; height; height -= 2) {
+        src_orig += src_stride_2x;
+
+        DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus11, shuf1, zeros, src10,
+                  shuf1, src_zero0, src_zero1);
+        DUP2_ARG3(__lsx_vshuf_b, zeros, src10, shuf2, zeros, src11, shuf2,
+                  src_plus0, src_plus1);
+
+        DUP2_ARG2(__lsx_vilvl_b, src_plus0, src_minus10, src_plus1,
+                  src_minus11, src_minus10, src_minus11);
+        DUP2_ARG2(__lsx_vilvl_b, src_zero0, src_zero0, src_zero1,
+                  src_zero1, src_zero0, src_zero1);
+        DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1,
+                  src_minus11, cmp_minus10, cmp_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11,
+                  cmp_minus11, diff_minus10, diff_minus11);
+        DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1,
+                  src_minus11, cmp_minus10, cmp_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11,
+                  cmp_minus11, cmp_minus10, cmp_minus11);
+        DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10,
+             diff_minus11, const1, cmp_minus11, diff_minus10, diff_minus11);
+
+        DUP2_ARG2(__lsx_vhaddw_hu_bu, diff_minus10, diff_minus10, diff_minus11,
+                  diff_minus11, offset_mask0, offset_mask1);
+        DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2,
+                  offset_mask0, offset_mask1);
+        DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1,
+                  src_zero0, offset, dst0);
+        DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset,
+                  sao_offset, offset, offset, offset);
+        dst0 = __lsx_vxori_b(dst0, 128);
+        dst0 = __lsx_vsadd_b(dst0, offset);
+        dst0 = __lsx_vxori_b(dst0, 128);
+
+        src_minus10 = src10;
+        src_minus11 = src11;
+
+        /* load in advance */
+        DUP2_ARG2(__lsx_vldx, src_orig, src_stride, src_orig, src_stride_2x,
+                  src10, src11);
+
+        __lsx_vstelm_w(dst0, dst, 0, 0);
+        __lsx_vstelm_w(dst0, dst + dst_stride, 0, 2);
+        dst += dst_stride_2x;
+    }
+
+    DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus11, shuf1, zeros, src10, shuf1,
+              src_zero0, src_zero1);
+    DUP2_ARG3(__lsx_vshuf_b, zeros, src10, shuf2, zeros, src11, shuf2,
+              src_plus0, src_plus1);
+
+    DUP2_ARG2(__lsx_vilvl_b, src_plus0, src_minus10, src_plus1, src_minus11,
+              src_minus10, src_minus11);
+    DUP2_ARG2(__lsx_vilvl_b, src_zero0, src_zero0, src_zero1, src_zero1,
+              src_zero0, src_zero1);
+    DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
+              cmp_minus10, cmp_minus11);
+    DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+              diff_minus10, diff_minus11);
+    DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1, src_minus11,
+              cmp_minus10, cmp_minus11);
+    DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+              cmp_minus10, cmp_minus11);
+    DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
+              const1, cmp_minus11, diff_minus10, diff_minus11);
+
+    DUP2_ARG2(__lsx_vhaddw_hu_bu, diff_minus10, diff_minus10, diff_minus11,
+              diff_minus11, offset_mask0, offset_mask1);
+    DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2, offset_mask0,
+              offset_mask1);
+    DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1,
+              src_zero0, offset, dst0);
+    DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset,
+              sao_offset, offset, offset, offset);
+    dst0 = __lsx_vxori_b(dst0, 128);
+    dst0 = __lsx_vsadd_b(dst0, offset);
+    dst0 = __lsx_vxori_b(dst0, 128);
+
+    __lsx_vstelm_w(dst0, dst, 0, 0);
+    __lsx_vstelm_w(dst0, dst + dst_stride, 0, 2);
+}
+
+static void hevc_sao_edge_filter_45degree_8width_lsx(uint8_t *dst,
+                                                     int32_t dst_stride,
+                                                     uint8_t *src,
+                                                     int32_t src_stride,
+                                                     int16_t *sao_offset_val,
+                                                     int32_t height)
+{
+    uint8_t *src_orig;
+    const int32_t src_stride_2x = (src_stride << 1);
+    const int32_t dst_stride_2x = (dst_stride << 1);
+    __m128i shuf1 = {0x807060504030201, 0x100F0E0D0C0B0A09};
+    __m128i shuf2 = {0x908070605040302, 0x11100F0E0D0C0B0A};
+    __m128i edge_idx = {0x403000201, 0x0};
+    __m128i const1 = __lsx_vldi(1);
+    __m128i offset, sao_offset = __lsx_vld(sao_offset_val, 0);
+    __m128i cmp_minus10, diff_minus10, cmp_minus11, diff_minus11;
+    __m128i src_minus10, src10, src_minus11, src11;
+    __m128i src_zero0, src_plus10, src_zero1, src_plus11, dst0;
+    __m128i offset_mask0, offset_mask1;
+    __m128i zeros = {0};
+
+    sao_offset = __lsx_vpickev_b(sao_offset, sao_offset);
+    src_orig = src - 1;
+
+    /* load in advance */
+    DUP2_ARG2(__lsx_vld, src_orig - src_stride, 0, src_orig, 0, src_minus10,
+              src_minus11);
+    DUP2_ARG2(__lsx_vldx, src_orig, src_stride, src_orig, src_stride_2x,
+              src10, src11);
+
+    for (height -= 2; height; height -= 2) {
+        src_orig += src_stride_2x;
+
+        DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus11, shuf1, zeros, src10,
+                  shuf1, src_zero0, src_zero1);
+        DUP2_ARG3(__lsx_vshuf_b, zeros, src10, shuf2, zeros, src11, shuf2,
+                  src_plus10, src_plus11);
+
+        DUP2_ARG2(__lsx_vilvl_b, src_plus10, src_minus10, src_plus11,
+                  src_minus11, src_minus10, src_minus11);
+        DUP2_ARG2(__lsx_vilvl_b, src_zero0, src_zero0, src_zero1, src_zero1,
+                  src_zero0, src_zero1);
+        DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
+                  cmp_minus10, cmp_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11,
+                  cmp_minus11, diff_minus10, diff_minus11);
+        DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1,
+                  src_minus11, cmp_minus10, cmp_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11,
+                  cmp_minus11, cmp_minus10, cmp_minus11);
+        DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10,
+               diff_minus11, const1, cmp_minus11,  diff_minus10, diff_minus11);
+
+        DUP2_ARG2(__lsx_vhaddw_hu_bu, diff_minus10, diff_minus10, diff_minus11,
+                  diff_minus11, offset_mask0, offset_mask1);
+        DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2,
+                  offset_mask0, offset_mask1);
+        DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1,
+                  src_zero0, offset, dst0);
+        DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset,
+                  sao_offset, offset, offset, offset);
+        dst0 = __lsx_vxori_b(dst0, 128);
+        dst0 = __lsx_vsadd_b(dst0, offset);
+        dst0 = __lsx_vxori_b(dst0, 128);
+
+        src_minus10 = src10;
+        src_minus11 = src11;
+
+        /* load in advance */
+        DUP2_ARG2(__lsx_vldx, src_orig, src_stride, src_orig, src_stride_2x,
+                  src10, src11)
+        __lsx_vstelm_d(dst0, dst, 0, 0);
+        __lsx_vstelm_d(dst0, dst + dst_stride, 0, 1);
+        dst += dst_stride_2x;
+    }
+
+    DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus11, shuf1, zeros, src10, shuf1,
+              src_zero0, src_zero1);
+    DUP2_ARG3(__lsx_vshuf_b, zeros, src10, shuf2, zeros, src11, shuf2,
+              src_plus10, src_plus11);
+    DUP2_ARG2(__lsx_vilvl_b, src_plus10, src_minus10, src_plus11, src_minus11,
+              src_minus10, src_minus11);
+    DUP2_ARG2(__lsx_vilvl_b, src_zero0, src_zero0, src_zero1, src_zero1,
+              src_zero0, src_zero1);
+
+    DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
+              cmp_minus10, cmp_minus11);
+    DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11,
+              cmp_minus11, diff_minus10, diff_minus11);
+    DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1, src_minus11,
+              cmp_minus10, cmp_minus11);
+    DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+              cmp_minus10, cmp_minus11);
+    DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
+              const1, cmp_minus11, diff_minus10, diff_minus11);
+
+    DUP2_ARG2(__lsx_vhaddw_hu_bu, diff_minus10, diff_minus10, diff_minus11,
+              diff_minus11, offset_mask0, offset_mask1);
+    DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2, offset_mask0,
+              offset_mask1);
+    DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1,
+              src_zero0, offset, dst0);
+    DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset,
+              sao_offset, offset, offset, offset);
+    dst0 = __lsx_vxori_b(dst0, 128);
+    dst0 = __lsx_vsadd_b(dst0, offset);
+    dst0 = __lsx_vxori_b(dst0, 128);
+
+    src_minus10 = src10;
+    src_minus11 = src11;
+
+    /* load in advance */
+    DUP2_ARG2(__lsx_vldx, src_orig, src_stride, src_orig, src_stride_2x,
+              src10, src11);
+
+    __lsx_vstelm_d(dst0, dst, 0, 0);
+    __lsx_vstelm_d(dst0, dst + dst_stride, 0, 1);
+}
+
+static void hevc_sao_edge_filter_45degree_16multiple_lsx(uint8_t *dst,
+                                                         int32_t dst_stride,
+                                                         uint8_t *src,
+                                                         int32_t src_stride,
+                                                         int16_t *
+                                                         sao_offset_val,
+                                                         int32_t width,
+                                                         int32_t height)
+{
+    uint8_t *src_orig = src;
+    uint8_t *dst_orig = dst;
+    int32_t v_cnt;
+    const int32_t src_stride_2x = (src_stride << 1);
+    const int32_t dst_stride_2x = (dst_stride << 1);
+    const int32_t src_stride_4x = (src_stride << 2);
+    const int32_t dst_stride_4x = (dst_stride << 2);
+    const int32_t src_stride_3x = src_stride_2x + src_stride;
+    const int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+
+    __m128i shuf1 = {0x807060504030201, 0x100F0E0D0C0B0A09};
+    __m128i shuf2 = {0x908070605040302, 0x11100F0E0D0C0B0A};
+    __m128i edge_idx = {0x403000201, 0x0};
+    __m128i const1 = __lsx_vldi(1);
+    __m128i cmp_minus10, cmp_plus10, diff_minus10, diff_plus10, cmp_minus11;
+    __m128i cmp_plus11, diff_minus11, diff_plus11, cmp_minus12, cmp_plus12;
+    __m128i diff_minus12, diff_plus12, cmp_minus13, cmp_plus13, diff_minus13;
+    __m128i diff_plus13, src_minus14, src_plus13;
+    __m128i offset_mask0, offset_mask1, offset_mask2, offset_mask3;
+    __m128i src10, src_minus10, dst0, src11, src_minus11, dst1;
+    __m128i src12, src_minus12, dst2, src13, src_minus13, dst3;
+    __m128i src_zero0, src_plus10, src_zero1, src_plus11, src_zero2;
+    __m128i src_zero3, sao_offset, src_plus12;
+
+    sao_offset = __lsx_vld(sao_offset_val, 0);
+    sao_offset = __lsx_vpickev_b(sao_offset, sao_offset);
+
+    for (; height; height -= 4) {
+        src_orig = src - 1;
+        dst_orig = dst;
+        src_minus11 = __lsx_vld(src_orig, 0);
+        DUP2_ARG2(__lsx_vldx, src_orig, src_stride, src_orig, src_stride_2x,
+                  src_minus12, src_minus13);
+        src_minus14 = __lsx_vldx(src_orig, src_stride_3x);
+
+        for (v_cnt = 0; v_cnt < width; v_cnt += 16) {
+            src_minus10 = __lsx_vld(src_orig - src_stride, 0);
+            src_orig += 16;
+            src10 = __lsx_vld(src_orig, 0);
+            DUP2_ARG2(__lsx_vldx, src_orig, src_stride, src_orig,
+                      src_stride_2x, src11, src12);
+            src13 = __lsx_vldx(src_orig, src_stride_3x);
+            src_plus13 = __lsx_vld(src + v_cnt + src_stride_4x, 1);
+
+            DUP4_ARG3(__lsx_vshuf_b, src10, src_minus11, shuf1, src11,
+                      src_minus12, shuf1, src12, src_minus13, shuf1,
+                      src13, src_minus14, shuf1, src_zero0, src_zero1,
+                      src_zero2, src_zero3);
+            DUP2_ARG3(__lsx_vshuf_b, src11, src_minus12, shuf2, src12,
+                      src_minus13, shuf2, src_plus10, src_plus11);
+            src_plus12 = __lsx_vshuf_b(src13, src_minus14, shuf2);
+
+            DUP4_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero0,
+                      src_plus10, src_zero1, src_minus11, src_zero1,
+                      src_plus11, cmp_minus10, cmp_plus10,
+                      cmp_minus11, cmp_plus11);
+            DUP4_ARG2(__lsx_vseq_b, src_zero2, src_minus12, src_zero2,
+                      src_plus12, src_zero3, src_minus13, src_zero3,
+                      src_plus13, cmp_minus12, cmp_plus12,
+                      cmp_minus13, cmp_plus13);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_plus10,
+                      cmp_plus10, cmp_minus11, cmp_minus11, cmp_plus11,
+                      cmp_plus11, diff_minus10, diff_plus10, diff_minus11,
+                      diff_plus11);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus12, cmp_minus12, cmp_plus12,
+                      cmp_plus12, cmp_minus13, cmp_minus13, cmp_plus13,
+                      cmp_plus13, diff_minus12, diff_plus12, diff_minus13,
+                      diff_plus13);
+            DUP4_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero0,
+                      src_plus10, src_zero1, src_minus11, src_zero1,
+                      src_plus11, cmp_minus10, cmp_plus10, cmp_minus11,
+                      cmp_plus11);
+            DUP4_ARG2(__lsx_vsle_bu, src_zero2, src_minus12, src_zero2,
+                      src_plus12, src_zero3, src_minus13, src_zero3,
+                      src_plus13, cmp_minus12, cmp_plus12, cmp_minus13,
+                      cmp_plus13);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_plus10,
+                      cmp_plus10, cmp_minus11, cmp_minus11, cmp_plus11,
+                      cmp_plus11, cmp_minus10, cmp_plus10, cmp_minus11,
+                      cmp_plus11);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus12, cmp_minus12, cmp_plus12,
+                      cmp_plus12, cmp_minus13, cmp_minus13, cmp_plus13,
+                      cmp_plus13, cmp_minus12, cmp_plus12, cmp_minus13,
+                      cmp_plus13);
+            DUP4_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10,
+                      diff_plus10, const1, cmp_plus10, diff_minus11, const1,
+                      cmp_minus11, diff_plus11, const1, cmp_plus11,
+                      diff_minus10, diff_plus10, diff_minus11, diff_plus11);
+            DUP4_ARG3(__lsx_vbitsel_v, diff_minus12, const1, cmp_minus12,
+                      diff_plus12, const1, cmp_plus12, diff_minus13, const1,
+                      cmp_minus13, diff_plus13, const1, cmp_plus13,
+                      diff_minus12, diff_plus12, diff_minus13, diff_plus13);
+
+            DUP4_ARG2(__lsx_vadd_b, diff_minus10, diff_plus10, diff_minus11,
+                      diff_plus11, diff_minus12, diff_plus12, diff_minus13,
+                      diff_plus13, offset_mask0, offset_mask1, offset_mask2,
+                      offset_mask3);
+            DUP4_ARG2(__lsx_vaddi_bu, offset_mask0, 2, offset_mask1, 2,
+                      offset_mask2, 2, offset_mask3, 2, offset_mask0,
+                      offset_mask1, offset_mask2, offset_mask3);
+
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask0,
+                      sao_offset, sao_offset, offset_mask0, offset_mask0,
+                      offset_mask0);
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask1,
+                      sao_offset, sao_offset, offset_mask1, offset_mask1,
+                      offset_mask1);
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask2,
+                      sao_offset, sao_offset, offset_mask2, offset_mask2,
+                      offset_mask2);
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask3,
+                      sao_offset, sao_offset, offset_mask3, offset_mask3,
+                      offset_mask3);
+
+            DUP4_ARG2(__lsx_vxori_b, src_zero0, 128, src_zero1, 128, src_zero2,
+                      128, src_zero3, 128, src_zero0, src_zero1, src_zero2,
+                      src_zero3);
+            DUP4_ARG2(__lsx_vsadd_b, src_zero0, offset_mask0, src_zero1,
+                      offset_mask1, src_zero2, offset_mask2, src_zero3,
+                      offset_mask3, dst0, dst1, dst2, dst3);
+            DUP4_ARG2(__lsx_vxori_b, dst0, 128, dst1, 128, dst2, 128, dst3,
+                      128, dst0, dst1, dst2, dst3);
+
+            src_minus11 = src10;
+            src_minus12 = src11;
+            src_minus13 = src12;
+            src_minus14 = src13;
+
+            __lsx_vst(dst0, dst_orig, 0);
+            __lsx_vstx(dst1, dst_orig, dst_stride);
+            __lsx_vstx(dst2, dst_orig, dst_stride_2x);
+            __lsx_vstx(dst3, dst_orig, dst_stride_3x);
+            dst_orig += 16;
+        }
+        src += src_stride_4x;
+        dst += dst_stride_4x;
+    }
+}
+
+static void hevc_sao_edge_filter_135degree_4width_lsx(uint8_t *dst,
+                                                      int32_t dst_stride,
+                                                      uint8_t *src,
+                                                      int32_t src_stride,
+                                                      int16_t *sao_offset_val,
+                                                      int32_t height)
+{
+    uint8_t *src_orig;
+    const int32_t src_stride_2x = (src_stride << 1);
+    const int32_t dst_stride_2x = (dst_stride << 1);
+
+    __m128i shuf1 = {0x807060504030201, 0x100F0E0D0C0B0A09};
+    __m128i shuf2 = {0x908070605040302, 0x11100F0E0D0C0B0A};
+    __m128i edge_idx = {0x403000201, 0x0};
+    __m128i const1 = __lsx_vldi(1);
+    __m128i offset, sao_offset = __lsx_vld(sao_offset_val, 0);
+    __m128i src_zero0, src_zero1, dst0;
+    __m128i cmp_minus10, diff_minus10, cmp_minus11, diff_minus11;
+    __m128i src_minus10, src10, src_minus11, src11;
+    __m128i offset_mask0, offset_mask1;
+    __m128i zeros = {0};
+
+    sao_offset = __lsx_vpickev_b(sao_offset, sao_offset);
+    src_orig = src - 1;
+
+    /* load in advance */
+    DUP2_ARG2(__lsx_vld, src_orig - src_stride, 0, src_orig, 0,
+              src_minus10, src_minus11);
+    DUP2_ARG2(__lsx_vldx, src_orig, src_stride, src_orig, src_stride_2x,
+              src10, src11);
+
+    for (height -= 2; height; height -= 2) {
+        src_orig += src_stride_2x;
+
+        DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus11, shuf1, zeros, src10,
+                  shuf1, src_zero0, src_zero1);
+        DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus10, shuf2, zeros, src_minus11,
+                  shuf2, src_minus10, src_minus11);
+
+        DUP2_ARG2(__lsx_vilvl_b, src10, src_minus10, src11, src_minus11,
+                  src_minus10, src_minus11);
+        DUP2_ARG2(__lsx_vilvl_b, src_zero0, src_zero0, src_zero1, src_zero1,
+                  src_zero0, src_zero1);
+        DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
+                  cmp_minus10, cmp_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11,
+                  cmp_minus11, diff_minus10, diff_minus11);
+        DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1,
+                  src_minus11, cmp_minus10, cmp_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11,
+                  cmp_minus11, cmp_minus10, cmp_minus11);
+        DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10,
+               diff_minus11, const1, cmp_minus11,  diff_minus10, diff_minus11);
+
+        DUP2_ARG2(__lsx_vhaddw_hu_bu, diff_minus10, diff_minus10, diff_minus11,
+                  diff_minus11, offset_mask0, offset_mask1);
+        DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2,
+                  offset_mask0, offset_mask1);
+        DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1,
+                  src_zero0, offset, dst0);
+        DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset,
+                  sao_offset, offset, offset, offset);
+        dst0 = __lsx_vxori_b(dst0, 128);
+        dst0 = __lsx_vsadd_b(dst0, offset);
+        dst0 = __lsx_vxori_b(dst0, 128);
+
+        src_minus10 = src10;
+        src_minus11 = src11;
+
+        /* load in advance */
+        DUP2_ARG2(__lsx_vldx, src_orig, src_stride, src_orig, src_stride_2x,
+                  src10, src11);
+
+        __lsx_vstelm_w(dst0, dst, 0, 0);
+        __lsx_vstelm_w(dst0, dst + dst_stride, 0, 2);
+        dst += dst_stride_2x;
+    }
+
+    DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus11, shuf1, zeros, src10, shuf1,
+              src_zero0, src_zero1);
+    DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus10, shuf2, zeros, src_minus11,
+              shuf2, src_minus10, src_minus11);
+
+    DUP2_ARG2(__lsx_vilvl_b, src10, src_minus10, src11, src_minus11,
+              src_minus10, src_minus11);
+    DUP2_ARG2(__lsx_vilvl_b, src_zero0, src_zero0, src_zero1, src_zero1,
+              src_zero0, src_zero1);
+    DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
+              cmp_minus10, cmp_minus11);
+    DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11,
+              cmp_minus11, diff_minus10, diff_minus11);
+    DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1, src_minus11,
+              cmp_minus10, cmp_minus11);
+    DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+              cmp_minus10, cmp_minus11);
+    DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
+              const1, cmp_minus11, diff_minus10, diff_minus11);
+
+    DUP2_ARG2(__lsx_vhaddw_hu_bu, diff_minus10, diff_minus10, diff_minus11,
+              diff_minus11, offset_mask0, offset_mask1);
+    DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2, offset_mask0,
+              offset_mask1);
+    DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1,
+              src_zero0, offset, dst0);
+    DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset,
+              sao_offset, offset, offset, offset);
+    dst0 = __lsx_vxori_b(dst0, 128);
+    dst0 = __lsx_vsadd_b(dst0, offset);
+    dst0 = __lsx_vxori_b(dst0, 128);
+
+    __lsx_vstelm_w(dst0, dst, 0, 0);
+    __lsx_vstelm_w(dst0, dst + dst_stride, 0, 2);
+    dst += dst_stride_2x;
+}
+
+static void hevc_sao_edge_filter_135degree_8width_lsx(uint8_t *dst,
+                                                      int32_t dst_stride,
+                                                      uint8_t *src,
+                                                      int32_t src_stride,
+                                                      int16_t *sao_offset_val,
+                                                      int32_t height)
+{
+    uint8_t *src_orig;
+    const int32_t src_stride_2x = (src_stride << 1);
+    const int32_t dst_stride_2x = (dst_stride << 1);
+
+    __m128i shuf1 = {0x807060504030201, 0x100F0E0D0C0B0A09};
+    __m128i shuf2 = {0x908070605040302, 0x11100F0E0D0C0B0A};
+    __m128i edge_idx = {0x403000201, 0x0};
+    __m128i const1 = __lsx_vldi(1);
+    __m128i offset, sao_offset = __lsx_vld(sao_offset_val, 0);
+    __m128i cmp_minus10, diff_minus10, cmp_minus11, diff_minus11;
+    __m128i src_minus10, src10, src_minus11, src11;
+    __m128i src_zero0, src_zero1, dst0;
+    __m128i offset_mask0, offset_mask1;
+    __m128i zeros = {0};
+
+    sao_offset = __lsx_vpickev_b(sao_offset, sao_offset);
+    src_orig = src - 1;
+
+    /* load in advance */
+    DUP2_ARG2(__lsx_vld, src_orig - src_stride, 0, src_orig, 0,
+              src_minus10, src_minus11);
+    DUP2_ARG2(__lsx_vldx, src_orig, src_stride, src_orig, src_stride_2x,
+              src10, src11);
+
+    for (height -= 2; height; height -= 2) {
+        src_orig += src_stride_2x;
+
+        DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus11, shuf1, zeros, src10,
+                  shuf1, src_zero0, src_zero1);
+        DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus10, shuf2, zeros, src_minus11,
+                  shuf2, src_minus10, src_minus11);
+
+        DUP2_ARG2(__lsx_vilvl_b, src10, src_minus10, src11, src_minus11,
+                  src_minus10, src_minus11);
+        DUP2_ARG2(__lsx_vilvl_b, src_zero0, src_zero0, src_zero1, src_zero1,
+                  src_zero0, src_zero1);
+        DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
+                  cmp_minus10, cmp_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11,
+                  cmp_minus11, diff_minus10, diff_minus11);
+        DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1,
+                  src_minus11, cmp_minus10, cmp_minus11);
+        DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11,
+                  cmp_minus11, cmp_minus10, cmp_minus11);
+        DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10,
+              diff_minus11, const1, cmp_minus11,  diff_minus10, diff_minus11);
+
+        DUP2_ARG2(__lsx_vhaddw_hu_bu, diff_minus10, diff_minus10, diff_minus11,
+                  diff_minus11, offset_mask0, offset_mask1);
+        DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2,
+                  offset_mask0, offset_mask1);
+        DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1,
+                  src_zero0, offset, dst0);
+        DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset,
+                  sao_offset, offset, offset, offset);
+        dst0 = __lsx_vxori_b(dst0, 128);
+        dst0 = __lsx_vsadd_b(dst0, offset);
+        dst0 = __lsx_vxori_b(dst0, 128);
+
+        src_minus10 = src10;
+        src_minus11 = src11;
+
+        /* load in advance */
+        DUP2_ARG2(__lsx_vldx, src_orig, src_stride, src_orig, src_stride_2x,
+                  src10, src11);
+
+        __lsx_vstelm_d(dst0, dst, 0, 0);
+        __lsx_vstelm_d(dst0, dst + dst_stride, 0, 1);
+        dst += dst_stride_2x;
+    }
+
+    DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus11, shuf1, zeros, src10, shuf1,
+              src_zero0, src_zero1);
+    DUP2_ARG3(__lsx_vshuf_b, zeros, src_minus10, shuf2, zeros, src_minus11,
+              shuf2, src_minus10, src_minus11);
+
+    DUP2_ARG2(__lsx_vilvl_b, src10, src_minus10, src11, src_minus11,
+              src_minus10, src_minus11);
+    DUP2_ARG2(__lsx_vilvl_b, src_zero0, src_zero0, src_zero1, src_zero1,
+              src_zero0, src_zero1);
+    DUP2_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero1, src_minus11,
+              cmp_minus10, cmp_minus11);
+    DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+              diff_minus10, diff_minus11);
+    DUP2_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero1, src_minus11,
+              cmp_minus10, cmp_minus11);
+    DUP2_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_minus11, cmp_minus11,
+              cmp_minus10, cmp_minus11);
+    DUP2_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10, diff_minus11,
+              const1, cmp_minus11, diff_minus10, diff_minus11);
+
+    DUP2_ARG2(__lsx_vhaddw_hu_bu, diff_minus10, diff_minus10, diff_minus11,
+              diff_minus11, offset_mask0, offset_mask1);
+    DUP2_ARG2(__lsx_vaddi_hu, offset_mask0, 2, offset_mask1, 2, offset_mask0,
+              offset_mask1);
+    DUP2_ARG2(__lsx_vpickev_b, offset_mask1, offset_mask0, src_zero1,
+              src_zero0, offset, dst0);
+    DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset, sao_offset,
+              sao_offset, offset, offset, offset);
+    dst0 = __lsx_vxori_b(dst0, 128);
+    dst0 = __lsx_vsadd_b(dst0, offset);
+    dst0 = __lsx_vxori_b(dst0, 128);
+
+    __lsx_vstelm_d(dst0, dst, 0, 0);
+    __lsx_vstelm_d(dst0, dst + dst_stride, 0, 1);
+}
+
+static void hevc_sao_edge_filter_135degree_16multiple_lsx(uint8_t *dst,
+                                                          int32_t dst_stride,
+                                                          uint8_t *src,
+                                                          int32_t src_stride,
+                                                          int16_t *sao_offset_val,
+                                                          int32_t width,
+                                                          int32_t height)
+{
+    uint8_t *src_orig, *dst_orig;
+    int32_t v_cnt;
+    const int32_t src_stride_2x = (src_stride << 1);
+    const int32_t dst_stride_2x = (dst_stride << 1);
+    const int32_t src_stride_4x = (src_stride << 2);
+    const int32_t dst_stride_4x = (dst_stride << 2);
+    const int32_t src_stride_3x = src_stride_2x + src_stride;
+    const int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+
+    __m128i shuf1 = {0x807060504030201, 0x100F0E0D0C0B0A09};
+    __m128i shuf2 = {0x908070605040302, 0x11100F0E0D0C0B0A};
+    __m128i edge_idx = {0x403000201, 0x0};
+    __m128i const1 = __lsx_vldi(1);
+    __m128i dst0, dst1, dst2, dst3;
+    __m128i cmp_minus10, cmp_minus11, cmp_minus12, cmp_minus13, cmp_plus10;
+    __m128i cmp_plus11, cmp_plus12, cmp_plus13, diff_minus10, diff_minus11;
+    __m128i diff_minus12, diff_minus13, diff_plus10, diff_plus11, diff_plus12;
+    __m128i diff_plus13, src10, src11, src12, src13, src_minus10, src_minus11;
+    __m128i src_plus10, src_plus11, src_plus12, src_plus13;
+    __m128i src_minus12, src_minus13, src_zero0, src_zero1, src_zero2, src_zero3;
+    __m128i offset_mask0, offset_mask1, offset_mask2, offset_mask3, sao_offset;
+
+    sao_offset = __lsx_vld(sao_offset_val, 0);
+    sao_offset = __lsx_vpickev_b(sao_offset, sao_offset);
+
+    for (; height; height -= 4) {
+        src_orig = src - 1;
+        dst_orig = dst;
+
+        src_minus11 = __lsx_vld(src_orig, 0);
+        DUP2_ARG2(__lsx_vldx, src_orig, src_stride, src_orig, src_stride_2x,
+                  src_plus10, src_plus11);
+        src_plus12 = __lsx_vldx(src_orig, src_stride_3x);
+
+        for (v_cnt = 0; v_cnt < width; v_cnt += 16) {
+            src_minus10 = __lsx_vld(src_orig - src_stride, 2);
+            src_plus13 = __lsx_vldx(src_orig, src_stride_4x);
+            src_orig += 16;
+            src10 = __lsx_vld(src_orig, 0);
+            DUP2_ARG2(__lsx_vldx, src_orig, src_stride, src_orig, src_stride_2x,
+                      src11, src12);
+            src13 =__lsx_vldx(src_orig, src_stride_3x);
+
+            DUP4_ARG3(__lsx_vshuf_b, src10, src_minus11, shuf1, src11,
+                      src_plus10,  shuf1, src12, src_plus11, shuf1, src13,
+                      src_plus12, shuf1, src_zero0, src_zero1, src_zero2,
+                      src_zero3);
+            src_minus11 = __lsx_vshuf_b(src10, src_minus11, shuf2);
+            DUP2_ARG3(__lsx_vshuf_b, src11, src_plus10, shuf2, src12,
+                      src_plus11, shuf2, src_minus12, src_minus13);
+
+            DUP4_ARG2(__lsx_vseq_b, src_zero0, src_minus10, src_zero0,
+                      src_plus10,  src_zero1, src_minus11, src_zero1,
+                      src_plus11, cmp_minus10, cmp_plus10, cmp_minus11,
+                      cmp_plus11);
+            DUP4_ARG2(__lsx_vseq_b, src_zero2, src_minus12, src_zero2,
+                      src_plus12, src_zero3, src_minus13, src_zero3,
+                      src_plus13, cmp_minus12, cmp_plus12, cmp_minus13,
+                      cmp_plus13);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_plus10,
+                      cmp_plus10, cmp_minus11, cmp_minus11, cmp_plus11,
+                      cmp_plus11, diff_minus10, diff_plus10, diff_minus11,
+                      diff_plus11);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus12, cmp_minus12, cmp_plus12,
+                      cmp_plus12, cmp_minus13, cmp_minus13, cmp_plus13,
+                      cmp_plus13, diff_minus12, diff_plus12, diff_minus13,
+                      diff_plus13);
+            DUP4_ARG2(__lsx_vsle_bu, src_zero0, src_minus10, src_zero0,
+                      src_plus10, src_zero1, src_minus11, src_zero1, src_plus11,
+                      cmp_minus10, cmp_plus10, cmp_minus11, cmp_plus11);
+            DUP4_ARG2(__lsx_vsle_bu, src_zero2, src_minus12, src_zero2,
+                      src_plus12, src_zero3, src_minus13, src_zero3, src_plus13,
+                      cmp_minus12, cmp_plus12, cmp_minus13, cmp_plus13);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus10, cmp_minus10, cmp_plus10,
+                      cmp_plus10, cmp_minus11, cmp_minus11, cmp_plus11,
+                      cmp_plus11, cmp_minus10, cmp_plus10, cmp_minus11,
+                      cmp_plus11);
+            DUP4_ARG2(__lsx_vnor_v, cmp_minus12, cmp_minus12, cmp_plus12,
+                      cmp_plus12, cmp_minus13, cmp_minus13, cmp_plus13,
+                      cmp_plus13, cmp_minus12, cmp_plus12, cmp_minus13,
+                      cmp_plus13);
+            DUP4_ARG3(__lsx_vbitsel_v, diff_minus10, const1, cmp_minus10,
+                      diff_plus10, const1, cmp_plus10, diff_minus11, const1,
+                      cmp_minus11, diff_plus11, const1, cmp_plus11,
+                      diff_minus10, diff_plus10, diff_minus11, diff_plus11);
+            DUP4_ARG3(__lsx_vbitsel_v, diff_minus12, const1, cmp_minus12,
+                      diff_plus12, const1, cmp_plus12, diff_minus13, const1,
+                      cmp_minus13, diff_plus13, const1, cmp_plus13,
+                      diff_minus12, diff_plus12, diff_minus13, diff_plus13);
+
+            DUP4_ARG2(__lsx_vadd_b, diff_minus10, diff_plus10, diff_minus11,
+                      diff_plus11, diff_minus12, diff_plus12, diff_minus13,
+                      diff_plus13, offset_mask0, offset_mask1, offset_mask2,
+                      offset_mask3);
+            DUP4_ARG2(__lsx_vaddi_bu, offset_mask0, 2, offset_mask1, 2,
+                      offset_mask2, 2, offset_mask3, 2, offset_mask0,
+                      offset_mask1, offset_mask2, offset_mask3);
+
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask0,
+                      sao_offset, sao_offset, offset_mask0, offset_mask0,
+                      offset_mask0);
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask1,
+                      sao_offset, sao_offset, offset_mask1, offset_mask1,
+                      offset_mask1);
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask2,
+                      sao_offset, sao_offset, offset_mask2, offset_mask2,
+                      offset_mask2);
+            DUP2_ARG3(__lsx_vshuf_b, edge_idx, edge_idx, offset_mask3,
+                      sao_offset, sao_offset, offset_mask3, offset_mask3,
+                      offset_mask3);
+
+            DUP4_ARG2(__lsx_vxori_b, src_zero0, 128, src_zero1, 128,
+                      src_zero2, 128, src_zero3, 128, src_zero0, src_zero1,
+                      src_zero2, src_zero3);
+            DUP4_ARG2(__lsx_vsadd_b, src_zero0, offset_mask0, src_zero1,
+                      offset_mask1, src_zero2, offset_mask2, src_zero3,
+                      offset_mask3, dst0, dst1, dst2, dst3);
+            DUP4_ARG2(__lsx_vxori_b, dst0, 128, dst1, 128, dst2, 128, dst3,
+                      128, dst0, dst1, dst2, dst3);
+
+            src_minus11 = src10;
+            src_plus10 = src11;
+            src_plus11 = src12;
+            src_plus12 = src13;
+
+            __lsx_vst(dst0, dst_orig, 0);
+            __lsx_vstx(dst1, dst_orig, dst_stride);
+            __lsx_vstx(dst2, dst_orig, dst_stride_2x);
+            __lsx_vstx(dst3, dst_orig, dst_stride_3x);
+            dst_orig += 16;
+        }
+
+        src += src_stride_4x;
+        dst += dst_stride_4x;
+    }
+}
+
+void ff_hevc_sao_edge_filter_8_lsx(uint8_t *dst, uint8_t *src,
+                                   ptrdiff_t stride_dst,
+                                   int16_t *sao_offset_val,
+                                   int eo, int width, int height)
+{
+    ptrdiff_t stride_src = (2 * MAX_PB_SIZE + AV_INPUT_BUFFER_PADDING_SIZE);
+
+    switch (eo) {
+    case 0:
+        if (width >> 4) {
+            hevc_sao_edge_filter_0degree_16multiple_lsx(dst, stride_dst,
+                                                        src, stride_src,
+                                                        sao_offset_val,
+                                                        width - (width & 0x0F),
+                                                        height);
+            dst += width & 0xFFFFFFF0;
+            src += width & 0xFFFFFFF0;
+            width &= 0x0F;
+        }
+
+        if (width >> 3) {
+            hevc_sao_edge_filter_0degree_8width_lsx(dst, stride_dst,
+                                                    src, stride_src,
+                                                    sao_offset_val, height);
+            dst += 8;
+            src += 8;
+            width &= 0x07;
+        }
+
+        if (width) {
+            hevc_sao_edge_filter_0degree_4width_lsx(dst, stride_dst,
+                                                    src, stride_src,
+                                                    sao_offset_val, height);
+        }
+        break;
+
+    case 1:
+        if (width >> 4) {
+            hevc_sao_edge_filter_90degree_16multiple_lsx(dst, stride_dst,
+                                                         src, stride_src,
+                                                         sao_offset_val,
+                                                         width - (width & 0x0F),
+                                                         height);
+            dst += width & 0xFFFFFFF0;
+            src += width & 0xFFFFFFF0;
+            width &= 0x0F;
+        }
+
+        if (width >> 3) {
+            hevc_sao_edge_filter_90degree_8width_lsx(dst, stride_dst,
+                                                     src, stride_src,
+                                                     sao_offset_val, height);
+            dst += 8;
+            src += 8;
+            width &= 0x07;
+        }
+
+        if (width) {
+            hevc_sao_edge_filter_90degree_4width_lsx(dst, stride_dst,
+                                                     src, stride_src,
+                                                     sao_offset_val, height);
+        }
+        break;
+
+    case 2:
+        if (width >> 4) {
+            hevc_sao_edge_filter_45degree_16multiple_lsx(dst, stride_dst,
+                                                         src, stride_src,
+                                                         sao_offset_val,
+                                                         width - (width & 0x0F),
+                                                         height);
+            dst += width & 0xFFFFFFF0;
+            src += width & 0xFFFFFFF0;
+            width &= 0x0F;
+        }
+
+        if (width >> 3) {
+            hevc_sao_edge_filter_45degree_8width_lsx(dst, stride_dst,
+                                                     src, stride_src,
+                                                     sao_offset_val, height);
+            dst += 8;
+            src += 8;
+            width &= 0x07;
+        }
+
+        if (width) {
+            hevc_sao_edge_filter_45degree_4width_lsx(dst, stride_dst,
+                                                     src, stride_src,
+                                                     sao_offset_val, height);
+        }
+        break;
+
+    case 3:
+        if (width >> 4) {
+            hevc_sao_edge_filter_135degree_16multiple_lsx(dst, stride_dst,
+                                                          src, stride_src,
+                                                          sao_offset_val,
+                                                          width - (width & 0x0F),
+                                                          height);
+            dst += width & 0xFFFFFFF0;
+            src += width & 0xFFFFFFF0;
+            width &= 0x0F;
+        }
+
+        if (width >> 3) {
+            hevc_sao_edge_filter_135degree_8width_lsx(dst, stride_dst,
+                                                      src, stride_src,
+                                                      sao_offset_val, height);
+            dst += 8;
+            src += 8;
+            width &= 0x07;
+        }
+
+        if (width) {
+            hevc_sao_edge_filter_135degree_4width_lsx(dst, stride_dst,
+                                                      src, stride_src,
+                                                      sao_offset_val, height);
+        }
+        break;
+    }
+}
diff --git a/libavcodec/loongarch/hevc_mc_bi_lsx.c b/libavcodec/loongarch/hevc_mc_bi_lsx.c
new file mode 100644
index 0000000000..9092fdccb2
--- /dev/null
+++ b/libavcodec/loongarch/hevc_mc_bi_lsx.c
@@ -0,0 +1,2289 @@
+/*
+ * Copyright (c) 2022 Loongson Technology Corporation Limited
+ * Contributed by Lu Wang <wanglu@loongson.cn>
+ *                Hao Chen <chenhao@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "libavutil/loongarch/loongson_intrinsics.h"
+#include "hevcdsp_lsx.h"
+
+static const uint8_t ff_hevc_mask_arr[16 * 2] __attribute__((aligned(0x40))) = {
+    /* 8 width cases */
+    0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8,
+    0, 1, 1, 2, 2, 3, 3, 4, 16, 17, 17, 18, 18, 19, 19, 20
+};
+
+static av_always_inline __m128i
+hevc_bi_rnd_clip(__m128i in0, __m128i vec0, __m128i in1, __m128i vec1)
+{
+    __m128i out;
+
+    vec0 = __lsx_vsadd_h(in0, vec0);
+    vec1 = __lsx_vsadd_h(in1, vec1);
+    out  = __lsx_vssrarni_bu_h(vec1, vec0, 7);
+    return out;
+}
+
+/* hevc_bi_copy: dst = av_clip_uint8((src0 << 6 + src1) >> 7) */
+static
+void hevc_bi_copy_4w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                         int16_t *src1_ptr, int32_t src2_stride,
+                         uint8_t *dst, int32_t dst_stride, int32_t height)
+{
+    int32_t loop_cnt = height >> 3;
+    int32_t res = (height & 0x07) >> 1;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t dst_stride_2x = (dst_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t dst_stride_4x = (dst_stride << 2);
+    int32_t src2_stride_2x = (src2_stride << 1);
+    int32_t src2_stride_4x = (src2_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    int32_t src2_stride_3x = src2_stride_2x + src2_stride;
+    __m128i src0, src1;
+    __m128i zero = __lsx_vldi(0);
+    __m128i in0, in1, in2, in3;
+    __m128i tmp0, tmp1, tmp2, tmp3;
+    __m128i reg0, reg1, reg2, reg3;
+    __m128i dst0, dst1, dst2, dst3;
+
+    for (;loop_cnt--;) {
+        reg0 = __lsx_vldrepl_w(src0_ptr, 0);
+        reg1 = __lsx_vldrepl_w(src0_ptr + src_stride, 0);
+        reg2 = __lsx_vldrepl_w(src0_ptr + src_stride_2x, 0);
+        reg3 = __lsx_vldrepl_w(src0_ptr + src_stride_3x, 0);
+        src0_ptr += src_stride_4x;
+        DUP2_ARG2(__lsx_vilvl_w, reg1, reg0, reg3, reg2, tmp0, tmp1);
+        src0 = __lsx_vilvl_d(tmp1, tmp0);
+        reg0 = __lsx_vldrepl_w(src0_ptr, 0);
+        reg1 = __lsx_vldrepl_w(src0_ptr + src_stride, 0);
+        reg2 = __lsx_vldrepl_w(src0_ptr + src_stride_2x, 0);
+        reg3 = __lsx_vldrepl_w(src0_ptr + src_stride_3x, 0);
+        DUP2_ARG2(__lsx_vilvl_w, reg1, reg0, reg3, reg2, tmp0, tmp1);
+        src1 = __lsx_vilvl_d(tmp1, tmp0);
+        src0_ptr += src_stride_4x;
+
+        tmp0 = __lsx_vldrepl_d(src1_ptr, 0);
+        tmp1 = __lsx_vldrepl_d(src1_ptr + src2_stride, 0);
+        tmp2 = __lsx_vldrepl_d(src1_ptr + src2_stride_2x, 0);
+        tmp3 = __lsx_vldrepl_d(src1_ptr + src2_stride_3x, 0);
+        src1_ptr += src2_stride_4x;
+        DUP2_ARG2(__lsx_vilvl_d, tmp1, tmp0, tmp3, tmp2, in0, in1);
+        tmp0 = __lsx_vldrepl_d(src1_ptr, 0);
+        tmp1 = __lsx_vldrepl_d(src1_ptr + src2_stride, 0);
+        tmp2 = __lsx_vldrepl_d(src1_ptr + src2_stride_2x, 0);
+        tmp3 = __lsx_vldrepl_d(src1_ptr + src2_stride_3x, 0);
+        src1_ptr += src2_stride_4x;
+        DUP2_ARG2(__lsx_vilvl_d, tmp1, tmp0, tmp3, tmp2, in2, in3);
+        DUP2_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, dst0, dst2);
+        DUP2_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, dst1, dst3);
+        DUP2_ARG2(__lsx_vslli_h, dst1, 6, dst3, 6, dst1, dst3);
+        dst0 = hevc_bi_rnd_clip(in0, dst0, in1, dst1);
+        dst1 = hevc_bi_rnd_clip(in2, dst2, in3, dst3);
+        __lsx_vstelm_w(dst0, dst, 0, 0);
+        __lsx_vstelm_w(dst0, dst + dst_stride, 0, 1);
+        __lsx_vstelm_w(dst0, dst + dst_stride_2x, 0, 2);
+        __lsx_vstelm_w(dst0, dst + dst_stride_3x, 0, 3);
+        dst += dst_stride_4x;
+        __lsx_vstelm_w(dst1, dst, 0, 0);
+        __lsx_vstelm_w(dst1, dst + dst_stride, 0, 1);
+        __lsx_vstelm_w(dst1, dst + dst_stride_2x, 0, 2);
+        __lsx_vstelm_w(dst1, dst + dst_stride_3x, 0, 3);
+        dst += dst_stride_4x;
+    }
+    for(;res--;) {
+        reg0 = __lsx_vldrepl_w(src0_ptr, 0);
+        reg1 = __lsx_vldrepl_w(src0_ptr + src_stride, 0);
+        reg2 = __lsx_vldrepl_d(src1_ptr, 0);
+        reg3 = __lsx_vldrepl_d(src1_ptr + src2_stride, 0);
+        src0 = __lsx_vilvl_w(reg1, reg0);
+        in0  = __lsx_vilvl_d(reg3, reg2);
+        dst0 = __lsx_vsllwil_hu_bu(src0, 6);
+        dst0 = __lsx_vsadd_h(dst0, in0);
+        dst0 = __lsx_vssrarni_bu_h(dst0, dst0, 7);
+        __lsx_vstelm_w(dst0, dst, 0, 0);
+        __lsx_vstelm_w(dst0, dst + dst_stride, 0, 1);
+        src0_ptr += src_stride_2x;
+        src1_ptr += src2_stride_2x;
+        dst += dst_stride_2x;
+    }
+}
+
+static
+void hevc_bi_copy_6w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                         int16_t *src1_ptr, int32_t src2_stride,
+                         uint8_t *dst, int32_t dst_stride, int32_t height)
+{
+    int32_t loop_cnt;
+    int32_t res = (height & 0x07) >> 1;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t dst_stride_2x = (dst_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t dst_stride_4x = (dst_stride << 2);
+    int32_t src2_stride_x = (src2_stride << 1);
+    int32_t src2_stride_2x = (src2_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    int32_t src2_stride_3x = src2_stride_2x + src2_stride_x;
+    __m128i out0, out1, out2, out3;
+    __m128i zero = __lsx_vldi(0);
+    __m128i src0, src1, src2, src3;
+    __m128i in0, in1, in2, in3, in4, in5, in6, in7;
+    __m128i dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    __m128i reg0, reg1, reg2, reg3;
+
+    for (loop_cnt = (height >> 3); loop_cnt--;) {
+        reg0 = __lsx_vldrepl_d(src0_ptr, 0);
+        reg1 = __lsx_vldrepl_d(src0_ptr + src_stride, 0);
+        reg2 = __lsx_vldrepl_d(src0_ptr + src_stride_2x, 0);
+        reg3 = __lsx_vldrepl_d(src0_ptr + src_stride_3x, 0);
+        DUP2_ARG2(__lsx_vilvl_d, reg1, reg0, reg3, reg2, src0, src1);
+        src0_ptr += src_stride_4x;
+        reg0 = __lsx_vldrepl_d(src0_ptr, 0);
+        reg1 = __lsx_vldrepl_d(src0_ptr + src_stride, 0);
+        reg2 = __lsx_vldrepl_d(src0_ptr + src_stride_2x, 0);
+        reg3 = __lsx_vldrepl_d(src0_ptr + src_stride_3x, 0);
+        DUP2_ARG2(__lsx_vilvl_d, reg1, reg0, reg3, reg2, src2, src3);
+        src0_ptr += src_stride_4x;
+        in0 = __lsx_vld(src1_ptr, 0);
+        DUP2_ARG2(__lsx_vldx, src1_ptr, src2_stride_x, src1_ptr,
+                  src2_stride_2x, in1, in2);
+        in3 = __lsx_vldx(src1_ptr, src2_stride_3x);
+        src1_ptr += src2_stride_2x;
+        in4 = __lsx_vld(src1_ptr, 0);
+        DUP2_ARG2(__lsx_vldx, src1_ptr, src2_stride_x, src1_ptr,
+                  src2_stride_2x, in5, in6);
+        in7 = __lsx_vldx(src1_ptr, src2_stride_3x);
+        src1_ptr += src2_stride_2x;
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, src2, 6, src3, 6,
+                  dst0, dst2, dst4, dst6);
+        DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero, src3,
+                  dst1, dst3, dst5, dst7);
+        DUP4_ARG2(__lsx_vslli_h, dst1, 6, dst3, 6, dst5, 6, dst7, 6, dst1, dst3,
+                  dst5, dst7);
+        out0 = hevc_bi_rnd_clip(in0, dst0, in1, dst1);
+        out1 = hevc_bi_rnd_clip(in2, dst2, in3, dst3);
+        out2 = hevc_bi_rnd_clip(in4, dst4, in5, dst5);
+        out3 = hevc_bi_rnd_clip(in6, dst6, in7, dst7);
+        __lsx_vstelm_w(out0, dst, 0, 0);
+        __lsx_vstelm_w(out0, dst + dst_stride, 0, 2);
+        __lsx_vstelm_h(out0, dst, 4, 2);
+        __lsx_vstelm_h(out0, dst + dst_stride, 4, 6);
+        __lsx_vstelm_w(out1, dst + dst_stride_2x, 0, 0);
+        __lsx_vstelm_w(out1, dst + dst_stride_3x, 0, 2);
+        __lsx_vstelm_h(out1, dst + dst_stride_2x, 4, 2);
+        __lsx_vstelm_h(out1, dst + dst_stride_3x, 4, 6);
+        dst += dst_stride_4x;
+        __lsx_vstelm_w(out2, dst, 0, 0);
+        __lsx_vstelm_w(out2, dst + dst_stride, 0, 2);
+        __lsx_vstelm_h(out2, dst, 4, 2);
+        __lsx_vstelm_h(out2, dst + dst_stride, 4, 6);
+        __lsx_vstelm_w(out3, dst + dst_stride_2x, 0, 0);
+        __lsx_vstelm_w(out3, dst + dst_stride_3x, 0, 2);
+        __lsx_vstelm_h(out3, dst + dst_stride_2x, 4, 2);
+        __lsx_vstelm_h(out3, dst + dst_stride_3x, 4, 6);
+        dst += dst_stride_4x;
+    }
+    for (;res--;) {
+        reg0 = __lsx_vldrepl_d(src0_ptr, 0);
+        reg1 = __lsx_vldrepl_d(src0_ptr + src_stride, 0);
+        src0 = __lsx_vilvl_d(reg1, reg0);
+        src0_ptr += src_stride_2x;
+        in0 = __lsx_vld(src1_ptr, 0);
+        in1 = __lsx_vldx(src1_ptr, src2_stride_x);
+        src1_ptr += src2_stride_x;
+        dst0 = __lsx_vsllwil_hu_bu(src0, 6);
+        dst1 = __lsx_vilvh_b(zero, src0);
+        dst1 = __lsx_vslli_h(dst1, 6);
+        out0 = hevc_bi_rnd_clip(in0, dst0, in1, dst1);
+        __lsx_vstelm_w(out0, dst, 0, 0);
+        __lsx_vstelm_h(out0, dst, 4, 2);
+        dst += dst_stride;
+        __lsx_vstelm_w(out0, dst, 0, 2);
+        __lsx_vstelm_h(out0, dst, 4, 6);
+        dst += dst_stride;
+    }
+}
+
+static
+void hevc_bi_copy_8w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                         int16_t *src1_ptr, int32_t src2_stride,
+                         uint8_t *dst, int32_t dst_stride, int32_t height)
+{
+    int32_t loop_cnt = height >> 3;
+    int32_t res = (height & 7) >> 1;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t dst_stride_2x = (dst_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t dst_stride_4x = (dst_stride << 2);
+    int32_t src2_stride_x = (src2_stride << 1);
+    int32_t src2_stride_2x = (src2_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    int32_t src2_stride_3x = src2_stride_2x + src2_stride_x;
+    __m128i out0, out1, out2, out3;
+    __m128i src0, src1, src2, src3;
+    __m128i zero = __lsx_vldi(0);
+    __m128i in0, in1, in2, in3, in4, in5, in6, in7;
+    __m128i dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    __m128i reg0, reg1, reg2, reg3;
+
+    for (loop_cnt = (height >> 3); loop_cnt--;) {
+        reg0 = __lsx_vldrepl_d(src0_ptr, 0);
+        reg1 = __lsx_vldrepl_d(src0_ptr + src_stride, 0);
+        reg2 = __lsx_vldrepl_d(src0_ptr + src_stride_2x, 0);
+        reg3 = __lsx_vldrepl_d(src0_ptr + src_stride_3x, 0);
+        DUP2_ARG2(__lsx_vilvl_d, reg1, reg0, reg3, reg2, src0, src1);
+        src0_ptr += src_stride_4x;
+        reg0 = __lsx_vldrepl_d(src0_ptr, 0);
+        reg1 = __lsx_vldrepl_d(src0_ptr + src_stride, 0);
+        reg2 = __lsx_vldrepl_d(src0_ptr + src_stride_2x, 0);
+        reg3 = __lsx_vldrepl_d(src0_ptr + src_stride_3x, 0);
+        DUP2_ARG2(__lsx_vilvl_d, reg1, reg0, reg3, reg2, src2, src3);
+        src0_ptr += src_stride_4x;
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, src2, 6, src3, 6,
+                  dst0, dst2, dst4, dst6);
+        DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero,
+                  src3, dst1, dst3, dst5, dst7);
+        DUP4_ARG2(__lsx_vslli_h, dst1, 6, dst3, 6, dst5, 6, dst7, 6, dst1,
+                  dst3, dst5, dst7);
+        in0 = __lsx_vld(src1_ptr, 0);
+        DUP2_ARG2(__lsx_vldx, src1_ptr, src2_stride_x, src1_ptr,
+                  src2_stride_2x, in1, in2);
+        in3 = __lsx_vldx(src1_ptr, src2_stride_3x);
+        src1_ptr += src2_stride_2x;
+        in4 = __lsx_vld(src1_ptr, 0);
+        DUP2_ARG2(__lsx_vldx, src1_ptr, src2_stride_x, src1_ptr,
+                  src2_stride_2x, in5, in6);
+        in7 = __lsx_vldx(src1_ptr, src2_stride_3x);
+        src1_ptr += src2_stride_2x;
+        out0 = hevc_bi_rnd_clip(in0, dst0, in1, dst1);
+        out1 = hevc_bi_rnd_clip(in2, dst2, in3, dst3);
+        out2 = hevc_bi_rnd_clip(in4, dst4, in5, dst5);
+        out3 = hevc_bi_rnd_clip(in6, dst6, in7, dst7);
+        __lsx_vstelm_d(out0, dst, 0, 0);
+        __lsx_vstelm_d(out0, dst + dst_stride, 0, 1);
+        __lsx_vstelm_d(out1, dst + dst_stride_2x, 0, 0);
+        __lsx_vstelm_d(out1, dst + dst_stride_3x, 0, 1);
+        dst += dst_stride_4x;
+        __lsx_vstelm_d(out2, dst, 0, 0);
+        __lsx_vstelm_d(out2, dst + dst_stride, 0, 1);
+        __lsx_vstelm_d(out3, dst + dst_stride_2x, 0, 0);
+        __lsx_vstelm_d(out3, dst + dst_stride_3x, 0, 1);
+        dst += dst_stride_4x;
+    }
+    for (;res--;) {
+        reg0 = __lsx_vldrepl_d(src0_ptr, 0);
+        reg1 = __lsx_vldrepl_d(src0_ptr + src_stride, 0);
+        src0 = __lsx_vilvl_d(reg1, reg0);
+        in0  = __lsx_vld(src1_ptr, 0);
+        in1  = __lsx_vldx(src1_ptr, src2_stride_x);
+        dst0 = __lsx_vsllwil_hu_bu(src0, 6);
+        dst1 = __lsx_vilvh_b(zero, src0);
+        dst1 = __lsx_vslli_h(dst1, 6);
+        out0 = hevc_bi_rnd_clip(in0, dst0, in1, dst1);
+        __lsx_vstelm_d(out0, dst, 0, 0);
+        __lsx_vstelm_d(out0, dst + dst_stride, 0, 1);
+        src0_ptr += src_stride_2x;
+        src1_ptr += src2_stride_x;
+        dst += dst_stride_2x;
+    }
+}
+
+static
+void hevc_bi_copy_12w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                          int16_t *src1_ptr, int32_t src2_stride,
+                          uint8_t *dst, int32_t dst_stride, int32_t height)
+{
+    uint32_t loop_cnt;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t dst_stride_2x = (dst_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t dst_stride_4x = (dst_stride << 2);
+    int32_t src2_stride_x = (src2_stride << 1);
+    int32_t src2_stride_2x = (src2_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    int32_t src2_stride_3x = src2_stride_2x + src2_stride_x;
+    int16_t* _src1 = src1_ptr + 8;
+    __m128i out0, out1, out2;
+    __m128i src0, src1, src2, src3;
+    __m128i in0, in1, in2, in3, in4, in5, in6, in7;
+    __m128i dst0, dst1, dst2, dst3, dst4, dst5;
+
+    for (loop_cnt = 4; loop_cnt--;) {
+        src0 = __lsx_vld(src0_ptr, 0);
+        DUP2_ARG2(__lsx_vldx, src0_ptr, src_stride, src0_ptr, src_stride_2x,
+                  src1, src2);
+        src3 = __lsx_vldx(src0_ptr, src_stride_3x);
+        src0_ptr += src_stride_4x;
+        in0 = __lsx_vld(src1_ptr, 0);
+        DUP2_ARG2(__lsx_vldx, src1_ptr, src2_stride_x, src1_ptr,
+                  src2_stride_2x, in1, in2);
+        in3 = __lsx_vldx(src1_ptr, src2_stride_3x);
+        src1_ptr += src2_stride_2x;
+        in4 = __lsx_vld(_src1, 0);
+        DUP2_ARG2(__lsx_vldx, _src1, src2_stride_x, _src1, src2_stride_2x,
+                  in5, in6);
+        in7 = __lsx_vldx(_src1, src2_stride_3x);
+        _src1 += src2_stride_2x;
+
+        DUP2_ARG2(__lsx_vilvl_d, in5, in4, in7, in6, in4, in5);
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, src2, 6, src3, 6,
+                  dst0, dst1, dst2, dst3)
+        DUP2_ARG2(__lsx_vilvh_w, src1, src0, src3, src2, src0, src1);
+        DUP2_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, dst4, dst5)
+        out0 = hevc_bi_rnd_clip(in0, dst0, in1, dst1);
+        out1 = hevc_bi_rnd_clip(in2, dst2, in3, dst3);
+        out2 = hevc_bi_rnd_clip(in4, dst4, in5, dst5);
+        __lsx_vstelm_d(out0, dst, 0, 0);
+        __lsx_vstelm_d(out0, dst + dst_stride, 0, 1);
+        __lsx_vstelm_d(out1, dst + dst_stride_2x, 0, 0);
+        __lsx_vstelm_d(out1, dst + dst_stride_3x, 0, 1);
+        __lsx_vstelm_w(out2, dst, 8, 0);
+        __lsx_vstelm_w(out2, dst + dst_stride, 8, 1);
+        __lsx_vstelm_w(out2, dst + dst_stride_2x, 8, 2);
+        __lsx_vstelm_w(out2, dst + dst_stride_3x, 8, 3);
+        dst += dst_stride_4x;
+    }
+}
+
+static
+void hevc_bi_copy_16w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                          int16_t *src1_ptr, int32_t src2_stride,
+                          uint8_t *dst, int32_t dst_stride, int32_t height)
+{
+    uint32_t loop_cnt;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t dst_stride_2x = (dst_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t dst_stride_4x = (dst_stride << 2);
+    int32_t src2_stride_x = (src2_stride << 1);
+    int32_t src2_stride_2x = (src2_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    int32_t src2_stride_3x = src2_stride_2x + src2_stride_x;
+    int16_t *_src1 = src1_ptr + 8;
+    __m128i out0, out1, out2, out3;
+    __m128i src0, src1, src2, src3;
+    __m128i in0, in1, in2, in3, in4, in5, in6, in7;
+    __m128i dst0_r, dst1_r, dst2_r, dst3_r, dst0_l, dst1_l, dst2_l, dst3_l;
+    __m128i zero = {0};
+
+    for (loop_cnt = (height >> 2); loop_cnt--;) {
+        src0 = __lsx_vld(src0_ptr, 0);
+        DUP2_ARG2(__lsx_vldx, src0_ptr, src_stride, src0_ptr, src_stride_2x,
+                  src1, src2);
+        src3 = __lsx_vldx(src0_ptr, src_stride_3x);
+        src0_ptr += src_stride_4x;
+        in0 = __lsx_vld(src1_ptr, 0);
+        DUP2_ARG2(__lsx_vldx, src1_ptr, src2_stride_x, src1_ptr,
+                  src2_stride_2x, in1, in2);
+        in3 = __lsx_vldx(src1_ptr, src2_stride_3x);
+        src1_ptr += src2_stride_2x;
+        in4 = __lsx_vld(_src1, 0);
+        DUP2_ARG2(__lsx_vldx, _src1, src2_stride_x, _src1, src2_stride_2x,
+                  in5, in6);
+        in7 = __lsx_vldx(_src1, src2_stride_3x);
+        _src1 += src2_stride_2x;
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, src2, 6, src3, 6,
+                  dst0_r, dst1_r, dst2_r, dst3_r)
+        DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero, src3,
+                  dst0_l, dst1_l, dst2_l, dst3_l);
+        DUP4_ARG2(__lsx_vslli_h, dst0_l, 6, dst1_l, 6, dst2_l, 6, dst3_l, 6,
+                  dst0_l, dst1_l, dst2_l, dst3_l);
+
+        out0 = hevc_bi_rnd_clip(in0, dst0_r, in4, dst0_l);
+        out1 = hevc_bi_rnd_clip(in1, dst1_r, in5, dst1_l);
+        out2 = hevc_bi_rnd_clip(in2, dst2_r, in6, dst2_l);
+        out3 = hevc_bi_rnd_clip(in3, dst3_r, in7, dst3_l);
+        __lsx_vst(out0, dst, 0);
+        __lsx_vstx(out1, dst, dst_stride);
+        __lsx_vstx(out2, dst, dst_stride_2x);
+        __lsx_vstx(out3, dst, dst_stride_3x);
+        dst += dst_stride_4x;
+    }
+}
+
+static
+void hevc_bi_copy_24w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                          int16_t *src1_ptr, int32_t src2_stride,
+                          uint8_t *dst, int32_t dst_stride, int32_t height)
+{
+    hevc_bi_copy_16w_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                         dst, dst_stride, height);
+    hevc_bi_copy_8w_lsx(src0_ptr + 16, src_stride, src1_ptr + 16, src2_stride,
+                         dst + 16, dst_stride, height);
+}
+
+static
+void hevc_bi_copy_32w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                          int16_t *src1_ptr, int32_t src2_stride,
+                          uint8_t *dst, int32_t dst_stride, int32_t height)
+{
+    hevc_bi_copy_16w_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                         dst, dst_stride, height);
+    hevc_bi_copy_16w_lsx(src0_ptr + 16, src_stride, src1_ptr + 16, src2_stride,
+                         dst + 16, dst_stride, height);
+}
+
+static
+void hevc_bi_copy_48w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                          int16_t *src1_ptr, int32_t src2_stride,
+                          uint8_t *dst, int32_t dst_stride, int32_t height)
+{
+    hevc_bi_copy_16w_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                         dst, dst_stride, height);
+    hevc_bi_copy_32w_lsx(src0_ptr + 16, src_stride, src1_ptr + 16, src2_stride,
+                         dst + 16, dst_stride, height);
+}
+
+static
+void hevc_bi_copy_64w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                          int16_t *src1_ptr, int32_t src2_stride,
+                          uint8_t *dst, int32_t dst_stride, int32_t height)
+{
+    hevc_bi_copy_32w_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                         dst, dst_stride, height);
+    hevc_bi_copy_32w_lsx(src0_ptr + 32, src_stride, src1_ptr + 32, src2_stride,
+                         dst + 32, dst_stride, height);
+}
+
+static void hevc_hz_8t_16w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr,  int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter, int32_t height)
+{
+    uint32_t loop_cnt;
+    const int32_t dst_stride_2x = (dst_stride << 1);
+    __m128i src0, src1, src2, src3;
+    __m128i filt0, filt1, filt2, filt3;
+    __m128i mask1, mask2, mask3;
+    __m128i vec0, vec1, vec2, vec3;
+    __m128i dst0, dst1, dst2, dst3;
+    __m128i in0, in1, in2, in3;
+    __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
+
+    src0_ptr -= 3;
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filt0, filt1, filt2, filt3);
+
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    mask3 = __lsx_vaddi_bu(mask0, 6);
+
+    for (loop_cnt = (height >> 1); loop_cnt--;) {
+        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 8, src0, src1);
+        src0_ptr += src_stride;
+        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 8, src2, src3);
+        src0_ptr += src_stride;
+        DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, in0, in1);
+        src1_ptr += src2_stride;
+        DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, in2, in3);
+        src1_ptr += src2_stride;
+
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src1, mask0,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src3, src3, mask0,
+                  vec2, vec3);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec1, filt0, vec2, filt0,
+                  vec3, filt0, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src1, mask1,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask1, src3, src3, mask1,
+                  vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec0, filt1, dst1, vec1, filt1,
+                  dst2, vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src1, mask2,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask2, src3, src3, mask2,
+                  vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec0, filt2, dst1, vec1, filt2,
+                  dst2, vec2, filt2, dst3, vec3, filt2, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src1, mask3,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask3, src3, src3, mask3,
+                  vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec0, filt3, dst1, vec1, filt3,
+                  dst2, vec2, filt3, dst3, vec3, filt3, dst0, dst1, dst2, dst3);
+
+        dst0 = hevc_bi_rnd_clip(in0, dst0, in1, dst1);
+        dst1 = hevc_bi_rnd_clip(in2, dst2, in3, dst3);
+        __lsx_vst(dst0, dst, 0);
+        __lsx_vstx(dst1, dst, dst_stride);
+        dst += dst_stride_2x;
+    }
+}
+
+static void hevc_hz_8t_24w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr, int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter, int32_t height)
+{
+    uint32_t loop_cnt;
+    __m128i src0, src1, tmp0, tmp1;
+    __m128i filt0, filt1, filt2, filt3;
+    __m128i mask1, mask2, mask3, mask4, mask5, mask6, mask7;
+    __m128i vec0, vec1, vec2, vec3;
+    __m128i dst0, dst1, dst2;
+    __m128i in0, in1, in2;
+    __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
+
+    src0_ptr -= 3;
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filt0, filt1, filt2, filt3);
+
+    DUP4_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask0, 6, mask0, 8, mask1,
+              mask2, mask3, mask4);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 10, mask0, 12, mask5, mask6);
+    mask7 = __lsx_vaddi_bu(mask0, 14);
+
+    for (loop_cnt = height; loop_cnt--;) {
+        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src0, src1);
+        src0_ptr += src_stride;
+        DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, in0, in1);
+        in2 = __lsx_vld(src1_ptr, 32);
+        src1_ptr += src2_stride;
+
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src0, mask4, src1,
+                  src1, mask0, src0, src0, mask1, vec0, vec1, vec2, vec3);
+        DUP2_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec1, filt0, dst0, dst1);
+        dst2 = __lsx_vdp2_h_bu_b(vec2, filt0);
+        dst0 = __lsx_vdp2add_h_bu_b(dst0, vec3, filt1);
+        DUP4_ARG3(__lsx_vshuf_b, src1, src0, mask5, src1, src1, mask1, src0,
+                  src0, mask2, src1, src0, mask6, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst1, vec0, filt1, dst2, vec1, filt1,
+                  dst0, vec2, filt2, dst1, vec3, filt2, dst1, dst2, dst0, dst1);
+        DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask2, src0, src0, mask3, src1, src0,
+                  mask7, src1, src1, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst2, vec0, filt2, dst0, vec1, filt3,
+                  dst1, vec2, filt3, dst2, vec3, filt3, dst2, dst0, dst1, dst2);
+
+        tmp0 = hevc_bi_rnd_clip(in0, dst0, in1, dst1);
+        dst2 = __lsx_vsadd_h(dst2, in2);
+        tmp1 = __lsx_vssrarni_bu_h(dst2, dst2, 7);
+
+        __lsx_vst(tmp0, dst, 0);
+        __lsx_vstelm_d(tmp1, dst, 16, 0);
+        dst += dst_stride;
+    }
+}
+
+static void hevc_hz_8t_32w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr, int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter, int32_t height)
+{
+    hevc_hz_8t_16w_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                       dst, dst_stride, filter, height);
+    hevc_hz_8t_16w_lsx(src0_ptr + 16, src_stride, src1_ptr + 16, src2_stride,
+                       dst + 16, dst_stride, filter, height);
+}
+
+static void hevc_hz_8t_48w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr, int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter, int32_t height)
+{
+    hevc_hz_8t_16w_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                       dst, dst_stride, filter, height);
+    hevc_hz_8t_32w_lsx(src0_ptr + 16, src_stride, src1_ptr + 16, src2_stride,
+                       dst + 16, dst_stride, filter, height);
+}
+
+static void hevc_hz_8t_64w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr, int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter, int32_t height)
+{
+    hevc_hz_8t_32w_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                       dst, dst_stride, filter, height);
+    hevc_hz_8t_32w_lsx(src0_ptr + 32, src_stride, src1_ptr + 32, src2_stride,
+                       dst + 32, dst_stride, filter, height);
+}
+
+static av_always_inline
+void hevc_vt_8t_8w_lsx(uint8_t *src0_ptr, int32_t src_stride, int16_t *src1_ptr,
+                       int32_t src2_stride, uint8_t *dst, int32_t dst_stride,\
+                       const int8_t *filter, int32_t height)
+{
+    int32_t loop_cnt;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t dst_stride_2x = (dst_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t dst_stride_4x = (dst_stride << 2);
+    int32_t src2_stride_x = (src2_stride << 1);
+    int32_t src2_stride_2x = (src2_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    int32_t src2_stride_3x = src2_stride_2x + src2_stride_x;
+    __m128i src0, src1, src2, src3, src4, src5;
+    __m128i src6, src7, src8, src9, src10;
+    __m128i in0, in1, in2, in3;
+    __m128i src10_r, src32_r, src54_r, src76_r, src98_r;
+    __m128i src21_r, src43_r, src65_r, src87_r, src109_r;
+    __m128i dst0_r, dst1_r, dst2_r, dst3_r;
+    __m128i filt0, filt1, filt2, filt3;
+
+    src0_ptr -= src_stride_3x;
+
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filt0, filt1, filt2, filt3);
+
+    src0 = __lsx_vld(src0_ptr, 0);
+    DUP2_ARG2(__lsx_vldx, src0_ptr, src_stride, src0_ptr, src_stride_2x,
+              src1, src2);
+    src3 = __lsx_vldx(src0_ptr, src_stride_3x);
+    src0_ptr += src_stride_4x;
+    src4 = __lsx_vld(src0_ptr, 0);
+    DUP2_ARG2(__lsx_vldx, src0_ptr, src_stride, src0_ptr, src_stride_2x,
+              src5, src6);
+    src0_ptr += src_stride_3x;
+    DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1,
+              src10_r, src32_r, src54_r, src21_r);
+    DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, src43_r, src65_r);
+
+    for (loop_cnt = (height >> 2); loop_cnt--;) {
+        src7 = __lsx_vld(src0_ptr, 0);
+        DUP2_ARG2(__lsx_vldx, src0_ptr, src_stride, src0_ptr, src_stride_2x,
+                  src8, src9);
+        src10 = __lsx_vldx(src0_ptr, src_stride_3x);
+        src0_ptr += src_stride_4x;
+        in0 = __lsx_vld(src1_ptr, 0);
+        DUP2_ARG2(__lsx_vldx, src1_ptr, src2_stride_x, src1_ptr, src2_stride_2x,
+                  in1, in2);
+        in3 = __lsx_vldx(src1_ptr, src2_stride_3x);
+        src1_ptr += src2_stride_2x;
+        DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10, src9,
+                  src76_r, src87_r, src98_r, src109_r);
+
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, src10_r, filt0, src21_r, filt0, src32_r,
+                  filt0, src43_r, filt0, dst0_r, dst1_r, dst2_r, dst3_r);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0_r, src32_r, filt1, dst1_r, src43_r,
+                  filt1, dst2_r, src54_r, filt1, dst3_r, src65_r, filt1,
+                  dst0_r, dst1_r, dst2_r, dst3_r);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0_r, src54_r, filt2, dst1_r, src65_r,
+                  filt2, dst2_r, src76_r, filt2, dst3_r, src87_r, filt2,
+                  dst0_r, dst1_r, dst2_r, dst3_r);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0_r, src76_r, filt3, dst1_r, src87_r,
+                  filt3, dst2_r, src98_r, filt3, dst3_r, src109_r, filt3,
+                  dst0_r, dst1_r, dst2_r, dst3_r);
+
+        dst0_r = hevc_bi_rnd_clip(in0, dst0_r, in1, dst1_r);
+        dst1_r = hevc_bi_rnd_clip(in2, dst2_r, in3, dst3_r);
+        __lsx_vstelm_d(dst0_r, dst, 0, 0);
+        __lsx_vstelm_d(dst0_r, dst + dst_stride, 0, 1);
+        __lsx_vstelm_d(dst1_r, dst + dst_stride_2x, 0, 0);
+        __lsx_vstelm_d(dst1_r, dst + dst_stride_3x, 0, 1);
+        dst += dst_stride_4x;
+
+        src10_r = src54_r;
+        src32_r = src76_r;
+        src54_r = src98_r;
+        src21_r = src65_r;
+        src43_r = src87_r;
+        src65_r = src109_r;
+
+        src6 = src10;
+    }
+}
+
+static av_always_inline
+void hevc_vt_8t_16multx2mult_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                                 int16_t *src1_ptr, int32_t src2_stride,
+                                 uint8_t *dst, int32_t dst_stride,
+                                 const int8_t *filter, int32_t height,
+                                 int32_t width)
+{
+    uint8_t *src0_ptr_tmp;
+    int16_t *src1_ptr_tmp;
+    uint8_t *dst_tmp;
+    uint32_t loop_cnt;
+    uint32_t cnt;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t dst_stride_2x = (dst_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8;
+    __m128i in0, in1, in2, in3;
+    __m128i src10_r, src32_r, src54_r, src76_r;
+    __m128i src21_r, src43_r, src65_r, src87_r;
+    __m128i dst0_r, dst1_r;
+    __m128i src10_l, src32_l, src54_l, src76_l;
+    __m128i src21_l, src43_l, src65_l, src87_l;
+    __m128i dst0_l, dst1_l;
+    __m128i filt0, filt1, filt2, filt3;
+
+    src0_ptr -= src_stride_3x;
+
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filt0, filt1, filt2, filt3);
+
+    for (cnt = (width >> 4); cnt--;) {
+        src0_ptr_tmp = src0_ptr;
+        src1_ptr_tmp = src1_ptr;
+        dst_tmp = dst;
+
+        src0 = __lsx_vld(src0_ptr_tmp, 0);
+        DUP2_ARG2(__lsx_vldx, src0_ptr_tmp, src_stride, src0_ptr_tmp,
+                  src_stride_2x, src1, src2);
+        src3 = __lsx_vldx(src0_ptr_tmp, src_stride_3x);
+        src0_ptr_tmp += src_stride_4x;
+        src4 = __lsx_vld(src0_ptr_tmp, 0);
+        DUP2_ARG2(__lsx_vldx, src0_ptr_tmp, src_stride, src0_ptr_tmp,
+                  src_stride_2x, src5, src6);
+        src0_ptr_tmp += src_stride_3x;
+
+        DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1,
+                  src10_r, src32_r, src54_r, src21_r);
+        DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, src43_r, src65_r);
+        DUP4_ARG2(__lsx_vilvh_b, src1, src0, src3, src2, src5, src4, src2, src1,
+                  src10_l, src32_l, src54_l, src21_l);
+        DUP2_ARG2(__lsx_vilvh_b, src4, src3, src6, src5, src43_l, src65_l);
+
+        for (loop_cnt = (height >> 1); loop_cnt--;) {
+            src7 = __lsx_vld(src0_ptr_tmp, 0);
+            src8 = __lsx_vldx(src0_ptr_tmp, src_stride);
+            src0_ptr_tmp += src_stride_2x;
+            DUP2_ARG2(__lsx_vld, src1_ptr_tmp, 0, src1_ptr_tmp, 16, in0, in2);
+            src1_ptr_tmp += src2_stride;
+            DUP2_ARG2(__lsx_vld, src1_ptr_tmp, 0, src1_ptr_tmp, 16, in1, in3);
+            src1_ptr_tmp += src2_stride;
+
+            DUP2_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src76_r, src87_r);
+            DUP2_ARG2(__lsx_vilvh_b, src7, src6, src8, src7, src76_l, src87_l);
+
+            DUP4_ARG2(__lsx_vdp2_h_bu_b, src10_r, filt0, src21_r, filt0, src10_l,
+                      filt0, src21_l, filt0, dst0_r, dst1_r, dst0_l, dst1_l);
+            DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0_r, src32_r, filt1, dst1_r,
+                      src43_r, filt1, dst0_l, src32_l, filt1, dst1_l, src43_l,
+                      filt1, dst0_r, dst1_r, dst0_l, dst1_l);
+            DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0_r, src54_r, filt2, dst1_r,
+                      src65_r, filt2, dst0_l, src54_l, filt2, dst1_l, src65_l,
+                      filt2, dst0_r, dst1_r, dst0_l, dst1_l);
+            DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0_r, src76_r, filt3, dst1_r,
+                      src87_r, filt3, dst0_l, src76_l, filt3, dst1_l, src87_l,
+                      filt3, dst0_r, dst1_r, dst0_l, dst1_l);
+            dst0_r = hevc_bi_rnd_clip(in0, dst0_r, in2, dst0_l);
+            dst1_r = hevc_bi_rnd_clip(in1, dst1_r, in3, dst1_l);
+
+            __lsx_vst(dst0_r, dst_tmp, 0);
+            __lsx_vstx(dst1_r, dst_tmp, dst_stride);
+            dst_tmp += dst_stride_2x;
+
+            src10_r = src32_r;
+            src32_r = src54_r;
+            src54_r = src76_r;
+            src21_r = src43_r;
+            src43_r = src65_r;
+            src65_r = src87_r;
+            src10_l = src32_l;
+            src32_l = src54_l;
+            src54_l = src76_l;
+            src21_l = src43_l;
+            src43_l = src65_l;
+            src65_l = src87_l;
+            src6 = src8;
+        }
+
+        src0_ptr += 16;
+        src1_ptr += 16;
+        dst += 16;
+    }
+}
+
+static void hevc_vt_8t_16w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr, int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter, int32_t height)
+{
+    hevc_vt_8t_16multx2mult_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                                dst, dst_stride, filter, height, 16);
+}
+
+static void hevc_vt_8t_24w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr, int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter, int32_t height)
+{
+    hevc_vt_8t_16multx2mult_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                                dst, dst_stride, filter, height, 16);
+    hevc_vt_8t_8w_lsx(src0_ptr + 16, src_stride, src1_ptr + 16, src2_stride,
+                      dst + 16, dst_stride, filter, height);
+}
+
+static void hevc_vt_8t_32w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr, int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter, int32_t height)
+{
+    hevc_vt_8t_16multx2mult_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                                dst, dst_stride, filter, height, 32);
+}
+
+static void hevc_vt_8t_48w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr, int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter, int32_t height)
+{
+    hevc_vt_8t_16multx2mult_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                                dst, dst_stride, filter, height, 48);
+}
+
+static void hevc_vt_8t_64w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr, int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter, int32_t height)
+{
+    hevc_vt_8t_16multx2mult_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                                dst, dst_stride, filter, height, 64);
+}
+
+static av_always_inline
+void hevc_hv_8t_8multx1mult_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                                int16_t *src1_ptr, int32_t src2_stride,
+                                uint8_t *dst, int32_t dst_stride,
+                                const int8_t *filter_x, const int8_t *filter_y,
+                                int32_t height, int32_t width)
+{
+    uint32_t loop_cnt;
+    uint32_t cnt;
+    uint8_t *src0_ptr_tmp;
+    int16_t *src1_ptr_tmp;
+    uint8_t *dst_tmp;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    __m128i out;
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m128i in0, tmp;
+    __m128i filt0, filt1, filt2, filt3;
+    __m128i filt_h0, filt_h1, filt_h2, filt_h3;
+    __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
+    __m128i mask1, mask2, mask3;
+    __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
+    __m128i vec8, vec9, vec10, vec11, vec12, vec13, vec14, vec15;
+    __m128i dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    __m128i dst0_r, dst0_l;
+    __m128i dst10_r, dst32_r, dst54_r, dst76_r;
+    __m128i dst10_l, dst32_l, dst54_l, dst76_l;
+
+    src0_ptr -= src_stride_3x + 3;
+
+    DUP4_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filter_x, 4, filter_x,
+              6, filt0, filt1, filt2, filt3);
+    filt_h3 = __lsx_vld(filter_y, 0);
+    filt_h3 = __lsx_vsllwil_h_b(filt_h3, 0);
+
+    DUP4_ARG2(__lsx_vreplvei_w, filt_h3, 0, filt_h3, 1, filt_h3, 2, filt_h3, 3,
+              filt_h0, filt_h1, filt_h2, filt_h3);
+
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    mask3 = __lsx_vaddi_bu(mask0, 6);
+
+    for (cnt = width >> 3; cnt--;) {
+        src0_ptr_tmp = src0_ptr;
+        dst_tmp = dst;
+        src1_ptr_tmp = src1_ptr;
+
+        src0 = __lsx_vld(src0_ptr_tmp, 0);
+        DUP2_ARG2(__lsx_vldx, src0_ptr_tmp, src_stride, src0_ptr_tmp,
+                  src_stride_2x, src1, src2);
+        src3 = __lsx_vldx(src0_ptr_tmp, src_stride_3x);
+        src0_ptr_tmp += src_stride_4x;
+        src4 = __lsx_vld(src0_ptr_tmp, 0);
+        DUP2_ARG2(__lsx_vldx, src0_ptr_tmp, src_stride, src0_ptr_tmp,
+                  src_stride_2x, src5, src6);
+        src0_ptr_tmp += src_stride_3x;
+
+        /* row 0 row 1 row 2 row 3 */
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src0,
+                  src0, mask2, src0, src0, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, src1,
+                  src1, mask2, src1, src1, mask3, vec4, vec5, vec6, vec7);
+        DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, src2,
+                  src2, mask2, src2, src2, mask3, vec8, vec9, vec10, vec11);
+        DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src3,
+                  src3, mask2, src3, src3, mask3, vec12, vec13, vec14, vec15);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec4, filt0, vec8, filt0,
+                  vec12, filt0, dst0, dst1, dst2, dst3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec1, filt1, dst1, vec5, filt1,
+                  dst2, vec9, filt1, dst3, vec13, filt1, dst0, dst1, dst2, dst3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec2, filt2, dst1, vec6, filt2,
+                  dst2, vec10, filt2, dst3, vec14, filt2, dst0, dst1, dst2, dst3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec3, filt3, dst1, vec7, filt3,
+                  dst2, vec11, filt3, dst3, vec15, filt3, dst0, dst1, dst2, dst3);
+
+        DUP4_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, src4,
+                  src4, mask2, src4, src4, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, src5,
+                  src5, mask2, src5, src5, mask3, vec4, vec5, vec6, vec7);
+        DUP4_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, src6,
+                  src6, mask2, src6, src6, mask3, vec8, vec9, vec10, vec11);
+        DUP2_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec4, filt0, dst4, dst5);
+        dst6 = __lsx_vdp2_h_bu_b(vec8, filt0);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst4, vec1, filt1, dst5, vec5, filt1,
+                  dst6, vec9, filt1, dst4, vec2, filt2, dst4, dst5, dst6, dst4);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst5, vec6, filt2, dst6, vec10, filt2,
+                  dst4, vec3, filt3, dst5, vec7, filt3, dst5, dst6, dst4, dst5);
+        dst6 = __lsx_vdp2add_h_bu_b(dst6, vec11, filt3);
+
+        for (loop_cnt = height; loop_cnt--;) {
+            src7 = __lsx_vld(src0_ptr_tmp, 0);
+            src0_ptr_tmp += src_stride;
+
+            in0 = __lsx_vld(src1_ptr_tmp, 0);
+            src1_ptr_tmp += src2_stride;
+
+            DUP4_ARG3(__lsx_vshuf_b, src7, src7, mask0, src7, src7, mask1, src7,
+                      src7, mask2, src7, src7, mask3, vec0, vec1, vec2, vec3);
+            dst7 = __lsx_vdp2_h_bu_b(vec0, filt0);
+            DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst7, vec1, filt1, dst7, vec2,
+                      filt2, dst7, dst7);
+            dst7 = __lsx_vdp2add_h_bu_b(dst7, vec3, filt3);
+            DUP4_ARG2(__lsx_vilvl_h, dst1, dst0, dst3, dst2, dst5, dst4, dst7,
+                      dst6, dst10_r, dst32_r, dst54_r, dst76_r);
+            DUP4_ARG2(__lsx_vilvh_h, dst1, dst0, dst3, dst2, dst5, dst4, dst7,
+                      dst6, dst10_l, dst32_l, dst54_l, dst76_l);
+
+            DUP2_ARG2(__lsx_vdp2_w_h, dst10_r, filt_h0, dst10_l, filt_h0,
+                      dst0_r, dst0_l);
+            DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst32_r, filt_h1, dst0_l,
+                      dst32_l, filt_h1, dst0_r, dst54_r, filt_h2, dst0_l,
+                      dst54_l, filt_h2, dst0_r, dst0_l, dst0_r, dst0_l);
+            DUP2_ARG3(__lsx_vdp2add_w_h, dst0_r, dst76_r, filt_h3, dst0_l,
+                      dst76_l, filt_h3, dst0_r, dst0_l);
+            dst0_r = __lsx_vsrli_w(dst0_r, 6);
+            dst0_l = __lsx_vsrli_w(dst0_l, 6);
+
+            tmp = __lsx_vpickev_h(dst0_l, dst0_r);
+            tmp = __lsx_vsadd_h(tmp, in0);
+            tmp = __lsx_vmaxi_h(tmp, 0);
+            out = __lsx_vssrlrni_bu_h(tmp, tmp, 7);
+            __lsx_vstelm_d(out, dst_tmp, 0, 0);
+            dst_tmp += dst_stride;
+
+            dst0 = dst1;
+            dst1 = dst2;
+            dst2 = dst3;
+            dst3 = dst4;
+            dst4 = dst5;
+            dst5 = dst6;
+            dst6 = dst7;
+        }
+
+        src0_ptr += 8;
+        dst += 8;
+        src1_ptr += 8;
+    }
+}
+
+static void hevc_hv_8t_8w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                              int16_t *src1_ptr, int32_t src2_stride,
+                              uint8_t *dst, int32_t dst_stride,
+                              const int8_t *filter_x, const int8_t *filter_y,
+                              int32_t height)
+{
+    hevc_hv_8t_8multx1mult_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                               dst, dst_stride, filter_x, filter_y, height, 8);
+}
+
+static void hevc_hv_8t_16w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr, int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter_x, const int8_t *filter_y,
+                               int32_t height)
+{
+    hevc_hv_8t_8multx1mult_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                               dst, dst_stride, filter_x, filter_y, height, 16);
+}
+
+static void hevc_hv_8t_24w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr, int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter_x, const int8_t *filter_y,
+                               int32_t height)
+{
+    hevc_hv_8t_8multx1mult_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                               dst, dst_stride, filter_x, filter_y, height, 24);
+}
+
+static void hevc_hv_8t_32w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr, int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter_x, const int8_t *filter_y,
+                               int32_t height)
+{
+    hevc_hv_8t_8multx1mult_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                               dst, dst_stride, filter_x, filter_y, height, 32);
+}
+
+static void hevc_hv_8t_48w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr, int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter_x, const int8_t *filter_y,
+                               int32_t height)
+{
+    hevc_hv_8t_8multx1mult_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                               dst, dst_stride, filter_x, filter_y, height, 48);
+}
+
+static void hevc_hv_8t_64w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr, int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter_x, const int8_t *filter_y,
+                               int32_t height)
+{
+    hevc_hv_8t_8multx1mult_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                               dst, dst_stride, filter_x, filter_y, height, 64);
+}
+
+static void hevc_hz_4t_24w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr, int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter, int32_t height)
+{
+    int16_t *src1_ptr_tmp;
+    uint8_t *dst_tmp;
+    uint32_t loop_cnt;
+    int32_t dst_stride_2x = (dst_stride << 1);
+    int32_t dst_stride_4x = (dst_stride << 2);
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    int32_t src2_stride_x = src2_stride << 1;
+    int32_t src2_stride_2x = src2_stride << 2;
+    int32_t src2_stride_3x = src2_stride_2x + src2_stride_x;
+
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m128i in0, in1, in2, in3, in4, in5, in6, in7;
+    __m128i filt0, filt1;
+    __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
+    __m128i mask1, mask2, mask3;
+    __m128i vec0, vec1, vec2, vec3;
+    __m128i dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+
+    src0_ptr -= 1;
+    DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
+
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 8, mask1, mask2);
+    mask3 = __lsx_vaddi_bu(mask0, 10);
+
+    dst_tmp = dst + 16;
+    src1_ptr_tmp = src1_ptr + 16;
+
+    for (loop_cnt = (height >> 2); loop_cnt--;) {
+        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src0, src1);
+        src0_ptr += src_stride;
+        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src2, src3);
+        src0_ptr += src_stride;
+        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src4, src5);
+        src0_ptr += src_stride;
+        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src6, src7);
+        src0_ptr += src_stride;
+
+        DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, in0, in1);
+        src1_ptr += src2_stride;
+        DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, in2, in3);
+        src1_ptr += src2_stride;
+        DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, in4, in5);
+        src1_ptr += src2_stride;
+        DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, in6, in7);
+        src1_ptr += src2_stride;
+
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src0, mask2, src2,
+                  src2, mask0, src3, src2, mask2, vec0, vec1, vec2, vec3);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec1, filt0, vec2, filt0,
+                  vec3, filt0, dst0, dst1, dst2, dst3);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src0, mask3, src2,
+                  src2, mask1, src3, src2, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec0, filt1, dst1, vec1, filt1,
+                  dst2, vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
+
+        DUP4_ARG3(__lsx_vshuf_b, src4, src4, mask0, src5, src4, mask2, src6,
+                  src6, mask0, src7, src6, mask2, vec0, vec1, vec2, vec3);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec1, filt0, vec2, filt0,
+                  vec3, filt0, dst4, dst5, dst6, dst7);
+        DUP4_ARG3(__lsx_vshuf_b, src4, src4, mask1, src5, src4, mask3, src6,
+                  src6, mask1, src7, src6, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst4, vec0, filt1, dst5, vec1, filt1,
+                  dst6, vec2, filt1, dst7, vec3, filt1, dst4, dst5, dst6, dst7);
+
+        dst0 = hevc_bi_rnd_clip(in0, dst0, in1, dst1);
+        dst1 = hevc_bi_rnd_clip(in2, dst2, in3, dst3);
+        dst2 = hevc_bi_rnd_clip(in4, dst4, in5, dst5);
+        dst3 = hevc_bi_rnd_clip(in6, dst6, in7, dst7);
+        __lsx_vst(dst0, dst, 0);
+        __lsx_vstx(dst1, dst, dst_stride);
+        __lsx_vstx(dst2, dst, dst_stride_2x);
+        __lsx_vstx(dst3, dst, dst_stride_3x);
+        dst += dst_stride_4x;
+
+        in0 = __lsx_vld(src1_ptr_tmp, 0);
+        DUP2_ARG2(__lsx_vldx, src1_ptr_tmp, src2_stride_x, src1_ptr_tmp,
+                  src2_stride_2x, in1, in2);
+        in3 = __lsx_vldx(src1_ptr_tmp, src2_stride_3x);
+        src1_ptr_tmp += src2_stride_2x;
+
+        DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask0, src3, src3, mask0, src5,
+                  src5, mask0, src7, src7, mask0, vec0, vec1, vec2, vec3);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec1, filt0, vec2, filt0,
+                  vec3, filt0, dst0, dst1, dst2, dst3);
+        DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask1, src3, src3, mask1, src5,
+                  src5, mask1, src7, src7, mask1, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec0, filt1, dst1, vec1, filt1,
+                  dst2, vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
+        dst0 = hevc_bi_rnd_clip(in0, dst0, in1, dst1);
+        dst1 = hevc_bi_rnd_clip(in2, dst2, in3, dst3);
+        __lsx_vstelm_d(dst0, dst_tmp, 0, 0);
+        __lsx_vstelm_d(dst0, dst_tmp + dst_stride, 0, 1);
+        __lsx_vstelm_d(dst1, dst_tmp + dst_stride_2x, 0, 0);
+        __lsx_vstelm_d(dst1, dst_tmp + dst_stride_3x, 0, 1);
+        dst_tmp += dst_stride_4x;
+    }
+}
+
+static void hevc_hz_4t_32w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr, int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter, int32_t height)
+{
+    uint32_t loop_cnt;
+    __m128i src0, src1, src2;
+    __m128i in0, in1, in2, in3;
+    __m128i filt0, filt1;
+    __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
+    __m128i mask1, mask2, mask3;
+    __m128i dst0, dst1, dst2, dst3;
+    __m128i vec0, vec1, vec2, vec3;
+
+    src0_ptr -= 1;
+
+    DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
+
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 8, mask1, mask2);
+    mask3 = __lsx_vaddi_bu(mask0, 10);
+
+    for (loop_cnt = height; loop_cnt--;) {
+        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src0, src1);
+        src2 = __lsx_vld(src0_ptr, 24);
+        src0_ptr += src_stride;
+        DUP4_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, src1_ptr, 32,
+                  src1_ptr, 48, in0, in1, in2, in3);
+        src1_ptr += src2_stride;
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src0, mask2, src1,
+                  src1, mask0, src2, src2, mask0, vec0, vec1, vec2, vec3);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec1, filt0, vec2, filt0,
+                  vec3, filt0, dst0, dst1, dst2, dst3);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src0, mask3, src1,
+                  src1, mask1, src2, src2, mask1, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec0, filt1, dst1, vec1, filt1,
+                  dst2, vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
+        dst0 = hevc_bi_rnd_clip(in0, dst0, in1, dst1);
+        dst1 = hevc_bi_rnd_clip(in2, dst2, in3, dst3);
+        __lsx_vst(dst0, dst, 0);
+        __lsx_vst(dst1, dst, 16);
+        dst += dst_stride;
+    }
+}
+
+static void hevc_vt_4t_12w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr, int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter, int32_t height)
+{
+    int32_t loop_cnt;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t dst_stride_2x = (dst_stride << 1);
+    int32_t dst_stride_4x = (dst_stride << 2);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t src2_stride_x = (src2_stride << 1);
+    int32_t src2_stride_2x = (src2_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    int32_t src2_stride_3x = src2_stride_2x + src2_stride_x;
+    int16_t *_src1 = src1_ptr + 8;
+    __m128i src0, src1, src2, src3, src4, src5, src6;
+    __m128i in0, in1, in2, in3, in4, in5, in6, in7;
+    __m128i src10_r, src32_r, src21_r, src43_r, src54_r, src65_r;
+    __m128i dst0_r, dst1_r, dst2_r, dst3_r;
+    __m128i src10_l, src32_l, src54_l, src21_l, src43_l, src65_l;
+    __m128i src2110, src4332, src6554;
+    __m128i dst0_l, dst1_l, filt0, filt1;
+
+    src0_ptr -= src_stride;
+    DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
+
+    src0 = __lsx_vld(src0_ptr, 0);
+    DUP2_ARG2(__lsx_vldx, src0_ptr, src_stride, src0_ptr, src_stride_2x,
+              src1, src2);
+    src0_ptr += src_stride_3x;
+    DUP2_ARG2(__lsx_vilvl_b, src1, src0, src2, src1, src10_r, src21_r);
+    DUP2_ARG2(__lsx_vilvh_b, src1, src0, src2, src1, src10_l, src21_l);
+    src2110 = __lsx_vilvl_d(src21_l, src10_l);
+
+    for (loop_cnt = (height >> 2); loop_cnt--;) {
+        src3 = __lsx_vld(src0_ptr, 0);
+        DUP2_ARG2(__lsx_vldx, src0_ptr, src_stride, src0_ptr, src_stride_2x,
+                  src4, src5);
+        src6 = __lsx_vldx(src0_ptr, src_stride_3x);
+        src0_ptr += src_stride_4x;
+        in0 = __lsx_vld(src1_ptr, 0);
+        DUP2_ARG2(__lsx_vldx, src1_ptr, src2_stride_x, src1_ptr,
+                  src2_stride_2x, in1, in2);
+        in3 = __lsx_vldx(src1_ptr, src2_stride_3x);
+        src1_ptr += src2_stride_2x;
+        in4 = __lsx_vld(_src1, 0);
+        DUP2_ARG2(__lsx_vldx, _src1, src2_stride_x, _src1, src2_stride_2x,
+                  in5, in6);
+        in7 = __lsx_vldx(_src1, src2_stride_3x);
+        _src1 += src2_stride_2x;
+        DUP2_ARG2(__lsx_vilvl_d, in5, in4, in7, in6, in4, in5);
+
+        DUP2_ARG2(__lsx_vilvl_b, src3, src2, src4, src3, src32_r, src43_r);
+        DUP2_ARG2(__lsx_vilvh_b, src3, src2, src4, src3, src32_l, src43_l);
+        src4332 = __lsx_vilvl_d(src43_l, src32_l);
+        DUP2_ARG2(__lsx_vilvl_b, src5, src4, src6, src5, src54_r, src65_r);
+        DUP2_ARG2(__lsx_vilvh_b, src5, src4, src6, src5, src54_l, src65_l);
+        src6554 = __lsx_vilvl_d(src65_l, src54_l);
+
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, src10_r, filt0, src21_r, filt0, src2110,
+                  filt0, src32_r, filt0, dst0_r, dst1_r, dst0_l, dst2_r);
+        DUP2_ARG2(__lsx_vdp2_h_bu_b, src43_r, filt0, src4332, filt0,
+                  dst3_r, dst1_l);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0_r, src32_r, filt1, dst1_r,
+                  src43_r, filt1, dst0_l, src4332, filt1, dst2_r, src54_r,
+                  filt1, dst0_r, dst1_r, dst0_l, dst2_r);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst3_r, src65_r, filt1, dst1_l,
+                  src6554, filt1, dst3_r, dst1_l);
+        dst0_r = hevc_bi_rnd_clip(in0, dst0_r, in1, dst1_r);
+        dst1_r = hevc_bi_rnd_clip(in2, dst2_r, in3, dst3_r);
+        dst0_l = hevc_bi_rnd_clip(in4, dst0_l, in5, dst1_l);
+        __lsx_vstelm_d(dst0_r, dst, 0, 0);
+        __lsx_vstelm_d(dst0_r, dst + dst_stride, 0, 1);
+        __lsx_vstelm_d(dst1_r, dst + dst_stride_2x, 0, 0);
+        __lsx_vstelm_d(dst1_r, dst + dst_stride_3x, 0, 1);
+        __lsx_vstelm_w(dst0_l, dst, 8, 0);
+        __lsx_vstelm_w(dst0_l, dst + dst_stride, 8, 1);
+        __lsx_vstelm_w(dst0_l, dst + dst_stride_2x, 8, 2);
+        __lsx_vstelm_w(dst0_l, dst + dst_stride_3x, 8, 3);
+        dst += dst_stride_4x;
+
+        src2 = src6;
+        src10_r = src54_r;
+        src21_r = src65_r;
+        src2110 = src6554;
+    }
+}
+
+static void hevc_vt_4t_16w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr, int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter, int32_t height)
+{
+    int32_t loop_cnt;
+    const int32_t src_stride_2x = (src_stride << 1);
+    const int32_t dst_stride_2x = (dst_stride << 1);
+    const int32_t src_stride_3x = src_stride_2x + src_stride;
+    __m128i src0, src1, src2, src3, src4, src5;
+    __m128i in0, in1, in2, in3;
+    __m128i src10_r, src32_r, src21_r, src43_r;
+    __m128i src10_l, src32_l, src21_l, src43_l;
+    __m128i dst0_r, dst1_r, dst0_l, dst1_l;
+    __m128i filt0, filt1;
+
+    src0_ptr -= src_stride;
+    DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
+
+    src0 = __lsx_vld(src0_ptr, 0);
+    DUP2_ARG2(__lsx_vldx, src0_ptr, src_stride, src0_ptr, src_stride_2x,
+              src1, src2);
+    src0_ptr += src_stride_3x;
+    DUP2_ARG2(__lsx_vilvl_b, src1, src0, src2, src1, src10_r, src21_r);
+    DUP2_ARG2(__lsx_vilvh_b, src1, src0, src2, src1, src10_l, src21_l);
+
+    for (loop_cnt = (height >> 2); loop_cnt--;) {
+        src3 = __lsx_vld(src0_ptr, 0);
+        src4 = __lsx_vldx(src0_ptr, src_stride);
+        src0_ptr += src_stride_2x;
+        DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, in0, in2);
+        src1_ptr += src2_stride;
+        DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, in1, in3);
+        src1_ptr += src2_stride;
+        DUP2_ARG2(__lsx_vilvl_b, src3, src2, src4, src3, src32_r, src43_r);
+        DUP2_ARG2(__lsx_vilvh_b, src3, src2, src4, src3, src32_l, src43_l);
+
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, src10_r, filt0, src21_r, filt0, src10_l,
+                  filt0, src21_l, filt0, dst0_r, dst1_r, dst0_l, dst1_l);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0_r, src32_r, filt1, dst1_r, src43_r,
+                  filt1, dst0_l, src32_l, filt1, dst1_l, src43_l, filt1,
+                  dst0_r, dst1_r, dst0_l, dst1_l);
+
+        dst0_r = hevc_bi_rnd_clip(in0, dst0_r, in2, dst0_l);
+        dst1_r = hevc_bi_rnd_clip(in1, dst1_r, in3, dst1_l);
+        __lsx_vst(dst0_r, dst, 0);
+        __lsx_vstx(dst1_r, dst, dst_stride);
+        dst += dst_stride_2x;
+
+        src5 = __lsx_vld(src0_ptr, 0);
+        src2 = __lsx_vldx(src0_ptr, src_stride);
+        src0_ptr += src_stride_2x;
+        DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, in0, in2);
+        src1_ptr += src2_stride;
+        DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, in1, in3);
+        src1_ptr += src2_stride;
+        DUP2_ARG2(__lsx_vilvl_b, src5, src4, src2, src5, src10_r, src21_r);
+        DUP2_ARG2(__lsx_vilvh_b, src5, src4, src2, src5, src10_l, src21_l);
+
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, src32_r, filt0, src32_l, filt0, src43_r,
+                  filt0, src43_l, filt0, dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0_r, src10_r, filt1, dst0_l,
+                  src10_l, filt1, dst1_r, src21_r, filt1, dst1_l, src21_l,
+                  filt1, dst0_r, dst0_l, dst1_r, dst1_l);
+        dst0_r = hevc_bi_rnd_clip(in0, dst0_r, in2, dst0_l);
+        dst1_r = hevc_bi_rnd_clip(in1, dst1_r, in3, dst1_l);
+        __lsx_vst(dst0_r, dst, 0);
+        __lsx_vstx(dst1_r, dst, dst_stride);
+        dst += dst_stride_2x;
+    }
+}
+
+static void hevc_vt_4t_24w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr, int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter, int32_t height)
+{
+    uint32_t loop_cnt;
+    int32_t dst_stride_2x = dst_stride << 1;
+    __m128i src0, src1, src2, src3, src4, src5;
+    __m128i src6, src7, src8, src9, src10, src11;
+    __m128i in0, in1, in2, in3, in4, in5;
+    __m128i src10_r, src32_r, src76_r, src98_r;
+    __m128i src21_r, src43_r, src87_r, src109_r;
+    __m128i src10_l, src32_l, src21_l, src43_l;
+    __m128i dst0_r, dst1_r, dst2_r, dst3_r;
+    __m128i dst0_l, dst1_l;
+    __m128i filt0, filt1;
+
+    src0_ptr -= src_stride;
+    DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
+
+    /* 16width */
+    DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src0, src6);
+    src0_ptr += src_stride;
+    DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src1, src7);
+    src0_ptr += src_stride;
+    DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src2, src8);
+    src0_ptr += src_stride;
+    DUP2_ARG2(__lsx_vilvl_b, src1, src0, src2, src1, src10_r, src21_r);
+    DUP2_ARG2(__lsx_vilvh_b, src1, src0, src2, src1, src10_l, src21_l);
+    /* 8width */
+    DUP2_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src76_r, src87_r);
+
+    for (loop_cnt = (height >> 2); loop_cnt--;) {
+        /* 16width */
+        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src3, src9);
+        src0_ptr += src_stride;
+        DUP2_ARG2(__lsx_vld, src0_ptr, 0, src0_ptr, 16, src4, src10);
+        src0_ptr += src_stride;
+        DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, in0, in2);
+        in4 = __lsx_vld(src1_ptr, 32);
+        src1_ptr += src2_stride;
+        DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr, 16, in1, in3);
+        in5 = __lsx_vld(src1_ptr, 32);
+        src1_ptr += src2_stride;
+        DUP2_ARG2(__lsx_vilvl_b, src3, src2, src4, src3, src32_r, src43_r);
+        DUP2_ARG2(__lsx_vilvh_b, src3, src2, src4, src3, src32_l, src43_l);
+        /* 8width */
+        DUP2_ARG2(__lsx_vilvl_b, src9, src8, src10, src9, src98_r, src109_r);
+        /* 16width */
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, src10_r, filt0, src10_l, filt0, src21_r,
+                  filt0, src21_l, filt0, dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0_r, src32_r, filt1,  dst0_l,
+                  src32_l, filt1, dst1_r, src43_r, filt1, dst1_l, src43_l, filt1,
+                  dst0_r, dst0_l, dst1_r, dst1_l);
+        /* 8width */
+        DUP2_ARG2(__lsx_vdp2_h_bu_b, src76_r, filt0, src87_r, filt0,
+                  dst2_r, dst3_r);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst2_r, src98_r, filt1, dst3_r,
+                  src109_r, filt1, dst2_r, dst3_r);
+        /* 16width */
+        dst0_r = hevc_bi_rnd_clip(in0, dst0_r, in2, dst0_l);
+        dst1_r = hevc_bi_rnd_clip(in1, dst1_r, in3, dst1_l);
+        dst2_r = hevc_bi_rnd_clip(in4, dst2_r, in5, dst3_r);
+        __lsx_vst(dst0_r, dst, 0);
+        __lsx_vstx(dst1_r, dst, dst_stride);
+        __lsx_vstelm_d(dst2_r, dst, 16, 0);
+        __lsx_vstelm_d(dst2_r, dst + dst_stride, 16, 1);
+        dst += dst_stride_2x;
+
+        /* 16width */
+        DUP4_ARG2(__lsx_vld, src0_ptr, 0, src1_ptr, 0, src1_ptr, 16, src1_ptr,
+                  32, src5, in0, in2, in4);
+        src1_ptr += src2_stride;
+        DUP4_ARG2(__lsx_vld, src0_ptr, 16,  src1_ptr, 0, src1_ptr, 16, src1_ptr,
+                  32, src11, in1, in3, in5);
+        src1_ptr += src2_stride;
+        src0_ptr += src_stride;
+        DUP2_ARG2(__lsx_vld, src0_ptr, 0,  src0_ptr, 16, src2, src8);
+        src0_ptr += src_stride;
+        DUP2_ARG2(__lsx_vilvl_b, src5, src4, src2, src5, src10_r, src21_r);
+        DUP2_ARG2(__lsx_vilvh_b, src5, src4, src2, src5, src10_l, src21_l);
+        /* 8width */
+        DUP2_ARG2(__lsx_vilvl_b, src11, src10, src8, src11, src76_r, src87_r);
+        /* 16width */
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, src32_r, filt0, src32_l, filt0, src43_r,
+                  filt0, src43_l, filt0, dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0_r, src10_r, filt1, dst0_l,
+                  src10_l, filt1, dst1_r, src21_r, filt1, dst1_l, src21_l,
+                  filt1, dst0_r, dst0_l, dst1_r, dst1_l);
+
+        /* 8width */
+        DUP2_ARG2(__lsx_vdp2_h_bu_b, src98_r, filt0, src109_r, filt0,
+                  dst2_r, dst3_r);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b,  dst2_r, src76_r, filt1, dst3_r,
+                  src87_r, filt1, dst2_r, dst3_r);
+
+        dst0_r = hevc_bi_rnd_clip(in0, dst0_r, in2, dst0_l);
+        dst1_r = hevc_bi_rnd_clip(in1, dst1_r, in3, dst1_l);
+        dst2_r = hevc_bi_rnd_clip(in4, dst2_r, in5, dst3_r);
+        __lsx_vst(dst0_r, dst, 0);
+        __lsx_vstx(dst1_r, dst, dst_stride);
+        __lsx_vstelm_d(dst2_r, dst, 16, 0);
+        __lsx_vstelm_d(dst2_r, dst + dst_stride, 16, 1);
+        dst += dst_stride_2x;
+    }
+}
+
+static void hevc_vt_4t_32w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr, int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter, int32_t height)
+{
+    hevc_vt_4t_16w_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                       dst, dst_stride, filter, height);
+    hevc_vt_4t_16w_lsx(src0_ptr + 16, src_stride, src1_ptr + 16, src2_stride,
+                       dst + 16, dst_stride, filter, height);
+}
+
+static void hevc_hv_4t_6w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                              int16_t *src1_ptr, int32_t src2_stride,
+                              uint8_t *dst, int32_t dst_stride,
+                              const int8_t *filter_x, const int8_t *filter_y,
+                              int32_t height)
+{
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t dst_stride_2x = (dst_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t dst_stride_4x = (dst_stride << 2);
+    int32_t src2_stride_2x = (src2_stride << 1);
+    int32_t src2_stride_4x = (src2_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    int32_t src2_stride_3x = src2_stride_2x + src2_stride;
+    __m128i out0, out1;
+    __m128i src0, src1, src2, src3, src4, src5, src6;
+    __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7, mask1;
+    __m128i filt0, filt1, filt_h0, filt_h1;
+    __m128i dsth0, dsth1, dsth2, dsth3, dsth4, dsth5;
+    __m128i dsth6, dsth7, dsth8, dsth9, dsth10;
+    __m128i dst0_r, dst0_l, dst1_r, dst1_l, dst2_r, dst2_l, dst3_r, dst3_l;
+    __m128i dst4_r, dst5_r, dst6_r, dst7_r;
+    __m128i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7, tmp8;
+    __m128i reg0, reg1, reg2, reg3;
+    __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
+
+    src0_ptr -= (src_stride + 1);
+    DUP2_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filt0, filt1);
+
+    filt_h1 = __lsx_vld(filter_y, 0);
+    filt_h1 = __lsx_vsllwil_h_b(filt_h1, 0);
+    DUP2_ARG2(__lsx_vreplvei_w, filt_h1, 0, filt_h1, 1, filt_h0, filt_h1);
+
+    mask1 = __lsx_vaddi_bu(mask0, 2);
+
+    src0 = __lsx_vld(src0_ptr, 0);
+    DUP2_ARG2(__lsx_vldx, src0_ptr, src_stride, src0_ptr, src_stride_2x,
+              src1, src2);
+    src0_ptr += src_stride_3x;
+
+    DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, vec0, vec1);
+    DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, vec2, vec3);
+    DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, vec4, vec5);
+
+    DUP2_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, dsth0, dsth1);
+    dsth2 = __lsx_vdp2_h_bu_b(vec4, filt0);
+    DUP2_ARG3(__lsx_vdp2add_h_bu_b, dsth0, vec1, filt1, dsth1, vec3, filt1,
+              dsth0, dsth1);
+    dsth2 = __lsx_vdp2add_h_bu_b(dsth2, vec5, filt1);
+
+    DUP2_ARG2(__lsx_vilvl_h, dsth1, dsth0, dsth2, dsth1, tmp0, tmp2);
+    DUP2_ARG2(__lsx_vilvh_h, dsth1, dsth0, dsth2, dsth1, tmp1, tmp3);
+
+    src3 = __lsx_vld(src0_ptr, 0);
+    DUP2_ARG2(__lsx_vldx, src0_ptr, src_stride, src0_ptr, src_stride_2x,
+              src4, src5);
+    src6 = __lsx_vldx(src0_ptr, src_stride_3x);
+    src0_ptr += src_stride_4x;
+    DUP2_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, vec0, vec1);
+    DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, vec2, vec3);
+    DUP2_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, vec4, vec5);
+    DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, vec6, vec7);
+
+    DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, vec4, filt0, vec6,
+              filt0, dsth3, dsth4, dsth5, dsth6);
+    DUP4_ARG3(__lsx_vdp2add_h_bu_b, dsth3, vec1, filt1, dsth4, vec3, filt1, dsth5,
+              vec5, filt1, dsth6, vec7, filt1, dsth3, dsth4, dsth5, dsth6);
+
+    src3 = __lsx_vld(src0_ptr, 0);
+    DUP2_ARG2(__lsx_vldx, src0_ptr, src_stride, src0_ptr, src_stride_2x,
+              src4, src5);
+    src6 = __lsx_vldx(src0_ptr, src_stride_3x);
+
+    DUP2_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, vec0, vec1);
+    DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, vec2, vec3);
+    DUP2_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, vec4, vec5);
+    DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, vec6, vec7);
+
+    DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, vec4, filt0, vec6,
+              filt0, dsth7, dsth8, dsth9, dsth10);
+    DUP4_ARG3(__lsx_vdp2add_h_bu_b, dsth7, vec1, filt1, dsth8, vec3, filt1, dsth9,
+              vec5, filt1, dsth10, vec7, filt1, dsth7, dsth8, dsth9, dsth10);
+
+    DUP2_ARG2(__lsx_vilvl_h, dsth3, dsth2, dsth4, dsth3, tmp4, tmp6);
+    DUP2_ARG2(__lsx_vilvh_h, dsth3, dsth2, dsth4, dsth3, tmp5, tmp7);
+    DUP2_ARG2(__lsx_vilvl_h, dsth5, dsth4, dsth6, dsth5, dsth0, dsth2);
+    DUP2_ARG2(__lsx_vilvh_h, dsth5, dsth4, dsth6, dsth5, dsth1, dsth3);
+    DUP4_ARG2(__lsx_vdp2_w_h, tmp0, filt_h0, tmp2, filt_h0, tmp4, filt_h0,
+              tmp6, filt_h0, dst0_r, dst1_r, dst2_r, dst3_r);
+    DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, tmp4, filt_h1, dst1_r, tmp6,
+              filt_h1, dst2_r, dsth0, filt_h1, dst3_r, dsth2, filt_h1,
+              dst0_r, dst1_r, dst2_r, dst3_r);
+    DUP2_ARG2(__lsx_vpickev_d, tmp3, tmp1, tmp7, tmp5, tmp0, tmp8);
+    dst0_l = __lsx_vdp2_w_h(tmp0, filt_h0);
+    dst0_l = __lsx_vdp2add_w_h(dst0_l, tmp8, filt_h1);
+
+    DUP2_ARG2(__lsx_vilvl_h, dsth7, dsth6, dsth8, dsth7, tmp0, tmp2);
+    DUP2_ARG2(__lsx_vilvh_h, dsth7, dsth6, dsth8, dsth7, tmp1, tmp3);
+    DUP2_ARG2(__lsx_vilvl_h, dsth9, dsth8, dsth10, dsth9, tmp4, tmp6);
+    DUP2_ARG2(__lsx_vilvh_h, dsth9, dsth8, dsth10, dsth9, tmp5, tmp7);
+    DUP4_ARG2(__lsx_vdp2_w_h, dsth0, filt_h0, dsth2, filt_h0, tmp0, filt_h0,
+              tmp2, filt_h0, dst4_r, dst5_r, dst6_r, dst7_r);
+    DUP4_ARG3(__lsx_vdp2add_w_h, dst4_r, tmp0, filt_h1, dst5_r, tmp2,
+              filt_h1, dst6_r, tmp4, filt_h1, dst7_r, tmp6, filt_h1,
+              dst4_r, dst5_r, dst6_r, dst7_r);
+    DUP2_ARG2(__lsx_vpickev_d, dsth3, dsth1, tmp3, tmp1, tmp0, tmp1);
+    tmp2 = __lsx_vpickev_d(tmp7, tmp5);
+
+    DUP2_ARG2(__lsx_vdp2_w_h, tmp8, filt_h0, tmp0, filt_h0, dst1_l, dst2_l);
+    dst3_l = __lsx_vdp2_w_h(tmp1, filt_h0);
+    DUP2_ARG3(__lsx_vdp2add_w_h, dst1_l, tmp0, filt_h1, dst2_l, tmp1, filt_h1,
+              dst1_l, dst2_l);
+    dst3_l = __lsx_vdp2add_w_h(dst3_l, tmp2, filt_h1);
+
+    DUP4_ARG2(__lsx_vsrai_d, dst0_r, 6, dst1_r, 6, dst2_r, 6, dst3_r, 6,
+              dst0_r, dst1_r, dst2_r, dst3_r);
+    DUP4_ARG2(__lsx_vsrai_d, dst4_r, 6, dst5_r, 6, dst6_r, 6, dst7_r, 6,
+              dst4_r, dst5_r, dst6_r, dst7_r);
+    DUP4_ARG2(__lsx_vsrai_d, dst0_l, 6, dst1_l, 6, dst2_l, 6, dst3_l, 6,
+              dst0_l, dst1_l, dst2_l, dst3_l);
+    DUP2_ARG2(__lsx_vpickev_h, dst1_r, dst0_r, dst3_r, dst2_r, tmp0, tmp1);
+    DUP2_ARG2(__lsx_vpickev_h, dst5_r, dst4_r, dst7_r, dst6_r, tmp2, tmp3);
+    DUP2_ARG2(__lsx_vpickev_h, dst1_l, dst0_l, dst3_l, dst2_l, tmp4, tmp5);
+
+    reg0 = __lsx_vldrepl_d(src1_ptr, 0);
+    reg1 = __lsx_vldrepl_d(src1_ptr + src2_stride, 0);
+    dsth0 = __lsx_vilvl_d(reg1, reg0);
+    reg0 = __lsx_vldrepl_d(src1_ptr + src2_stride_2x, 0);
+    reg1 = __lsx_vldrepl_d(src1_ptr + src2_stride_3x, 0);
+    dsth1 = __lsx_vilvl_d(reg1, reg0);
+    src1_ptr += src2_stride_4x;
+    reg0 = __lsx_vldrepl_d(src1_ptr, 0);
+    reg1 = __lsx_vldrepl_d(src1_ptr + src2_stride, 0);
+    dsth2 = __lsx_vilvl_d(reg1, reg0);
+    reg0 = __lsx_vldrepl_d(src1_ptr + src2_stride_2x, 0);
+    reg1 = __lsx_vldrepl_d(src1_ptr + src2_stride_3x, 0);
+    dsth3 = __lsx_vilvl_d(reg1, reg0);
+
+    DUP4_ARG2(__lsx_vsadd_h, dsth0, tmp0, dsth1, tmp1, dsth2, tmp2, dsth3,
+              tmp3, tmp0, tmp1, tmp2, tmp3);
+    DUP4_ARG2(__lsx_vmaxi_h, tmp0, 0, tmp1, 0, tmp2, 0, tmp3, 0,
+              tmp0, tmp1, tmp2, tmp3);
+    DUP2_ARG3(__lsx_vssrlrni_bu_h, tmp1, tmp0, 7, tmp3, tmp2, 7, out0, out1);
+
+    __lsx_vstelm_w(out0, dst, 0, 0);
+    __lsx_vstelm_w(out0, dst + dst_stride, 0, 1);
+    __lsx_vstelm_w(out0, dst + dst_stride_2x, 0, 2);
+    __lsx_vstelm_w(out0, dst + dst_stride_3x, 0, 3);
+    dst += dst_stride_4x;
+    __lsx_vstelm_w(out1, dst, 0, 0);
+    __lsx_vstelm_w(out1, dst + dst_stride, 0, 1);
+    __lsx_vstelm_w(out1, dst + dst_stride_2x, 0, 2);
+    __lsx_vstelm_w(out1, dst + dst_stride_3x, 0, 3);
+    dst -= dst_stride_4x;
+
+    src1_ptr -= src2_stride_4x;
+
+    reg0 = __lsx_vldrepl_w(src1_ptr, 8);
+    reg1 = __lsx_vldrepl_w(src1_ptr + src2_stride, 8);
+    reg2 = __lsx_vldrepl_w(src1_ptr + src2_stride_2x, 8);
+    reg3 = __lsx_vldrepl_w(src1_ptr + src2_stride_3x, 8);
+    DUP2_ARG2(__lsx_vilvl_w, reg1, reg0, reg3, reg2, tmp0, tmp1);
+    dsth4 = __lsx_vilvl_d(tmp1, tmp0);
+    src1_ptr += src2_stride_4x;
+
+    reg0 = __lsx_vldrepl_w(src1_ptr, 8);
+    reg1 = __lsx_vldrepl_w(src1_ptr + src2_stride, 8);
+    reg2 = __lsx_vldrepl_w(src1_ptr + src2_stride_2x, 8);
+    reg3 = __lsx_vldrepl_w(src1_ptr + src2_stride_3x, 8);
+    DUP2_ARG2(__lsx_vilvl_w, reg1, reg0, reg3, reg2, tmp0, tmp1);
+    dsth5 = __lsx_vilvl_d(tmp1, tmp0);
+    DUP2_ARG2(__lsx_vsadd_h, dsth4, tmp4, dsth5, tmp5, tmp4, tmp5);
+    DUP2_ARG2(__lsx_vmaxi_h, tmp4, 0, tmp5, 7, tmp4, tmp5);
+    out0 = __lsx_vssrlrni_bu_h(tmp5, tmp4, 7);
+
+    __lsx_vstelm_h(out0, dst, 4, 0);
+    __lsx_vstelm_h(out0, dst + dst_stride, 4, 1);
+    __lsx_vstelm_h(out0, dst + dst_stride_2x, 4, 2);
+    __lsx_vstelm_h(out0, dst + dst_stride_3x, 4, 3);
+    dst += dst_stride_4x;
+    __lsx_vstelm_h(out0, dst, 4, 4);
+    __lsx_vstelm_h(out0, dst + dst_stride, 4, 5);
+    __lsx_vstelm_h(out0, dst + dst_stride_2x, 4, 6);
+    __lsx_vstelm_h(out0, dst + dst_stride_3x, 4, 7);
+}
+
+static av_always_inline
+void hevc_hv_4t_8x2_lsx(uint8_t *src0_ptr, int32_t src_stride, int16_t *src1_ptr,
+                        int32_t src2_stride, uint8_t *dst, int32_t dst_stride,
+                        const int8_t *filter_x, const int8_t *filter_y)
+{
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+
+    __m128i out;
+    __m128i src0, src1, src2, src3, src4;
+    __m128i filt0, filt1;
+    __m128i filt_h0, filt_h1;
+    __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
+    __m128i mask1, filter_vec;
+    __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7, vec8, vec9;
+    __m128i dst0, dst1, dst2, dst3, dst4;
+    __m128i dst0_r, dst0_l, dst1_r, dst1_l;
+    __m128i dst10_r, dst32_r, dst21_r, dst43_r;
+    __m128i dst10_l, dst32_l, dst21_l, dst43_l;
+    __m128i tmp0, tmp1;
+    __m128i in0, in1;
+
+    src0_ptr -= (src_stride + 1);
+    DUP2_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filt0, filt1);
+
+    filter_vec = __lsx_vld(filter_y, 0);
+    filter_vec = __lsx_vsllwil_h_b(filter_vec, 0);
+    DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
+
+    mask1 = __lsx_vaddi_bu(mask0, 2);
+
+    src0 = __lsx_vld(src0_ptr, 0);
+    DUP4_ARG2(__lsx_vldx, src0_ptr, src_stride, src0_ptr, src_stride_2x,
+              src0_ptr, src_stride_3x, src0_ptr, src_stride_4x,
+              src1, src2, src3, src4);
+
+    DUP2_ARG2(__lsx_vld, src1_ptr, 0, src1_ptr + src2_stride, 0, in0, in1);
+
+    DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, vec0, vec1);
+    DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, vec2, vec3);
+    DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, vec4, vec5);
+    DUP2_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, vec6, vec7);
+    DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, vec8, vec9);
+
+    DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, vec4, filt0, vec6,
+              filt0, dst0, dst1, dst2, dst3);
+    dst4 = __lsx_vdp2_h_bu_b(vec8, filt0);
+    DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec1, filt1, dst1, vec3, filt1, dst2,
+              vec5, filt1, dst3, vec7, filt1, dst0, dst1, dst2, dst3);
+    dst4 = __lsx_vdp2add_h_bu_b(dst4, vec9, filt1);
+
+    DUP2_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst10_r, dst21_r);
+    DUP2_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst10_l, dst21_l);
+    DUP2_ARG2(__lsx_vilvl_h, dst3, dst2, dst4, dst3, dst32_r, dst43_r);
+    DUP2_ARG2(__lsx_vilvh_h, dst3, dst2, dst4, dst3, dst32_l, dst43_l);
+    DUP4_ARG2(__lsx_vdp2_w_h, dst10_r, filt_h0, dst10_l, filt_h0, dst21_r,
+              filt_h0, dst21_l, filt_h0, dst0_r, dst0_l, dst1_r, dst1_l);
+    DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst32_r, filt_h1, dst0_l, dst32_l,
+              filt_h1, dst1_r, dst43_r, filt_h1, dst1_l, dst43_l, filt_h1,
+              dst0_r, dst0_l, dst1_r, dst1_l);
+    DUP4_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst1_r, 6, dst1_l, 6,
+              dst0_r, dst0_l, dst1_r, dst1_l);
+    DUP2_ARG2(__lsx_vpickev_h, dst0_l, dst0_r, dst1_l, dst1_r, tmp0, tmp1);
+    DUP2_ARG2(__lsx_vsadd_h, in0, tmp0, in1, tmp1, tmp0, tmp1);
+    DUP2_ARG2(__lsx_vmaxi_h, tmp0, 0, tmp1, 0, tmp0, tmp1);
+    out = __lsx_vssrlrni_bu_h(tmp1, tmp0, 7);
+    __lsx_vstelm_d(out, dst, 0, 0);
+    __lsx_vstelm_d(out, dst + dst_stride, 0, 1);
+}
+
+static av_always_inline
+void hevc_hv_4t_8multx4_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                            int16_t *src1_ptr, int32_t src2_stride,
+                            uint8_t *dst, int32_t dst_stride,
+                            const int8_t *filter_x, const int8_t *filter_y,
+                            int32_t width8mult)
+{
+    uint32_t cnt;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t dst_stride_2x = (dst_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t src2_stride_x = (src2_stride << 1);
+    int32_t src2_stride_2x = (src2_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    int32_t src2_stride_3x = src2_stride_2x + src2_stride_x;
+
+    __m128i out0, out1;
+    __m128i src0, src1, src2, src3, src4, src5, src6, mask0, mask1;
+    __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
+    __m128i filt0, filt1, filt_h0, filt_h1, filter_vec;
+    __m128i dst0, dst1, dst2, dst3, dst4, dst5, dst6, tmp0, tmp1, tmp2, tmp3;
+    __m128i in0, in1, in2, in3;
+    __m128i dst0_r, dst0_l, dst1_r, dst1_l, dst2_r, dst2_l, dst3_r, dst3_l;
+    __m128i dst10_r, dst32_r, dst54_r, dst21_r, dst43_r, dst65_r;
+    __m128i dst10_l, dst32_l, dst54_l, dst21_l, dst43_l, dst65_l;
+
+    src0_ptr -= (src_stride + 1);
+    DUP2_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filt0, filt1);
+
+    filter_vec = __lsx_vld(filter_y, 0);
+    filter_vec = __lsx_vsllwil_h_b(filter_vec, 0);
+    DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
+
+    mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
+    mask1 = __lsx_vaddi_bu(mask0, 2);
+
+    for (cnt = width8mult; cnt--;) {
+        src0 = __lsx_vld(src0_ptr, 0);
+        DUP2_ARG2(__lsx_vldx, src0_ptr, src_stride, src0_ptr, src_stride_2x,
+                  src1, src2);
+        src3 = __lsx_vldx(src0_ptr, src_stride_3x);
+        src0_ptr += src_stride_4x;
+        src4 = __lsx_vld(src0_ptr, 0);
+        DUP2_ARG2(__lsx_vldx, src0_ptr, src_stride, src0_ptr, src_stride_2x,
+                  src5, src6);
+        src0_ptr += (8 - src_stride_4x);
+
+        in0 = __lsx_vld(src1_ptr, 0);
+        DUP2_ARG2(__lsx_vldx, src1_ptr, src2_stride_x, src1_ptr,
+                  src2_stride_2x, in1, in2);
+        in3 = __lsx_vldx(src1_ptr, src2_stride_3x);
+        src1_ptr += 8;
+
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1,
+                  vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1,
+                  vec4, vec5);
+
+        DUP2_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, dst0, dst1);
+        dst2 = __lsx_vdp2_h_bu_b(vec4, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec1, filt1, dst1, vec3, filt1,
+                  dst0, dst1);
+        dst2 = __lsx_vdp2add_h_bu_b(dst2, vec5, filt1);
+
+        DUP2_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst10_r, dst21_r);
+        DUP2_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst10_l, dst21_l);
+
+        DUP2_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1,
+                  vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1,
+                  vec4, vec5);
+        DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1,
+                  vec6, vec7);
+
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, vec4, filt0,
+                  vec6, filt0, dst3, dst4, dst5, dst6);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst3, vec1, filt1, dst4, vec3, filt1,
+                  dst5, vec5, filt1, dst6, vec7, filt1, dst3, dst4, dst5, dst6);
+
+        DUP2_ARG2(__lsx_vilvl_h, dst3, dst2, dst4, dst3, dst32_r, dst43_r);
+        DUP2_ARG2(__lsx_vilvh_h, dst3, dst2, dst4, dst3, dst32_l, dst43_l);
+        DUP2_ARG2(__lsx_vilvl_h, dst5, dst4, dst6, dst5, dst54_r, dst65_r);
+        DUP2_ARG2(__lsx_vilvh_h, dst5, dst4, dst6, dst5, dst54_l, dst65_l);
+
+        DUP4_ARG2(__lsx_vdp2_w_h, dst10_r, filt_h0, dst10_l, filt_h0, dst21_r,
+                  filt_h0, dst21_l, filt_h0, dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP4_ARG2(__lsx_vdp2_w_h, dst32_r, filt_h0, dst32_l, filt_h0, dst43_r,
+                  filt_h0, dst43_l, filt_h0, dst2_r, dst2_l, dst3_r, dst3_l);
+        DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst32_r, filt_h1, dst0_l, dst32_l,
+                  filt_h1, dst1_r, dst43_r, filt_h1, dst1_l, dst43_l, filt_h1,
+                  dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP4_ARG3(__lsx_vdp2add_w_h, dst2_r, dst54_r, filt_h1, dst2_l, dst54_l,
+                  filt_h1, dst3_r, dst65_r, filt_h1, dst3_l, dst65_l, filt_h1,
+                  dst2_r, dst2_l, dst3_r, dst3_l);
+
+        DUP4_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst1_r, 6, dst1_l, 6,
+                  dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP4_ARG2(__lsx_vsrai_w, dst2_r, 6, dst2_l, 6, dst3_r, 6, dst3_l, 6,
+                  dst2_r, dst2_l, dst3_r, dst3_l);
+        DUP4_ARG2(__lsx_vpickev_h, dst0_l, dst0_r, dst1_l, dst1_r, dst2_l,
+                  dst2_r, dst3_l, dst3_r, tmp0, tmp1, tmp2, tmp3);
+        DUP4_ARG2(__lsx_vsadd_h, in0, tmp0, in1, tmp1, in2, tmp2, in3, tmp3,
+                  tmp0, tmp1, tmp2, tmp3);
+        DUP4_ARG2(__lsx_vmaxi_h, tmp0, 0, tmp1, 0, tmp2, 0, tmp3, 0,
+                  tmp0, tmp1, tmp2, tmp3);
+        DUP2_ARG3(__lsx_vssrlrni_bu_h, tmp1, tmp0, 7, tmp3, tmp2, 7, out0, out1);
+        __lsx_vstelm_d(out0, dst, 0, 0);
+        __lsx_vstelm_d(out0, dst + dst_stride, 0, 1);
+        __lsx_vstelm_d(out1, dst + dst_stride_2x, 0, 0);
+        __lsx_vstelm_d(out1, dst + dst_stride_3x, 0, 1);
+        dst += 8;
+    }
+}
+
+static av_always_inline
+void hevc_hv_4t_8x6_lsx(uint8_t *src0_ptr, int32_t src_stride, int16_t *src1_ptr,
+                        int32_t src2_stride, uint8_t *dst, int32_t dst_stride,
+                        const int8_t *filter_x, const int8_t *filter_y)
+{
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t dst_stride_2x = (dst_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t dst_stride_4x = (dst_stride << 2);
+    int32_t src2_stride_x = (src2_stride << 1);
+    int32_t src2_stride_2x = (src2_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    int32_t src2_stride_3x = src2_stride_2x + src2_stride_x;
+
+    __m128i out0, out1, out2;
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8;
+    __m128i in0, in1, in2, in3, in4, in5;
+    __m128i filt0, filt1;
+    __m128i filt_h0, filt_h1;
+    __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
+    __m128i mask1, filter_vec;
+    __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7, vec8, vec9;
+    __m128i vec10, vec11, vec12, vec13, vec14, vec15, vec16, vec17;
+    __m128i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5;
+    __m128i dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7, dst8;
+    __m128i dst0_r, dst0_l, dst1_r, dst1_l, dst2_r, dst2_l, dst3_r, dst3_l;
+    __m128i dst4_r, dst4_l, dst5_r, dst5_l;
+    __m128i dst10_r, dst32_r, dst10_l, dst32_l;
+    __m128i dst21_r, dst43_r, dst21_l, dst43_l;
+    __m128i dst54_r, dst54_l, dst65_r, dst65_l;
+    __m128i dst76_r, dst76_l, dst87_r, dst87_l;
+
+    src0_ptr -= (src_stride + 1);
+    DUP2_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filt0, filt1);
+
+    filter_vec = __lsx_vld(filter_y, 0);
+    filter_vec = __lsx_vsllwil_h_b(filter_vec, 0);
+    DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
+
+    mask1 = __lsx_vaddi_bu(mask0, 2);
+
+    src0 = __lsx_vld(src0_ptr, 0);
+    DUP2_ARG2(__lsx_vldx, src0_ptr, src_stride, src0_ptr, src_stride_2x,
+              src1, src2);
+    src3 = __lsx_vldx(src0_ptr, src_stride_3x);
+    src0_ptr += src_stride_4x;
+    src4 = __lsx_vld(src0_ptr, 0);
+    DUP4_ARG2(__lsx_vldx, src0_ptr, src_stride, src0_ptr, src_stride_2x,
+              src0_ptr, src_stride_3x, src0_ptr, src_stride_4x,
+              src5, src6, src7, src8);
+
+    in0 = __lsx_vld(src1_ptr, 0);
+    DUP2_ARG2(__lsx_vldx, src1_ptr, src2_stride_x, src1_ptr, src2_stride_2x,
+              in1, in2);
+    in3 = __lsx_vldx(src1_ptr, src2_stride_3x);
+    src1_ptr += src2_stride_2x;
+    in4 = __lsx_vld(src1_ptr, 0);
+    in5 = __lsx_vldx(src1_ptr, src2_stride_x);
+
+    DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, vec0, vec1);
+    DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, vec2, vec3);
+    DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, vec4, vec5);
+    DUP2_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, vec6, vec7);
+    DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, vec8, vec9);
+    DUP2_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, vec10, vec11);
+    DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, vec12, vec13);
+    DUP2_ARG3(__lsx_vshuf_b, src7, src7, mask0, src7, src7, mask1, vec14, vec15);
+    DUP2_ARG3(__lsx_vshuf_b, src8, src8, mask0, src8, src8, mask1, vec16, vec17);
+
+    DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, vec4, filt0, vec6,
+              filt0, dst0, dst1, dst2, dst3);
+    dst4 = __lsx_vdp2_h_bu_b(vec8, filt0);
+    DUP4_ARG2(__lsx_vdp2_h_bu_b, vec10, filt0, vec12, filt0, vec14, filt0,
+              vec16, filt0, dst5, dst6, dst7, dst8);
+    DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec1, filt1, dst1, vec3, filt1, dst2,
+              vec5, filt1, dst3, vec7, filt1, dst0, dst1, dst2, dst3);
+    dst4 = __lsx_vdp2add_h_bu_b(dst4, vec9, filt1);
+    DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst5, vec11, filt1, dst6, vec13, filt1,
+              dst7, vec15, filt1, dst8, vec17, filt1, dst5, dst6, dst7, dst8);
+
+    DUP4_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst3, dst2, dst4, dst3,
+              dst10_r, dst21_r, dst32_r, dst43_r);
+    DUP4_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst3, dst2, dst4, dst3,
+              dst10_l, dst21_l, dst32_l, dst43_l);
+    DUP4_ARG2(__lsx_vilvl_h, dst5, dst4, dst6, dst5, dst7, dst6, dst8, dst7,
+              dst54_r, dst65_r, dst76_r, dst87_r);
+    DUP4_ARG2(__lsx_vilvh_h, dst5, dst4, dst6, dst5, dst7, dst6, dst8, dst7,
+              dst54_l, dst65_l, dst76_l, dst87_l);
+
+    DUP4_ARG2(__lsx_vdp2_w_h, dst10_r, filt_h0, dst10_l, filt_h0, dst21_r,
+              filt_h0, dst21_l, filt_h0, dst0_r, dst0_l, dst1_r, dst1_l);
+    DUP4_ARG2(__lsx_vdp2_w_h, dst32_r, filt_h0, dst32_l, filt_h0, dst43_r,
+              filt_h0, dst43_l, filt_h0, dst2_r, dst2_l, dst3_r, dst3_l);
+    DUP4_ARG2(__lsx_vdp2_w_h, dst54_r, filt_h0, dst54_l, filt_h0, dst65_r,
+              filt_h0, dst65_l, filt_h0, dst4_r, dst4_l, dst5_r, dst5_l);
+    DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst32_r, filt_h1, dst0_l, dst32_l,
+              filt_h1, dst1_r, dst43_r, filt_h1, dst1_l, dst43_l, filt_h1,
+              dst0_r, dst0_l, dst1_r, dst1_l);
+    DUP4_ARG3(__lsx_vdp2add_w_h, dst2_r, dst54_r, filt_h1, dst2_l, dst54_l,
+              filt_h1, dst3_r, dst65_r, filt_h1, dst3_l, dst65_l, filt_h1,
+              dst2_r, dst2_l, dst3_r, dst3_l);
+    DUP4_ARG3(__lsx_vdp2add_w_h, dst4_r, dst76_r, filt_h1, dst4_l, dst76_l,
+              filt_h1, dst5_r, dst87_r, filt_h1, dst5_l, dst87_l, filt_h1,
+              dst4_r, dst4_l, dst5_r, dst5_l);
+
+    DUP4_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst1_r, 6, dst1_l, 6,
+              dst0_r, dst0_l, dst1_r, dst1_l);
+    DUP4_ARG2(__lsx_vsrai_w, dst2_r, 6, dst2_l, 6, dst3_r, 6, dst3_l, 6,
+              dst2_r, dst2_l, dst3_r, dst3_l);
+    DUP4_ARG2(__lsx_vsrai_w, dst4_r, 6, dst4_l, 6, dst5_r, 6, dst5_l, 6,
+              dst4_r, dst4_l, dst5_r, dst5_l);
+    DUP4_ARG2(__lsx_vpickev_h, dst0_l, dst0_r, dst1_l, dst1_r, dst2_l, dst2_r,
+              dst3_l, dst3_r, tmp0, tmp1, tmp2, tmp3);
+    DUP2_ARG2(__lsx_vpickev_h, dst4_l, dst4_r, dst5_l, dst5_r, tmp4, tmp5);
+    DUP4_ARG2(__lsx_vsadd_h, in0, tmp0, in1, tmp1, in2, tmp2, in3, tmp3,
+              tmp0, tmp1, tmp2, tmp3);
+    DUP2_ARG2(__lsx_vsadd_h, in4, tmp4, in5, tmp5, tmp4, tmp5);
+    DUP4_ARG2(__lsx_vmaxi_h, tmp0, 0, tmp1, 0, tmp2, 0, tmp3, 0,
+              tmp0, tmp1, tmp2, tmp3);
+    DUP2_ARG2(__lsx_vmaxi_h, tmp4, 0, tmp5, 0, tmp4, tmp5);
+    DUP2_ARG3(__lsx_vssrlrni_bu_h, tmp1, tmp0, 7, tmp3, tmp2, 7, out0, out1);
+    out2 = __lsx_vssrlrni_bu_h(tmp5, tmp4, 7);
+    __lsx_vstelm_d(out0, dst, 0, 0);
+    __lsx_vstelm_d(out0, dst + dst_stride, 0, 1);
+    __lsx_vstelm_d(out1, dst + dst_stride_2x, 0, 0);
+    __lsx_vstelm_d(out1, dst + dst_stride_3x, 0, 1);
+    dst += dst_stride_4x;
+    __lsx_vstelm_d(out2, dst, 0, 0);
+    __lsx_vstelm_d(out2, dst + dst_stride, 0, 1);
+}
+
+static av_always_inline
+void hevc_hv_4t_8multx4mult_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                                int16_t *src1_ptr, int32_t src2_stride,
+                                uint8_t *dst, int32_t dst_stride,
+                                const int8_t *filter_x, const int8_t *filter_y,
+                                int32_t height, int32_t width)
+{
+    uint32_t loop_cnt, cnt;
+    uint8_t *src0_ptr_tmp;
+    int16_t *src1_ptr_tmp;
+    uint8_t *dst_tmp;
+    const int32_t src_stride_2x = (src_stride << 1);
+    const int32_t dst_stride_2x = (dst_stride << 1);
+    const int32_t src_stride_4x = (src_stride << 2);
+    const int32_t dst_stride_4x = (dst_stride << 2);
+    const int32_t src2_stride_x = (src2_stride << 1);
+    const int32_t src2_stride_2x = (src2_stride << 2);
+    const int32_t src_stride_3x = src_stride_2x + src_stride;
+    const int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    const int32_t src2_stride_3x = src2_stride_2x + src2_stride_x;
+    __m128i out0, out1;
+    __m128i src0, src1, src2, src3, src4, src5, src6;
+    __m128i in0, in1, in2, in3;
+    __m128i filt0, filt1;
+    __m128i filt_h0, filt_h1;
+    __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
+    __m128i mask1, filter_vec;
+    __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
+    __m128i dst0, dst1, dst2, dst3, dst4, dst5;
+    __m128i dst0_r, dst0_l, dst1_r, dst1_l, dst2_r, dst2_l, dst3_r, dst3_l;
+    __m128i tmp0, tmp1, tmp2, tmp3;
+    __m128i dst10_r, dst32_r, dst21_r, dst43_r;
+    __m128i dst10_l, dst32_l, dst21_l, dst43_l;
+    __m128i dst54_r, dst54_l, dst65_r, dst65_l, dst6;
+
+    src0_ptr -= (src_stride + 1);
+
+    DUP2_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filt0, filt1);
+
+    filter_vec = __lsx_vld(filter_y, 0);
+    filter_vec = __lsx_vsllwil_h_b(filter_vec, 0);
+
+    DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
+
+    mask1 = __lsx_vaddi_bu(mask0, 2);
+
+    for (cnt = width >> 3; cnt--;) {
+        src0_ptr_tmp = src0_ptr;
+        dst_tmp = dst;
+        src1_ptr_tmp = src1_ptr;
+
+        src0 = __lsx_vld(src0_ptr_tmp, 0);
+        DUP2_ARG2(__lsx_vldx, src0_ptr_tmp, src_stride, src0_ptr_tmp,
+                  src_stride_2x, src1, src2);
+        src0_ptr_tmp += src_stride_3x;
+
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1,
+                  vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1,
+                  vec4, vec5);
+
+        DUP2_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, dst0, dst1);
+        dst2 = __lsx_vdp2_h_bu_b(vec4, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec1, filt1, dst1, vec3, filt1,
+                  dst0, dst1);
+        dst2 = __lsx_vdp2add_h_bu_b(dst2, vec5, filt1);
+
+        DUP2_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst10_r, dst21_r);
+        DUP2_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst10_l, dst21_l);
+
+        for (loop_cnt = height >> 2; loop_cnt--;) {
+            src3 = __lsx_vld(src0_ptr_tmp, 0);
+            DUP2_ARG2(__lsx_vldx, src0_ptr_tmp, src_stride, src0_ptr_tmp,
+                      src_stride_2x, src4, src5);
+            src6 = __lsx_vldx(src0_ptr_tmp, src_stride_3x);
+            src0_ptr_tmp += src_stride_4x;
+            in0 = __lsx_vld(src1_ptr_tmp, 0);
+            DUP2_ARG2(__lsx_vldx, src1_ptr_tmp, src2_stride_x, src1_ptr_tmp,
+                      src2_stride_2x, in1, in2);
+            in3 = __lsx_vldx(src1_ptr_tmp, src2_stride_3x);
+            src1_ptr_tmp += src2_stride_2x;
+
+            DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src4,
+                      src4, mask0, src4, src4, mask1, vec0, vec1, vec2, vec3);
+            DUP4_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, src6,
+                      src6, mask0, src6, src6, mask1, vec4, vec5, vec6, vec7);
+
+            DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, vec4, filt0,
+                      vec6, filt0, dst3, dst4, dst5, dst6);
+            DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst3, vec1, filt1, dst4, vec3,
+                      filt1, dst5, vec5, filt1, dst6, vec7, filt1,
+                      dst3, dst4, dst5, dst6);
+
+            DUP2_ARG2(__lsx_vilvl_h, dst3, dst2, dst4, dst3, dst32_r, dst43_r);
+            DUP2_ARG2(__lsx_vilvh_h, dst3, dst2, dst4, dst3, dst32_l, dst43_l);
+            DUP2_ARG2(__lsx_vilvl_h, dst5, dst4, dst6, dst5, dst54_r, dst65_r);
+            DUP2_ARG2(__lsx_vilvh_h, dst5, dst4, dst6, dst5, dst54_l, dst65_l);
+
+            DUP4_ARG2(__lsx_vdp2_w_h, dst10_r, filt_h0, dst10_l, filt_h0, dst21_r,
+                      filt_h0, dst21_l, filt_h0, dst0_r, dst0_l, dst1_r, dst1_l);
+            DUP4_ARG2(__lsx_vdp2_w_h, dst32_r, filt_h0, dst32_l, filt_h0, dst43_r,
+                      filt_h0, dst43_l, filt_h0, dst2_r, dst2_l, dst3_r, dst3_l);
+            DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst32_r, filt_h1, dst0_l,
+                      dst32_l, filt_h1, dst1_r, dst43_r, filt_h1, dst1_l,
+                      dst43_l, filt_h1, dst0_r, dst0_l, dst1_r, dst1_l);
+            DUP4_ARG3(__lsx_vdp2add_w_h, dst2_r, dst54_r, filt_h1, dst2_l,
+                      dst54_l, filt_h1, dst3_r, dst65_r, filt_h1, dst3_l,
+                      dst65_l, filt_h1, dst2_r, dst2_l, dst3_r, dst3_l);
+
+            DUP4_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst1_r, 6, dst1_l, 6,
+                      dst0_r, dst0_l, dst1_r, dst1_l);
+            DUP4_ARG2(__lsx_vsrai_w, dst2_r, 6, dst2_l, 6, dst3_r, 6, dst3_l, 6,
+                      dst2_r, dst2_l, dst3_r, dst3_l);
+            DUP4_ARG2(__lsx_vpickev_h, dst0_l, dst0_r, dst1_l, dst1_r, dst2_l,
+                      dst2_r, dst3_l, dst3_r, tmp0, tmp1, tmp2, tmp3);
+            DUP4_ARG2(__lsx_vsadd_h, in0, tmp0, in1, tmp1, in2, tmp2, in3, tmp3,
+                      tmp0, tmp1, tmp2, tmp3);
+            DUP4_ARG2(__lsx_vmaxi_h, tmp0, 0, tmp1, 0, tmp2, 0, tmp3, 0, tmp0,
+                      tmp1, tmp2, tmp3);
+            DUP2_ARG3(__lsx_vssrlrni_bu_h, tmp1, tmp0, 7, tmp3, tmp2, 7, out0, out1);
+            __lsx_vstelm_d(out0, dst_tmp, 0, 0);
+            __lsx_vstelm_d(out0, dst_tmp + dst_stride, 0, 1);
+            __lsx_vstelm_d(out1, dst_tmp + dst_stride_2x, 0, 0);
+            __lsx_vstelm_d(out1, dst_tmp + dst_stride_3x, 0, 1);
+            dst_tmp += dst_stride_4x;
+
+            dst10_r = dst54_r;
+            dst10_l = dst54_l;
+            dst21_r = dst65_r;
+            dst21_l = dst65_l;
+            dst2 = dst6;
+        }
+
+        src0_ptr += 8;
+        dst += 8;
+        src1_ptr += 8;
+    }
+}
+
+static void hevc_hv_4t_8w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                              int16_t *src1_ptr, int32_t src2_stride,
+                              uint8_t *dst, int32_t dst_stride,
+                              const int8_t *filter_x, const int8_t *filter_y,
+                              int32_t height)
+{
+    if (2 == height) {
+        hevc_hv_4t_8x2_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                           dst, dst_stride, filter_x, filter_y);
+    } else if (4 == height) {
+        hevc_hv_4t_8multx4_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                               dst, dst_stride, filter_x, filter_y, 1);
+    } else if (6 == height) {
+        hevc_hv_4t_8x6_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                           dst, dst_stride, filter_x, filter_y);
+    } else {
+        hevc_hv_4t_8multx4mult_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                                dst, dst_stride, filter_x, filter_y, height, 8);
+    }
+}
+
+static void hevc_hv_4t_16w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr, int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter_x, const int8_t *filter_y,
+                               int32_t height)
+{
+    if (4 == height) {
+        hevc_hv_4t_8multx4_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                               dst, dst_stride, filter_x, filter_y, 2);
+    } else {
+        hevc_hv_4t_8multx4mult_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                                dst, dst_stride, filter_x, filter_y, height, 16);
+    }
+}
+
+static void hevc_hv_4t_24w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr, int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter_x, const int8_t *filter_y,
+                               int32_t height)
+{
+    hevc_hv_4t_8multx4mult_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                            dst, dst_stride, filter_x, filter_y, height, 24);
+}
+
+static void hevc_hv_4t_32w_lsx(uint8_t *src0_ptr, int32_t src_stride,
+                               int16_t *src1_ptr, int32_t src2_stride,
+                               uint8_t *dst, int32_t dst_stride,
+                               const int8_t *filter_x, const int8_t *filter_y,
+                               int32_t height)
+{
+    hevc_hv_4t_8multx4mult_lsx(src0_ptr, src_stride, src1_ptr, src2_stride,
+                            dst, dst_stride, filter_x, filter_y, height, 32);
+}
+
+#define BI_MC_COPY(WIDTH)                                                 \
+void ff_hevc_put_hevc_bi_pel_pixels##WIDTH##_8_lsx(uint8_t *dst,          \
+                                                   ptrdiff_t dst_stride,  \
+                                                   uint8_t *src,          \
+                                                   ptrdiff_t src_stride,  \
+                                                   int16_t *src_16bit,    \
+                                                   int height,            \
+                                                   intptr_t mx,           \
+                                                   intptr_t my,           \
+                                                   int width)             \
+{                                                                         \
+    hevc_bi_copy_##WIDTH##w_lsx(src, src_stride, src_16bit, MAX_PB_SIZE,  \
+                                dst, dst_stride, height);                 \
+}
+
+BI_MC_COPY(4);
+BI_MC_COPY(6);
+BI_MC_COPY(8);
+BI_MC_COPY(12);
+BI_MC_COPY(16);
+BI_MC_COPY(24);
+BI_MC_COPY(32);
+BI_MC_COPY(48);
+BI_MC_COPY(64);
+
+#undef BI_MC_COPY
+
+#define BI_MC(PEL, DIR, WIDTH, TAP, DIR1, FILT_DIR)                          \
+void ff_hevc_put_hevc_bi_##PEL##_##DIR##WIDTH##_8_lsx(uint8_t *dst,          \
+                                                      ptrdiff_t dst_stride,  \
+                                                      uint8_t *src,          \
+                                                      ptrdiff_t src_stride,  \
+                                                      int16_t *src_16bit,    \
+                                                      int height,            \
+                                                      intptr_t mx,           \
+                                                      intptr_t my,           \
+                                                      int width)             \
+{                                                                            \
+    const int8_t *filter = ff_hevc_##PEL##_filters[FILT_DIR - 1];            \
+                                                                             \
+    hevc_##DIR1##_##TAP##t_##WIDTH##w_lsx(src, src_stride, src_16bit,        \
+                                          MAX_PB_SIZE, dst, dst_stride,      \
+                                          filter, height);                   \
+}
+
+BI_MC(qpel, h, 16, 8, hz, mx);
+BI_MC(qpel, h, 24, 8, hz, mx);
+BI_MC(qpel, h, 32, 8, hz, mx);
+BI_MC(qpel, h, 48, 8, hz, mx);
+BI_MC(qpel, h, 64, 8, hz, mx);
+
+BI_MC(qpel, v, 8, 8, vt, my);
+BI_MC(qpel, v, 16, 8, vt, my);
+BI_MC(qpel, v, 24, 8, vt, my);
+BI_MC(qpel, v, 32, 8, vt, my);
+BI_MC(qpel, v, 48, 8, vt, my);
+BI_MC(qpel, v, 64, 8, vt, my);
+
+BI_MC(epel, h, 24, 4, hz, mx);
+BI_MC(epel, h, 32, 4, hz, mx);
+
+BI_MC(epel, v, 12, 4, vt, my);
+BI_MC(epel, v, 16, 4, vt, my);
+BI_MC(epel, v, 24, 4, vt, my);
+BI_MC(epel, v, 32, 4, vt, my);
+
+#undef BI_MC
+
+#define BI_MC_HV(PEL, WIDTH, TAP)                                         \
+void ff_hevc_put_hevc_bi_##PEL##_hv##WIDTH##_8_lsx(uint8_t *dst,          \
+                                                   ptrdiff_t dst_stride,  \
+                                                   uint8_t *src,          \
+                                                   ptrdiff_t src_stride,  \
+                                                   int16_t *src_16bit,    \
+                                                   int height,            \
+                                                   intptr_t mx,           \
+                                                   intptr_t my,           \
+                                                   int width)             \
+{                                                                         \
+    const int8_t *filter_x = ff_hevc_##PEL##_filters[mx - 1];             \
+    const int8_t *filter_y = ff_hevc_##PEL##_filters[my - 1];             \
+                                                                          \
+    hevc_hv_##TAP##t_##WIDTH##w_lsx(src, src_stride, src_16bit,           \
+                                    MAX_PB_SIZE, dst, dst_stride,         \
+                                    filter_x, filter_y, height);          \
+}
+
+BI_MC_HV(qpel, 8, 8);
+BI_MC_HV(qpel, 16, 8);
+BI_MC_HV(qpel, 24, 8);
+BI_MC_HV(qpel, 32, 8);
+BI_MC_HV(qpel, 48, 8);
+BI_MC_HV(qpel, 64, 8);
+
+BI_MC_HV(epel, 8, 4);
+BI_MC_HV(epel, 6, 4);
+BI_MC_HV(epel, 16, 4);
+BI_MC_HV(epel, 24, 4);
+BI_MC_HV(epel, 32, 4);
+
+#undef BI_MC_HV
diff --git a/libavcodec/loongarch/hevc_mc_uni_lsx.c b/libavcodec/loongarch/hevc_mc_uni_lsx.c
new file mode 100644
index 0000000000..a15c86268f
--- /dev/null
+++ b/libavcodec/loongarch/hevc_mc_uni_lsx.c
@@ -0,0 +1,1423 @@
+/*
+ * Copyright (c) 2022 Loongson Technology Corporation Limited
+ * Contributed by Lu Wang <wanglu@loongson.cn>
+ *                Hao Chen <chenhao@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "libavutil/loongarch/loongson_intrinsics.h"
+#include "hevcdsp_lsx.h"
+
+static const uint8_t ff_hevc_mask_arr[16 * 3] __attribute__((aligned(0x40))) = {
+    /* 8 width cases */
+    0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8,
+    /* 4 width cases */
+    0, 1, 1, 2, 2, 3, 3, 4, 16, 17, 17, 18, 18, 19, 19, 20,
+    /* 4 width cases */
+    8, 9, 9, 10, 10, 11, 11, 12, 24, 25, 25, 26, 26, 27, 27, 28
+};
+
+static av_always_inline
+void common_hz_8t_64w_lsx(uint8_t *src, int32_t src_stride,
+                          uint8_t *dst, int32_t dst_stride,
+                          const int8_t *filter, int32_t height)
+{
+    int32_t loop_cnt;
+    __m128i mask0, mask1, mask2, mask3, out1, out2;
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
+    __m128i filt0, filt1, filt2, filt3;
+    __m128i res0, res1, res2, res3;
+
+    mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
+    src -= 3;
+
+    /* rearranging filter */
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filt0, filt1, filt2, filt3);
+
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    mask3 = __lsx_vaddi_bu(mask0, 6);
+
+    for (loop_cnt = height; loop_cnt--;) {
+        DUP4_ARG2(__lsx_vld, src, 0, src, 8, src, 16, src, 24,
+                  src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vld, src, 32, src, 40, src, 48, src, 56,
+                  src4, src5, src6, src7);
+        src += src_stride;
+
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src1, mask0,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src3, src3, mask0,
+                  vec2, vec3);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec1, filt0, vec2, filt0,
+                  vec3, filt0, res0, res1, res2, res3);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src1, mask2,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask2, src3, src3, mask2,
+                  vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, res0, vec0, filt2, res1, vec1, filt2,
+                  res2, vec2, filt2, res3, vec3, filt2, res0, res1, res2, res3);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src1, mask1,
+                  vec4, vec5);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask1, src3, src3, mask1,
+                  vec6, vec7);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, res0, vec4, filt1, res1, vec5, filt1,
+                  res2, vec6, filt1, res3, vec7, filt1, res0, res1, res2, res3);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src1, mask3,
+                  vec4, vec5);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask3, src3, src3, mask3,
+                  vec6, vec7);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, res0, vec4, filt3, res1, vec5, filt3,
+                  res2, vec6, filt3, res3, vec7, filt3, res0, res1, res2, res3);
+
+        DUP2_ARG3(__lsx_vssrarni_bu_h, res1, res0, 6, res3, res2, 6,
+                  out1, out2);
+        __lsx_vst(out1, dst, 0);
+        __lsx_vst(out2, dst, 16);
+
+        DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src5, src5, mask0,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask0, src7, src7, mask0,
+                  vec2, vec3);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec1, filt0, vec2, filt0,
+                  vec3, filt0, res0, res1, res2, res3);
+        DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask2, src5, src5, mask2,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask2, src7, src7, mask2,
+                  vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, res0, vec0, filt2, res1, vec1, filt2,
+                  res2, vec2, filt2, res3, vec3, filt2, res0, res1, res2, res3);
+        DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask1, src5, src5, mask1,
+                  vec4, vec5);
+        DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask1, src7, src7, mask1,
+                  vec6, vec7);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, res0, vec4, filt1, res1, vec5, filt1,
+                  res2, vec6, filt1, res3, vec7, filt1, res0, res1, res2, res3);
+        DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask3, src5, src5, mask3,
+                  vec4, vec5);
+        DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask3, src7, src7, mask3,
+                  vec6, vec7);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, res0, vec4, filt3, res1, vec5, filt3,
+                  res2, vec6, filt3, res3, vec7, filt3, res0, res1, res2, res3);
+
+        DUP2_ARG3(__lsx_vssrarni_bu_h, res1, res0, 6, res3, res2, 6,
+                  out1, out2);
+        __lsx_vst(out1, dst, 32);
+        __lsx_vst(out2, dst, 48);
+        dst += dst_stride;
+    }
+}
+
+static av_always_inline
+void common_vt_8t_8w_lsx(uint8_t *src, int32_t src_stride,
+                         uint8_t *dst, int32_t dst_stride,
+                         const int8_t *filter, int32_t height)
+{
+    uint32_t loop_cnt;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t dst_stride_2x = (dst_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t dst_stride_4x = (dst_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8, src9, src10;
+    __m128i src10_r, src32_r, src54_r, src76_r, src98_r, src21_r, src43_r;
+    __m128i src65_r, src87_r, src109_r, filt0, filt1, filt2, filt3;
+    __m128i tmp0, tmp1;
+    __m128i out0_r, out1_r, out2_r, out3_r;
+
+    src -= src_stride_3x;
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filt0, filt1, filt2, filt3);
+
+    src0 = __lsx_vld(src, 0);
+    DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src1, src2);
+    src3 = __lsx_vldx(src, src_stride_3x);
+    src += src_stride_4x;
+    src4 = __lsx_vld(src, 0);
+    DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src5, src6);
+    src += src_stride_3x;
+    DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1,
+              src10_r, src32_r, src54_r, src21_r);
+    DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, src43_r, src65_r);
+
+    for (loop_cnt = (height >> 2); loop_cnt--;) {
+        src7 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src8, src9);
+        src10 = __lsx_vldx(src, src_stride_3x);
+        src += src_stride_4x;
+
+        DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10,
+                  src9, src76_r, src87_r, src98_r, src109_r);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, src10_r, filt0, src21_r, filt0, src32_r,
+                  filt0, src43_r, filt0, out0_r, out1_r, out2_r, out3_r);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, out0_r, src32_r, filt1, out1_r,
+                  src43_r, filt1, out2_r, src54_r, filt1, out3_r, src65_r,
+                  filt1, out0_r, out1_r, out2_r, out3_r);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, out0_r, src54_r, filt2, out1_r,
+                  src65_r, filt2, out2_r, src76_r, filt2, out3_r, src87_r,
+                  filt2, out0_r, out1_r, out2_r, out3_r);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, out0_r, src76_r, filt3, out1_r,
+                  src87_r, filt3, out2_r, src98_r, filt3, out3_r, src109_r,
+                  filt3, out0_r, out1_r, out2_r, out3_r);
+
+        DUP2_ARG3(__lsx_vssrarni_bu_h, out1_r, out0_r, 6, out3_r, out2_r, 6,
+                  tmp0, tmp1)
+        __lsx_vstelm_d(tmp0, dst, 0, 0);
+        __lsx_vstelm_d(tmp0, dst + dst_stride, 0, 1);
+        __lsx_vstelm_d(tmp1, dst + dst_stride_2x, 0, 0);
+        __lsx_vstelm_d(tmp1, dst + dst_stride_3x, 0, 1);
+        dst += dst_stride_4x;
+
+        src10_r = src54_r;
+        src32_r = src76_r;
+        src54_r = src98_r;
+        src21_r = src65_r;
+        src43_r = src87_r;
+        src65_r = src109_r;
+        src6 = src10;
+    }
+}
+
+static av_always_inline
+void common_vt_8t_16w_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                          int32_t dst_stride, const int8_t *filter,
+                          int32_t height, int32_t width)
+{
+    uint8_t *src_tmp;
+    uint8_t *dst_tmp;
+    uint32_t loop_cnt, cnt;
+    const int32_t src_stride_2x = (src_stride << 1);
+    const int32_t dst_stride_2x = (dst_stride << 1);
+    const int32_t src_stride_4x = (src_stride << 2);
+    const int32_t dst_stride_4x = (dst_stride << 2);
+    const int32_t src_stride_3x = src_stride_2x + src_stride;
+    const int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8, src9, src10;
+    __m128i filt0, filt1, filt2, filt3;
+    __m128i src10_r, src32_r, src54_r, src76_r, src98_r, src21_r, src43_r;
+    __m128i src65_r, src87_r, src109_r, src10_l, src32_l, src54_l, src76_l;
+    __m128i src98_l, src21_l, src43_l, src65_l, src87_l, src109_l;
+    __m128i tmp0, tmp1, tmp2, tmp3;
+    __m128i out0_r, out1_r, out2_r, out3_r, out0_l, out1_l, out2_l, out3_l;
+
+    src -= src_stride_3x;
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6, filt0,
+              filt1, filt2, filt3);
+
+    for (cnt = (width >> 4); cnt--;) {
+        src_tmp = src;
+        dst_tmp = dst;
+
+        src0 = __lsx_vld(src_tmp, 0);
+        DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride_2x,
+                  src1, src2);
+        src3 = __lsx_vldx(src_tmp, src_stride_3x);
+        src_tmp += src_stride_4x;
+        src4 = __lsx_vld(src_tmp, 0);
+        DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride_2x,
+                  src5, src6);
+        src_tmp += src_stride_3x;
+        DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1,
+                  src10_r, src32_r, src54_r, src21_r);
+        DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, src43_r, src65_r);
+        DUP4_ARG2(__lsx_vilvh_b, src1, src0, src3, src2, src5, src4, src2, src1,
+                  src10_l, src32_l, src54_l, src21_l);
+        DUP2_ARG2(__lsx_vilvh_b, src4, src3, src6, src5, src43_l, src65_l);
+
+        for (loop_cnt = (height >> 2); loop_cnt--;) {
+            src7 = __lsx_vld(src_tmp, 0);
+            DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride_2x,
+                      src8, src9);
+            src10 = __lsx_vldx(src_tmp, src_stride_3x);
+            src_tmp += src_stride_4x;
+            DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10,
+                      src9, src76_r, src87_r, src98_r, src109_r);
+            DUP4_ARG2(__lsx_vilvh_b, src7, src6, src8, src7, src9, src8, src10,
+                      src9, src76_l, src87_l, src98_l, src109_l);
+            DUP4_ARG2(__lsx_vdp2_h_bu_b, src10_r, filt0, src21_r, filt0, src32_r,
+                      filt0, src43_r, filt0, out0_r, out1_r, out2_r, out3_r);
+            DUP4_ARG3(__lsx_vdp2add_h_bu_b, out0_r, src32_r, filt1, out1_r,
+                      src43_r, filt1, out2_r, src54_r, filt1, out3_r, src65_r,
+                      filt1, out0_r, out1_r, out2_r, out3_r);
+            DUP4_ARG3(__lsx_vdp2add_h_bu_b, out0_r, src54_r, filt2, out1_r,
+                      src65_r, filt2, out2_r, src76_r, filt2, out3_r, src87_r,
+                      filt2, out0_r, out1_r, out2_r, out3_r);
+            DUP4_ARG3(__lsx_vdp2add_h_bu_b, out0_r, src76_r, filt3, out1_r,
+                      src87_r, filt3, out2_r, src98_r, filt3, out3_r, src109_r,
+                      filt3, out0_r, out1_r, out2_r, out3_r);
+            DUP4_ARG2(__lsx_vdp2_h_bu_b, src10_l, filt0, src21_l, filt0, src32_l,
+                      filt0, src43_l, filt0, out0_l, out1_l, out2_l, out3_l);
+            DUP4_ARG3(__lsx_vdp2add_h_bu_b, out0_l, src32_l, filt1, out1_l,
+                      src43_l, filt1, out2_l, src54_l, filt1, out3_l, src65_l,
+                      filt1, out0_l, out1_l, out2_l, out3_l);
+            DUP4_ARG3(__lsx_vdp2add_h_bu_b, out0_l, src54_l, filt2, out1_l,
+                      src65_l, filt2, out2_l, src76_l, filt2, out3_l, src87_l,
+                      filt2, out0_l, out1_l, out2_l, out3_l);
+            DUP4_ARG3(__lsx_vdp2add_h_bu_b, out0_l, src76_l, filt3, out1_l,
+                      src87_l, filt3, out2_l, src98_l, filt3, out3_l, src109_l,
+                      filt3, out0_l, out1_l, out2_l, out3_l);
+            DUP4_ARG3(__lsx_vssrarni_bu_h, out0_l, out0_r, 6, out1_l, out1_r,
+                      6, out2_l, out2_r, 6, out3_l, out3_r, 6,
+                      tmp0, tmp1, tmp2, tmp3);
+            __lsx_vst(tmp0, dst_tmp, 0);
+            __lsx_vstx(tmp1, dst_tmp, dst_stride);
+            __lsx_vstx(tmp2, dst_tmp, dst_stride_2x);
+            __lsx_vstx(tmp3, dst_tmp, dst_stride_3x);
+            dst_tmp += dst_stride_4x;
+
+            src10_r = src54_r;
+            src32_r = src76_r;
+            src54_r = src98_r;
+            src21_r = src65_r;
+            src43_r = src87_r;
+            src65_r = src109_r;
+            src10_l = src54_l;
+            src32_l = src76_l;
+            src54_l = src98_l;
+            src21_l = src65_l;
+            src43_l = src87_l;
+            src65_l = src109_l;
+            src6 = src10;
+        }
+
+        src += 16;
+        dst += 16;
+    }
+}
+
+static void common_vt_8t_24w_lsx(uint8_t *src, int32_t src_stride,
+                                 uint8_t *dst, int32_t dst_stride,
+                                 const int8_t *filter, int32_t height)
+{
+    common_vt_8t_16w_lsx(src, src_stride, dst, dst_stride, filter, height, 16);
+    common_vt_8t_8w_lsx(src + 16, src_stride, dst + 16, dst_stride, filter,
+                        height);
+}
+
+static void common_vt_8t_32w_lsx(uint8_t *src, int32_t src_stride,
+                                 uint8_t *dst, int32_t dst_stride,
+                                 const int8_t *filter, int32_t height)
+{
+    common_vt_8t_16w_lsx(src, src_stride, dst, dst_stride, filter, height, 32);
+}
+
+static void common_vt_8t_48w_lsx(uint8_t *src, int32_t src_stride,
+                                 uint8_t *dst, int32_t dst_stride,
+                                 const int8_t *filter, int32_t height)
+{
+    common_vt_8t_16w_lsx(src, src_stride, dst, dst_stride, filter, height, 48);
+}
+
+static void common_vt_8t_64w_lsx(uint8_t *src, int32_t src_stride,
+                                 uint8_t *dst, int32_t dst_stride,
+                                 const int8_t *filter, int32_t height)
+{
+    common_vt_8t_16w_lsx(src, src_stride, dst, dst_stride, filter, height, 64);
+}
+
+static av_always_inline
+void hevc_hv_8t_8x2_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                        int32_t dst_stride, const int8_t *filter_x,
+                        const int8_t *filter_y, int32_t height, int32_t width)
+{
+    uint32_t loop_cnt, cnt;
+    uint8_t *src_tmp;
+    uint8_t *dst_tmp;
+    const int32_t src_stride_2x = (src_stride << 1);
+    const int32_t dst_stride_2x = (dst_stride << 1);
+    const int32_t src_stride_4x = (src_stride << 2);
+    const int32_t src_stride_3x = src_stride_2x + src_stride;
+
+    __m128i out;
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8;
+    __m128i filt0, filt1, filt2, filt3;
+    __m128i filt_h0, filt_h1, filt_h2, filt_h3;
+    __m128i mask1, mask2, mask3;
+    __m128i filter_vec;
+    __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
+    __m128i vec8, vec9, vec10, vec11, vec12, vec13, vec14, vec15;
+    __m128i dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7, dst8;
+    __m128i dst0_r, dst0_l, dst1_r, dst1_l;
+    __m128i dst10_r, dst32_r, dst54_r, dst76_r;
+    __m128i dst10_l, dst32_l, dst54_l, dst76_l;
+    __m128i dst21_r, dst43_r, dst65_r, dst87_r;
+    __m128i dst21_l, dst43_l, dst65_l, dst87_l;
+    __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
+
+    src -= (src_stride_3x + 3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filter_x, 4,
+              filter_x, 6, filt0, filt1, filt2, filt3);
+
+    filter_vec = __lsx_vld(filter_y, 0);
+    filter_vec = __lsx_vsllwil_h_b(filter_vec, 0);
+    DUP4_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filter_vec, 2,
+              filter_vec, 3, filt_h0, filt_h1, filt_h2, filt_h3);
+
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    mask3 = __lsx_vaddi_bu(mask0, 6);
+
+    for (cnt = width >> 3; cnt--;) {
+        src_tmp = src;
+        dst_tmp = dst;
+
+        src0 = __lsx_vld(src_tmp, 0);
+        DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride_2x,
+                  src1, src2);
+        src3 = __lsx_vldx(src_tmp, src_stride_3x);
+        src_tmp += src_stride_4x;
+        src4 = __lsx_vld(src_tmp, 0);
+        DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride_2x,
+                  src5, src6);
+        src_tmp += src_stride_3x;
+
+        /* row 0 row 1 row 2 row 3 */
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src0,
+                  src0, mask2, src0, src0, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, src1,
+                  src1, mask2, src1, src1, mask3, vec4, vec5, vec6, vec7);
+        DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, src2,
+                  src2, mask2, src2, src2, mask3, vec8, vec9, vec10, vec11);
+        DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src3,
+                  src3, mask2, src3, src3, mask3, vec12, vec13, vec14, vec15);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec4, filt0, vec8, filt0,
+                  vec12, filt0, dst0, dst1, dst2, dst3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec1, filt1, dst1, vec5, filt1,
+                  dst2, vec9, filt1, dst3, vec13, filt1, dst0, dst1, dst2, dst3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec2, filt2, dst1, vec6, filt2,
+                  dst2, vec10, filt2, dst3, vec14, filt2, dst0, dst1, dst2, dst3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec3, filt3, dst1, vec7, filt3,
+                  dst2, vec11, filt3, dst3, vec15, filt3, dst0, dst1, dst2, dst3);
+
+        DUP4_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, src4,
+                  src4, mask2, src4, src4, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, src5,
+                  src5, mask2, src5, src5, mask3, vec4, vec5, vec6, vec7);
+        DUP4_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, src6,
+                  src6, mask2, src6, src6, mask3, vec8, vec9, vec10, vec11);
+        DUP2_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec4, filt0, dst4, dst5);
+        dst6 = __lsx_vdp2_h_bu_b(vec8, filt0);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst4, vec1, filt1, dst5, vec5, filt1,
+                  dst6, vec9, filt1, dst4, vec2, filt2, dst4, dst5, dst6, dst4);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst5, vec6, filt2, dst6, vec10, filt2,
+                  dst4, vec3, filt3, dst5, vec7, filt3, dst5, dst6, dst4, dst5);
+        dst6 = __lsx_vdp2add_h_bu_b(dst6, vec11, filt3);
+        DUP4_ARG2(__lsx_vilvl_h, dst1, dst0, dst3, dst2, dst5, dst4, dst2,
+                  dst1, dst10_r, dst32_r, dst54_r, dst21_r);
+        DUP4_ARG2(__lsx_vilvh_h, dst1, dst0, dst3, dst2, dst5, dst4, dst2,
+                  dst1, dst10_l, dst32_l, dst54_l, dst21_l);
+        DUP2_ARG2(__lsx_vilvl_h, dst4, dst3, dst6, dst5, dst43_r, dst65_r);
+        DUP2_ARG2(__lsx_vilvh_h, dst4, dst3, dst6, dst5, dst43_l, dst65_l);
+
+        for (loop_cnt = height >> 1; loop_cnt--;) {
+            src7 = __lsx_vld(src_tmp, 0);
+            src8 = __lsx_vldx(src_tmp, src_stride);
+            src_tmp += src_stride_2x;
+
+            DUP4_ARG3(__lsx_vshuf_b, src7, src7, mask0, src7, src7, mask1, src7,
+                      src7, mask2, src7, src7, mask3, vec0, vec1, vec2, vec3);
+            dst7 = __lsx_vdp2_h_bu_b(vec0, filt0);
+            DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst7, vec1, filt1, dst7, vec2,
+                      filt2, dst7, dst7);
+            dst7 = __lsx_vdp2add_h_bu_b(dst7, vec3, filt3);
+            dst76_r = __lsx_vilvl_h(dst7, dst6);
+            dst76_l = __lsx_vilvh_h(dst7, dst6);
+            DUP2_ARG2(__lsx_vdp2_w_h, dst10_r, filt_h0, dst10_l, filt_h0,
+                      dst0_r, dst0_l);
+            DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst32_r, filt_h1, dst0_l,
+                      dst32_l, filt_h1, dst0_r, dst54_r, filt_h2, dst0_l,
+                      dst54_l, filt_h2, dst0_r, dst0_l, dst0_r, dst0_l);
+            DUP2_ARG3(__lsx_vdp2add_w_h, dst0_r, dst76_r, filt_h3, dst0_l,
+                      dst76_l, filt_h3, dst0_r, dst0_l);
+            DUP2_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst0_r, dst0_l);
+
+            DUP4_ARG3(__lsx_vshuf_b, src8, src8, mask0, src8, src8, mask1, src8,
+                      src8, mask2, src8, src8, mask3, vec0, vec1, vec2, vec3);
+            dst8 = __lsx_vdp2_h_bu_b(vec0, filt0);
+            DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst8, vec1, filt1, dst8, vec2,
+                      filt2, dst8, dst8);
+            dst8 = __lsx_vdp2add_h_bu_b(dst8, vec3, filt3);
+
+            dst87_r = __lsx_vilvl_h(dst8, dst7);
+            dst87_l = __lsx_vilvh_h(dst8, dst7);
+            DUP2_ARG2(__lsx_vdp2_w_h, dst21_r, filt_h0, dst21_l, filt_h0,
+                      dst1_r, dst1_l);
+            DUP4_ARG3(__lsx_vdp2add_w_h, dst1_r, dst43_r, filt_h1, dst1_l,
+                      dst43_l, filt_h1, dst1_r, dst65_r, filt_h2, dst1_l,
+                      dst65_l, filt_h2, dst1_r, dst1_l, dst1_r, dst1_l);
+            DUP2_ARG3(__lsx_vdp2add_w_h, dst1_r, dst87_r, filt_h3, dst1_l,
+                      dst87_l, filt_h3, dst1_r, dst1_l);
+            DUP2_ARG2(__lsx_vsrai_w, dst1_r, 6, dst1_l, 6, dst1_r, dst1_l);
+            DUP4_ARG2(__lsx_vsrari_w, dst0_r, 6, dst0_l, 6,dst1_r, 6, dst1_l,
+                      6, dst0_r, dst0_l, dst1_r, dst1_l);
+            DUP4_ARG1(__lsx_vclip255_w, dst0_l, dst0_r, dst1_l, dst1_r,
+                      dst0_l, dst0_r, dst1_l, dst1_r);
+            DUP2_ARG2(__lsx_vpickev_h, dst0_l, dst0_r, dst1_l, dst1_r,
+                      dst0, dst1);
+            out = __lsx_vpickev_b(dst1, dst0);
+            __lsx_vstelm_d(out, dst_tmp, 0, 0);
+            __lsx_vstelm_d(out, dst_tmp + dst_stride, 0, 1);
+            dst_tmp += dst_stride_2x;
+
+            dst10_r = dst32_r;
+            dst32_r = dst54_r;
+            dst54_r = dst76_r;
+            dst10_l = dst32_l;
+            dst32_l = dst54_l;
+            dst54_l = dst76_l;
+            dst21_r = dst43_r;
+            dst43_r = dst65_r;
+            dst65_r = dst87_r;
+            dst21_l = dst43_l;
+            dst43_l = dst65_l;
+            dst65_l = dst87_l;
+            dst6 = dst8;
+        }
+        src += 8;
+        dst += 8;
+    }
+}
+
+static void hevc_hv_8t_8w_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                              int32_t dst_stride, const int8_t *filter_x,
+                              const int8_t *filter_y, int32_t height)
+{
+    hevc_hv_8t_8x2_lsx(src, src_stride, dst, dst_stride,
+                       filter_x, filter_y, height, 8);
+}
+
+static void hevc_hv_8t_16w_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                               int32_t dst_stride, const int8_t *filter_x,
+                               const int8_t *filter_y, int32_t height)
+{
+    hevc_hv_8t_8x2_lsx(src, src_stride, dst, dst_stride,
+                      filter_x, filter_y, height, 16);
+}
+
+static void hevc_hv_8t_24w_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                               int32_t dst_stride, const int8_t *filter_x,
+                               const int8_t *filter_y, int32_t height)
+{
+    hevc_hv_8t_8x2_lsx(src, src_stride, dst, dst_stride,
+                       filter_x, filter_y, height, 24);
+}
+
+static void hevc_hv_8t_32w_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                               int32_t dst_stride, const int8_t *filter_x,
+                               const int8_t *filter_y, int32_t height)
+{
+    hevc_hv_8t_8x2_lsx(src, src_stride, dst, dst_stride,
+                       filter_x, filter_y, height, 32);
+}
+
+static void hevc_hv_8t_48w_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                               int32_t dst_stride, const int8_t *filter_x,
+                               const int8_t *filter_y, int32_t height)
+{
+    hevc_hv_8t_8x2_lsx(src, src_stride, dst, dst_stride,
+                       filter_x, filter_y, height, 48);
+}
+
+static void hevc_hv_8t_64w_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                               int32_t dst_stride, const int8_t *filter_x,
+                               const int8_t *filter_y, int32_t height)
+{
+    hevc_hv_8t_8x2_lsx(src, src_stride, dst, dst_stride,
+                       filter_x, filter_y, height, 64);
+}
+
+static av_always_inline
+void common_vt_4t_24w_lsx(uint8_t *src, int32_t src_stride,
+                          uint8_t *dst, int32_t dst_stride,
+                          const int8_t *filter, int32_t height)
+{
+    uint32_t loop_cnt;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    uint8_t *_src;
+
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8, src9, src10;
+    __m128i src11, filt0, filt1;
+    __m128i src10_r, src32_r, src76_r, src98_r, src21_r, src43_r, src87_r;
+    __m128i src109_r, src10_l, src32_l, src21_l, src43_l;
+    __m128i out0_r, out1_r, out2_r, out3_r, out0_l, out1_l;
+    __m128i out1, out2, out3, out4;
+
+    src -= src_stride;
+    DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
+    _src = src + 16;
+
+    /* 16 width */
+    src0 = __lsx_vld(src, 0);
+    DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src1, src2);
+    DUP2_ARG2(__lsx_vilvl_b, src1, src0, src2, src1, src10_r, src21_r);
+    DUP2_ARG2(__lsx_vilvh_b, src1, src0, src2, src1, src10_l, src21_l);
+
+    /* 8 width */
+    src6 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride_2x, src7, src8);
+    src += src_stride_3x;
+    _src += src_stride_3x;
+    DUP2_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src76_r, src87_r);
+
+    for (loop_cnt = 8; loop_cnt--;) {
+        /* 16 width */
+        DUP2_ARG2(__lsx_vld, src, 0, _src, 0, src3, src9);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, _src, src_stride, src4, src10);
+        DUP2_ARG2(__lsx_vilvl_b, src3, src2, src4, src3, src32_r, src43_r);
+        DUP2_ARG2(__lsx_vilvh_b, src3, src2, src4, src3, src32_l, src43_l);
+
+        /* 8 width */
+        src += src_stride_2x;
+        _src += src_stride_2x;
+        DUP2_ARG2(__lsx_vilvl_b, src9, src8, src10, src9, src98_r, src109_r);
+
+        /* 16 width */
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, src10_r, filt0, src10_l, filt0, src21_r,
+                  filt0, src21_l, filt0, out0_r, out0_l, out1_r, out1_l);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, out0_r, src32_r, filt1, out0_l, src32_l,
+                  filt1, out1_r, src43_r, filt1, out1_l, src43_l, filt1,
+                  out0_r, out0_l, out1_r, out1_l);
+
+        /* 8 width */
+        DUP2_ARG2(__lsx_vdp2_h_bu_b, src76_r, filt0, src87_r, filt0,
+                  out2_r, out3_r);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, out2_r, src98_r, filt1, out3_r,
+                  src109_r, filt1, out2_r, out3_r);
+
+        /* 16 + 8 width */
+        DUP4_ARG3(__lsx_vssrarni_bu_h, out0_l, out0_r, 6, out2_r, out2_r, 6,
+                out3_r, out3_r, 6, out1_l, out1_r, 6, out1, out2, out3, out4);
+        __lsx_vst(out1, dst, 0);
+        __lsx_vstelm_d(out2, dst, 16, 0);
+        dst += dst_stride;
+        __lsx_vst(out4, dst, 0);
+        __lsx_vstelm_d(out3, dst, 16, 0);
+        dst += dst_stride;
+
+        /* 16 width */
+        DUP2_ARG2(__lsx_vld, src, 0, _src, 0, src5, src11);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, _src, src_stride, src2, src8);
+        DUP2_ARG2(__lsx_vilvl_b, src5, src4, src2, src5, src10_r, src21_r);
+        DUP2_ARG2(__lsx_vilvh_b, src5, src4, src2, src5, src10_l, src21_l);
+
+        /* 8 width */
+        src += src_stride_2x;
+        _src += src_stride_2x;
+        DUP2_ARG2(__lsx_vilvl_b, src11, src10, src8, src11, src76_r, src87_r);
+
+        /* 16 width */
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, src32_r, filt0, src32_l, filt0, src43_r,
+                  filt0, src43_l, filt0, out0_r, out0_l, out1_r, out1_l);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, out0_r, src10_r, filt1, out0_l, src10_l,
+                  filt1, out1_r, src21_r, filt1, out1_l, src21_l, filt1,
+                  out0_r, out0_l, out1_r, out1_l);
+
+        /* 8 width */
+        DUP2_ARG2(__lsx_vdp2_h_bu_b, src98_r, filt0, src109_r, filt0,
+                  out2_r, out3_r);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, out2_r, src76_r, filt1, out3_r,
+                  src87_r, filt1, out2_r, out3_r);
+
+        /* 16 + 8 width */
+        DUP4_ARG3(__lsx_vssrarni_bu_h, out0_l, out0_r, 6, out2_r, out2_r, 6,
+                  out1_l, out1_r, 6, out3_r, out3_r, 6, out1, out2, out3, out4);
+
+        __lsx_vst(out1, dst, 0);
+        __lsx_vstelm_d(out2, dst, 16, 0);
+        dst += dst_stride;
+        __lsx_vst(out3, dst, 0);
+        __lsx_vstelm_d(out4, dst, 16, 0);
+        dst += dst_stride;
+    }
+}
+
+static av_always_inline
+void common_vt_4t_32w_lsx(uint8_t *src, int32_t src_stride,
+                          uint8_t *dst, int32_t dst_stride,
+                          const int8_t *filter, int32_t height)
+{
+    uint32_t loop_cnt;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t dst_stride_2x = (dst_stride << 1);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    uint8_t *_src;
+
+    __m128i src0, src1, src2, src3, src4, src6, src7, src8, src9, src10;
+    __m128i src10_r, src32_r, src76_r, src98_r;
+    __m128i src21_r, src43_r, src87_r, src109_r;
+    __m128i out0_r, out1_r, out2_r, out3_r, out0_l, out1_l, out2_l, out3_l;
+    __m128i src10_l, src32_l, src76_l, src98_l;
+    __m128i src21_l, src43_l, src87_l, src109_l;
+    __m128i filt0, filt1;
+    __m128i out1, out2;
+
+    src -= src_stride;
+    DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
+    _src = src + 16;
+
+    /* 16 width */
+    src0 = __lsx_vld(src, 0);
+    DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src1, src2);
+
+    DUP2_ARG2(__lsx_vilvl_b, src1, src0, src2, src1, src10_r, src21_r);
+    DUP2_ARG2(__lsx_vilvh_b, src1, src0, src2, src1, src10_l, src21_l);
+
+    /* next 16 width */
+    src6 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride_2x, src7, src8);
+    src += src_stride_3x;
+    _src += src_stride_3x;
+
+    DUP2_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src76_r, src87_r);
+    DUP2_ARG2(__lsx_vilvh_b, src7, src6, src8, src7, src76_l, src87_l);
+
+    for (loop_cnt = (height >> 1); loop_cnt--;) {
+        /* 16 width */
+        DUP2_ARG2(__lsx_vld, src, 0, _src, 0, src3, src9);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, _src, src_stride, src4, src10);
+        DUP2_ARG2(__lsx_vilvl_b, src3, src2, src4, src3, src32_r, src43_r);
+        DUP2_ARG2(__lsx_vilvh_b, src3, src2, src4, src3, src32_l, src43_l);
+
+        /* 16 width */
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, src10_r, filt0, src10_l, filt0, src21_r,
+                  filt0, src21_l, filt0, out0_r, out0_l, out1_r, out1_l);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, out0_r, src32_r, filt1, out0_l, src32_l,
+                  filt1, out1_r, src43_r, filt1, out1_l, src43_l, filt1,
+                  out0_r, out0_l, out1_r, out1_l);
+
+        DUP2_ARG3(__lsx_vssrarni_bu_h, out0_l, out0_r, 6, out1_l, out1_r, 6,
+                  out1, out2);
+        __lsx_vst(out1, dst, 0);
+        __lsx_vstx(out2, dst, dst_stride);
+
+        src10_r = src32_r;
+        src21_r = src43_r;
+        src10_l = src32_l;
+        src21_l = src43_l;
+        src2 = src4;
+
+        /* next 16 width */
+        src += src_stride_2x;
+        _src += src_stride_2x;
+        DUP2_ARG2(__lsx_vilvl_b, src9, src8, src10, src9, src98_r, src109_r);
+        DUP2_ARG2(__lsx_vilvh_b, src9, src8, src10, src9, src98_l, src109_l);
+
+        /* next 16 width */
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, src76_r, filt0, src76_l, filt0, src87_r,
+                  filt0, src87_l, filt0, out2_r, out2_l, out3_r, out3_l);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, out2_r, src98_r, filt1, out2_l, src98_l,
+                  filt1, out3_r, src109_r, filt1, out3_l, src109_l, filt1,
+                  out2_r, out2_l, out3_r, out3_l);
+
+        /* next 16 width */
+        DUP2_ARG3(__lsx_vssrarni_bu_h, out2_l, out2_r, 6, out3_l, out3_r, 6,
+                  out1, out2);
+        __lsx_vst(out1, dst, 16);
+        __lsx_vst(out2, dst + dst_stride, 16);
+
+        dst += dst_stride_2x;
+
+        src76_r = src98_r;
+        src87_r = src109_r;
+        src76_l = src98_l;
+        src87_l = src109_l;
+        src8 = src10;
+    }
+}
+
+static av_always_inline
+void hevc_hv_4t_8x2_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                        int32_t dst_stride, const int8_t *filter_x,
+                        const int8_t *filter_y)
+{
+    const int32_t src_stride_2x = (src_stride << 1);
+    const int32_t src_stride_4x = (src_stride << 2);
+    const int32_t src_stride_3x = src_stride_2x + src_stride;
+    __m128i out;
+    __m128i src0, src1, src2, src3, src4;
+    __m128i filt0, filt1;
+    __m128i filt_h0, filt_h1, filter_vec;
+    __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
+    __m128i mask1;
+    __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7, vec8, vec9;
+    __m128i dst0, dst1, dst2, dst3, dst4;
+    __m128i dst0_r, dst0_l, dst1_r, dst1_l;
+    __m128i dst10_r, dst32_r, dst21_r, dst43_r;
+    __m128i dst10_l, dst32_l, dst21_l, dst43_l;
+    __m128i out0_r, out1_r;
+
+    src -= (src_stride + 1);
+    DUP2_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filt0, filt1);
+
+    filter_vec = __lsx_vld(filter_y, 0);
+    filter_vec = __lsx_vsllwil_h_b(filter_vec, 0);
+    DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
+
+    mask1 = __lsx_vaddi_bu(mask0, 2);
+    src0 = __lsx_vld(src, 0);
+    DUP4_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src,
+              src_stride_3x, src, src_stride_4x, src1, src2, src3, src4);
+
+    DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src1, src1,
+              mask0, src1, src1, mask1, vec0, vec1, vec2, vec3);
+    DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, src3, src3,
+              mask0, src3, src3, mask1, vec4, vec5, vec6, vec7);
+    DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, vec8, vec9);
+
+    DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, vec4, filt0, vec6,
+              filt0, dst0, dst1, dst2, dst3);
+    dst4 = __lsx_vdp2_h_bu_b(vec8, filt0);
+    DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec1, filt1, dst1, vec3, filt1, dst2,
+              vec5, filt1, dst3, vec7, filt1, dst0, dst1, dst2, dst3);
+    dst4 = __lsx_vdp2add_h_bu_b(dst4, vec9, filt1);
+    DUP4_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst3, dst2, dst4, dst3,
+              dst10_r, dst21_r, dst32_r, dst43_r);
+    DUP4_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst3, dst2, dst4, dst3,
+              dst10_l, dst21_l, dst32_l, dst43_l);
+    DUP4_ARG2(__lsx_vdp2_w_h, dst10_r, filt_h0, dst10_l, filt_h0, dst21_r,
+              filt_h0, dst21_l, filt_h0, dst0_r, dst0_l, dst1_r, dst1_l);
+    DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst32_r, filt_h1, dst0_l, dst32_l,
+              filt_h1, dst1_r, dst43_r, filt_h1, dst1_l, dst43_l, filt_h1,
+              dst0_r, dst0_l, dst1_r, dst1_l);
+    DUP2_ARG3(__lsx_vsrani_h_w, dst0_l, dst0_r, 6, dst1_l, dst1_r, 6,
+              out0_r, out1_r);
+    out = __lsx_vssrarni_bu_h(out1_r, out0_r, 6);
+    __lsx_vstelm_d(out, dst, 0, 0);
+    __lsx_vstelm_d(out, dst + dst_stride, 0, 1);
+}
+
+static av_always_inline
+void hevc_hv_4t_8multx4_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                            int32_t dst_stride, const int8_t *filter_x,
+                            const int8_t *filter_y, int32_t width8mult)
+{
+    uint32_t cnt;
+    const int32_t src_stride_2x = (src_stride << 1);
+    const int32_t dst_stride_2x = (dst_stride << 1);
+    const int32_t src_stride_4x = (src_stride << 2);
+    const int32_t src_stride_3x = src_stride_2x + src_stride;
+    const int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+
+    __m128i out0, out1;
+    __m128i src0, src1, src2, src3, src4, src5, src6, mask0, mask1;
+    __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
+    __m128i filt0, filt1, filt_h0, filt_h1, filter_vec;
+    __m128i dst0, dst1, dst2, dst3, dst4, dst5, dst6, tmp0, tmp1, tmp2, tmp3;
+    __m128i dst0_r, dst0_l, dst1_r, dst1_l, dst2_r, dst2_l, dst3_r, dst3_l;
+    __m128i dst10_r, dst32_r, dst54_r, dst21_r, dst43_r, dst65_r;
+    __m128i dst10_l, dst32_l, dst54_l, dst21_l, dst43_l, dst65_l;
+
+    src -= (src_stride + 1);
+    DUP2_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filt0, filt1);
+
+    filter_vec = __lsx_vld(filter_y, 0);
+    filter_vec = __lsx_vsllwil_h_b(filter_vec, 0);
+    DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
+
+    mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
+    mask1 = __lsx_vaddi_bu(mask0, 2);
+
+    for (cnt = width8mult; cnt--;) {
+        src0 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src1, src2);
+        src3 = __lsx_vldx(src, src_stride_3x);
+        src += src_stride_4x;
+        src4 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src5, src6);
+        src += (8 - src_stride_4x);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1,
+                  vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1,
+                  vec4, vec5);
+
+        DUP2_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, dst0, dst1);
+        dst2 = __lsx_vdp2_h_bu_b(vec4, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec1, filt1, dst1, vec3, filt1,
+                  dst0, dst1);
+        dst2 = __lsx_vdp2add_h_bu_b(dst2, vec5, filt1);
+
+        DUP2_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst10_r, dst21_r);
+        DUP2_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst10_l, dst21_l);
+
+        DUP2_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1,
+                  vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1,
+                  vec4, vec5);
+        DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1,
+                  vec6, vec7);
+
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, vec4, filt0,
+                  vec6, filt0, dst3, dst4, dst5, dst6);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst3, vec1, filt1, dst4, vec3, filt1,
+                  dst5, vec5, filt1, dst6, vec7, filt1, dst3, dst4, dst5, dst6);
+
+        DUP4_ARG2(__lsx_vilvl_h, dst3, dst2, dst4, dst3, dst5, dst4, dst6,
+                  dst5, dst32_r, dst43_r, dst54_r, dst65_r);
+        DUP4_ARG2(__lsx_vilvh_h, dst3, dst2, dst4, dst3, dst5, dst4, dst6,
+                  dst5, dst32_l, dst43_l, dst54_l, dst65_l);
+
+        DUP4_ARG2(__lsx_vdp2_w_h, dst10_r, filt_h0, dst10_l, filt_h0, dst21_r,
+                  filt_h0, dst21_l, filt_h0, dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP4_ARG2(__lsx_vdp2_w_h, dst32_r, filt_h0, dst32_l, filt_h0, dst43_r,
+                  filt_h0, dst43_l, filt_h0, dst2_r, dst2_l, dst3_r, dst3_l);
+        DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst32_r, filt_h1, dst0_l, dst32_l,
+                  filt_h1, dst1_r, dst43_r, filt_h1, dst1_l, dst43_l, filt_h1,
+                  dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP4_ARG3(__lsx_vdp2add_w_h, dst2_r, dst54_r, filt_h1, dst2_l, dst54_l,
+                  filt_h1, dst3_r, dst65_r, filt_h1, dst3_l, dst65_l, filt_h1,
+                  dst2_r, dst2_l, dst3_r, dst3_l);
+
+        DUP4_ARG3(__lsx_vsrani_h_w, dst0_l, dst0_r, 6, dst1_l, dst1_r, 6,
+                  dst2_l, dst2_r, 6, dst3_l, dst3_r, 6, tmp0, tmp1, tmp2, tmp3);
+        DUP2_ARG3(__lsx_vssrarni_bu_h, tmp1, tmp0, 6, tmp3, tmp2, 6, out0, out1);
+        __lsx_vstelm_d(out0, dst, 0, 0);
+        __lsx_vstelm_d(out0, dst + dst_stride, 0, 1);
+        __lsx_vstelm_d(out1, dst + dst_stride_2x, 0, 0);
+        __lsx_vstelm_d(out1, dst + dst_stride_3x, 0, 1);
+        dst += 8;
+    }
+}
+
+static av_always_inline
+void hevc_hv_4t_8x6_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                        int32_t dst_stride, const int8_t *filter_x,
+                        const int8_t *filter_y)
+{
+    const int32_t src_stride_2x = (src_stride << 1);
+    const int32_t dst_stride_2x = (dst_stride << 1);
+    const int32_t src_stride_4x = (src_stride << 2);
+    const int32_t dst_stride_4x = (dst_stride << 2);
+    const int32_t src_stride_3x = src_stride_2x + src_stride;
+    const int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    __m128i out0, out1, out2;
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8;
+    __m128i filt0, filt1;
+    __m128i filt_h0, filt_h1, filter_vec;
+    __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
+    __m128i mask1;
+    __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7, vec8, vec9;
+    __m128i vec10, vec11, vec12, vec13, vec14, vec15, vec16, vec17;
+    __m128i dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7, dst8;
+    __m128i dst0_r, dst0_l, dst1_r, dst1_l, dst2_r, dst2_l, dst3_r, dst3_l;
+    __m128i dst4_r, dst4_l, dst5_r, dst5_l;
+    __m128i dst10_r, dst32_r, dst10_l, dst32_l;
+    __m128i dst21_r, dst43_r, dst21_l, dst43_l;
+    __m128i dst54_r, dst54_l, dst65_r, dst65_l;
+    __m128i dst76_r, dst76_l, dst87_r, dst87_l;
+    __m128i out0_r, out1_r, out2_r, out3_r, out4_r, out5_r;
+
+    src -= (src_stride + 1);
+    DUP2_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filt0, filt1);
+
+    filter_vec = __lsx_vld(filter_y, 0);
+    filter_vec = __lsx_vsllwil_h_b(filter_vec, 0);
+    DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
+
+    mask1 = __lsx_vaddi_bu(mask0, 2);
+
+    src0 = __lsx_vld(src, 0);
+    DUP4_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x,src,
+              src_stride_3x, src, src_stride_4x, src1, src2, src3, src4);
+    src += src_stride_4x;
+    DUP4_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x,src,
+              src_stride_3x, src, src_stride_4x, src5, src6, src7, src8);
+
+    DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src1, src1,
+              mask0, src1, src1, mask1, vec0, vec1, vec2, vec3);
+    DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, src3, src3,
+              mask0, src3, src3, mask1, vec4, vec5, vec6, vec7);
+    DUP4_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, src5, src5,
+              mask0, src5, src5, mask1, vec8, vec9, vec10, vec11);
+    DUP4_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, src7, src7,
+              mask0, src7, src7, mask1, vec12, vec13, vec14, vec15);
+    DUP2_ARG3(__lsx_vshuf_b, src8, src8, mask0, src8, src8, mask1, vec16, vec17);
+
+    DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, vec4, filt0, vec6,
+              filt0, dst0, dst1, dst2, dst3);
+    DUP4_ARG2(__lsx_vdp2_h_bu_b, vec8, filt0, vec10, filt0, vec12, filt0, vec14,
+              filt0, dst4, dst5, dst6, dst7);
+    dst8 = __lsx_vdp2_h_bu_b(vec16, filt0);
+    DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec1, filt1, dst1, vec3, filt1, dst2,
+              vec5, filt1, dst3, vec7, filt1, dst0, dst1, dst2, dst3);
+    DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst4, vec9, filt1, dst5, vec11, filt1, dst6,
+              vec13, filt1, dst7, vec15, filt1, dst4, dst5, dst6, dst7);
+    dst8 = __lsx_vdp2add_h_bu_b(dst8, vec17, filt1);
+
+    DUP4_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst3, dst2, dst4, dst3,
+              dst10_r, dst21_r, dst32_r, dst43_r);
+    DUP4_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst3, dst2, dst4, dst3,
+              dst10_l, dst21_l, dst32_l, dst43_l);
+    DUP4_ARG2(__lsx_vilvl_h, dst5, dst4, dst6, dst5, dst7, dst6, dst8, dst7,
+              dst54_r, dst65_r, dst76_r, dst87_r);
+    DUP4_ARG2(__lsx_vilvh_h, dst5, dst4, dst6, dst5, dst7, dst6, dst8, dst7,
+              dst54_l, dst65_l, dst76_l, dst87_l);
+
+    DUP4_ARG2(__lsx_vdp2_w_h, dst10_r, filt_h0, dst10_l, filt_h0, dst21_r,
+              filt_h0, dst21_l, filt_h0, dst0_r, dst0_l, dst1_r, dst1_l);
+    DUP4_ARG2(__lsx_vdp2_w_h, dst32_r, filt_h0, dst32_l, filt_h0, dst43_r,
+              filt_h0, dst43_l, filt_h0, dst2_r, dst2_l, dst3_r, dst3_l);
+    DUP4_ARG2(__lsx_vdp2_w_h, dst54_r, filt_h0, dst54_l, filt_h0, dst65_r,
+              filt_h0, dst65_l, filt_h0, dst4_r, dst4_l, dst5_r, dst5_l);
+    DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst32_r, filt_h1, dst0_l, dst32_l,
+              filt_h1, dst1_r, dst43_r, filt_h1, dst1_l, dst43_l, filt_h1,
+              dst0_r, dst0_l, dst1_r, dst1_l);
+    DUP4_ARG3(__lsx_vdp2add_w_h, dst2_r, dst54_r, filt_h1, dst2_l, dst54_l,
+              filt_h1, dst3_r, dst65_r, filt_h1, dst3_l, dst65_l, filt_h1,
+              dst2_r, dst2_l, dst3_r, dst3_l);
+    DUP4_ARG3(__lsx_vdp2add_w_h, dst4_r, dst76_r, filt_h1, dst4_l, dst76_l,
+              filt_h1, dst5_r, dst87_r, filt_h1, dst5_l, dst87_l, filt_h1,
+              dst4_r, dst4_l, dst5_r, dst5_l);
+
+    DUP4_ARG3(__lsx_vsrani_h_w, dst0_l, dst0_r, 6, dst1_l, dst1_r, 6, dst2_l,
+              dst2_r, 6, dst3_l, dst3_r, 6, out0_r, out1_r, out2_r, out3_r);
+    DUP2_ARG3(__lsx_vsrani_h_w, dst4_l, dst4_r, 6, dst5_l, dst5_r, 6,
+              out4_r, out5_r);
+    DUP2_ARG3(__lsx_vssrarni_bu_h, out1_r, out0_r, 6, out3_r, out2_r, 6,
+              out0, out1);
+    out2 = __lsx_vssrarni_bu_h(out5_r, out4_r, 6);
+
+    __lsx_vstelm_d(out0, dst, 0, 0);
+    __lsx_vstelm_d(out0, dst + dst_stride, 0, 1);
+    __lsx_vstelm_d(out1, dst + dst_stride_2x, 0, 0);
+    __lsx_vstelm_d(out1, dst + dst_stride_3x, 0, 1);
+    dst += dst_stride_4x;
+    __lsx_vstelm_d(out2, dst, 0, 0);
+    __lsx_vstelm_d(out2, dst + dst_stride, 0, 1);
+}
+
+static av_always_inline
+void hevc_hv_4t_8multx4mult_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                                int32_t dst_stride, const int8_t *filter_x,
+                                const int8_t *filter_y, int32_t height,
+                                int32_t width8mult)
+{
+    uint32_t loop_cnt, cnt;
+    uint8_t *src_tmp;
+    uint8_t *dst_tmp;
+    const int32_t src_stride_2x = (src_stride << 1);
+    const int32_t dst_stride_2x = (dst_stride << 1);
+    const int32_t src_stride_4x = (src_stride << 2);
+    const int32_t dst_stride_4x = (dst_stride << 2);
+    const int32_t src_stride_3x = src_stride_2x + src_stride;
+    const int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+
+    __m128i out0, out1;
+    __m128i src0, src1, src2, src3, src4, src5, src6;
+    __m128i filt0, filt1;
+    __m128i filt_h0, filt_h1, filter_vec;
+    __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
+    __m128i mask1;
+    __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
+    __m128i dst0, dst1, dst2, dst3, dst4, dst5;
+    __m128i dst0_r, dst0_l, dst1_r, dst1_l, dst2_r, dst2_l, dst3_r, dst3_l;
+    __m128i dst10_r, dst32_r, dst21_r, dst43_r;
+    __m128i dst10_l, dst32_l, dst21_l, dst43_l;
+    __m128i dst54_r, dst54_l, dst65_r, dst65_l, dst6;
+    __m128i out0_r, out1_r, out2_r, out3_r;
+
+    src -= (src_stride + 1);
+    DUP2_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filt0, filt1);
+
+    filter_vec = __lsx_vld(filter_y, 0);
+    filter_vec = __lsx_vsllwil_h_b(filter_vec, 0);
+    DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
+    mask1 = __lsx_vaddi_bu(mask0, 2);
+
+    for (cnt = width8mult; cnt--;) {
+        src_tmp = src;
+        dst_tmp = dst;
+
+        src0 = __lsx_vld(src_tmp, 0);
+        DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride_2x,
+                  src1, src2);
+        src_tmp += src_stride_3x;
+
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1,
+                  vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1,
+                  vec4, vec5);
+
+        DUP2_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, dst0, dst1);
+        dst2 = __lsx_vdp2_h_bu_b(vec4, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec1, filt1, dst1, vec3, filt1,
+                  dst0, dst1);
+        dst2 = __lsx_vdp2add_h_bu_b(dst2, vec5, filt1);
+
+        DUP2_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst10_r, dst21_r);
+        DUP2_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst10_l, dst21_l);
+
+        for (loop_cnt = (height >> 2); loop_cnt--;) {
+            src3 = __lsx_vld(src_tmp, 0);
+            DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride_2x,
+                      src4, src5);
+            src6 = __lsx_vldx(src_tmp, src_stride_3x);
+            src_tmp += src_stride_4x;
+
+            DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src4,
+                      src4, mask0, src4, src4, mask1, vec0, vec1, vec2, vec3);
+            DUP4_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, src6,
+                      src6, mask0, src6, src6, mask1, vec4, vec5, vec6, vec7);
+
+            DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, vec4, filt0,
+                      vec6, filt0, dst3, dst4, dst5, dst6);
+            DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst3, vec1, filt1, dst4, vec3,
+                      filt1, dst5, vec5, filt1, dst6, vec7, filt1,
+                      dst3, dst4, dst5, dst6);
+
+            DUP4_ARG2(__lsx_vilvl_h, dst3, dst2, dst4, dst3, dst5, dst4,
+                      dst6, dst5, dst32_r, dst43_r, dst54_r, dst65_r);
+            DUP4_ARG2(__lsx_vilvh_h, dst3, dst2, dst4, dst3, dst5, dst4,
+                      dst6, dst5, dst32_l, dst43_l, dst54_l, dst65_l);
+
+            DUP4_ARG2(__lsx_vdp2_w_h, dst10_r, filt_h0, dst10_l, filt_h0, dst21_r,
+                      filt_h0, dst21_l, filt_h0, dst0_r, dst0_l, dst1_r, dst1_l);
+            DUP4_ARG2(__lsx_vdp2_w_h, dst32_r, filt_h0, dst32_l, filt_h0, dst43_r,
+                      filt_h0, dst43_l, filt_h0, dst2_r, dst2_l, dst3_r, dst3_l);
+            DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst32_r, filt_h1, dst0_l,
+                      dst32_l, filt_h1, dst1_r, dst43_r, filt_h1, dst1_l,
+                      dst43_l, filt_h1, dst0_r, dst0_l, dst1_r, dst1_l);
+            DUP4_ARG3(__lsx_vdp2add_w_h, dst2_r, dst54_r, filt_h1, dst2_l,
+                      dst54_l, filt_h1, dst3_r, dst65_r, filt_h1, dst3_l,
+                      dst65_l, filt_h1, dst2_r, dst2_l, dst3_r, dst3_l);
+
+            DUP4_ARG3(__lsx_vsrani_h_w, dst0_l, dst0_r, 6, dst1_l, dst1_r, 6,
+                      dst2_l, dst2_r, 6, dst3_l, dst3_r, 6, out0_r, out1_r,
+                      out2_r, out3_r);
+            DUP2_ARG3(__lsx_vssrarni_bu_h, out1_r, out0_r, 6, out3_r, out2_r,
+                      6, out0, out1);
+            __lsx_vstelm_d(out0, dst_tmp, 0, 0);
+            __lsx_vstelm_d(out0, dst_tmp + dst_stride, 0, 1);
+            __lsx_vstelm_d(out1, dst_tmp + dst_stride_2x, 0, 0);
+            __lsx_vstelm_d(out1, dst_tmp + dst_stride_3x, 0, 1);
+            dst_tmp += dst_stride_4x;
+
+            dst10_r = dst54_r;
+            dst10_l = dst54_l;
+            dst21_r = dst65_r;
+            dst21_l = dst65_l;
+            dst2 = dst6;
+        }
+        src += 8;
+        dst += 8;
+    }
+}
+
+static
+void hevc_hv_4t_8w_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                       int32_t dst_stride, const int8_t *filter_x,
+                       const int8_t *filter_y, int32_t height)
+{
+    if (2 == height) {
+        hevc_hv_4t_8x2_lsx(src, src_stride, dst, dst_stride, filter_x, filter_y);
+    } else if (4 == height) {
+        hevc_hv_4t_8multx4_lsx(src, src_stride, dst, dst_stride,
+                               filter_x, filter_y, 1);
+    } else if (6 == height) {
+        hevc_hv_4t_8x6_lsx(src, src_stride, dst, dst_stride, filter_x, filter_y);
+    } else if (0 == (height & 0x03)) {
+        hevc_hv_4t_8multx4mult_lsx(src, src_stride, dst, dst_stride,
+                                   filter_x, filter_y, height, 1);
+    }
+}
+
+static av_always_inline
+void hevc_hv_4t_12w_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                        int32_t dst_stride, const int8_t *filter_x,
+                        const int8_t *filter_y, int32_t height)
+{
+    uint32_t loop_cnt;
+    uint8_t *src_tmp, *dst_tmp;
+    const int32_t src_stride_2x = (src_stride << 1);
+    const int32_t dst_stride_2x = (dst_stride << 1);
+    const int32_t src_stride_4x = (src_stride << 2);
+    const int32_t dst_stride_4x = (dst_stride << 2);
+    const int32_t src_stride_3x = src_stride_2x + src_stride;
+    const int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    __m128i out0, out1;
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8, src9, src10;
+    __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
+    __m128i mask0, mask1, mask2, mask3;
+    __m128i filt0, filt1, filt_h0, filt_h1, filter_vec, tmp0, tmp1, tmp2, tmp3;
+    __m128i dsth0, dsth1, dsth2, dsth3, dsth4, dsth5, dsth6;
+    __m128i dst10, dst21, dst22, dst73, dst84, dst95, dst106;
+    __m128i dst76_r, dst98_r, dst87_r, dst109_r;
+    __m128i dst10_r, dst32_r, dst54_r, dst21_r, dst43_r, dst65_r;
+    __m128i dst10_l, dst32_l, dst54_l, dst21_l, dst43_l, dst65_l;
+    __m128i dst0_r, dst0_l, dst1_r, dst1_l, dst2_r, dst2_l, dst3_r, dst3_l;
+    __m128i dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+
+    src -= (src_stride + 1);
+    DUP2_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filt0, filt1);
+
+    filter_vec = __lsx_vld(filter_y, 0);
+    filter_vec = __lsx_vsllwil_h_b(filter_vec, 0);
+    DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
+
+    mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
+    mask1 = __lsx_vaddi_bu(mask0, 2);
+
+    src_tmp = src;
+    dst_tmp = dst;
+
+    src0 = __lsx_vld(src_tmp, 0);
+    DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride_2x,
+              src1, src2);
+    src_tmp += src_stride_3x;
+
+    DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, vec0, vec1);
+    DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, vec2, vec3);
+    DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, vec4, vec5);
+
+    DUP2_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, dsth0, dsth1);
+    dsth2 = __lsx_vdp2_h_bu_b(vec4, filt0);
+    DUP2_ARG3(__lsx_vdp2add_h_bu_b, dsth0, vec1, filt1, dsth1, vec3, filt1,
+              dsth0, dsth1);
+    dsth2 = __lsx_vdp2add_h_bu_b(dsth2, vec5, filt1);
+
+    DUP2_ARG2(__lsx_vilvl_h, dsth1, dsth0, dsth2, dsth1, dst10_r, dst21_r);
+    DUP2_ARG2(__lsx_vilvh_h, dsth1, dsth0, dsth2, dsth1, dst10_l, dst21_l);
+
+    for (loop_cnt = 4; loop_cnt--;) {
+        src3 = __lsx_vld(src_tmp, 0);
+        DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride_2x,
+                  src4, src5);
+        src6 = __lsx_vldx(src_tmp, src_stride_3x);
+        src_tmp += src_stride_4x;
+
+        DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src4,
+                  src4, mask0, src4, src4, mask1, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, src6,
+                  src6, mask0, src6, src6, mask1, vec4, vec5, vec6, vec7);
+
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, vec4, filt0,
+                  vec6, filt0, dsth3, dsth4, dsth5, dsth6);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dsth3, vec1, filt1, dsth4,
+                  vec3, filt1, dsth5, vec5, filt1, dsth6, vec7, filt1,
+                  dsth3, dsth4, dsth5, dsth6);
+
+        DUP4_ARG2(__lsx_vilvl_h, dsth3, dsth2, dsth4, dsth3, dsth5, dsth4,
+                  dsth6, dsth5, dst32_r, dst43_r, dst54_r, dst65_r);
+        DUP4_ARG2(__lsx_vilvh_h, dsth3, dsth2, dsth4, dsth3, dsth5, dsth4,
+                  dsth6, dsth5, dst32_l, dst43_l, dst54_l, dst65_l);
+
+        DUP4_ARG2(__lsx_vdp2_w_h, dst10_r, filt_h0, dst10_l, filt_h0, dst21_r,
+                  filt_h0, dst21_l, filt_h0, dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP4_ARG2(__lsx_vdp2_w_h, dst32_r, filt_h0, dst32_l, filt_h0, dst43_r,
+                  filt_h0, dst43_l, filt_h0, dst2_r, dst2_l, dst3_r, dst3_l);
+        DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst32_r, filt_h1, dst0_l, dst32_l,
+                  filt_h1, dst1_r, dst43_r, filt_h1, dst1_l, dst43_l, filt_h1,
+                  dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP4_ARG3(__lsx_vdp2add_w_h, dst2_r, dst54_r, filt_h1, dst2_l, dst54_l,
+                  filt_h1, dst3_r, dst65_r, filt_h1, dst3_l, dst65_l, filt_h1,
+                  dst2_r, dst2_l, dst3_r, dst3_l);
+
+        DUP4_ARG3(__lsx_vsrani_h_w, dst0_l, dst0_r, 6, dst1_l, dst1_r, 6, dst2_l,
+                  dst2_r, 6, dst3_l, dst3_r, 6, tmp0, tmp1, tmp2, tmp3);
+        DUP2_ARG3(__lsx_vssrarni_bu_h, tmp1, tmp0, 6, tmp3, tmp2, 6, out0, out1);
+
+        __lsx_vstelm_d(out0, dst_tmp, 0, 0);
+        __lsx_vstelm_d(out0, dst_tmp + dst_stride, 0, 1);
+        __lsx_vstelm_d(out1, dst_tmp + dst_stride_2x, 0, 0);
+        __lsx_vstelm_d(out1, dst_tmp + dst_stride_3x, 0, 1);
+        dst_tmp += dst_stride_4x;
+
+        dst10_r = dst54_r;
+        dst10_l = dst54_l;
+        dst21_r = dst65_r;
+        dst21_l = dst65_l;
+        dsth2 = dsth6;
+    }
+
+    src += 8;
+    dst += 8;
+
+    mask2 = __lsx_vld(ff_hevc_mask_arr, 16);
+    mask3 = __lsx_vaddi_bu(mask2, 2);
+
+    src0 = __lsx_vld(src, 0);
+    DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src1, src2);
+    src += src_stride_3x;
+    DUP2_ARG3(__lsx_vshuf_b, src1, src0, mask2, src1, src0, mask3, vec0, vec1);
+    DUP2_ARG3(__lsx_vshuf_b, src2, src1, mask2, src2, src1, mask3, vec2, vec3);
+
+    DUP2_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, dst10, dst21);
+    DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst10, vec1, filt1, dst21, vec3, filt1,
+              dst10, dst21);
+
+    dst10_r = __lsx_vilvl_h(dst21, dst10);
+    dst21_r = __lsx_vilvh_h(dst21, dst10);
+    dst22 = __lsx_vreplvei_d(dst21, 1);
+
+    for (loop_cnt = 2; loop_cnt--;) {
+        src3 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src4, src5);
+        src6 = __lsx_vldx(src, src_stride_3x);
+        src += src_stride_4x;
+        src7 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src8, src9);
+        src10 = __lsx_vldx(src, src_stride_3x);
+        src += src_stride_4x;
+        DUP4_ARG3(__lsx_vshuf_b, src7, src3, mask2, src7, src3, mask3, src8,
+                  src4, mask2, src8, src4, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vshuf_b, src9, src5, mask2, src9, src5, mask3, src10,
+                  src6, mask2, src10, src6, mask3, vec4, vec5, vec6, vec7);
+
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, vec4, filt0,
+                  vec6, filt0, dst73, dst84, dst95, dst106);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst73, vec1, filt1, dst84, vec3,
+                  filt1, dst95, vec5, filt1, dst106, vec7, filt1,
+                  dst73, dst84, dst95, dst106);
+
+        dst32_r = __lsx_vilvl_h(dst73, dst22);
+        DUP2_ARG2(__lsx_vilvl_h, dst84, dst73, dst95, dst84, dst43_r, dst54_r);
+        DUP2_ARG2(__lsx_vilvh_h, dst84, dst73, dst95, dst84, dst87_r, dst98_r);
+        dst65_r = __lsx_vilvl_h(dst106, dst95);
+        dst109_r = __lsx_vilvh_h(dst106, dst95);
+        dst22 = __lsx_vreplvei_d(dst73, 1);
+        dst76_r = __lsx_vilvl_h(dst22, dst106);
+
+        DUP4_ARG2(__lsx_vdp2_w_h, dst10_r, filt_h0, dst21_r, filt_h0, dst32_r,
+                  filt_h0, dst43_r, filt_h0, dst0, dst1, dst2, dst3);
+        DUP4_ARG2(__lsx_vdp2_w_h, dst54_r, filt_h0, dst65_r, filt_h0, dst76_r,
+                  filt_h0, dst87_r, filt_h0, dst4, dst5, dst6, dst7);
+        DUP4_ARG3(__lsx_vdp2add_w_h, dst0, dst32_r, filt_h1, dst1, dst43_r,
+                  filt_h1, dst2, dst54_r, filt_h1, dst3, dst65_r, filt_h1,
+                  dst0, dst1, dst2, dst3);
+        DUP4_ARG3(__lsx_vdp2add_w_h, dst4, dst76_r, filt_h1, dst5, dst87_r,
+                  filt_h1, dst6, dst98_r, filt_h1, dst7, dst109_r, filt_h1,
+                  dst4, dst5, dst6, dst7);
+
+        DUP4_ARG3(__lsx_vsrani_h_w, dst1, dst0, 6, dst3, dst2, 6, dst5, dst4,
+                  6, dst7, dst6, 6, tmp0, tmp1, tmp2, tmp3);
+        DUP2_ARG3(__lsx_vssrarni_bu_h, tmp1, tmp0, 6, tmp3, tmp2, 6, out0, out1);
+
+        __lsx_vstelm_w(out0, dst, 0, 0);
+        __lsx_vstelm_w(out0, dst + dst_stride, 0, 1);
+        __lsx_vstelm_w(out0, dst + dst_stride_2x, 0, 2);
+        __lsx_vstelm_w(out0, dst + dst_stride_3x, 0, 3);
+        dst += dst_stride_4x;
+        __lsx_vstelm_w(out1, dst, 0, 0);
+        __lsx_vstelm_w(out1, dst + dst_stride, 0, 1);
+        __lsx_vstelm_w(out1, dst + dst_stride_2x, 0, 2);
+        __lsx_vstelm_w(out1, dst + dst_stride_3x, 0, 3);
+        dst += dst_stride_4x;
+
+        dst10_r = dst98_r;
+        dst21_r = dst109_r;
+        dst22 = __lsx_vreplvei_d(dst106, 1);
+    }
+}
+
+static void hevc_hv_4t_16w_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                               int32_t dst_stride, const int8_t *filter_x,
+                               const int8_t *filter_y, int32_t height)
+{
+    if (4 == height) {
+        hevc_hv_4t_8multx4_lsx(src, src_stride, dst, dst_stride, filter_x,
+                               filter_y, 2);
+    } else {
+        hevc_hv_4t_8multx4mult_lsx(src, src_stride, dst, dst_stride,
+                                   filter_x, filter_y, height, 2);
+    }
+}
+
+static void hevc_hv_4t_24w_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                               int32_t dst_stride, const int8_t *filter_x,
+                               const int8_t *filter_y, int32_t height)
+{
+    hevc_hv_4t_8multx4mult_lsx(src, src_stride, dst, dst_stride,
+                               filter_x, filter_y, height, 3);
+}
+
+static void hevc_hv_4t_32w_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                               int32_t dst_stride, const int8_t *filter_x,
+                               const int8_t *filter_y, int32_t height)
+{
+    hevc_hv_4t_8multx4mult_lsx(src, src_stride, dst, dst_stride,
+                               filter_x, filter_y, height, 4);
+}
+
+#define UNI_MC(PEL, DIR, WIDTH, TAP, DIR1, FILT_DIR)                           \
+void ff_hevc_put_hevc_uni_##PEL##_##DIR##WIDTH##_8_lsx(uint8_t *dst,           \
+                                                       ptrdiff_t dst_stride,   \
+                                                       uint8_t *src,           \
+                                                       ptrdiff_t src_stride,   \
+                                                       int height,             \
+                                                       intptr_t mx,            \
+                                                       intptr_t my,            \
+                                                       int width)              \
+{                                                                              \
+    const int8_t *filter = ff_hevc_##PEL##_filters[FILT_DIR - 1];              \
+                                                                               \
+    common_##DIR1##_##TAP##t_##WIDTH##w_lsx(src, src_stride, dst, dst_stride,  \
+                                            filter, height);                   \
+}
+
+UNI_MC(qpel, h, 64, 8, hz, mx);
+
+UNI_MC(qpel, v, 24, 8, vt, my);
+UNI_MC(qpel, v, 32, 8, vt, my);
+UNI_MC(qpel, v, 48, 8, vt, my);
+UNI_MC(qpel, v, 64, 8, vt, my);
+
+UNI_MC(epel, v, 24, 4, vt, my);
+UNI_MC(epel, v, 32, 4, vt, my);
+
+#undef UNI_MC
+
+#define UNI_MC_HV(PEL, WIDTH, TAP)                                         \
+void ff_hevc_put_hevc_uni_##PEL##_hv##WIDTH##_8_lsx(uint8_t *dst,          \
+                                                    ptrdiff_t dst_stride,  \
+                                                    uint8_t *src,          \
+                                                    ptrdiff_t src_stride,  \
+                                                    int height,            \
+                                                    intptr_t mx,           \
+                                                    intptr_t my,           \
+                                                    int width)             \
+{                                                                          \
+    const int8_t *filter_x = ff_hevc_##PEL##_filters[mx - 1];              \
+    const int8_t *filter_y = ff_hevc_##PEL##_filters[my - 1];              \
+                                                                           \
+    hevc_hv_##TAP##t_##WIDTH##w_lsx(src, src_stride, dst, dst_stride,  \
+                                    filter_x, filter_y, height);       \
+}
+
+UNI_MC_HV(qpel, 8, 8);
+UNI_MC_HV(qpel, 16, 8);
+UNI_MC_HV(qpel, 24, 8);
+UNI_MC_HV(qpel, 32, 8);
+UNI_MC_HV(qpel, 48, 8);
+UNI_MC_HV(qpel, 64, 8);
+
+UNI_MC_HV(epel, 8, 4);
+UNI_MC_HV(epel, 12, 4);
+UNI_MC_HV(epel, 16, 4);
+UNI_MC_HV(epel, 24, 4);
+UNI_MC_HV(epel, 32, 4);
+
+#undef UNI_MC_HV
diff --git a/libavcodec/loongarch/hevc_mc_uniw_lsx.c b/libavcodec/loongarch/hevc_mc_uniw_lsx.c
new file mode 100644
index 0000000000..118f5b820e
--- /dev/null
+++ b/libavcodec/loongarch/hevc_mc_uniw_lsx.c
@@ -0,0 +1,298 @@
+/*
+ * Copyright (c) 2022 Loongson Technology Corporation Limited
+ * Contributed by Lu Wang <wanglu@loongson.cn>
+ *                Hao Chen <chenhao@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "libavutil/loongarch/loongson_intrinsics.h"
+#include "hevcdsp_lsx.h"
+
+static const uint8_t ff_hevc_mask_arr[16 * 2] __attribute__((aligned(0x40))) = {
+    /* 8 width cases */
+    0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8,
+    /* 4 width cases */
+    0, 1, 1, 2, 2, 3, 3, 4, 16, 17, 17, 18, 18, 19, 19, 20
+};
+
+static av_always_inline
+void hevc_hv_8t_8x2_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                        int32_t dst_stride, const int8_t *filter_x,
+                        const int8_t *filter_y, int32_t height, int32_t weight,
+                        int32_t offset, int32_t rnd_val, int32_t width)
+{
+    uint32_t loop_cnt, cnt;
+    uint8_t *src_tmp;
+    uint8_t *dst_tmp;
+    const int32_t src_stride_2x = (src_stride << 1);
+    const int32_t dst_stride_2x = (dst_stride << 1);
+    const int32_t src_stride_4x = (src_stride << 2);
+    const int32_t src_stride_3x = src_stride_2x + src_stride;
+
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8;
+    __m128i filt0, filt1, filt2, filt3;
+    __m128i filt_h0, filt_h1, filt_h2, filt_h3;
+    __m128i mask1, mask2, mask3;
+    __m128i filter_vec;
+    __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
+    __m128i vec8, vec9, vec10, vec11, vec12, vec13, vec14, vec15;
+    __m128i dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7, dst8;
+    __m128i dst0_r, dst0_l, dst1_r, dst1_l;
+    __m128i dst10_r, dst32_r, dst54_r, dst76_r;
+    __m128i dst10_l, dst32_l, dst54_l, dst76_l;
+    __m128i dst21_r, dst43_r, dst65_r, dst87_r;
+    __m128i dst21_l, dst43_l, dst65_l, dst87_l;
+    __m128i weight_vec, offset_vec, rnd_vec;
+    __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
+
+    src -= (src_stride_3x + 3);
+    weight_vec = __lsx_vreplgr2vr_w(weight);
+    offset_vec = __lsx_vreplgr2vr_w(offset);
+    rnd_vec = __lsx_vreplgr2vr_w(rnd_val);
+
+    DUP4_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filter_x, 4,
+              filter_x, 6, filt0, filt1, filt2, filt3);
+    filter_vec = __lsx_vld(filter_y, 0);
+    filter_vec = __lsx_vsllwil_h_b(filter_vec, 0);
+    DUP4_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filter_vec, 2,
+              filter_vec, 3, filt_h0, filt_h1, filt_h2, filt_h3);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    mask3 = __lsx_vaddi_bu(mask0, 6);
+
+    for (cnt = width >> 3; cnt--;) {
+        src_tmp = src;
+        dst_tmp = dst;
+
+        src0 = __lsx_vld(src_tmp, 0);
+        DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride_2x,
+                  src1, src2);
+        src3 = __lsx_vldx(src_tmp, src_stride_3x);
+        src_tmp += src_stride_4x;
+        src4 = __lsx_vld(src_tmp, 0);
+        DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride_2x,
+                  src5, src6);
+        src_tmp += src_stride_3x;
+
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src0,
+                  src0, mask2, src0, src0, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, src1,
+                  src1, mask2, src1, src1, mask3, vec4, vec5, vec6, vec7);
+        DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, src2,
+                  src2, mask2, src2, src2, mask3, vec8, vec9, vec10, vec11);
+        DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src3,
+                  src3, mask2, src3, src3, mask3, vec12, vec13, vec14, vec15);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec4, filt0, vec8, filt0,
+                  vec12, filt0, dst0, dst1, dst2, dst3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec1, filt1, dst1, vec5, filt1,
+                  dst2, vec9, filt1, dst3, vec13, filt1, dst0, dst1, dst2, dst3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec2, filt2, dst1, vec6, filt2,
+                  dst2, vec10, filt2, dst3, vec14, filt2, dst0, dst1, dst2, dst3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec3, filt3, dst1, vec7, filt3,
+                  dst2, vec11, filt3, dst3, vec15, filt3, dst0, dst1, dst2, dst3);
+        DUP4_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, src4,
+                  src4, mask2, src4, src4, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, src5,
+                  src5, mask2, src5, src5, mask3, vec4, vec5, vec6, vec7);
+        DUP4_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, src6,
+                  src6, mask2, src6, src6, mask3, vec8, vec9, vec10, vec11);
+        DUP2_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec4, filt0, dst4, dst5);
+        dst6 = __lsx_vdp2_h_bu_b(vec8, filt0);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst4, vec1, filt1, dst5, vec5, filt1,
+                  dst6, vec9, filt1, dst4, vec2, filt2, dst4, dst5, dst6, dst4);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst5, vec6, filt2, dst6, vec10, filt2,
+                  dst4, vec3, filt3, dst5, vec7, filt3, dst5, dst6, dst4, dst5);
+        dst6 = __lsx_vdp2add_h_bu_b(dst6, vec11, filt3);
+
+        DUP4_ARG2(__lsx_vilvl_h, dst1, dst0, dst3, dst2, dst5, dst4, dst2,
+                  dst1, dst10_r, dst32_r, dst54_r, dst21_r);
+        DUP2_ARG2(__lsx_vilvl_h, dst4, dst3, dst6, dst5, dst43_r, dst65_r);
+        DUP4_ARG2(__lsx_vilvh_h, dst1, dst0, dst3, dst2, dst5, dst4, dst2,
+                  dst1, dst10_l, dst32_l, dst54_l, dst21_l);
+        DUP2_ARG2(__lsx_vilvh_h, dst4, dst3, dst6, dst5, dst43_l, dst65_l);
+
+        for (loop_cnt = height >> 1; loop_cnt--;) {
+            src7 = __lsx_vld(src_tmp, 0);
+            src8 = __lsx_vldx(src_tmp, src_stride);
+            src_tmp += src_stride_2x;
+            DUP4_ARG3(__lsx_vshuf_b, src7, src7, mask0, src7, src7, mask1, src7,
+                      src7, mask2, src7, src7, mask3, vec0, vec1, vec2, vec3);
+            dst7 = __lsx_vdp2_h_bu_b(vec0, filt0);
+            DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst7, vec1, filt1, dst7, vec2,
+                      filt2, dst7, dst7);
+            dst7 = __lsx_vdp2add_h_bu_b(dst7, vec3, filt3);
+            dst76_r = __lsx_vilvl_h(dst7, dst6);
+            dst76_l = __lsx_vilvh_h(dst7, dst6);
+            DUP2_ARG2(__lsx_vdp2_w_h, dst10_r, filt_h0, dst10_l, filt_h0,
+                      dst0_r, dst0_l);
+            DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst32_r, filt_h1, dst0_l,
+                      dst32_l, filt_h1, dst0_r, dst54_r, filt_h2, dst0_l,
+                      dst54_l, filt_h2, dst0_r, dst0_l, dst0_r, dst0_l);
+            DUP2_ARG3(__lsx_vdp2add_w_h, dst0_r, dst76_r, filt_h3, dst0_l,
+                      dst76_l, filt_h3, dst0_r, dst0_l);
+            DUP2_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst0_r, dst0_l);
+
+            /* row 8 */
+            DUP4_ARG3(__lsx_vshuf_b, src8, src8, mask0, src8, src8, mask1, src8,
+                      src8, mask2, src8, src8, mask3, vec0, vec1, vec2, vec3);
+            dst8 = __lsx_vdp2_h_bu_b(vec0, filt0);
+            DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst8, vec1, filt1, dst8, vec2,
+                      filt2, dst8, dst8);
+            dst8 = __lsx_vdp2add_h_bu_b(dst8, vec3, filt3);
+
+            dst87_r = __lsx_vilvl_h(dst8, dst7);
+            dst87_l = __lsx_vilvh_h(dst8, dst7);
+            DUP2_ARG2(__lsx_vdp2_w_h, dst21_r, filt_h0, dst21_l, filt_h0,
+                      dst1_r, dst1_l);
+            DUP4_ARG3(__lsx_vdp2add_w_h, dst1_r, dst43_r, filt_h1, dst1_l,
+                      dst43_l, filt_h1, dst1_r, dst65_r, filt_h2, dst1_l,
+                      dst65_l, filt_h2, dst1_r, dst1_l, dst1_r, dst1_l);
+            DUP2_ARG3(__lsx_vdp2add_w_h, dst1_r, dst87_r, filt_h3, dst1_l,
+                      dst87_l, filt_h3, dst1_r, dst1_l);
+            DUP2_ARG2(__lsx_vsrai_w, dst1_r, 6, dst1_l, 6, dst1_r, dst1_l);
+
+            DUP2_ARG2(__lsx_vmul_w, dst0_r, weight_vec, dst0_l, weight_vec,
+                      dst0_r, dst0_l);
+            DUP2_ARG2(__lsx_vmul_w, dst1_r, weight_vec, dst1_l, weight_vec,
+                      dst1_r, dst1_l);
+            DUP4_ARG2(__lsx_vsrar_w, dst0_r, rnd_vec, dst1_r, rnd_vec, dst0_l,
+                     rnd_vec, dst1_l, rnd_vec, dst0_r, dst1_r, dst0_l, dst1_l);
+
+            DUP2_ARG2(__lsx_vadd_w, dst0_r, offset_vec, dst0_l, offset_vec,
+                      dst0_r, dst0_l);
+            DUP2_ARG2(__lsx_vadd_w, dst1_r, offset_vec, dst1_l, offset_vec,
+                      dst1_r, dst1_l);
+            DUP4_ARG1(__lsx_vclip255_w, dst0_r, dst1_r, dst0_l, dst1_l, dst0_r,
+                      dst1_r, dst0_l, dst1_l);
+            DUP2_ARG2(__lsx_vpickev_h, dst0_l, dst0_r, dst1_l, dst1_r,
+                      dst0_r, dst1_r);
+            dst0_r = __lsx_vpickev_b(dst1_r, dst0_r);
+
+            __lsx_vstelm_d(dst0_r, dst_tmp, 0, 0);
+            __lsx_vstelm_d(dst0_r, dst_tmp + dst_stride, 0, 1);
+            dst_tmp += dst_stride_2x;
+
+            dst10_r = dst32_r;
+            dst32_r = dst54_r;
+            dst54_r = dst76_r;
+            dst10_l = dst32_l;
+            dst32_l = dst54_l;
+            dst54_l = dst76_l;
+            dst21_r = dst43_r;
+            dst43_r = dst65_r;
+            dst65_r = dst87_r;
+            dst21_l = dst43_l;
+            dst43_l = dst65_l;
+            dst65_l = dst87_l;
+            dst6 = dst8;
+        }
+
+        src += 8;
+        dst += 8;
+    }
+}
+
+static
+void hevc_hv_8t_8w_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                       int32_t dst_stride, const int8_t *filter_x,
+                       const int8_t *filter_y, int32_t height, int32_t weight,
+                       int32_t offset, int32_t rnd_val)
+{
+    hevc_hv_8t_8x2_lsx(src, src_stride, dst, dst_stride, filter_x,
+                       filter_y, height, weight, offset, rnd_val, 8);
+}
+
+static
+void hevc_hv_8t_16w_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                        int32_t dst_stride, const int8_t *filter_x,
+                        const int8_t *filter_y, int32_t height, int32_t weight,
+                        int32_t offset, int32_t rnd_val)
+{
+    hevc_hv_8t_8x2_lsx(src, src_stride, dst, dst_stride, filter_x,
+                       filter_y, height, weight, offset, rnd_val, 16);
+}
+
+static
+void hevc_hv_8t_24w_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                        int32_t dst_stride, const int8_t *filter_x,
+                        const int8_t *filter_y, int32_t height, int32_t weight,
+                        int32_t offset, int32_t rnd_val)
+{
+    hevc_hv_8t_8x2_lsx(src, src_stride, dst, dst_stride, filter_x,
+                       filter_y, height, weight, offset, rnd_val, 24);
+}
+
+static
+void hevc_hv_8t_32w_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                        int32_t dst_stride, const int8_t *filter_x,
+                        const int8_t *filter_y, int32_t height, int32_t weight,
+                        int32_t offset, int32_t rnd_val)
+{
+    hevc_hv_8t_8x2_lsx(src, src_stride, dst, dst_stride, filter_x,
+                       filter_y, height, weight, offset, rnd_val, 32);
+}
+
+static
+void hevc_hv_8t_48w_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                        int32_t dst_stride, const int8_t *filter_x,
+                        const int8_t *filter_y, int32_t height, int32_t weight,
+                        int32_t offset, int32_t rnd_val)
+{
+    hevc_hv_8t_8x2_lsx(src, src_stride, dst, dst_stride, filter_x,
+                       filter_y, height, weight, offset, rnd_val, 48);
+}
+
+static
+void hevc_hv_8t_64w_lsx(uint8_t *src, int32_t src_stride, uint8_t *dst,
+                        int32_t dst_stride, const int8_t *filter_x,
+                        const int8_t *filter_y, int32_t height, int32_t weight,
+                        int32_t offset, int32_t rnd_val)
+{
+    hevc_hv_8t_8x2_lsx(src, src_stride, dst, dst_stride, filter_x,
+                       filter_y, height, weight, offset, rnd_val, 64);
+}
+
+
+#define UNI_W_MC_HV(PEL, WIDTH, TAP)                                           \
+void ff_hevc_put_hevc_uni_w_##PEL##_hv##WIDTH##_8_lsx(uint8_t *dst,            \
+                                                      ptrdiff_t dst_stride,    \
+                                                      uint8_t *src,            \
+                                                      ptrdiff_t src_stride,    \
+                                                      int height,              \
+                                                      int denom,               \
+                                                      int weight,              \
+                                                      int offset,              \
+                                                      intptr_t mx,             \
+                                                      intptr_t my,             \
+                                                      int width)               \
+{                                                                              \
+    const int8_t *filter_x = ff_hevc_##PEL##_filters[mx - 1];                  \
+    const int8_t *filter_y = ff_hevc_##PEL##_filters[my - 1];                  \
+    int shift = denom + 14 - 8;                                                \
+                                                                               \
+    hevc_hv_##TAP##t_##WIDTH##w_lsx(src, src_stride, dst, dst_stride, filter_x,\
+                                    filter_y,  height, weight, offset, shift); \
+}
+
+UNI_W_MC_HV(qpel, 8, 8);
+UNI_W_MC_HV(qpel, 16, 8);
+UNI_W_MC_HV(qpel, 24, 8);
+UNI_W_MC_HV(qpel, 32, 8);
+UNI_W_MC_HV(qpel, 48, 8);
+UNI_W_MC_HV(qpel, 64, 8);
+
+#undef UNI_W_MC_HV
diff --git a/libavcodec/loongarch/hevcdsp_init_loongarch.c b/libavcodec/loongarch/hevcdsp_init_loongarch.c
new file mode 100644
index 0000000000..22739c6f5b
--- /dev/null
+++ b/libavcodec/loongarch/hevcdsp_init_loongarch.c
@@ -0,0 +1,190 @@
+/*
+ * Copyright (c) 2022 Loongson Technology Corporation Limited
+ * Contributed by Lu Wang <wanglu@loongson.cn>
+ *                Hao Chen <chenhao@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "libavutil/loongarch/cpu.h"
+#include "hevcdsp_lsx.h"
+
+void ff_hevc_dsp_init_loongarch(HEVCDSPContext *c, const int bit_depth)
+{
+    int cpu_flags = av_get_cpu_flags();
+
+    if (have_lsx(cpu_flags)) {
+        if (bit_depth == 8) {
+            c->put_hevc_qpel[1][0][0] = ff_hevc_put_hevc_pel_pixels4_8_lsx;
+            c->put_hevc_qpel[2][0][0] = ff_hevc_put_hevc_pel_pixels6_8_lsx;
+            c->put_hevc_qpel[3][0][0] = ff_hevc_put_hevc_pel_pixels8_8_lsx;
+            c->put_hevc_qpel[4][0][0] = ff_hevc_put_hevc_pel_pixels12_8_lsx;
+            c->put_hevc_qpel[5][0][0] = ff_hevc_put_hevc_pel_pixels16_8_lsx;
+            c->put_hevc_qpel[6][0][0] = ff_hevc_put_hevc_pel_pixels24_8_lsx;
+            c->put_hevc_qpel[7][0][0] = ff_hevc_put_hevc_pel_pixels32_8_lsx;
+            c->put_hevc_qpel[8][0][0] = ff_hevc_put_hevc_pel_pixels48_8_lsx;
+            c->put_hevc_qpel[9][0][0] = ff_hevc_put_hevc_pel_pixels64_8_lsx;
+
+            c->put_hevc_epel[1][0][0] = ff_hevc_put_hevc_pel_pixels4_8_lsx;
+            c->put_hevc_epel[2][0][0] = ff_hevc_put_hevc_pel_pixels6_8_lsx;
+            c->put_hevc_epel[3][0][0] = ff_hevc_put_hevc_pel_pixels8_8_lsx;
+            c->put_hevc_epel[4][0][0] = ff_hevc_put_hevc_pel_pixels12_8_lsx;
+            c->put_hevc_epel[5][0][0] = ff_hevc_put_hevc_pel_pixels16_8_lsx;
+            c->put_hevc_epel[6][0][0] = ff_hevc_put_hevc_pel_pixels24_8_lsx;
+            c->put_hevc_epel[7][0][0] = ff_hevc_put_hevc_pel_pixels32_8_lsx;
+
+            c->put_hevc_qpel[1][0][1] = ff_hevc_put_hevc_qpel_h4_8_lsx;
+            c->put_hevc_qpel[3][0][1] = ff_hevc_put_hevc_qpel_h8_8_lsx;
+            c->put_hevc_qpel[4][0][1] = ff_hevc_put_hevc_qpel_h12_8_lsx;
+            c->put_hevc_qpel[5][0][1] = ff_hevc_put_hevc_qpel_h16_8_lsx;
+            c->put_hevc_qpel[6][0][1] = ff_hevc_put_hevc_qpel_h24_8_lsx;
+            c->put_hevc_qpel[7][0][1] = ff_hevc_put_hevc_qpel_h32_8_lsx;
+            c->put_hevc_qpel[8][0][1] = ff_hevc_put_hevc_qpel_h48_8_lsx;
+            c->put_hevc_qpel[9][0][1] = ff_hevc_put_hevc_qpel_h64_8_lsx;
+
+            c->put_hevc_qpel[1][1][0] = ff_hevc_put_hevc_qpel_v4_8_lsx;
+            c->put_hevc_qpel[3][1][0] = ff_hevc_put_hevc_qpel_v8_8_lsx;
+            c->put_hevc_qpel[4][1][0] = ff_hevc_put_hevc_qpel_v12_8_lsx;
+            c->put_hevc_qpel[5][1][0] = ff_hevc_put_hevc_qpel_v16_8_lsx;
+            c->put_hevc_qpel[6][1][0] = ff_hevc_put_hevc_qpel_v24_8_lsx;
+            c->put_hevc_qpel[7][1][0] = ff_hevc_put_hevc_qpel_v32_8_lsx;
+            c->put_hevc_qpel[8][1][0] = ff_hevc_put_hevc_qpel_v48_8_lsx;
+            c->put_hevc_qpel[9][1][0] = ff_hevc_put_hevc_qpel_v64_8_lsx;
+
+            c->put_hevc_qpel[1][1][1] = ff_hevc_put_hevc_qpel_hv4_8_lsx;
+            c->put_hevc_qpel[3][1][1] = ff_hevc_put_hevc_qpel_hv8_8_lsx;
+            c->put_hevc_qpel[4][1][1] = ff_hevc_put_hevc_qpel_hv12_8_lsx;
+            c->put_hevc_qpel[5][1][1] = ff_hevc_put_hevc_qpel_hv16_8_lsx;
+            c->put_hevc_qpel[6][1][1] = ff_hevc_put_hevc_qpel_hv24_8_lsx;
+            c->put_hevc_qpel[7][1][1] = ff_hevc_put_hevc_qpel_hv32_8_lsx;
+            c->put_hevc_qpel[8][1][1] = ff_hevc_put_hevc_qpel_hv48_8_lsx;
+            c->put_hevc_qpel[9][1][1] = ff_hevc_put_hevc_qpel_hv64_8_lsx;
+
+            c->put_hevc_epel[7][0][1] = ff_hevc_put_hevc_epel_h32_8_lsx;
+
+            c->put_hevc_epel[5][1][0] = ff_hevc_put_hevc_epel_v16_8_lsx;
+            c->put_hevc_epel[6][1][0] = ff_hevc_put_hevc_epel_v24_8_lsx;
+            c->put_hevc_epel[7][1][0] = ff_hevc_put_hevc_epel_v32_8_lsx;
+
+            c->put_hevc_epel[3][1][1] = ff_hevc_put_hevc_epel_hv8_8_lsx;
+            c->put_hevc_epel[4][1][1] = ff_hevc_put_hevc_epel_hv12_8_lsx;
+            c->put_hevc_epel[5][1][1] = ff_hevc_put_hevc_epel_hv16_8_lsx;
+            c->put_hevc_epel[6][1][1] = ff_hevc_put_hevc_epel_hv24_8_lsx;
+            c->put_hevc_epel[7][1][1] = ff_hevc_put_hevc_epel_hv32_8_lsx;
+
+            c->put_hevc_qpel_bi[1][0][0] = ff_hevc_put_hevc_bi_pel_pixels4_8_lsx;
+            c->put_hevc_qpel_bi[3][0][0] = ff_hevc_put_hevc_bi_pel_pixels8_8_lsx;
+            c->put_hevc_qpel_bi[4][0][0] = ff_hevc_put_hevc_bi_pel_pixels12_8_lsx;
+            c->put_hevc_qpel_bi[5][0][0] = ff_hevc_put_hevc_bi_pel_pixels16_8_lsx;
+            c->put_hevc_qpel_bi[6][0][0] = ff_hevc_put_hevc_bi_pel_pixels24_8_lsx;
+            c->put_hevc_qpel_bi[7][0][0] = ff_hevc_put_hevc_bi_pel_pixels32_8_lsx;
+            c->put_hevc_qpel_bi[8][0][0] = ff_hevc_put_hevc_bi_pel_pixels48_8_lsx;
+            c->put_hevc_qpel_bi[9][0][0] = ff_hevc_put_hevc_bi_pel_pixels64_8_lsx;
+
+            c->put_hevc_epel_bi[1][0][0] = ff_hevc_put_hevc_bi_pel_pixels4_8_lsx;
+            c->put_hevc_epel_bi[2][0][0] = ff_hevc_put_hevc_bi_pel_pixels6_8_lsx;
+            c->put_hevc_epel_bi[3][0][0] = ff_hevc_put_hevc_bi_pel_pixels8_8_lsx;
+            c->put_hevc_epel_bi[4][0][0] = ff_hevc_put_hevc_bi_pel_pixels12_8_lsx;
+            c->put_hevc_epel_bi[5][0][0] = ff_hevc_put_hevc_bi_pel_pixels16_8_lsx;
+            c->put_hevc_epel_bi[6][0][0] = ff_hevc_put_hevc_bi_pel_pixels24_8_lsx;
+            c->put_hevc_epel_bi[7][0][0] = ff_hevc_put_hevc_bi_pel_pixels32_8_lsx;
+
+            c->put_hevc_qpel_bi[3][1][0] = ff_hevc_put_hevc_bi_qpel_v8_8_lsx;
+            c->put_hevc_qpel_bi[5][1][0] = ff_hevc_put_hevc_bi_qpel_v16_8_lsx;
+            c->put_hevc_qpel_bi[6][1][0] = ff_hevc_put_hevc_bi_qpel_v24_8_lsx;
+            c->put_hevc_qpel_bi[7][1][0] = ff_hevc_put_hevc_bi_qpel_v32_8_lsx;
+            c->put_hevc_qpel_bi[8][1][0] = ff_hevc_put_hevc_bi_qpel_v48_8_lsx;
+            c->put_hevc_qpel_bi[9][1][0] = ff_hevc_put_hevc_bi_qpel_v64_8_lsx;
+
+            c->put_hevc_qpel_bi[3][1][1] = ff_hevc_put_hevc_bi_qpel_hv8_8_lsx;
+            c->put_hevc_qpel_bi[5][1][1] = ff_hevc_put_hevc_bi_qpel_hv16_8_lsx;
+            c->put_hevc_qpel_bi[6][1][1] = ff_hevc_put_hevc_bi_qpel_hv24_8_lsx;
+            c->put_hevc_qpel_bi[7][1][1] = ff_hevc_put_hevc_bi_qpel_hv32_8_lsx;
+            c->put_hevc_qpel_bi[8][1][1] = ff_hevc_put_hevc_bi_qpel_hv48_8_lsx;
+            c->put_hevc_qpel_bi[9][1][1] = ff_hevc_put_hevc_bi_qpel_hv64_8_lsx;
+
+            c->put_hevc_qpel_bi[5][0][1] = ff_hevc_put_hevc_bi_qpel_h16_8_lsx;
+            c->put_hevc_qpel_bi[6][0][1] = ff_hevc_put_hevc_bi_qpel_h24_8_lsx;
+            c->put_hevc_qpel_bi[7][0][1] = ff_hevc_put_hevc_bi_qpel_h32_8_lsx;
+            c->put_hevc_qpel_bi[8][0][1] = ff_hevc_put_hevc_bi_qpel_h48_8_lsx;
+            c->put_hevc_qpel_bi[9][0][1] = ff_hevc_put_hevc_bi_qpel_h64_8_lsx;
+
+            c->put_hevc_epel_bi[6][0][1] = ff_hevc_put_hevc_bi_epel_h24_8_lsx;
+            c->put_hevc_epel_bi[7][0][1] = ff_hevc_put_hevc_bi_epel_h32_8_lsx;
+
+            c->put_hevc_epel_bi[4][1][0] = ff_hevc_put_hevc_bi_epel_v12_8_lsx;
+            c->put_hevc_epel_bi[5][1][0] = ff_hevc_put_hevc_bi_epel_v16_8_lsx;
+            c->put_hevc_epel_bi[6][1][0] = ff_hevc_put_hevc_bi_epel_v24_8_lsx;
+            c->put_hevc_epel_bi[7][1][0] = ff_hevc_put_hevc_bi_epel_v32_8_lsx;
+
+            c->put_hevc_epel_bi[2][1][1] = ff_hevc_put_hevc_bi_epel_hv6_8_lsx;
+            c->put_hevc_epel_bi[3][1][1] = ff_hevc_put_hevc_bi_epel_hv8_8_lsx;
+            c->put_hevc_epel_bi[5][1][1] = ff_hevc_put_hevc_bi_epel_hv16_8_lsx;
+            c->put_hevc_epel_bi[6][1][1] = ff_hevc_put_hevc_bi_epel_hv24_8_lsx;
+            c->put_hevc_epel_bi[7][1][1] = ff_hevc_put_hevc_bi_epel_hv32_8_lsx;
+
+            c->put_hevc_qpel_uni[9][0][1] = ff_hevc_put_hevc_uni_qpel_h64_8_lsx;
+
+            c->put_hevc_qpel_uni[6][1][0] = ff_hevc_put_hevc_uni_qpel_v24_8_lsx;
+            c->put_hevc_qpel_uni[7][1][0] = ff_hevc_put_hevc_uni_qpel_v32_8_lsx;
+            c->put_hevc_qpel_uni[8][1][0] = ff_hevc_put_hevc_uni_qpel_v48_8_lsx;
+            c->put_hevc_qpel_uni[9][1][0] = ff_hevc_put_hevc_uni_qpel_v64_8_lsx;
+
+            c->put_hevc_qpel_uni[3][1][1] = ff_hevc_put_hevc_uni_qpel_hv8_8_lsx;
+            c->put_hevc_qpel_uni[5][1][1] = ff_hevc_put_hevc_uni_qpel_hv16_8_lsx;
+            c->put_hevc_qpel_uni[6][1][1] = ff_hevc_put_hevc_uni_qpel_hv24_8_lsx;
+            c->put_hevc_qpel_uni[7][1][1] = ff_hevc_put_hevc_uni_qpel_hv32_8_lsx;
+            c->put_hevc_qpel_uni[8][1][1] = ff_hevc_put_hevc_uni_qpel_hv48_8_lsx;
+            c->put_hevc_qpel_uni[9][1][1] = ff_hevc_put_hevc_uni_qpel_hv64_8_lsx;
+
+            c->put_hevc_epel_uni[6][1][0] = ff_hevc_put_hevc_uni_epel_v24_8_lsx;
+            c->put_hevc_epel_uni[7][1][0] = ff_hevc_put_hevc_uni_epel_v32_8_lsx;
+
+            c->put_hevc_epel_uni[3][1][1] = ff_hevc_put_hevc_uni_epel_hv8_8_lsx;
+            c->put_hevc_epel_uni[4][1][1] = ff_hevc_put_hevc_uni_epel_hv12_8_lsx;
+            c->put_hevc_epel_uni[5][1][1] = ff_hevc_put_hevc_uni_epel_hv16_8_lsx;
+            c->put_hevc_epel_uni[6][1][1] = ff_hevc_put_hevc_uni_epel_hv24_8_lsx;
+            c->put_hevc_epel_uni[7][1][1] = ff_hevc_put_hevc_uni_epel_hv32_8_lsx;
+
+            c->put_hevc_qpel_uni_w[3][1][1] = ff_hevc_put_hevc_uni_w_qpel_hv8_8_lsx;
+            c->put_hevc_qpel_uni_w[5][1][1] = ff_hevc_put_hevc_uni_w_qpel_hv16_8_lsx;
+            c->put_hevc_qpel_uni_w[6][1][1] = ff_hevc_put_hevc_uni_w_qpel_hv24_8_lsx;
+            c->put_hevc_qpel_uni_w[7][1][1] = ff_hevc_put_hevc_uni_w_qpel_hv32_8_lsx;
+            c->put_hevc_qpel_uni_w[8][1][1] = ff_hevc_put_hevc_uni_w_qpel_hv48_8_lsx;
+            c->put_hevc_qpel_uni_w[9][1][1] = ff_hevc_put_hevc_uni_w_qpel_hv64_8_lsx;
+
+            c->sao_edge_filter[4] = ff_hevc_sao_edge_filter_8_lsx;
+
+            c->hevc_h_loop_filter_luma = ff_hevc_loop_filter_luma_h_8_lsx;
+            c->hevc_v_loop_filter_luma = ff_hevc_loop_filter_luma_v_8_lsx;
+
+            c->hevc_h_loop_filter_luma_c = ff_hevc_loop_filter_luma_h_8_lsx;
+            c->hevc_v_loop_filter_luma_c = ff_hevc_loop_filter_luma_v_8_lsx;
+
+            c->hevc_h_loop_filter_chroma = ff_hevc_loop_filter_chroma_h_8_lsx;
+            c->hevc_v_loop_filter_chroma = ff_hevc_loop_filter_chroma_v_8_lsx;
+
+            c->hevc_h_loop_filter_chroma_c = ff_hevc_loop_filter_chroma_h_8_lsx;
+            c->hevc_v_loop_filter_chroma_c = ff_hevc_loop_filter_chroma_v_8_lsx;
+
+            c->idct[0] = ff_hevc_idct_4x4_lsx;
+            c->idct[1] = ff_hevc_idct_8x8_lsx;
+            c->idct[2] = ff_hevc_idct_16x16_lsx;
+            c->idct[3] = ff_hevc_idct_32x32_lsx;
+        }
+    }
+}
diff --git a/libavcodec/loongarch/hevcdsp_lsx.c b/libavcodec/loongarch/hevcdsp_lsx.c
new file mode 100644
index 0000000000..a520f02bd1
--- /dev/null
+++ b/libavcodec/loongarch/hevcdsp_lsx.c
@@ -0,0 +1,3299 @@
+/*
+ * Copyright (c) 2022 Loongson Technology Corporation Limited
+ * Contributed by Lu Wang <wanglu@loongson.cn>
+ *                Hao Chen <chenhao@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "libavutil/loongarch/loongson_intrinsics.h"
+#include "hevcdsp_lsx.h"
+
+static const uint8_t ff_hevc_mask_arr[16 * 2] __attribute__((aligned(0x40))) = {
+    /* 8 width cases */
+    0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8,
+    /* 4 width cases */
+    0, 1, 1, 2, 2, 3, 3, 4, 16, 17, 17, 18, 18, 19, 19, 20
+};
+
+/* hevc_copy: dst = src << 6 */
+static void hevc_copy_4w_lsx(uint8_t *src, int32_t src_stride,
+                             int16_t *dst, int32_t dst_stride,
+                             int32_t height)
+{
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t dst_stride_2x = (dst_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t dst_stride_4x = (dst_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    int32_t loop_cnt = height >> 3;
+    int32_t res = height & 0x07;
+
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m128i in0, in1, in2, in3;
+    for (; loop_cnt--;) {
+        src0 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x,
+                  src1, src2);
+        src3 = __lsx_vldx(src, src_stride_3x);
+        src += src_stride_4x;
+        src4 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x,
+                  src5, src6);
+        src7 = __lsx_vldx(src, src_stride_3x);
+        src += src_stride_4x;
+
+        DUP4_ARG2(__lsx_vilvl_w, src1, src0, src3, src2, src5, src4, src7, src6,
+                  src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, src2, 6, src3, 6,
+                  in0, in1, in2, in3);
+
+        __lsx_vstelm_d(in0, dst, 0, 0);
+        __lsx_vstelm_d(in0, dst + dst_stride, 0, 1);
+        __lsx_vstelm_d(in1, dst + dst_stride_2x, 0, 0);
+        __lsx_vstelm_d(in1, dst + dst_stride_3x, 0, 1);
+        dst += dst_stride_4x;
+        __lsx_vstelm_d(in2, dst, 0, 0);
+        __lsx_vstelm_d(in2, dst + dst_stride, 0, 1);
+        __lsx_vstelm_d(in3, dst + dst_stride_2x, 0, 0);
+        __lsx_vstelm_d(in3, dst + dst_stride_3x, 0, 1);
+        dst += dst_stride_4x;
+    }
+    for (;res--;) {
+        src0 = __lsx_vld(src, 0);
+        in0 = __lsx_vsllwil_hu_bu(src0, 6);
+        __lsx_vstelm_d(in0, dst, 0, 0);
+        src += src_stride;
+        dst += dst_stride;
+    }
+}
+
+static void hevc_copy_6w_lsx(uint8_t *src, int32_t src_stride,
+                             int16_t *dst, int32_t dst_stride,
+                             int32_t height)
+{
+    int32_t loop_cnt = (height >> 3);
+    int32_t res = height & 0x07;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m128i in0, in1, in2, in3, in4, in5, in6, in7;
+
+    for (loop_cnt = (height >> 3); loop_cnt--;) {
+        src0 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src1, src2);
+        src3 = __lsx_vldx(src, src_stride_3x);
+        src += src_stride_4x;
+        src4 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src5, src6);
+        src7 = __lsx_vldx(src, src_stride_3x);
+        src += src_stride_4x;
+
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, src2, 6, src3, 6,
+                  in0, in1, in2, in3);
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src4, 6, src5, 6, src6, 6, src7, 6,
+                  in4, in5, in6, in7);
+
+        __lsx_vstelm_d(in0, dst, 0, 0);
+        __lsx_vstelm_w(in0, dst, 8, 2);
+        dst += dst_stride;
+        __lsx_vstelm_d(in1, dst, 0, 0);
+        __lsx_vstelm_w(in1, dst, 8, 2);
+        dst += dst_stride;
+        __lsx_vstelm_d(in2, dst, 0, 0);
+        __lsx_vstelm_w(in2, dst, 8, 2);
+        dst += dst_stride;
+        __lsx_vstelm_d(in3, dst, 0, 0);
+        __lsx_vstelm_w(in3, dst, 8, 2);
+        dst += dst_stride;
+        __lsx_vstelm_d(in4, dst, 0, 0);
+        __lsx_vstelm_w(in4, dst, 8, 2);
+        dst += dst_stride;
+        __lsx_vstelm_d(in5, dst, 0, 0);
+        __lsx_vstelm_w(in5, dst, 8, 2);
+        dst += dst_stride;
+        __lsx_vstelm_d(in6, dst, 0, 0);
+        __lsx_vstelm_w(in6, dst, 8, 2);
+        dst += dst_stride;
+        __lsx_vstelm_d(in7, dst, 0, 0);
+        __lsx_vstelm_w(in7, dst, 8, 2);
+        dst += dst_stride;
+    }
+    for (;res--;) {
+        src0 = __lsx_vld(src, 0);
+        in0 = __lsx_vsllwil_hu_bu(src0, 6);
+        src += src_stride;
+        __lsx_vstelm_d(in0, dst, 0, 0);
+        __lsx_vstelm_w(in0, dst, 8, 2);
+        dst += dst_stride;
+    }
+}
+
+static void hevc_copy_8w_lsx(uint8_t *src, int32_t src_stride,
+                             int16_t *dst, int32_t dst_stride,
+                             int32_t height)
+{
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t dst_stride_x  = (dst_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t dst_stride_2x = (dst_stride_x << 1);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride_x;
+    int32_t loop_cnt = height >> 3;
+    int32_t res = height & 0x07;
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m128i in0, in1, in2, in3, in4, in5, in6, in7;
+
+    for (loop_cnt = (height >> 3); loop_cnt--;) {
+        src0 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x,
+                  src1, src2);
+        src3 = __lsx_vldx(src, src_stride_3x);
+        src += src_stride_4x;
+        src4 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x,
+                  src5, src6);
+        src7 = __lsx_vldx(src, src_stride_3x);
+        src += src_stride_4x;
+
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, src2, 6, src3, 6,
+                  in0, in1, in2, in3);
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src4, 6, src5, 6, src6, 6, src7, 6,
+                  in4, in5, in6, in7);
+        __lsx_vst(in0, dst, 0);
+        __lsx_vstx(in1, dst, dst_stride_x);
+        __lsx_vstx(in2, dst, dst_stride_2x);
+        __lsx_vstx(in3, dst, dst_stride_3x);
+        dst += dst_stride_2x;
+        __lsx_vst(in4, dst, 0);
+        __lsx_vstx(in5, dst, dst_stride_x);
+        __lsx_vstx(in6, dst, dst_stride_2x);
+        __lsx_vstx(in7, dst, dst_stride_3x);
+        dst += dst_stride_2x;
+    }
+    for (;res--;) {
+        src0 = __lsx_vld(src, 0);
+        in0 = __lsx_vsllwil_hu_bu(src0, 6);
+        __lsx_vst(in0, dst, 0);
+        src += src_stride;
+        dst += dst_stride;
+    }
+}
+
+static void hevc_copy_12w_lsx(uint8_t *src, int32_t src_stride,
+                              int16_t *dst, int32_t dst_stride,
+                              int32_t height)
+{
+    uint32_t loop_cnt;
+    uint32_t res = height & 0x07;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t dst_stride_x  = (dst_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t dst_stride_2x = (dst_stride_x << 1);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride_x;
+    __m128i zero = __lsx_vldi(0);
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m128i in0, in1, in0_r, in1_r, in2_r, in3_r;
+
+    for (loop_cnt = (height >> 3); loop_cnt--;) {
+        src0 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src1, src2);
+        src3 = __lsx_vldx(src, src_stride_3x);
+        src += src_stride_4x;
+        src4 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src5, src6);
+        src7 = __lsx_vldx(src, src_stride_3x);
+        src += src_stride_4x;
+
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, src2, 6, src3, 6,
+                  in0_r, in1_r, in2_r, in3_r);
+        DUP2_ARG2(__lsx_vilvh_w, src1, src0, src3, src2, src0, src1);
+        DUP2_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, in0, in1);
+        __lsx_vst(in0_r, dst, 0);
+        __lsx_vstx(in1_r, dst, dst_stride_x);
+        __lsx_vstx(in2_r, dst, dst_stride_2x);
+        __lsx_vstx(in3_r, dst, dst_stride_3x);
+        __lsx_vstelm_d(in0, dst, 16, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(in0, dst, 16, 1);
+        dst += dst_stride;
+        __lsx_vstelm_d(in1, dst, 16, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(in1, dst, 16, 1);
+        dst += dst_stride;
+
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src4, 6, src5, 6, src6, 6, src7, 6,
+                  in0_r, in1_r, in2_r, in3_r);
+        DUP2_ARG2(__lsx_vilvh_w, src5, src4, src7, src6, src0, src1);
+        DUP2_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, in0, in1);
+        __lsx_vst(in0_r, dst, 0);
+        __lsx_vstx(in1_r, dst, dst_stride_x);
+        __lsx_vstx(in2_r, dst, dst_stride_2x);
+        __lsx_vstx(in3_r, dst, dst_stride_3x);
+        __lsx_vstelm_d(in0, dst, 16, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(in0, dst, 16, 1);
+        dst += dst_stride;
+        __lsx_vstelm_d(in1, dst, 16, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(in1, dst, 16, 1);
+        dst += dst_stride;
+    }
+    for (;res--;) {
+        src0 = __lsx_vld(src, 0);
+        in0  = __lsx_vsllwil_hu_bu(src0, 6);
+        src1 = __lsx_vilvh_b(zero, src0);
+        in1  = __lsx_vslli_h(src1, 6);
+        __lsx_vst(in0, dst, 0);
+        __lsx_vstelm_d(in1, dst, 16, 0);
+        src += src_stride;
+        dst += dst_stride;
+    }
+}
+
+static void hevc_copy_16w_lsx(uint8_t *src, int32_t src_stride,
+                              int16_t *dst, int32_t dst_stride,
+                              int32_t height)
+{
+    __m128i zero = __lsx_vldi(0);
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t dst_stride_x  = (dst_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t dst_stride_2x = (dst_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride_x;
+    int32_t loop_cnt = height >> 3;
+    int32_t res = height & 0x07;
+    int16_t* dst1 = dst + 8;
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m128i in0_r, in1_r, in2_r, in3_r, in0_l, in1_l, in2_l, in3_l;
+
+    for (loop_cnt = (height >> 3); loop_cnt--;) {
+        src0 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x,
+                  src1, src2);
+        src3 = __lsx_vldx(src, src_stride_3x);
+        src += src_stride_4x;
+        src4 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x,
+                  src5, src6);
+        src7 = __lsx_vldx(src, src_stride_3x);
+        src += src_stride_4x;
+        DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero, src3,
+                  in0_l, in1_l, in2_l, in3_l);
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, src2, 6, src3, 6,
+                  in0_r, in1_r, in2_r, in3_r);
+        DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6, in0_l,
+                  in1_l, in2_l, in3_l);
+        __lsx_vst(in0_r, dst, 0);
+        __lsx_vstx(in1_r, dst, dst_stride_x);
+        __lsx_vstx(in2_r, dst, dst_stride_2x);
+        __lsx_vstx(in3_r, dst, dst_stride_3x);
+        __lsx_vst(in0_l, dst1, 0);
+        __lsx_vstx(in1_l, dst1, dst_stride_x);
+        __lsx_vstx(in2_l, dst1, dst_stride_2x);
+        __lsx_vstx(in3_l, dst1, dst_stride_3x);
+        dst += dst_stride_2x;
+        dst1 += dst_stride_2x;
+
+        DUP4_ARG2(__lsx_vilvh_b, zero, src4, zero, src5, zero, src6, zero, src7,
+                  in0_l, in1_l, in2_l, in3_l);
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src4, 6, src5, 6, src6, 6, src7, 6,
+                  in0_r, in1_r, in2_r, in3_r);
+        DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6, in0_l,
+                  in1_l, in2_l, in3_l);
+        __lsx_vst(in0_r, dst, 0);
+        __lsx_vstx(in1_r, dst, dst_stride_x);
+        __lsx_vstx(in2_r, dst, dst_stride_2x);
+        __lsx_vstx(in3_r, dst, dst_stride_3x);
+        __lsx_vst(in0_l, dst1, 0);
+        __lsx_vstx(in1_l, dst1, dst_stride_x);
+        __lsx_vstx(in2_l, dst1, dst_stride_2x);
+        __lsx_vstx(in3_l, dst1, dst_stride_3x);
+        dst += dst_stride_2x;
+        dst1 += dst_stride_2x;
+    }
+    if (res) {
+        src0 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src1, src2);
+        src3 = __lsx_vldx(src, src_stride_3x);
+
+        DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero, src3,
+                  in0_l, in1_l, in2_l, in3_l);
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, src2, 6, src3, 6,
+                  in0_r, in1_r, in2_r, in3_r);
+        DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6, in0_l,
+                  in1_l, in2_l, in3_l);
+        __lsx_vst(in0_r, dst, 0);
+        __lsx_vstx(in1_r, dst, dst_stride_x);
+        __lsx_vstx(in2_r, dst, dst_stride_2x);
+        __lsx_vstx(in3_r, dst, dst_stride_3x);
+        dst += 8;
+        __lsx_vst(in0_l, dst, 0);
+        __lsx_vstx(in1_l, dst, dst_stride_x);
+        __lsx_vstx(in2_l, dst, dst_stride_2x);
+        __lsx_vstx(in3_l, dst, dst_stride_3x);
+    }
+}
+
+static void hevc_copy_24w_lsx(uint8_t *src, int32_t src_stride,
+                              int16_t *dst, int32_t dst_stride,
+                              int32_t height)
+{
+    uint32_t loop_cnt;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t dst_stride_x = (dst_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t dst_stride_2x = (dst_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride_x;
+    uint8_t *_src = src + 16;
+    int16_t *dst1 = dst;
+    __m128i zero = __lsx_vldi(0);
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m128i in0_r, in1_r, in2_r, in3_r, in0_l, in1_l, in2_l, in3_l;
+
+    for (loop_cnt = (height >> 2); loop_cnt--;) {
+        src0 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src1, src2);
+        src3 = __lsx_vldx(src, src_stride_3x);
+        src += src_stride_4x;
+        src4 = __lsx_vld(_src, 0);
+        DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride_2x,
+                  src5, src6);
+        src7 = __lsx_vldx(_src, src_stride_3x);
+        _src += src_stride_4x;
+
+        DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero,
+                  src3, in0_l, in1_l, in2_l, in3_l);
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, src2, 6, src3, 6,
+                  in0_r, in1_r, in2_r, in3_r);
+        DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6,
+                  in0_l, in1_l, in2_l, in3_l);
+        __lsx_vst(in0_r, dst, 0);
+        __lsx_vstx(in1_r, dst, dst_stride_x);
+        __lsx_vstx(in2_r, dst, dst_stride_2x);
+        __lsx_vstx(in3_r, dst, dst_stride_3x);
+        dst1 = dst + 8;
+        __lsx_vst(in0_l, dst1, 0);
+        __lsx_vstx(in1_l, dst1, dst_stride_x);
+        __lsx_vstx(in2_l, dst1, dst_stride_2x);
+        __lsx_vstx(in3_l, dst1, dst_stride_3x);
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src4, 6, src5, 6, src6, 6, src7, 6,
+                  in0_r, in1_r, in2_r, in3_r);
+        dst1 = dst1 + 8;
+        __lsx_vst(in0_r, dst1, 0);
+        __lsx_vstx(in1_r, dst1, dst_stride_x);
+        __lsx_vstx(in2_r, dst1, dst_stride_2x);
+        __lsx_vstx(in3_r, dst1, dst_stride_3x);
+        dst += dst_stride_2x;
+    }
+}
+
+static void hevc_copy_32w_lsx(uint8_t *src, int32_t src_stride,
+                              int16_t *dst, int32_t dst_stride,
+                              int32_t height)
+{
+    uint32_t loop_cnt;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    uint8_t *_src = src + 16;
+    __m128i zero = {0};
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m128i in0_r, in1_r, in2_r, in3_r, in0_l, in1_l, in2_l, in3_l;
+
+    for (loop_cnt = (height >> 2); loop_cnt--;) {
+        src0 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src2, src4);
+        src6 = __lsx_vldx(src, src_stride_3x);
+        src += src_stride_4x;
+        src1 = __lsx_vld(_src, 0);
+        DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride_2x,
+                  src3, src5);
+        src7 = __lsx_vldx(_src, src_stride_3x);
+        _src += src_stride_4x;
+
+        DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero,
+                  src3, in0_l, in1_l, in2_l, in3_l);
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, src2, 6, src3, 6,
+                  in0_r, in1_r, in2_r, in3_r);
+        DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6,
+                  in0_l, in1_l, in2_l, in3_l);
+        __lsx_vst(in0_r, dst, 0);
+        __lsx_vst(in0_l, dst, 16);
+        __lsx_vst(in1_r, dst, 32);
+        __lsx_vst(in1_l, dst, 48);
+        dst += dst_stride;
+        __lsx_vst(in2_r, dst, 0);
+        __lsx_vst(in2_l, dst, 16);
+        __lsx_vst(in3_r, dst, 32);
+        __lsx_vst(in3_l, dst, 48);
+        dst += dst_stride;
+
+        DUP4_ARG2(__lsx_vilvh_b, zero, src4, zero, src5, zero, src6, zero, src7,
+                  in0_l, in1_l, in2_l, in3_l);
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src4, 6, src5, 6, src6, 6, src7, 6,
+                  in0_r, in1_r, in2_r, in3_r);
+        DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6, in0_l,
+                  in1_l, in2_l, in3_l);
+        __lsx_vst(in0_r, dst, 0);
+        __lsx_vst(in0_l, dst, 16);
+        __lsx_vst(in1_r, dst, 32);
+        __lsx_vst(in1_l, dst, 48);
+        dst += dst_stride;
+        __lsx_vst(in2_r, dst, 0);
+        __lsx_vst(in2_l, dst, 16);
+        __lsx_vst(in3_r, dst, 32);
+        __lsx_vst(in3_l, dst, 48);
+        dst += dst_stride;
+    }
+}
+
+static void hevc_copy_48w_lsx(uint8_t *src, int32_t src_stride,
+                              int16_t *dst, int32_t dst_stride,
+                              int32_t height)
+{
+    uint32_t loop_cnt;
+    __m128i zero = {0};
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m128i src8, src9, src10, src11;
+    __m128i in0_r, in1_r, in2_r, in3_r, in4_r, in5_r;
+    __m128i in0_l, in1_l, in2_l, in3_l, in4_l, in5_l;
+
+    for (loop_cnt = (height >> 2); loop_cnt--;) {
+        DUP2_ARG2(__lsx_vld, src, 0, src, 16, src0, src1);
+        src2 = __lsx_vld(src, 32);
+        src += src_stride;
+        DUP2_ARG2(__lsx_vld, src, 0, src, 16, src3, src4);
+        src5 = __lsx_vld(src, 32);
+        src += src_stride;
+        DUP2_ARG2(__lsx_vld, src, 0, src, 16, src6, src7);
+        src8 = __lsx_vld(src, 32);
+        src += src_stride;
+        DUP2_ARG2(__lsx_vld, src, 0, src, 16, src9, src10);
+        src11 = __lsx_vld(src, 32);
+        src += src_stride;
+
+        DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero,
+                  src3, in0_l, in1_l, in2_l, in3_l);
+        DUP2_ARG2(__lsx_vilvh_b, zero, src4, zero, src5, in4_l, in5_l);
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, src2, 6, src3, 6,
+                  in0_r, in1_r, in2_r, in3_r);
+        DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6, in0_l,
+                  in1_l, in2_l, in3_l);
+        DUP2_ARG2(__lsx_vsllwil_hu_bu, src4, 6, src5, 6, in4_r, in5_r);
+        DUP2_ARG2(__lsx_vslli_h, in4_l, 6, in5_l, 6, in4_l, in5_l);
+        __lsx_vst(in0_r, dst, 0);
+        __lsx_vst(in0_l, dst, 16);
+        __lsx_vst(in1_r, dst, 32);
+        __lsx_vst(in1_l, dst, 48);
+        __lsx_vst(in2_r, dst, 64);
+        __lsx_vst(in2_l, dst, 80);
+        dst += dst_stride;
+        __lsx_vst(in3_r, dst, 0);
+        __lsx_vst(in3_l, dst, 16);
+        __lsx_vst(in4_r, dst, 32);
+        __lsx_vst(in4_l, dst, 48);
+        __lsx_vst(in5_r, dst, 64);
+        __lsx_vst(in5_l, dst, 80);
+        dst += dst_stride;
+
+        DUP4_ARG2(__lsx_vilvh_b, zero, src6, zero, src7, zero, src8, zero, src9,
+                  in0_l, in1_l, in2_l, in3_l);
+        DUP2_ARG2(__lsx_vilvh_b, zero, src10, zero, src11, in4_l, in5_l);
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src6, 6, src7, 6, src8, 6, src9, 6,
+                  in0_r, in1_r, in2_r, in3_r);
+        DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6, in0_l,
+                  in1_l, in2_l, in3_l);
+        DUP2_ARG2(__lsx_vsllwil_hu_bu, src10, 6, src11, 6, in4_r, in5_r);
+        DUP2_ARG2(__lsx_vslli_h, in4_l, 6, in5_l, 6, in4_l, in5_l);
+        __lsx_vst(in0_r, dst, 0);
+        __lsx_vst(in0_l, dst, 16);
+        __lsx_vst(in1_r, dst, 32);
+        __lsx_vst(in1_l, dst, 48);
+        __lsx_vst(in2_r, dst, 64);
+        __lsx_vst(in2_l, dst, 80);
+        dst += dst_stride;
+        __lsx_vst(in3_r, dst, 0);
+        __lsx_vst(in3_l, dst, 16);
+        __lsx_vst(in4_r, dst, 32);
+        __lsx_vst(in4_l, dst, 48);
+        __lsx_vst(in5_r, dst, 64);
+        __lsx_vst(in5_l, dst, 80);
+        dst += dst_stride;
+    }
+}
+
+static void hevc_copy_64w_lsx(uint8_t *src, int32_t src_stride,
+                              int16_t *dst, int32_t dst_stride,
+                              int32_t height)
+{
+    uint32_t loop_cnt;
+    __m128i zero = {0};
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m128i in0_r, in1_r, in2_r, in3_r, in0_l, in1_l, in2_l, in3_l;
+
+
+    for (loop_cnt = (height >> 1); loop_cnt--;) {
+        DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48,
+                  src0, src1, src2, src3);
+        src += src_stride;
+        DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48,
+                  src4, src5, src6, src7);
+        src += src_stride;
+
+        DUP4_ARG2(__lsx_vilvh_b, zero, src0, zero, src1, zero, src2, zero,
+                  src3, in0_l, in1_l, in2_l, in3_l);
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src0, 6, src1, 6, src2, 6, src3, 6,
+                  in0_r, in1_r, in2_r, in3_r);
+        DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6,
+                  in0_l, in1_l, in2_l, in3_l);
+        __lsx_vst(in0_r, dst, 0);
+        __lsx_vst(in0_l, dst, 16);
+        __lsx_vst(in1_r, dst, 32);
+        __lsx_vst(in1_l, dst, 48);
+        __lsx_vst(in2_r, dst, 64);
+        __lsx_vst(in2_l, dst, 80);
+        __lsx_vst(in3_r, dst, 96);
+        __lsx_vst(in3_l, dst, 112);
+        dst += dst_stride;
+
+        DUP4_ARG2(__lsx_vilvh_b, zero, src4, zero, src5, zero, src6, zero,
+                  src7, in0_l, in1_l, in2_l, in3_l);
+        DUP4_ARG2(__lsx_vsllwil_hu_bu, src4, 6, src5, 6, src6, 6, src7, 6,
+                  in0_r, in1_r, in2_r, in3_r);
+        DUP4_ARG2(__lsx_vslli_h, in0_l, 6, in1_l, 6, in2_l, 6, in3_l, 6, in0_l,
+                  in1_l, in2_l, in3_l);
+        __lsx_vst(in0_r, dst, 0);
+        __lsx_vst(in0_l, dst, 16);
+        __lsx_vst(in1_r, dst, 32);
+        __lsx_vst(in1_l, dst, 48);
+        __lsx_vst(in2_r, dst, 64);
+        __lsx_vst(in2_l, dst, 80);
+        __lsx_vst(in3_r, dst, 96);
+        __lsx_vst(in3_l, dst, 112);
+        dst += dst_stride;
+    }
+}
+
+static void hevc_hz_8t_4w_lsx(uint8_t *src, int32_t src_stride,
+                              int16_t *dst, int32_t dst_stride,
+                              const int8_t *filter, int32_t height)
+{
+    uint32_t loop_cnt = height >> 3;
+    uint32_t res = (height & 0x7) >> 1;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t dst_stride_2x = (dst_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t dst_stride_4x = (dst_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m128i filt0, filt1, filt2, filt3;
+    __m128i mask1, mask2, mask3;
+    __m128i vec0, vec1, vec2, vec3;
+    __m128i dst0, dst1, dst2, dst3;
+    __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 16);
+
+    src -= 3;
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filt0, filt1, filt2, filt3);
+
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    mask3 = __lsx_vaddi_bu(mask0, 6);
+
+    for (;loop_cnt--;) {
+        src0 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src1, src2);
+        src3 = __lsx_vldx(src, src_stride_3x);
+        src += src_stride_4x;
+        src4 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src5, src6);
+        src7 = __lsx_vldx(src, src_stride_3x);
+        src += src_stride_4x;
+        DUP4_ARG3(__lsx_vshuf_b, src1, src0, mask0, src1, src0, mask1, src1,
+                  src0, mask2, src1, src0, mask3, vec0, vec1, vec2, vec3);
+        dst0 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec1, filt1, dst0, vec2, filt2,
+                  dst0, dst0);
+        dst0 = __lsx_vdp2add_h_bu_b(dst0, vec3, filt3);
+        DUP4_ARG3(__lsx_vshuf_b, src3, src2, mask0, src3, src2, mask1, src3,
+                  src2, mask2, src3, src2, mask3, vec0, vec1, vec2, vec3);
+        dst1 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst1, vec1, filt1, dst1, vec2, filt2,
+                  dst1, dst1);
+        dst1 = __lsx_vdp2add_h_bu_b(dst1, vec3, filt3);
+        DUP4_ARG3(__lsx_vshuf_b, src5, src4, mask0, src5, src4, mask1, src5,
+                  src4, mask2, src5, src4, mask3, vec0, vec1, vec2, vec3);
+        dst2 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst2, vec1, filt1, dst2, vec2, filt2,
+                  dst2, dst2);
+        dst2 = __lsx_vdp2add_h_bu_b(dst2, vec3, filt3);
+        DUP4_ARG3(__lsx_vshuf_b, src7, src6, mask0, src7, src6, mask1, src7,
+                  src6, mask2, src7, src6, mask3, vec0, vec1, vec2, vec3);
+        dst3 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst3, vec1, filt1, dst3, vec2, filt2,
+                  dst3, dst3);
+        dst3 = __lsx_vdp2add_h_bu_b(dst3, vec3, filt3);
+
+        __lsx_vstelm_d(dst0, dst, 0, 0);
+        __lsx_vstelm_d(dst0, dst + dst_stride, 0, 1);
+        __lsx_vstelm_d(dst1, dst + dst_stride_2x, 0, 0);
+        __lsx_vstelm_d(dst1, dst + dst_stride_3x, 0, 1);
+        dst += dst_stride_4x;
+        __lsx_vstelm_d(dst2, dst, 0, 0);
+        __lsx_vstelm_d(dst2, dst + dst_stride, 0, 1);
+        __lsx_vstelm_d(dst3, dst + dst_stride_2x, 0, 0);
+        __lsx_vstelm_d(dst3, dst + dst_stride_3x, 0, 1);
+        dst += dst_stride_4x;
+    }
+    for (;res--;) {
+        src0 = __lsx_vld(src, 0);
+        src1 = __lsx_vldx(src, src_stride);
+        DUP4_ARG3(__lsx_vshuf_b, src1, src0, mask0, src1, src0, mask1, src1,
+                  src0, mask2, src1, src0, mask3, vec0, vec1, vec2, vec3);
+        dst0 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec1, filt1, dst0, vec2, filt2,
+                  dst0, dst0);
+        dst0 = __lsx_vdp2add_h_bu_b(dst0, vec3, filt3);
+        __lsx_vstelm_d(dst0, dst, 0, 0);
+        __lsx_vstelm_d(dst0, dst + dst_stride, 0, 1);
+        src += src_stride_2x;
+        dst += dst_stride_2x;
+    }
+}
+
+static void hevc_hz_8t_8w_lsx(uint8_t *src, int32_t src_stride,
+                              int16_t *dst, int32_t dst_stride,
+                              const int8_t *filter, int32_t height)
+{
+    uint32_t loop_cnt;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t dst_stride_x  = (dst_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t dst_stride_2x = (dst_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride_x;
+    __m128i src0, src1, src2, src3;
+    __m128i filt0, filt1, filt2, filt3;
+    __m128i mask1, mask2, mask3;
+    __m128i vec0, vec1, vec2, vec3;
+    __m128i dst0, dst1, dst2, dst3;
+    __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
+
+    src -= 3;
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filt0, filt1, filt2, filt3);
+
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    mask3 = __lsx_vaddi_bu(mask0, 6);
+
+    for (loop_cnt = (height >> 2); loop_cnt--;) {
+        src0 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src1, src2);
+        src3 = __lsx_vldx(src, src_stride_3x);
+        src += src_stride_4x;
+
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src0,
+                  src0, mask2, src0, src0, mask3, vec0, vec1, vec2, vec3);
+        dst0 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec1, filt1, dst0, vec2, filt2,
+                  dst0, dst0);
+        dst0 = __lsx_vdp2add_h_bu_b(dst0, vec3, filt3);
+        DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, src1,
+                  src1, mask2, src1, src1, mask3, vec0, vec1, vec2, vec3);
+        dst1 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst1, vec1, filt1, dst1, vec2, filt2,
+                  dst1, dst1);
+        dst1 = __lsx_vdp2add_h_bu_b(dst1, vec3, filt3);
+        DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, src2,
+                  src2, mask2, src2, src2, mask3, vec0, vec1, vec2, vec3);
+        dst2 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst2, vec1, filt1, dst2, vec2, filt2,
+                  dst2, dst2);
+        dst2 = __lsx_vdp2add_h_bu_b(dst2, vec3, filt3);
+        DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src3,
+                  src3, mask2, src3, src3, mask3, vec0, vec1, vec2, vec3);
+        dst3 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst3, vec1, filt1, dst3, vec2, filt2,
+                  dst3, dst3);
+        dst3 = __lsx_vdp2add_h_bu_b(dst3, vec3, filt3);
+
+        __lsx_vst(dst0, dst, 0);
+        __lsx_vstx(dst1, dst, dst_stride_x);
+        __lsx_vstx(dst2, dst, dst_stride_2x);
+        __lsx_vstx(dst3, dst, dst_stride_3x);
+        dst += dst_stride_2x;
+    }
+}
+
+static void hevc_hz_8t_12w_lsx(uint8_t *src, int32_t src_stride,
+                               int16_t *dst, int32_t dst_stride,
+                               const int8_t *filter, int32_t height)
+{
+    uint32_t loop_cnt;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    uint8_t *_src;
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m128i mask0, mask1, mask2, mask3, mask4, mask5, mask6, mask7;
+    __m128i vec0, vec1, vec2, vec3, vec4, vec5;
+    __m128i filt0, filt1, filt2, filt3, dst0, dst1, dst2, dst3, dst4, dst5;
+
+    src -= 3;
+    _src = src + 8;
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filt0, filt1, filt2, filt3);
+
+    mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    mask3 = __lsx_vaddi_bu(mask0, 6);
+    mask4 = __lsx_vld(ff_hevc_mask_arr, 16);
+    DUP2_ARG2(__lsx_vaddi_bu, mask4, 2, mask4, 4, mask5, mask6);
+    mask7 = __lsx_vaddi_bu(mask4, 6);
+
+    for (loop_cnt = 4; loop_cnt--;) {
+        src0 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src1, src2);
+        src3 = __lsx_vldx(src, src_stride_3x);
+        src4 = __lsx_vld(_src, 0);
+        DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride_2x,
+                  src5, src6);
+        src7 = __lsx_vldx(_src, src_stride_3x);
+        src += src_stride_4x;
+        _src += src_stride_4x;
+
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src1, mask0,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src3, src3, mask0,
+                  vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src5, src4, mask4, src7, src6, mask4,
+                  vec4, vec5);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec1, filt0, vec2, filt0,
+                  vec3, filt0, dst0, dst1, dst2, dst3);
+        DUP2_ARG2(__lsx_vdp2_h_bu_b, vec4, filt0, vec5, filt0, dst4, dst5);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src1, mask1,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask1, src3, src3, mask1,
+                  vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src5, src4, mask5, src7, src6, mask5,
+                  vec4, vec5);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec0, filt1, dst1, vec1, filt1,
+                  dst2, vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst4, vec4, filt1, dst5, vec5, filt1,
+                  dst4, dst5);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src1, mask2,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask2, src3, src3, mask2,
+                  vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src5, src4, mask6, src7, src6, mask6,
+                  vec4, vec5);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec0, filt2, dst1, vec1, filt2,
+                  dst2, vec2, filt2, dst3, vec3, filt2, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst4, vec4, filt2, dst5, vec5, filt2,
+                  dst4, dst5);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src1, mask3,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask3, src3, src3, mask3,
+                  vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src5, src4, mask7, src7, src6, mask7,
+                  vec4, vec5);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec0, filt3, dst1, vec1, filt3,
+                  dst2, vec2, filt3, dst3, vec3, filt3, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst4, vec4, filt3, dst5, vec5, filt3,
+                  dst4, dst5);
+
+        __lsx_vst(dst0, dst, 0);
+        __lsx_vstelm_d(dst4, dst, 16, 0);
+        dst += dst_stride;
+        __lsx_vst(dst1, dst, 0);
+        __lsx_vstelm_d(dst4, dst, 16, 1);
+        dst += dst_stride;
+        __lsx_vst(dst2, dst, 0);
+        __lsx_vstelm_d(dst5, dst, 16, 0);
+        dst += dst_stride;
+        __lsx_vst(dst3, dst, 0);
+        __lsx_vstelm_d(dst5, dst, 16, 1);
+        dst += dst_stride;
+    }
+}
+
+static void hevc_hz_8t_16w_lsx(uint8_t *src, int32_t src_stride,
+                               int16_t *dst, int32_t dst_stride,
+                               const int8_t *filter, int32_t height)
+{
+    uint32_t loop_cnt;
+    __m128i src0, src1, src2, src3;
+    __m128i filt0, filt1, filt2, filt3;
+    __m128i mask1, mask2, mask3;
+    __m128i vec0, vec1, vec2, vec3;
+    __m128i dst0, dst1, dst2, dst3;
+    __m128i mask0;
+
+    src -= 3;
+    mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filt0, filt1, filt2, filt3);
+
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    mask3 = __lsx_vaddi_bu(mask0, 6);
+
+    for (loop_cnt = (height >> 1); loop_cnt--;) {
+        DUP2_ARG2(__lsx_vld, src, 0, src, 8, src0, src1);
+        src += src_stride;
+        DUP2_ARG2(__lsx_vld, src, 0, src, 8, src2, src3);
+        src += src_stride;
+
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src1, mask0,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src3, src3, mask0,
+                  vec2, vec3);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec1, filt0, vec2, filt0,
+                  vec3, filt0, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src1, mask1,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask1, src3, src3, mask1,
+                  vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec0, filt1, dst1, vec1, filt1,
+                  dst2, vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src1, mask2,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask2, src3, src3, mask2,
+                  vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec0, filt2, dst1, vec1, filt2,
+                  dst2, vec2, filt2, dst3, vec3, filt2, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src1, mask3,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask3, src3, src3, mask3,
+                  vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec0, filt3, dst1, vec1, filt3,
+                  dst2, vec2, filt3, dst3, vec3, filt3, dst0, dst1, dst2, dst3);
+
+        __lsx_vst(dst0, dst, 0);
+        __lsx_vst(dst1, dst, 16);
+        dst += dst_stride;
+        __lsx_vst(dst2, dst, 0);
+        __lsx_vst(dst3, dst, 16);
+        dst += dst_stride;
+    }
+}
+
+static void hevc_hz_8t_24w_lsx(uint8_t *src, int32_t src_stride,
+                               int16_t *dst, int32_t dst_stride,
+                               const int8_t *filter, int32_t height)
+{
+    uint32_t loop_cnt;
+    __m128i src0, src1, src2, src3;
+    __m128i filt0, filt1, filt2, filt3;
+    __m128i mask1, mask2, mask3, mask4, mask5, mask6, mask7;
+    __m128i vec0, vec1, vec2, vec3, vec4, vec5;
+    __m128i dst0, dst1, dst2, dst3, dst4, dst5;
+    __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
+
+    src -= 3;
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filt0, filt1, filt2, filt3);
+
+    DUP4_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask0, 6, mask0, 8, mask1,
+              mask2, mask3, mask4);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 10, mask0, 12, mask5, mask6);
+    mask7 = __lsx_vaddi_bu(mask0, 14);
+
+    for (loop_cnt = (height >> 1); loop_cnt--;) {
+        DUP2_ARG2(__lsx_vld, src, 0, src, 16, src0, src1);
+        src += src_stride;
+        DUP2_ARG2(__lsx_vld, src, 0, src, 16, src2, src3);
+        src += src_stride;
+
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src0, mask4, src1,
+                  src1, mask0, src2, src2, mask0, vec0, vec1, vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src3, src2, mask4, src3, src3, mask0,
+                  vec4, vec5);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec1, filt0, vec2, filt0,
+                  vec3, filt0, dst0, dst1, dst2, dst3);
+        DUP2_ARG2(__lsx_vdp2_h_bu_b, vec4, filt0, vec5, filt0, dst4, dst5);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src0, mask5, src1,
+                  src1, mask1, src2, src2, mask1, vec0, vec1, vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src3, src2, mask5, src3, src3, mask1,
+                  vec4, vec5);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec0, filt1, dst1, vec1, filt1,
+                  dst2, vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst4, vec4, filt1, dst5, vec5, filt1,
+                  dst4, dst5);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src0, mask6, src1,
+                  src1, mask2, src2, src2, mask2, vec0, vec1, vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src3, src2, mask6, src3, src3, mask2,
+                  vec4, vec5);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec0, filt2, dst1, vec1, filt2,
+                  dst2, vec2, filt2, dst3, vec3, filt2, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst4, vec4, filt2, dst5, vec5, filt2,
+                  dst4, dst5);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src0, mask7, src1,
+                  src1, mask3, src2, src2, mask3, vec0, vec1, vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src3, src2, mask7, src3, src3, mask3,
+                  vec4, vec5);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec0, filt3, dst1, vec1, filt3,
+                  dst2, vec2, filt3, dst3, vec3, filt3, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst4, vec4, filt3, dst5, vec5, filt3,
+                  dst4, dst5);
+
+        __lsx_vst(dst0, dst, 0);
+        __lsx_vst(dst1, dst, 16);
+        __lsx_vst(dst2, dst, 32);
+        dst += dst_stride;
+        __lsx_vst(dst3, dst, 0);
+        __lsx_vst(dst4, dst, 16);
+        __lsx_vst(dst5, dst, 32);
+        dst += dst_stride;
+    }
+}
+
+static void hevc_hz_8t_32w_lsx(uint8_t *src, int32_t src_stride,
+                               int16_t *dst, int32_t dst_stride,
+                               const int8_t *filter, int32_t height)
+{
+    uint32_t loop_cnt;
+    __m128i src0, src1, src2;
+    __m128i filt0, filt1, filt2, filt3;
+    __m128i mask1, mask2, mask3, mask4, mask5, mask6, mask7;
+    __m128i vec0, vec1, vec2, vec3;
+    __m128i dst0, dst1, dst2, dst3;
+    __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
+
+    src -= 3;
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2,  filter, 4, filter, 6,
+              filt0, filt1, filt2, filt3);
+
+    DUP4_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask0, 6, mask0, 8,
+              mask1, mask2, mask3, mask4);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 10, mask0, 12, mask5, mask6);
+    mask7 = __lsx_vaddi_bu(mask0, 14);
+
+    for (loop_cnt = height; loop_cnt--;) {
+        DUP2_ARG2(__lsx_vld, src, 0, src, 16, src0, src1);
+        src2 = __lsx_vld(src, 24);
+        src += src_stride;
+
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src0,
+                  src0, mask2, src0, src0, mask3, vec0, vec1, vec2, vec3);
+        dst0 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec1, filt1, dst0, vec2, filt2,
+                  dst0, dst0);
+        dst0 = __lsx_vdp2add_h_bu_b(dst0, vec3, filt3);
+
+        DUP4_ARG3(__lsx_vshuf_b, src1, src0, mask4, src1, src0, mask5, src1,
+                  src0, mask6, src1, src0, mask7, vec0, vec1, vec2, vec3);
+        dst1 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst1, vec1, filt1, dst1, vec2, filt2,
+                  dst1, dst1);
+        dst1 = __lsx_vdp2add_h_bu_b(dst1, vec3, filt3);
+        DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, src1, src1,
+                  mask2, src1, src1, mask3, vec0, vec1, vec2, vec3);
+        dst2 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst2, vec1, filt1, dst2, vec2, filt2,
+                  dst2, dst2);
+        dst2 = __lsx_vdp2add_h_bu_b(dst2, vec3, filt3);
+        DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, src2, src2,
+                  mask2, src2, src2, mask3, vec0, vec1, vec2, vec3);
+        dst3 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst3, vec1, filt1, dst3, vec2, filt2,
+                  dst3, dst3);
+        dst3 = __lsx_vdp2add_h_bu_b(dst3, vec3, filt3);
+
+        __lsx_vst(dst0, dst, 0);
+        __lsx_vst(dst1, dst, 16);
+        __lsx_vst(dst2, dst, 32);
+        __lsx_vst(dst3, dst, 48);
+        dst += dst_stride;
+    }
+}
+
+static void hevc_hz_8t_48w_lsx(uint8_t *src, int32_t src_stride,
+                               int16_t *dst, int32_t dst_stride,
+                               const int8_t *filter, int32_t height)
+{
+    uint32_t loop_cnt;
+    __m128i src0, src1, src2, src3;
+    __m128i filt0, filt1, filt2, filt3;
+    __m128i mask1, mask2, mask3, mask4, mask5, mask6, mask7;
+    __m128i vec0, vec1, vec2, vec3, vec4, vec5;
+    __m128i dst0, dst1, dst2, dst3, dst4, dst5;
+    __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
+
+    src -= 3;
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filt0, filt1, filt2, filt3);
+
+    DUP4_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask0, 6, mask0, 8, mask1,
+              mask2, mask3, mask4);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 10, mask0, 12, mask5, mask6);
+    mask7 = __lsx_vaddi_bu(mask0, 14);
+
+    for (loop_cnt = height; loop_cnt--;) {
+        DUP2_ARG2(__lsx_vld, src, 0, src, 16, src0, src1);
+        src2 = __lsx_vld(src, 32);
+        src3 = __lsx_vld(src, 40);
+        src += src_stride;
+
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src0, mask4, src1, src1,
+                  mask0, src2, src1, mask4, vec0, vec1, vec2, vec3);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec1, filt0, vec2, filt0,
+                  vec3, filt0, dst0, dst1, dst2, dst3);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src0, mask5, src1,
+                  src1, mask1, src2, src1, mask5, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec0, filt1, dst1, vec1, filt1,
+                  dst2, vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src0, mask6, src1,
+                  src1, mask2, src2, src1, mask6, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec0, filt2, dst1, vec1, filt2,
+                  dst2, vec2, filt2, dst3, vec3, filt2, dst0, dst1, dst2, dst3);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src0, mask7, src1,
+                  src1, mask3, src2, src1, mask7, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec0, filt3, dst1, vec1, filt3,
+                  dst2, vec2, filt3, dst3, vec3, filt3, dst0, dst1, dst2, dst3);
+        __lsx_vst(dst0, dst, 0);
+        __lsx_vst(dst1, dst, 16);
+        __lsx_vst(dst2, dst, 32);
+        __lsx_vst(dst3, dst, 48);
+
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src3, src3, mask0,
+                  vec4, vec5);
+        DUP2_ARG2(__lsx_vdp2_h_bu_b, vec4, filt0, vec5, filt0, dst4, dst5);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask1, src3, src3, mask1,
+                  vec4, vec5);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst4, vec4, filt1, dst5, vec5, filt1,
+                  dst4, dst5);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask2, src3, src3, mask2,
+                  vec4, vec5);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst4, vec4, filt2, dst5, vec5, filt2,
+                  dst4, dst5);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask3, src3, src3, mask3,
+                  vec4, vec5);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst4, vec4, filt3, dst5, vec5, filt3,
+                  dst4, dst5);
+        __lsx_vst(dst4, dst, 64);
+        __lsx_vst(dst5, dst, 80);
+        dst += dst_stride;
+    }
+}
+
+static void hevc_hz_8t_64w_lsx(uint8_t *src, int32_t src_stride,
+                               int16_t *dst, int32_t dst_stride,
+                               const int8_t *filter, int32_t height)
+{
+    uint32_t loop_cnt;
+    __m128i src0, src1, src2, src3, src4;
+    __m128i filt0, filt1, filt2, filt3;
+    __m128i mask1, mask2, mask3, mask4, mask5, mask6, mask7;
+    __m128i vec0, vec1, vec2, vec3;
+    __m128i dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
+
+    src -= 3;
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filt0, filt1, filt2, filt3);
+
+    DUP4_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask0, 6, mask0, 8, mask1,
+              mask2, mask3, mask4);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 10, mask0, 12, mask5, mask6)
+    mask7 = __lsx_vaddi_bu(mask0, 14);
+
+    for (loop_cnt = height; loop_cnt--;) {
+        DUP4_ARG2(__lsx_vld, src, 0, src, 16,  src, 32, src, 48,
+                  src0, src1, src2, src3);
+        src4 = __lsx_vld(src, 56);
+        src += src_stride;
+
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src0,
+                  src0, mask2, src0, src0, mask3, vec0, vec1, vec2, vec3);
+        dst0 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec1, filt1, dst0, vec2, filt2,
+                  dst0, dst0);
+        dst0 = __lsx_vdp2add_h_bu_b(dst0, vec3, filt3);
+        __lsx_vst(dst0, dst, 0);
+
+        DUP4_ARG3(__lsx_vshuf_b, src1, src0, mask4, src1, src0, mask5, src1,
+                  src0, mask6, src1, src0, mask7, vec0, vec1, vec2, vec3);
+        dst1 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst1, vec1, filt1, dst1, vec2, filt2,
+                  dst1, dst1);
+        dst1 = __lsx_vdp2add_h_bu_b(dst1, vec3, filt3);
+        __lsx_vst(dst1, dst, 16);
+
+        DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, src1,
+                  src1, mask2, src1, src1, mask3, vec0, vec1, vec2, vec3);
+        dst2 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst2, vec1, filt1, dst2, vec2, filt2,
+                  dst2, dst2);
+        dst2 = __lsx_vdp2add_h_bu_b(dst2, vec3, filt3);
+        __lsx_vst(dst2, dst, 32);
+
+        DUP4_ARG3(__lsx_vshuf_b, src2, src1, mask4, src2, src1, mask5, src2,
+                  src1, mask6, src2, src1, mask7, vec0, vec1, vec2, vec3);
+        dst3 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst3, vec1, filt1, dst3, vec2, filt2,
+                  dst3, dst3);
+        dst3 = __lsx_vdp2add_h_bu_b(dst3, vec3, filt3);
+        __lsx_vst(dst3, dst, 48);
+
+        DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, src2,
+                  src2, mask2, src2, src2, mask3, vec0, vec1, vec2, vec3);
+        dst4 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst4, vec1, filt1, dst4, vec2, filt2,
+                  dst4, dst4);
+        dst4 = __lsx_vdp2add_h_bu_b(dst4, vec3, filt3);
+        __lsx_vst(dst4, dst, 64);
+
+        DUP4_ARG3(__lsx_vshuf_b, src3, src2, mask4, src3, src2, mask5, src3,
+                  src2, mask6, src3, src2, mask7, vec0, vec1, vec2, vec3);
+        dst5 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst5, vec1, filt1, dst5, vec2, filt2,
+                  dst5, dst5);
+        dst5 = __lsx_vdp2add_h_bu_b(dst5, vec3, filt3);
+        __lsx_vst(dst5, dst, 80);
+
+        DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src3,
+                  src3, mask2, src3, src3, mask3, vec0, vec1, vec2, vec3);
+        dst6 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst6, vec1, filt1, dst6, vec2, filt2,
+                  dst6, dst6);
+        dst6 = __lsx_vdp2add_h_bu_b(dst6, vec3, filt3);
+        __lsx_vst(dst6, dst, 96);
+
+        DUP4_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, src4,
+                  src4, mask2, src4, src4, mask3, vec0, vec1, vec2, vec3);
+        dst7 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst7, vec1, filt1, dst7, vec2, filt2,
+                  dst7, dst7);
+        dst7 = __lsx_vdp2add_h_bu_b(dst7, vec3, filt3);
+        __lsx_vst(dst7, dst, 112);
+        dst += dst_stride;
+    }
+}
+
+static void hevc_vt_8t_4w_lsx(uint8_t *src, int32_t src_stride,
+                              int16_t *dst, int32_t dst_stride,
+                              const int8_t *filter, int32_t height)
+{
+    int32_t loop_cnt;
+    int32_t res = (height & 0x07) >> 1;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8;
+    __m128i src9, src10, src11, src12, src13, src14;
+    __m128i src10_r, src32_r, src54_r, src76_r, src98_r;
+    __m128i src21_r, src43_r, src65_r, src87_r, src109_r;
+    __m128i src1110_r, src1211_r, src1312_r, src1413_r;
+    __m128i src2110, src4332, src6554, src8776, src10998;
+    __m128i src12111110, src14131312;
+    __m128i dst10, dst32, dst54, dst76;
+    __m128i filt0, filt1, filt2, filt3;
+
+    src -= src_stride_3x;
+
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filt0, filt1, filt2, filt3);
+
+    src0 = __lsx_vld(src, 0);
+    DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src1, src2);
+    src3 = __lsx_vldx(src, src_stride_3x);
+    src += src_stride_4x;
+    src4 = __lsx_vld(src, 0);
+    DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src5, src6);
+    src += src_stride_3x;
+    DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1,
+              src10_r, src32_r, src54_r, src21_r);
+    DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, src43_r, src65_r);
+    DUP2_ARG2(__lsx_vilvl_d, src21_r, src10_r, src43_r, src32_r,
+              src2110, src4332);
+    src6554 = __lsx_vilvl_d(src65_r, src54_r);
+
+    for (loop_cnt = (height >> 3); loop_cnt--;) {
+        src7 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src8, src9);
+        src10 = __lsx_vldx(src, src_stride_3x);
+        src += src_stride_4x;
+        src11 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x,
+                  src12, src13);
+        src14 = __lsx_vldx(src, src_stride_3x);
+        src += src_stride_4x;
+
+        DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10, src9,
+                  src76_r, src87_r, src98_r, src109_r);
+        DUP4_ARG2(__lsx_vilvl_b, src11, src10, src12, src11, src13, src12, src14,
+                  src13, src1110_r, src1211_r, src1312_r, src1413_r);
+        DUP4_ARG2(__lsx_vilvl_d, src87_r, src76_r, src109_r, src98_r, src1211_r,
+                  src1110_r, src1413_r, src1312_r, src8776, src10998,
+                  src12111110, src14131312);
+
+        dst10 = __lsx_vdp2_h_bu_b(src2110, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst10, src4332, filt1, dst10, src6554,
+                  filt2, dst10, dst10);
+        dst10 = __lsx_vdp2add_h_bu_b(dst10, src8776, filt3);
+        dst32 = __lsx_vdp2_h_bu_b(src4332, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst32, src6554, filt1, dst32, src8776,
+                  filt2, dst32, dst32);
+        dst32 = __lsx_vdp2add_h_bu_b(dst32, src10998, filt3);
+        dst54 = __lsx_vdp2_h_bu_b(src6554, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst54, src8776, filt1,
+                  dst54, src10998, filt2, dst54, dst54);
+        dst54 = __lsx_vdp2add_h_bu_b(dst54, src12111110, filt3);
+        dst76 = __lsx_vdp2_h_bu_b(src8776, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst76, src10998, filt1, dst76,
+                  src12111110, filt2, dst76, dst76);
+        dst76 = __lsx_vdp2add_h_bu_b(dst76, src14131312, filt3);
+
+        __lsx_vstelm_d(dst10, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(dst10, dst, 0, 1);
+        dst += dst_stride;
+        __lsx_vstelm_d(dst32, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(dst32, dst, 0, 1);
+        dst += dst_stride;
+        __lsx_vstelm_d(dst54, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(dst54, dst, 0, 1);
+        dst += dst_stride;
+        __lsx_vstelm_d(dst76, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(dst76, dst, 0, 1);
+        dst += dst_stride;
+
+        src2110 = src10998;
+        src4332 = src12111110;
+        src6554 = src14131312;
+        src6 = src14;
+    }
+    for (;res--;) {
+        src7 = __lsx_vld(src, 0);
+        src8 = __lsx_vldx(src, src_stride);
+        DUP2_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src76_r, src87_r);
+        src += src_stride_2x;
+        src8776 = __lsx_vilvl_d(src87_r, src76_r);
+
+        dst10 = __lsx_vdp2_h_bu_b(src2110, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst10, src4332, filt1, dst10, src6554,
+                  filt2, dst10, dst10);
+        dst10 = __lsx_vdp2add_h_bu_b(dst10, src8776, filt3);
+
+        __lsx_vstelm_d(dst10, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(dst10, dst, 0, 1);
+        dst += dst_stride;
+
+        src2110 = src4332;
+        src4332 = src6554;
+        src6554 = src8776;
+        src6 = src8;
+    }
+}
+
+static void hevc_vt_8t_8w_lsx(uint8_t *src, int32_t src_stride,
+                              int16_t *dst, int32_t dst_stride,
+                              const int8_t *filter, int32_t height)
+{
+    int32_t loop_cnt;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t dst_stride_x  = (dst_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t dst_stride_2x = (dst_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride_x;
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8, src9, src10;
+    __m128i src10_r, src32_r, src54_r, src76_r, src98_r;
+    __m128i src21_r, src43_r, src65_r, src87_r, src109_r;
+    __m128i dst0_r, dst1_r, dst2_r, dst3_r;
+    __m128i filt0, filt1, filt2, filt3;
+
+    src -= src_stride_3x;
+
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filt0, filt1, filt2, filt3);
+
+    src0 = __lsx_vld(src, 0);
+    DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src1, src2);
+    src3 = __lsx_vldx(src, src_stride_3x);
+    src += src_stride_4x;
+    src4 = __lsx_vld(src, 0);
+    DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src5, src6);
+    src += src_stride_3x;
+    DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1,
+              src10_r, src32_r, src54_r, src21_r);
+    DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, src43_r, src65_r);
+
+    for (loop_cnt = (height >> 2); loop_cnt--;) {
+        src7 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src8, src9);
+        src10 = __lsx_vldx(src, src_stride_3x);
+        src += src_stride_4x;
+        DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10,
+                  src9, src76_r, src87_r, src98_r, src109_r);
+
+        dst0_r = __lsx_vdp2_h_bu_b(src10_r, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst0_r, src32_r, filt1, dst0_r,
+                  src54_r, filt2, dst0_r, dst0_r);
+        dst0_r = __lsx_vdp2add_h_bu_b(dst0_r, src76_r, filt3);
+        dst1_r = __lsx_vdp2_h_bu_b(src21_r, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst1_r, src43_r, filt1, dst1_r,
+                  src65_r, filt2, dst1_r, dst1_r);
+        dst1_r = __lsx_vdp2add_h_bu_b(dst1_r, src87_r, filt3);
+        dst2_r = __lsx_vdp2_h_bu_b(src32_r, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst2_r, src54_r, filt1, dst2_r,
+                  src76_r, filt2, dst2_r, dst2_r);
+        dst2_r = __lsx_vdp2add_h_bu_b(dst2_r, src98_r, filt3);
+        dst3_r = __lsx_vdp2_h_bu_b(src43_r, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst3_r, src65_r, filt1, dst3_r,
+                  src87_r, filt2, dst3_r, dst3_r);
+        dst3_r = __lsx_vdp2add_h_bu_b(dst3_r, src109_r, filt3);
+
+        __lsx_vst(dst0_r, dst, 0);
+        __lsx_vstx(dst1_r, dst, dst_stride_x);
+        __lsx_vstx(dst2_r, dst, dst_stride_2x);
+        __lsx_vstx(dst3_r, dst, dst_stride_3x);
+        dst += dst_stride_2x;
+
+        src10_r = src54_r;
+        src32_r = src76_r;
+        src54_r = src98_r;
+        src21_r = src65_r;
+        src43_r = src87_r;
+        src65_r = src109_r;
+        src6 = src10;
+    }
+}
+
+static void hevc_vt_8t_12w_lsx(uint8_t *src, int32_t src_stride,
+                               int16_t *dst, int32_t dst_stride,
+                               const int8_t *filter, int32_t height)
+{
+    int32_t loop_cnt;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8, src9, src10;
+    __m128i src10_r, src32_r, src54_r, src76_r, src98_r;
+    __m128i src21_r, src43_r, src65_r, src87_r, src109_r;
+    __m128i dst0_r, dst1_r, dst2_r, dst3_r;
+    __m128i src10_l, src32_l, src54_l, src76_l, src98_l;
+    __m128i src21_l, src43_l, src65_l, src87_l, src109_l;
+    __m128i src2110, src4332, src6554, src8776, src10998;
+    __m128i dst0_l, dst1_l;
+    __m128i filt0, filt1, filt2, filt3;
+
+    src -= src_stride_3x;
+
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filt0, filt1, filt2, filt3);
+    src0 = __lsx_vld(src, 0);
+    DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src1, src2);
+    src3 = __lsx_vldx(src, src_stride_3x);
+    src += src_stride_4x;
+    src4 = __lsx_vld(src, 0);
+    DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src5, src6);
+    src += src_stride_3x;
+    DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1,
+              src10_r, src32_r, src54_r, src21_r);
+    DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, src43_r, src65_r);
+    DUP4_ARG2(__lsx_vilvh_b, src1, src0, src3, src2, src5, src4, src2, src1,
+              src10_l, src32_l, src54_l, src21_l);
+    DUP2_ARG2(__lsx_vilvh_b, src4, src3, src6, src5, src43_l, src65_l);
+    DUP2_ARG2(__lsx_vilvl_d, src21_l, src10_l, src43_l, src32_l,
+              src2110, src4332);
+    src6554 = __lsx_vilvl_d(src65_l, src54_l);
+
+    for (loop_cnt = (height >> 2); loop_cnt--;) {
+        src7 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src8, src9);
+        src10 = __lsx_vldx(src, src_stride_3x);
+        src += src_stride_4x;
+        DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10,
+                  src9, src76_r, src87_r, src98_r, src109_r);
+        DUP4_ARG2(__lsx_vilvh_b, src7, src6, src8, src7, src9, src8, src10,
+                  src9, src76_l, src87_l, src98_l, src109_l);
+        DUP2_ARG2(__lsx_vilvl_d, src87_l, src76_l, src109_l, src98_l,
+                  src8776, src10998);
+
+        dst0_r = __lsx_vdp2_h_bu_b(src10_r, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst0_r, src32_r, filt1, dst0_r,
+                  src54_r, filt2, dst0_r, dst0_r);
+        dst0_r = __lsx_vdp2add_h_bu_b(dst0_r, src76_r, filt3);
+        dst1_r = __lsx_vdp2_h_bu_b(src21_r, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst1_r, src43_r, filt1, dst1_r,
+                  src65_r, filt2, dst1_r, dst1_r);
+        dst1_r = __lsx_vdp2add_h_bu_b(dst1_r, src87_r, filt3);
+        dst2_r = __lsx_vdp2_h_bu_b(src32_r, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst2_r, src54_r, filt1, dst2_r,
+                  src76_r, filt2, dst2_r, dst2_r);
+        dst2_r = __lsx_vdp2add_h_bu_b(dst2_r, src98_r, filt3);
+        dst3_r = __lsx_vdp2_h_bu_b(src43_r, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst3_r, src65_r, filt1, dst3_r,
+                  src87_r, filt2, dst3_r, dst3_r);
+        dst3_r = __lsx_vdp2add_h_bu_b(dst3_r, src109_r, filt3);
+        dst0_l = __lsx_vdp2_h_bu_b(src2110, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst0_l, src4332, filt1, dst0_l,
+                  src6554, filt2, dst0_l, dst0_l);
+        dst0_l = __lsx_vdp2add_h_bu_b(dst0_l, src8776, filt3);
+        dst1_l = __lsx_vdp2_h_bu_b(src4332, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst1_l, src6554, filt1, dst1_l,
+                  src8776, filt2, dst1_l, dst1_l);
+        dst1_l = __lsx_vdp2add_h_bu_b(dst1_l, src10998, filt3);
+
+        __lsx_vst(dst0_r, dst, 0);
+        __lsx_vstelm_d(dst0_l, dst, 16, 0);
+        dst += dst_stride;
+        __lsx_vst(dst1_r, dst, 0);
+        __lsx_vstelm_d(dst0_l, dst, 16, 1);
+        dst += dst_stride;
+        __lsx_vst(dst2_r, dst, 0);
+        __lsx_vstelm_d(dst1_l, dst, 16, 0);
+        dst += dst_stride;
+        __lsx_vst(dst3_r, dst, 0);
+        __lsx_vstelm_d(dst1_l, dst, 16, 1);
+        dst += dst_stride;
+
+        src10_r = src54_r;
+        src32_r = src76_r;
+        src54_r = src98_r;
+        src21_r = src65_r;
+        src43_r = src87_r;
+        src65_r = src109_r;
+        src2110 = src6554;
+        src4332 = src8776;
+        src6554 = src10998;
+        src6 = src10;
+    }
+}
+
+static void hevc_vt_8t_16multx4mult_lsx(uint8_t *src,
+                                        int32_t src_stride,
+                                        int16_t *dst,
+                                        int32_t dst_stride,
+                                        const int8_t *filter,
+                                        int32_t height,
+                                        int32_t width)
+{
+    uint8_t *src_tmp;
+    int16_t *dst_tmp;
+    int32_t loop_cnt, cnt;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8, src9, src10;
+    __m128i src10_r, src32_r, src54_r, src76_r, src98_r;
+    __m128i src21_r, src43_r, src65_r, src87_r, src109_r;
+    __m128i dst0_r, dst1_r, dst2_r, dst3_r;
+    __m128i src10_l, src32_l, src54_l, src76_l, src98_l;
+    __m128i src21_l, src43_l, src65_l, src87_l, src109_l;
+    __m128i dst0_l, dst1_l, dst2_l, dst3_l;
+    __m128i filt0, filt1, filt2, filt3;
+
+    src -= src_stride_3x;
+
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filt0, filt1, filt2, filt3);
+
+    for (cnt = width >> 4; cnt--;) {
+        src_tmp = src;
+        dst_tmp = dst;
+
+        src0 = __lsx_vld(src_tmp, 0);
+        DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride_2x,
+                  src1, src2);
+        src3 = __lsx_vldx(src_tmp, src_stride_3x);
+        src_tmp += src_stride_4x;
+        src4 = __lsx_vld(src_tmp, 0);
+        DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride_2x,
+                  src5, src6);
+        src_tmp += src_stride_3x;
+        DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1,
+                  src10_r, src32_r, src54_r, src21_r);
+        DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, src43_r, src65_r);
+        DUP4_ARG2(__lsx_vilvh_b, src1, src0, src3, src2, src5, src4, src2, src1,
+                  src10_l, src32_l, src54_l, src21_l);
+        DUP2_ARG2(__lsx_vilvh_b, src4, src3, src6, src5, src43_l, src65_l);
+
+        for (loop_cnt = (height >> 2); loop_cnt--;) {
+            src7 = __lsx_vld(src_tmp, 0);
+            DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride_2x,
+                      src8, src9);
+            src10 = __lsx_vldx(src_tmp, src_stride_3x);
+            src_tmp += src_stride_4x;
+            DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8,
+                      src10, src9, src76_r, src87_r, src98_r, src109_r);
+            DUP4_ARG2(__lsx_vilvh_b, src7, src6, src8, src7, src9, src8,
+                      src10, src9, src76_l, src87_l, src98_l, src109_l);
+
+            dst0_r = __lsx_vdp2_h_bu_b(src10_r, filt0);
+            DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst0_r, src32_r, filt1, dst0_r,
+                      src54_r, filt2, dst0_r, dst0_r);
+            dst0_r = __lsx_vdp2add_h_bu_b(dst0_r, src76_r, filt3);
+            dst1_r = __lsx_vdp2_h_bu_b(src21_r, filt0);
+            DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst1_r, src43_r, filt1, dst1_r,
+                      src65_r, filt2, dst1_r, dst1_r);
+            dst1_r = __lsx_vdp2add_h_bu_b(dst1_r, src87_r, filt3);
+            dst2_r = __lsx_vdp2_h_bu_b(src32_r, filt0);
+            DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst2_r, src54_r, filt1, dst2_r,
+                      src76_r, filt2, dst2_r, dst2_r);
+            dst2_r = __lsx_vdp2add_h_bu_b(dst2_r, src98_r, filt3);
+            dst3_r = __lsx_vdp2_h_bu_b(src43_r, filt0);
+            DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst3_r, src65_r, filt1, dst3_r,
+                      src87_r, filt2, dst3_r, dst3_r);
+            dst3_r = __lsx_vdp2add_h_bu_b(dst3_r, src109_r, filt3);
+            dst0_l = __lsx_vdp2_h_bu_b(src10_l, filt0);
+            DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst0_l, src32_l, filt1, dst0_l,
+                      src54_l, filt2, dst0_l, dst0_l);
+            dst0_l = __lsx_vdp2add_h_bu_b(dst0_l, src76_l, filt3);
+            dst1_l = __lsx_vdp2_h_bu_b(src21_l, filt0);
+            DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst1_l, src43_l, filt1, dst1_l,
+                      src65_l, filt2, dst1_l, dst1_l);
+            dst1_l = __lsx_vdp2add_h_bu_b(dst1_l, src87_l, filt3);
+            dst2_l = __lsx_vdp2_h_bu_b(src32_l, filt0);
+            DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst2_l, src54_l, filt1, dst2_l,
+                      src76_l, filt2, dst2_l, dst2_l);
+            dst2_l = __lsx_vdp2add_h_bu_b(dst2_l, src98_l, filt3);
+            dst3_l = __lsx_vdp2_h_bu_b(src43_l, filt0);
+            DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst3_l, src65_l, filt1, dst3_l,
+                      src87_l, filt2, dst3_l, dst3_l);
+            dst3_l = __lsx_vdp2add_h_bu_b(dst3_l, src109_l, filt3);
+
+            __lsx_vst(dst0_r, dst_tmp, 0);
+            __lsx_vst(dst0_l, dst_tmp, 16);
+            dst_tmp += dst_stride;
+            __lsx_vst(dst1_r, dst_tmp, 0);
+            __lsx_vst(dst1_l, dst_tmp, 16);
+            dst_tmp += dst_stride;
+            __lsx_vst(dst2_r, dst_tmp, 0);
+            __lsx_vst(dst2_l, dst_tmp, 16);
+            dst_tmp += dst_stride;
+            __lsx_vst(dst3_r, dst_tmp, 0);
+            __lsx_vst(dst3_l, dst_tmp, 16);
+            dst_tmp += dst_stride;
+
+            src10_r = src54_r;
+            src32_r = src76_r;
+            src54_r = src98_r;
+            src21_r = src65_r;
+            src43_r = src87_r;
+            src65_r = src109_r;
+            src10_l = src54_l;
+            src32_l = src76_l;
+            src54_l = src98_l;
+            src21_l = src65_l;
+            src43_l = src87_l;
+            src65_l = src109_l;
+            src6 = src10;
+        }
+        src += 16;
+        dst += 16;
+    }
+}
+
+static void hevc_vt_8t_16w_lsx(uint8_t *src, int32_t src_stride,
+                               int16_t *dst, int32_t dst_stride,
+                               const int8_t *filter, int32_t height)
+{
+    hevc_vt_8t_16multx4mult_lsx(src, src_stride, dst, dst_stride,
+                                filter, height, 16);
+}
+
+static void hevc_vt_8t_24w_lsx(uint8_t *src, int32_t src_stride,
+                               int16_t *dst, int32_t dst_stride,
+                               const int8_t *filter, int32_t height)
+{
+    hevc_vt_8t_16multx4mult_lsx(src, src_stride, dst, dst_stride,
+                                filter, height, 16);
+    hevc_vt_8t_8w_lsx(src + 16, src_stride, dst + 16, dst_stride,
+                      filter, height);
+}
+
+static void hevc_vt_8t_32w_lsx(uint8_t *src, int32_t src_stride,
+                               int16_t *dst, int32_t dst_stride,
+                               const int8_t *filter, int32_t height)
+{
+    hevc_vt_8t_16multx4mult_lsx(src, src_stride, dst, dst_stride,
+                                filter, height, 32);
+}
+
+static void hevc_vt_8t_48w_lsx(uint8_t *src, int32_t src_stride,
+                               int16_t *dst, int32_t dst_stride,
+                               const int8_t *filter, int32_t height)
+{
+    hevc_vt_8t_16multx4mult_lsx(src, src_stride, dst, dst_stride,
+                                filter, height, 48);
+}
+
+static void hevc_vt_8t_64w_lsx(uint8_t *src, int32_t src_stride,
+                               int16_t *dst, int32_t dst_stride,
+                               const int8_t *filter, int32_t height)
+{
+    hevc_vt_8t_16multx4mult_lsx(src, src_stride, dst, dst_stride,
+                                filter, height, 64);
+}
+
+static void hevc_hv_8t_4w_lsx(uint8_t *src, int32_t src_stride,
+                              int16_t *dst, int32_t dst_stride,
+                              const int8_t *filter_x, const int8_t *filter_y,
+                              int32_t height)
+{
+    uint32_t loop_cnt;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8, src9, src10;
+    __m128i filt0, filt1, filt2, filt3;
+    __m128i filt_h0, filt_h1, filt_h2, filt_h3;
+    __m128i mask1, mask2, mask3;
+    __m128i filter_vec;
+    __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
+    __m128i vec8, vec9, vec10, vec11, vec12, vec13, vec14, vec15;
+    __m128i dst30, dst41, dst52, dst63, dst66, dst97, dst108;
+    __m128i dst0_r, dst1_r, dst2_r, dst3_r;
+    __m128i dst10_r, dst32_r, dst54_r, dst76_r, dst98_r;
+    __m128i dst21_r, dst43_r, dst65_r, dst87_r, dst109_r;
+    __m128i mask0;
+
+    mask0 = __lsx_vld(ff_hevc_mask_arr, 16);
+
+    src -= src_stride_3x + 3;
+    DUP4_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filter_x, 4,
+              filter_x, 6, filt0, filt1, filt2, filt3);
+    filter_vec = __lsx_vld(filter_y, 0);
+    filter_vec = __lsx_vsllwil_h_b(filter_vec, 0);
+
+    DUP4_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filter_vec, 2,
+              filter_vec, 3, filt_h0, filt_h1, filt_h2, filt_h3);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    mask3 = __lsx_vaddi_bu(mask0, 6);
+
+    src0 = __lsx_vld(src, 0);
+    DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src1, src2);
+    src3 = __lsx_vldx(src, src_stride_3x);
+    src += src_stride_4x;
+    src4 = __lsx_vld(src, 0);
+    DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src5, src6);
+    src += src_stride_3x;
+
+    DUP4_ARG3(__lsx_vshuf_b, src3, src0, mask0, src3, src0, mask1, src3, src0,
+              mask2, src3, src0, mask3, vec0, vec1, vec2, vec3);
+    DUP4_ARG3(__lsx_vshuf_b, src4, src1, mask0, src4, src1, mask1, src4, src1,
+              mask2, src4, src1, mask3, vec4, vec5, vec6, vec7);
+    DUP4_ARG3(__lsx_vshuf_b, src5, src2, mask0, src5, src2, mask1, src5, src2,
+              mask2, src5, src2, mask3, vec8, vec9, vec10, vec11);
+    DUP4_ARG3(__lsx_vshuf_b, src6, src3, mask0, src6, src3, mask1, src6, src3,
+              mask2, src6, src3, mask3, vec12, vec13, vec14, vec15);
+    dst30 = __lsx_vdp2_h_bu_b(vec0, filt0);
+    DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst30, vec1, filt1, dst30, vec2, filt2,
+              dst30, dst30);
+    dst30 = __lsx_vdp2add_h_bu_b(dst30, vec3, filt3);
+    dst41 = __lsx_vdp2_h_bu_b(vec4, filt0);
+    DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst41, vec5, filt1, dst41, vec6, filt2,
+              dst41, dst41);
+    dst41 = __lsx_vdp2add_h_bu_b(dst41, vec7, filt3);
+    dst52 = __lsx_vdp2_h_bu_b(vec8, filt0);
+    DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst52, vec9, filt1, dst52, vec10, filt2,
+              dst52, dst52);
+    dst52 = __lsx_vdp2add_h_bu_b(dst52, vec11, filt3);
+    dst63 = __lsx_vdp2_h_bu_b(vec12, filt0);
+    DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst63, vec13, filt1, dst63, vec14, filt2,
+              dst63, dst63);
+    dst63 = __lsx_vdp2add_h_bu_b(dst63, vec15, filt3);
+
+    DUP2_ARG2(__lsx_vilvl_h, dst41, dst30, dst52, dst41, dst10_r, dst21_r);
+    DUP2_ARG2(__lsx_vilvh_h, dst41, dst30, dst52, dst41, dst43_r, dst54_r);
+    dst32_r = __lsx_vilvl_h(dst63, dst52);
+    dst65_r = __lsx_vilvh_h(dst63, dst52);
+    dst66 = __lsx_vreplvei_d(dst63, 1);
+
+    for (loop_cnt = height >> 2; loop_cnt--;) {
+        src7 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src8, src9);
+        src10 = __lsx_vldx(src, src_stride_3x);
+        src += src_stride_4x;
+
+        DUP4_ARG3(__lsx_vshuf_b, src9, src7, mask0, src9, src7, mask1, src9, src7,
+                  mask2, src9, src7, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vshuf_b, src10, src8, mask0, src10, src8, mask1, src10, src8,
+                  mask2, src10, src8, mask3, vec4, vec5, vec6, vec7);
+
+        dst97 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst97, vec1, filt1, dst97, vec2, filt2,
+                  dst97, dst97);
+        dst97 = __lsx_vdp2add_h_bu_b(dst97, vec3, filt3);
+        dst108 = __lsx_vdp2_h_bu_b(vec4, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst108, vec5, filt1, dst108, vec6,
+                  filt2, dst108, dst108);
+        dst108 = __lsx_vdp2add_h_bu_b(dst108, vec7, filt3);
+
+        DUP2_ARG2(__lsx_vilvl_h, dst97, dst66, dst108, dst97, dst76_r, dst87_r);
+        dst109_r = __lsx_vilvh_h(dst108, dst97);
+        dst66 = __lsx_vreplvei_d(dst97, 1);
+        dst98_r = __lsx_vilvl_h(dst66, dst108);
+
+        DUP4_ARG2(__lsx_vdp2_w_h, dst10_r, filt_h0, dst21_r, filt_h0, dst32_r,
+                  filt_h0, dst43_r, filt_h0, dst0_r, dst1_r, dst2_r, dst3_r);
+        DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst32_r, filt_h1, dst1_r, dst43_r,
+                  filt_h1, dst2_r, dst54_r, filt_h1, dst3_r, dst65_r, filt_h1,
+                  dst0_r, dst1_r, dst2_r, dst3_r);
+        DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst54_r, filt_h2, dst1_r, dst65_r,
+                  filt_h2, dst2_r, dst76_r, filt_h2, dst3_r, dst87_r, filt_h2,
+                  dst0_r, dst1_r, dst2_r, dst3_r);
+        DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst76_r, filt_h3, dst1_r, dst87_r,
+                  filt_h3, dst2_r, dst98_r, filt_h3, dst3_r, dst109_r, filt_h3,
+                  dst0_r, dst1_r, dst2_r, dst3_r);
+        DUP4_ARG2(__lsx_vsrai_w, dst0_r, 6, dst1_r, 6, dst2_r, 6, dst3_r, 6,
+                  dst0_r, dst1_r, dst2_r, dst3_r);
+        DUP2_ARG2(__lsx_vpickev_h, dst1_r, dst0_r, dst3_r, dst2_r, dst0_r, dst2_r);
+        __lsx_vstelm_d(dst0_r, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(dst0_r, dst, 0, 1);
+        dst += dst_stride;
+        __lsx_vstelm_d(dst2_r, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(dst2_r, dst, 0, 1);
+        dst += dst_stride;
+
+        dst10_r = dst54_r;
+        dst32_r = dst76_r;
+        dst54_r = dst98_r;
+        dst21_r = dst65_r;
+        dst43_r = dst87_r;
+        dst65_r = dst109_r;
+        dst66 = __lsx_vreplvei_d(dst108, 1);
+    }
+}
+
+static void hevc_hv_8t_8multx1mult_lsx(uint8_t *src,
+                                       int32_t src_stride,
+                                       int16_t *dst,
+                                       int32_t dst_stride,
+                                       const int8_t *filter_x,
+                                       const int8_t *filter_y,
+                                       int32_t height,
+                                       int32_t width)
+{
+    uint32_t loop_cnt, cnt;
+    uint8_t *src_tmp;
+    int16_t *dst_tmp;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m128i filt0, filt1, filt2, filt3;
+    __m128i filt_h0, filt_h1, filt_h2, filt_h3;
+    __m128i mask1, mask2, mask3;
+    __m128i filter_vec;
+    __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
+    __m128i vec8, vec9, vec10, vec11, vec12, vec13, vec14, vec15;
+    __m128i dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    __m128i dst0_r, dst0_l;
+    __m128i dst10_r, dst32_r, dst54_r, dst76_r;
+    __m128i dst10_l, dst32_l, dst54_l, dst76_l;
+    __m128i mask0 = {0x403030202010100, 0x807070606050504};
+
+    src -= src_stride_3x + 3;
+    DUP4_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filter_x, 4,
+              filter_x, 6, filt0, filt1, filt2, filt3);
+
+    filter_vec = __lsx_vld(filter_y, 0);
+    filter_vec = __lsx_vsllwil_h_b(filter_vec, 0);
+
+    DUP4_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filter_vec, 2,
+              filter_vec, 3, filt_h0, filt_h1, filt_h2, filt_h3);
+
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    mask3 = __lsx_vaddi_bu(mask0, 6);
+
+    for (cnt = width >> 3; cnt--;) {
+        src_tmp = src;
+        dst_tmp = dst;
+        src0 = __lsx_vld(src_tmp, 0);
+        DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride_2x,
+                  src1, src2);
+        src3 = __lsx_vldx(src_tmp, src_stride_3x);
+        src_tmp += src_stride_4x;
+        src4 = __lsx_vld(src_tmp, 0);
+        DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride_2x,
+                  src5, src6);
+        src_tmp += src_stride_3x;
+
+        /* row 0 row 1 row 2 row 3 */
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src0,
+                  src0, mask2, src0, src0, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, src1,
+                  src1, mask2, src1, src1, mask3, vec4, vec5, vec6, vec7);
+        DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, src2,
+                  src2, mask2, src2, src2, mask3, vec8, vec9, vec10, vec11);
+        DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src3,
+                  src3, mask2, src3, src3, mask3, vec12, vec13, vec14, vec15);
+        dst0 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec1, filt1, dst0, vec2, filt2,
+                  dst0, dst0);
+        dst0 = __lsx_vdp2add_h_bu_b(dst0, vec3, filt3);
+        dst1 = __lsx_vdp2_h_bu_b(vec4, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst1, vec5, filt1, dst1, vec6, filt2,
+                  dst1, dst1);
+        dst1 = __lsx_vdp2add_h_bu_b(dst1, vec7, filt3);
+        dst2 = __lsx_vdp2_h_bu_b(vec8, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst2, vec9, filt1, dst2, vec10, filt2,
+                  dst2, dst2);
+        dst2 = __lsx_vdp2add_h_bu_b(dst2, vec11, filt3);
+        dst3 = __lsx_vdp2_h_bu_b(vec12, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst3, vec13, filt1, dst3, vec14, filt2,
+                  dst3, dst3);
+        dst3 = __lsx_vdp2add_h_bu_b(dst3, vec15, filt3);
+
+        /* row 4 row 5 row 6 */
+        DUP4_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, src4,
+                  src4, mask2, src4, src4, mask3, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, src5,
+                  src5, mask2, src5, src5, mask3, vec4, vec5, vec6, vec7);
+        DUP4_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, src6,
+                  src6, mask2, src6, src6, mask3, vec8, vec9, vec10, vec11);
+        dst4 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst4, vec1, filt1, dst4, vec2, filt2,
+                  dst4, dst4);
+        dst4 = __lsx_vdp2add_h_bu_b(dst4, vec3, filt3);
+        dst5 = __lsx_vdp2_h_bu_b(vec4, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst5, vec5, filt1, dst5, vec6, filt2,
+                  dst5, dst5);
+        dst5 = __lsx_vdp2add_h_bu_b(dst5, vec7, filt3);
+        dst6 = __lsx_vdp2_h_bu_b(vec8, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst6, vec9, filt1, dst6, vec10, filt2,
+                  dst6, dst6);
+        dst6 = __lsx_vdp2add_h_bu_b(dst6, vec11, filt3);
+
+        for (loop_cnt = height; loop_cnt--;) {
+            src7 = __lsx_vld(src_tmp, 0);
+            src_tmp += src_stride;
+
+            DUP4_ARG3(__lsx_vshuf_b, src7, src7, mask0, src7, src7, mask1, src7,
+                      src7, mask2, src7, src7, mask3, vec0, vec1, vec2, vec3);
+            dst7 = __lsx_vdp2_h_bu_b(vec0, filt0);
+            DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst7, vec1, filt1, dst7, vec2,
+                      filt2, dst7, dst7);
+            dst7 = __lsx_vdp2add_h_bu_b(dst7, vec3, filt3);
+
+            DUP4_ARG2(__lsx_vilvl_h, dst1, dst0, dst3, dst2, dst5, dst4, dst7,
+                      dst6, dst10_r, dst32_r, dst54_r, dst76_r);
+            DUP4_ARG2(__lsx_vilvh_h, dst1, dst0, dst3, dst2, dst5, dst4, dst7,
+                      dst6, dst10_l, dst32_l, dst54_l, dst76_l);
+
+            DUP2_ARG2(__lsx_vdp2_w_h, dst10_r, filt_h0, dst10_l, filt_h0,
+                      dst0_r, dst0_l);
+            DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst32_r, filt_h1, dst0_l,
+                      dst32_l, filt_h1, dst0_r, dst54_r, filt_h2, dst0_l,
+                      dst54_l, filt_h2, dst0_r, dst0_l, dst0_r, dst0_l);
+            DUP2_ARG3(__lsx_vdp2add_w_h, dst0_r, dst76_r, filt_h3, dst0_l,
+                      dst76_l, filt_h3, dst0_r, dst0_l);
+            dst0_r = __lsx_vsrai_w(dst0_r, 6);
+            dst0_l = __lsx_vsrai_w(dst0_l, 6);
+
+            dst0_r = __lsx_vpickev_h(dst0_l, dst0_r);
+            __lsx_vst(dst0_r, dst_tmp, 0);
+            dst_tmp += dst_stride;
+
+            dst0 = dst1;
+            dst1 = dst2;
+            dst2 = dst3;
+            dst3 = dst4;
+            dst4 = dst5;
+            dst5 = dst6;
+            dst6 = dst7;
+        }
+        src += 8;
+        dst += 8;
+    }
+}
+
+static void hevc_hv_8t_8w_lsx(uint8_t *src, int32_t src_stride,
+                              int16_t *dst, int32_t dst_stride,
+                              const int8_t *filter_x, const int8_t *filter_y,
+                              int32_t height)
+{
+    hevc_hv_8t_8multx1mult_lsx(src, src_stride, dst, dst_stride,
+                               filter_x, filter_y, height, 8);
+}
+
+static void hevc_hv_8t_12w_lsx(uint8_t *src, int32_t src_stride,
+                               int16_t *dst, int32_t dst_stride,
+                               const int8_t *filter_x, const int8_t *filter_y,
+                               int32_t height)
+{
+    uint32_t loop_cnt;
+    uint8_t *src_tmp;
+    int16_t *dst_tmp;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8, src9, src10;
+    __m128i mask0, mask1, mask2, mask3, mask4, mask5, mask6, mask7;
+    __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
+    __m128i vec8, vec9, vec10, vec11, vec12, vec13, vec14, vec15;
+    __m128i filt0, filt1, filt2, filt3, filt_h0, filt_h1, filt_h2, filt_h3;
+    __m128i filter_vec;
+    __m128i dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    __m128i dst30, dst41, dst52, dst63, dst66, dst97, dst108;
+    __m128i dst10_r, dst32_r, dst54_r, dst76_r, dst98_r, dst21_r, dst43_r;
+    __m128i dst65_r, dst87_r, dst109_r, dst10_l, dst32_l, dst54_l, dst76_l;
+    __m128i dst0_r, dst0_l, dst1_r, dst2_r, dst3_r;
+
+    src -= src_stride_3x + 3;
+    DUP4_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filter_x, 4,
+              filter_x, 6, filt0, filt1, filt2, filt3);
+
+    filter_vec = __lsx_vld(filter_y, 0);
+    filter_vec = __lsx_vsllwil_h_b(filter_vec, 0);
+
+    DUP4_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filter_vec, 2,
+              filter_vec, 3, filt_h0, filt_h1, filt_h2, filt_h3);
+
+    mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    mask3 = __lsx_vaddi_bu(mask0, 6);
+
+    src_tmp = src;
+    dst_tmp = dst;
+
+    src0 = __lsx_vld(src_tmp, 0);
+    DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride_2x,
+              src1, src2);
+    src3 = __lsx_vldx(src_tmp, src_stride_3x);
+    src_tmp += src_stride_4x;
+    src4 = __lsx_vld(src_tmp, 0);
+    DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride_2x,
+              src5, src6);
+    src_tmp += src_stride_3x;
+
+    /* row 0 row 1 row 2 row 3 */
+    DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src0, src0,
+              mask2, src0, src0, mask3, vec0, vec1, vec2, vec3);
+    DUP4_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, src1, src1,
+              mask2, src1, src1, mask3, vec4, vec5, vec6, vec7);
+    DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, src2, src2,
+              mask2, src2, src2, mask3, vec8, vec9, vec10, vec11);
+    DUP4_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, src3, src3,
+              mask2, src3, src3, mask3, vec12, vec13, vec14, vec15);
+    dst0 = __lsx_vdp2_h_bu_b(vec0, filt0);
+    DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec1, filt1, dst0, vec2, filt2,
+              dst0, dst0);
+    dst0 = __lsx_vdp2add_h_bu_b(dst0, vec3, filt3);
+    dst1 = __lsx_vdp2_h_bu_b(vec4, filt0);
+    DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst1, vec5, filt1, dst1, vec6, filt2,
+              dst1, dst1);
+    dst1 = __lsx_vdp2add_h_bu_b(dst1, vec7, filt3);
+    dst2 = __lsx_vdp2_h_bu_b(vec8, filt0);
+    DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst2, vec9, filt1, dst2, vec10, filt2,
+              dst2, dst2);
+    dst2 = __lsx_vdp2add_h_bu_b(dst2, vec11, filt3);
+    dst3 = __lsx_vdp2_h_bu_b(vec12, filt0);
+    DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst3, vec13, filt1, dst3, vec14, filt2,
+              dst3, dst3);
+    dst3 = __lsx_vdp2add_h_bu_b(dst3, vec15, filt3);
+
+    /* row 4 row 5 row 6 */
+    DUP4_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, src4, src4,
+              mask2, src4, src4, mask3, vec0, vec1, vec2, vec3);
+    DUP4_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1, src5, src5,
+              mask2, src5, src5, mask3, vec4, vec5, vec6, vec7);
+    DUP4_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, src6, src6,
+              mask2, src6, src6, mask3, vec8, vec9, vec10, vec11);
+    dst4 = __lsx_vdp2_h_bu_b(vec0, filt0);
+    DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst4, vec1, filt1, dst4, vec2, filt2,
+              dst4, dst4);
+    dst4 = __lsx_vdp2add_h_bu_b(dst4, vec3, filt3);
+    dst5 = __lsx_vdp2_h_bu_b(vec4, filt0);
+    DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst5, vec5, filt1, dst5, vec6, filt2,
+              dst5, dst5);
+    dst5 = __lsx_vdp2add_h_bu_b(dst5, vec7, filt3);
+    dst6 = __lsx_vdp2_h_bu_b(vec8, filt0);
+    DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst6, vec9, filt1, dst6, vec10, filt2,
+              dst6, dst6);
+    dst6 = __lsx_vdp2add_h_bu_b(dst6, vec11, filt3);
+
+    for (loop_cnt = height; loop_cnt--;) {
+        src7 = __lsx_vld(src_tmp, 0);
+        src_tmp += src_stride;
+
+        DUP4_ARG3(__lsx_vshuf_b, src7, src7, mask0, src7, src7, mask1, src7,
+                  src7, mask2, src7, src7, mask3, vec0, vec1, vec2, vec3);
+        dst7 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst7, vec1, filt1, dst7, vec2, filt2,
+                  dst7, dst7);
+        dst7 = __lsx_vdp2add_h_bu_b(dst7, vec3, filt3);
+        DUP4_ARG2(__lsx_vilvl_h, dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
+                  dst10_r, dst32_r, dst54_r, dst76_r);
+        DUP4_ARG2(__lsx_vilvh_h, dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
+                  dst10_l, dst32_l, dst54_l, dst76_l);
+        DUP2_ARG2(__lsx_vdp2_w_h, dst10_r, filt_h0, dst10_l, filt_h0,
+                  dst0_r, dst0_l);
+        DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst32_r, filt_h1, dst0_l, dst32_l,
+                  filt_h1, dst0_r, dst54_r, filt_h2, dst0_l, dst54_l, filt_h2,
+                  dst0_r, dst0_l, dst0_r, dst0_l);
+        DUP2_ARG3(__lsx_vdp2add_w_h, dst0_r, dst76_r, filt_h3, dst0_l, dst76_l,
+                  filt_h3, dst0_r, dst0_l)
+        dst0_r = __lsx_vsrai_w(dst0_r, 6);
+        dst0_l = __lsx_vsrai_w(dst0_l, 6);
+
+        dst0_r = __lsx_vpickev_h(dst0_l, dst0_r);
+        __lsx_vst(dst0_r, dst_tmp, 0);
+        dst_tmp += dst_stride;
+
+        dst0 = dst1;
+        dst1 = dst2;
+        dst2 = dst3;
+        dst3 = dst4;
+        dst4 = dst5;
+        dst5 = dst6;
+        dst6 = dst7;
+    }
+    src += 8;
+    dst += 8;
+
+    mask4 = __lsx_vld(ff_hevc_mask_arr, 16);
+    DUP2_ARG2(__lsx_vaddi_bu, mask4, 2, mask4, 4, mask5, mask6);
+    mask7 = __lsx_vaddi_bu(mask4, 6);
+
+    src0 = __lsx_vld(src, 0);
+    DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src1, src2);
+    src3 = __lsx_vldx(src, src_stride_3x);
+    src += src_stride_4x;
+    src4 = __lsx_vld(src, 0);
+    DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src5, src6);
+    src += src_stride_3x;
+
+    DUP4_ARG3(__lsx_vshuf_b, src3, src0, mask4, src3, src0, mask5, src3, src0,
+              mask6, src3, src0, mask7, vec0, vec1, vec2, vec3);
+    DUP4_ARG3(__lsx_vshuf_b, src4, src1, mask4, src4, src1, mask5, src4, src1,
+              mask6, src4, src1, mask7, vec4, vec5, vec6, vec7);
+    DUP4_ARG3(__lsx_vshuf_b, src5, src2, mask4, src5, src2, mask5, src5, src2,
+              mask6, src5, src2, mask7, vec8, vec9, vec10, vec11);
+    DUP4_ARG3(__lsx_vshuf_b, src6, src3, mask4, src6, src3, mask5, src6, src3,
+              mask6, src6, src3, mask7, vec12, vec13, vec14, vec15);
+    dst30 = __lsx_vdp2_h_bu_b(vec0, filt0);
+    DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst30, vec1, filt1, dst30, vec2, filt2,
+              dst30, dst30);
+    dst30 = __lsx_vdp2add_h_bu_b(dst30, vec3, filt3);
+    dst41 = __lsx_vdp2_h_bu_b(vec4, filt0);
+    DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst41, vec5, filt1, dst41, vec6, filt2,
+              dst41, dst41);
+    dst41 = __lsx_vdp2add_h_bu_b(dst41, vec7, filt3);
+    dst52 = __lsx_vdp2_h_bu_b(vec8, filt0);
+    DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst52, vec9, filt1, dst52, vec10, filt2,
+              dst52, dst52);
+    dst52 = __lsx_vdp2add_h_bu_b(dst52, vec11, filt3);
+    dst63 = __lsx_vdp2_h_bu_b(vec12, filt0);
+    DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst63, vec13, filt1, dst63, vec14, filt2,
+              dst63, dst63);
+    dst63 = __lsx_vdp2add_h_bu_b(dst63, vec15, filt3);
+
+    DUP2_ARG2(__lsx_vilvl_h, dst41, dst30, dst52, dst41, dst10_r, dst21_r);
+    DUP2_ARG2(__lsx_vilvh_h, dst41, dst30, dst52, dst41, dst43_r, dst54_r);
+    dst32_r = __lsx_vilvl_h(dst63, dst52);
+    dst65_r = __lsx_vilvh_h(dst63, dst52);
+
+    dst66 = __lsx_vreplvei_d(dst63, 1);
+
+    for (loop_cnt = height >> 2; loop_cnt--;) {
+        src7 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src8, src9);
+        src10 = __lsx_vldx(src, src_stride_3x);
+        src += src_stride_4x;
+
+        DUP4_ARG3(__lsx_vshuf_b, src9, src7, mask4, src9, src7, mask5, src9,
+                  src7, mask6, src9, src7, mask7, vec0, vec1, vec2, vec3);
+        DUP4_ARG3(__lsx_vshuf_b, src10, src8, mask4, src10, src8, mask5, src10,
+                  src8, mask6, src10, src8, mask7, vec4, vec5, vec6, vec7);
+        dst97 = __lsx_vdp2_h_bu_b(vec0, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst97, vec1, filt1, dst97, vec2, filt2,
+                  dst97, dst97);
+        dst97 = __lsx_vdp2add_h_bu_b(dst97, vec3, filt3);
+        dst108 = __lsx_vdp2_h_bu_b(vec4, filt0);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst108, vec5, filt1, dst108, vec6,
+                  filt2, dst108, dst108);
+        dst108 = __lsx_vdp2add_h_bu_b(dst108, vec7, filt3);
+
+        DUP2_ARG2(__lsx_vilvl_h, dst97, dst66, dst108, dst97, dst76_r, dst87_r);
+        dst109_r = __lsx_vilvh_h(dst108, dst97);
+        dst66 = __lsx_vreplvei_d(dst97, 1);
+        dst98_r = __lsx_vilvl_h(dst66, dst108);
+
+        DUP4_ARG2(__lsx_vdp2_w_h, dst10_r, filt_h0, dst21_r, filt_h0, dst32_r,
+                  filt_h0, dst43_r, filt_h0, dst0_r, dst1_r, dst2_r, dst3_r);
+        DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst32_r, filt_h1, dst1_r, dst43_r,
+                  filt_h1, dst2_r, dst54_r, filt_h1, dst3_r, dst65_r, filt_h1,
+                  dst0_r, dst1_r, dst2_r, dst3_r);
+        DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst54_r, filt_h2, dst1_r, dst65_r,
+                  filt_h2, dst2_r, dst76_r, filt_h2, dst3_r, dst87_r, filt_h2,
+                  dst0_r, dst1_r, dst2_r, dst3_r);
+        DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst76_r, filt_h3, dst1_r, dst87_r,
+                  filt_h3, dst2_r, dst98_r, filt_h3, dst3_r, dst109_r, filt_h3,
+                  dst0_r, dst1_r, dst2_r, dst3_r);
+        DUP4_ARG2(__lsx_vsrai_w, dst0_r, 6, dst1_r, 6, dst2_r, 6, dst3_r, 6,
+                  dst0_r, dst1_r, dst2_r, dst3_r);
+        DUP2_ARG2(__lsx_vpickev_h, dst1_r, dst0_r, dst3_r, dst2_r,
+                  dst0_r, dst2_r);
+        __lsx_vstelm_d(dst0_r, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(dst0_r, dst, 0, 1);
+        dst += dst_stride;
+        __lsx_vstelm_d(dst2_r, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(dst2_r, dst, 0, 1);
+        dst += dst_stride;
+
+        dst10_r = dst54_r;
+        dst32_r = dst76_r;
+        dst54_r = dst98_r;
+        dst21_r = dst65_r;
+        dst43_r = dst87_r;
+        dst65_r = dst109_r;
+        dst66 = __lsx_vreplvei_d(dst108, 1);
+    }
+}
+
+static void hevc_hv_8t_16w_lsx(uint8_t *src, int32_t src_stride,
+                               int16_t *dst, int32_t dst_stride,
+                               const int8_t *filter_x, const int8_t *filter_y,
+                               int32_t height)
+{
+    hevc_hv_8t_8multx1mult_lsx(src, src_stride, dst, dst_stride,
+                               filter_x, filter_y, height, 16);
+}
+
+static void hevc_hv_8t_24w_lsx(uint8_t *src, int32_t src_stride,
+                               int16_t *dst, int32_t dst_stride,
+                               const int8_t *filter_x, const int8_t *filter_y,
+                               int32_t height)
+{
+    hevc_hv_8t_8multx1mult_lsx(src, src_stride, dst, dst_stride,
+                               filter_x, filter_y, height, 24);
+}
+
+static void hevc_hv_8t_32w_lsx(uint8_t *src, int32_t src_stride,
+                               int16_t *dst, int32_t dst_stride,
+                               const int8_t *filter_x, const int8_t *filter_y,
+                               int32_t height)
+{
+    hevc_hv_8t_8multx1mult_lsx(src, src_stride, dst, dst_stride,
+                               filter_x, filter_y, height, 32);
+}
+
+static void hevc_hv_8t_48w_lsx(uint8_t *src, int32_t src_stride,
+                               int16_t *dst, int32_t dst_stride,
+                               const int8_t *filter_x, const int8_t *filter_y,
+                               int32_t height)
+{
+    hevc_hv_8t_8multx1mult_lsx(src, src_stride, dst, dst_stride,
+                               filter_x, filter_y, height, 48);
+}
+
+static void hevc_hv_8t_64w_lsx(uint8_t *src, int32_t src_stride,
+                               int16_t *dst, int32_t dst_stride,
+                               const int8_t *filter_x, const int8_t *filter_y,
+                               int32_t height)
+{
+    hevc_hv_8t_8multx1mult_lsx(src, src_stride, dst, dst_stride,
+                               filter_x, filter_y, height, 64);
+}
+
+static void hevc_hz_4t_32w_lsx(uint8_t *src,
+                               int32_t src_stride,
+                               int16_t *dst,
+                               int32_t dst_stride,
+                               const int8_t *filter,
+                               int32_t height)
+{
+    uint32_t loop_cnt;
+    __m128i src0, src1, src2;
+    __m128i filt0, filt1;
+    __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
+    __m128i mask1, mask2, mask3;
+    __m128i dst0, dst1, dst2, dst3;
+    __m128i vec0, vec1, vec2, vec3;
+
+    src -= 1;
+    DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
+
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 8, mask1, mask2);
+    mask3 = __lsx_vaddi_bu(mask0, 10);
+
+    for (loop_cnt = height; loop_cnt--;) {
+        DUP2_ARG2(__lsx_vld, src, 0, src, 16, src0, src1);
+        src2 = __lsx_vld(src, 24);
+        src += src_stride;
+
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src0, mask2,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src2, src2, mask0,
+                  vec2, vec3);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec1, filt0, vec2, filt0,
+                  vec3, filt0, dst0, dst1, dst2, dst3);
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src0, mask3,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask1, src2, src2, mask1,
+                  vec2, vec3);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec0, filt1, dst1, vec1, filt1,
+                  dst2, vec2, filt1, dst3, vec3, filt1, dst0, dst1, dst2, dst3);
+        __lsx_vst(dst0, dst, 0);
+        __lsx_vst(dst1, dst, 16);
+        __lsx_vst(dst2, dst, 32);
+        __lsx_vst(dst3, dst, 48);
+        dst += dst_stride;
+    }
+}
+
+static void hevc_vt_4t_16w_lsx(uint8_t *src,
+                               int32_t src_stride,
+                               int16_t *dst,
+                               int32_t dst_stride,
+                               const int8_t *filter,
+                               int32_t height)
+{
+    int32_t loop_cnt;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    __m128i src0, src1, src2, src3, src4, src5;
+    __m128i src10_r, src32_r, src21_r, src43_r;
+    __m128i src10_l, src32_l, src21_l, src43_l;
+    __m128i dst0_r, dst1_r, dst0_l, dst1_l;
+    __m128i filt0, filt1;
+
+    src -= src_stride;
+    DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
+
+    src0 = __lsx_vld(src, 0);
+    DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src1, src2);
+    src += src_stride_3x;
+    DUP2_ARG2(__lsx_vilvl_b, src1, src0, src2, src1, src10_r, src21_r);
+    DUP2_ARG2(__lsx_vilvh_b, src1, src0, src2, src1, src10_l, src21_l);
+
+    for (loop_cnt = (height >> 2); loop_cnt--;) {
+        src3 = __lsx_vld(src, 0);
+        src4 = __lsx_vldx(src, src_stride);
+        src += src_stride_2x;
+        DUP2_ARG2(__lsx_vilvl_b, src3, src2, src4, src3, src32_r, src43_r);
+        DUP2_ARG2(__lsx_vilvh_b, src3, src2, src4, src3, src32_l, src43_l);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, src10_r, filt0, src10_l, filt0, src21_r,
+                  filt0, src21_l, filt0, dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0_r, src32_r, filt1, dst0_l,
+                  src32_l, filt1, dst1_r, src43_r, filt1, dst1_l, src43_l,
+                  filt1, dst0_r, dst0_l, dst1_r, dst1_l);
+        __lsx_vst(dst0_r, dst, 0);
+        __lsx_vst(dst0_l, dst, 16);
+        dst += dst_stride;
+        __lsx_vst(dst1_r, dst, 0);
+        __lsx_vst(dst1_l, dst, 16);
+        dst += dst_stride;
+
+        src5 = __lsx_vld(src, 0);
+        src2 = __lsx_vldx(src, src_stride);
+        src += src_stride_2x;
+        DUP2_ARG2(__lsx_vilvl_b, src5, src4, src2, src5, src10_r, src21_r);
+        DUP2_ARG2(__lsx_vilvh_b, src5, src4, src2, src5, src10_l, src21_l);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, src32_r, filt0, src32_l, filt0, src43_r,
+                  filt0, src43_l, filt0, dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0_r, src10_r, filt1, dst0_l,
+                  src10_l, filt1, dst1_r, src21_r, filt1, dst1_l, src21_l,
+                  filt1, dst0_r, dst0_l, dst1_r, dst1_l);
+        __lsx_vst(dst0_r, dst, 0);
+        __lsx_vst(dst0_l, dst, 16);
+        dst += dst_stride;
+        __lsx_vst(dst1_r, dst, 0);
+        __lsx_vst(dst1_l, dst, 16);
+        dst += dst_stride;
+    }
+}
+
+static void hevc_vt_4t_24w_lsx(uint8_t *src,
+                               int32_t src_stride,
+                               int16_t *dst,
+                               int32_t dst_stride,
+                               const int8_t *filter,
+                               int32_t height)
+{
+    int32_t loop_cnt;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    uint8_t *_src;
+
+    __m128i src0, src1, src2, src3, src4, src5;
+    __m128i src6, src7, src8, src9, src10, src11;
+    __m128i src10_r, src32_r, src76_r, src98_r;
+    __m128i src21_r, src43_r, src87_r, src109_r;
+    __m128i dst0_r, dst1_r, dst2_r, dst3_r;
+    __m128i src10_l, src32_l, src21_l, src43_l;
+    __m128i dst0_l, dst1_l;
+    __m128i filt0, filt1;
+
+    src -= src_stride;
+    _src = src + 16;
+    DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
+
+    src0 = __lsx_vld(src, 0);
+    DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src1, src2);
+    DUP2_ARG2(__lsx_vilvl_b, src1, src0, src2, src1, src10_r, src21_r);
+    DUP2_ARG2(__lsx_vilvh_b, src1, src0, src2, src1, src10_l, src21_l);
+
+    src6 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride_2x, src7, src8);
+    src  += src_stride_3x;
+    _src += src_stride_3x;
+    DUP2_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src76_r, src87_r);
+
+    for (loop_cnt = (height >> 2); loop_cnt--;) {
+        DUP2_ARG2(__lsx_vld, src, 0, _src, 0, src3, src9);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, _src, src_stride, src4, src10);
+        src  += src_stride_2x;
+        _src += src_stride_2x;
+        DUP2_ARG2(__lsx_vilvl_b, src3, src2, src4, src3, src32_r, src43_r);
+        DUP2_ARG2(__lsx_vilvh_b, src3, src2, src4, src3, src32_l, src43_l);
+
+        DUP2_ARG2(__lsx_vilvl_b, src9, src8, src10, src9, src98_r, src109_r);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, src10_r, filt0, src10_l, filt0, src21_r,
+                  filt0, src21_l, filt0, dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0_r, src32_r, filt1, dst0_l,
+                  src32_l, filt1, dst1_r, src43_r, filt1, dst1_l, src43_l,
+                  filt1, dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP2_ARG2(__lsx_vdp2_h_bu_b, src76_r, filt0, src87_r, filt0,
+                  dst2_r, dst3_r);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst2_r, src98_r, filt1, dst3_r,
+                  src109_r, filt1, dst2_r, dst3_r);
+
+        __lsx_vst(dst0_r, dst, 0);
+        __lsx_vst(dst0_l, dst, 16);
+        __lsx_vst(dst2_r, dst, 32);
+        dst += dst_stride;
+        __lsx_vst(dst1_r, dst, 0);
+        __lsx_vst(dst1_l, dst, 16);
+        __lsx_vst(dst3_r, dst, 32);
+        dst += dst_stride;
+
+        DUP2_ARG2(__lsx_vld, src, 0, _src, 0, src5, src11);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, _src, src_stride, src2, src8);
+        src  += src_stride_2x;
+        _src += src_stride_2x;
+        DUP2_ARG2(__lsx_vilvl_b, src5, src4, src2, src5, src10_r, src21_r);
+        DUP2_ARG2(__lsx_vilvh_b, src5, src4, src2, src5, src10_l, src21_l);
+
+        DUP2_ARG2(__lsx_vilvl_b, src11, src10, src8, src11, src76_r, src87_r);
+
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, src32_r, filt0, src32_l, filt0, src43_r,
+                  filt0, src43_l, filt0, dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0_r, src10_r, filt1, dst0_l, src10_l,
+                  filt1, dst1_r, src21_r, filt1, dst1_l, src21_l, filt1,
+                  dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP2_ARG2(__lsx_vdp2_h_bu_b, src98_r, filt0, src109_r, filt0,
+                  dst2_r, dst3_r);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst2_r, src76_r, filt1, dst3_r, src87_r,
+                  filt1, dst2_r, dst3_r);
+
+        __lsx_vst(dst0_r, dst, 0);
+        __lsx_vst(dst0_l, dst, 16);
+        __lsx_vst(dst2_r, dst, 32);
+        dst += dst_stride;
+        __lsx_vst(dst1_r, dst, 0);
+        __lsx_vst(dst1_l, dst, 16);
+        __lsx_vst(dst3_r, dst, 32);
+        dst += dst_stride;
+    }
+}
+
+static void hevc_vt_4t_32w_lsx(uint8_t *src,
+                               int32_t src_stride,
+                               int16_t *dst,
+                               int32_t dst_stride,
+                               const int8_t *filter,
+                               int32_t height)
+{
+    int32_t loop_cnt;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    uint8_t *_src;
+
+    __m128i src0, src1, src2, src3, src4, src5;
+    __m128i src6, src7, src8, src9, src10, src11;
+    __m128i src10_r, src32_r, src76_r, src98_r;
+    __m128i src21_r, src43_r, src87_r, src109_r;
+    __m128i dst0_r, dst1_r, dst2_r, dst3_r;
+    __m128i src10_l, src32_l, src76_l, src98_l;
+    __m128i src21_l, src43_l, src87_l, src109_l;
+    __m128i dst0_l, dst1_l, dst2_l, dst3_l;
+    __m128i filt0, filt1;
+
+    src -= src_stride;
+    _src = src + 16;
+    DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
+
+    src0 = __lsx_vld(src, 0);
+    DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src1, src2);
+    DUP2_ARG2(__lsx_vilvl_b, src1, src0, src2, src1, src10_r, src21_r);
+    DUP2_ARG2(__lsx_vilvh_b, src1, src0, src2, src1, src10_l, src21_l);
+
+    src6 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride_2x, src7, src8);
+    src  += src_stride_3x;
+    _src += src_stride_3x;
+    DUP2_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src76_r, src87_r);
+    DUP2_ARG2(__lsx_vilvh_b, src7, src6, src8, src7, src76_l, src87_l);
+
+    for (loop_cnt = (height >> 2); loop_cnt--;) {
+        DUP2_ARG2(__lsx_vld, src, 0, _src, 0, src3, src9);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, _src, src_stride, src4, src10);
+        src  += src_stride_2x;
+        _src += src_stride_2x;
+        DUP2_ARG2(__lsx_vilvl_b, src3, src2, src4, src3, src32_r, src43_r);
+        DUP2_ARG2(__lsx_vilvh_b, src3, src2, src4, src3, src32_l, src43_l);
+
+        DUP2_ARG2(__lsx_vilvl_b, src9, src8, src10, src9, src98_r, src109_r);
+        DUP2_ARG2(__lsx_vilvh_b, src9, src8, src10, src9, src98_l, src109_l);
+
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, src10_r, filt0, src10_l, filt0, src21_r,
+                  filt0, src21_l, filt0, dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0_r, src32_r, filt1, dst0_l,
+                  src32_l, filt1, dst1_r, src43_r, filt1, dst1_l,src43_l,
+                  filt1, dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, src76_r, filt0, src76_l, filt0, src87_r,
+                  filt0, src87_l, filt0, dst2_r, dst2_l, dst3_r, dst3_l);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst2_r, src98_r, filt1, dst2_l, src98_l,
+                  filt1, dst3_r, src109_r, filt1, dst3_l, src109_l, filt1,
+                  dst2_r, dst2_l, dst3_r, dst3_l);
+
+        __lsx_vst(dst0_r, dst, 0);
+        __lsx_vst(dst0_l, dst, 16);
+        __lsx_vst(dst2_r, dst, 32);
+        __lsx_vst(dst2_l, dst, 48);
+        dst += dst_stride;
+        __lsx_vst(dst1_r, dst, 0);
+        __lsx_vst(dst1_l, dst, 16);
+        __lsx_vst(dst3_r, dst, 32);
+        __lsx_vst(dst3_l, dst, 48);
+        dst += dst_stride;
+
+        DUP2_ARG2(__lsx_vld, src, 0, _src, 0, src5, src11);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, _src, src_stride, src2, src8);
+        src  += src_stride_2x;
+        _src += src_stride_2x;
+        DUP2_ARG2(__lsx_vilvl_b, src5, src4, src2, src5, src10_r, src21_r);
+        DUP2_ARG2(__lsx_vilvh_b, src5, src4, src2, src5, src10_l, src21_l);
+
+        DUP2_ARG2(__lsx_vilvl_b, src11, src10, src8, src11, src76_r, src87_r);
+        DUP2_ARG2(__lsx_vilvh_b, src11, src10, src8, src11, src76_l, src87_l);
+
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, src32_r, filt0, src32_l, filt0, src43_r,
+                  filt0, src43_l, filt0, dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0_r, src10_r, filt1, dst0_l,
+                  src10_l, filt1, dst1_r, src21_r, filt1, dst1_l, src21_l,
+                  filt1, dst0_r, dst0_l, dst1_r, dst1_l);
+
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, src98_r, filt0, src98_l, filt0, src109_r,
+                  filt0, src109_l, filt0, dst2_r, dst2_l, dst3_r, dst3_l);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst2_r, src76_r, filt1, dst2_l, src76_l,
+                  filt1, dst3_r, src87_r, filt1, dst3_l, src87_l, filt1,
+                  dst2_r, dst2_l, dst3_r, dst3_l);
+
+        __lsx_vst(dst0_r, dst, 0);
+        __lsx_vst(dst0_l, dst, 16);
+        __lsx_vst(dst2_r, dst, 32);
+        __lsx_vst(dst2_l, dst, 48);
+        dst += dst_stride;
+        __lsx_vst(dst1_r, dst, 0);
+        __lsx_vst(dst1_l, dst, 16);
+        __lsx_vst(dst3_r, dst, 32);
+        __lsx_vst(dst3_l, dst, 48);
+        dst += dst_stride;
+    }
+}
+
+static void hevc_hv_4t_8x2_lsx(uint8_t *src,
+                               int32_t src_stride,
+                               int16_t *dst,
+                               int32_t dst_stride,
+                               const int8_t *filter_x,
+                               const int8_t *filter_y)
+{
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+
+    __m128i src0, src1, src2, src3, src4;
+    __m128i filt0, filt1;
+    __m128i filt_h0, filt_h1;
+    __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
+    __m128i mask1;
+    __m128i filter_vec;
+    __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7, vec8, vec9;
+    __m128i dst0, dst1, dst2, dst3, dst4;
+    __m128i dst0_r, dst0_l, dst1_r, dst1_l;
+    __m128i dst10_r, dst32_r, dst21_r, dst43_r;
+    __m128i dst10_l, dst32_l, dst21_l, dst43_l;
+
+    src -= (src_stride + 1);
+    DUP2_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filt0, filt1);
+
+    filter_vec = __lsx_vld(filter_y, 0);
+    filter_vec = __lsx_vsllwil_h_b(filter_vec, 0);
+    DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
+
+    mask1 = __lsx_vaddi_bu(mask0, 2);
+
+    src0 = __lsx_vld(src, 0);
+    DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src1, src2);
+    src3 = __lsx_vldx(src, src_stride_3x);
+    src4 = __lsx_vldx(src, src_stride_4x);
+
+    DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, vec0, vec1);
+    DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, vec2, vec3);
+    DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, vec4, vec5);
+    DUP2_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1, vec6, vec7);
+    DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, vec8, vec9);
+
+    DUP4_ARG2(__lsx_vdp2_h_bu_b,  vec0, filt0, vec2, filt0, vec4, filt0,
+              vec6, filt0, dst0, dst1, dst2, dst3);
+    DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec1, filt1, dst1, vec3, filt1,
+              dst2, vec5, filt1, dst3, vec7, filt1, dst0, dst1, dst2, dst3);
+    dst4 = __lsx_vdp2_h_bu_b(vec8, filt0);
+    dst4 = __lsx_vdp2add_h_bu_b(dst4, vec9, filt1);
+
+    DUP2_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst10_r, dst21_r);
+    DUP2_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst10_l, dst21_l);
+    DUP2_ARG2(__lsx_vilvl_h, dst3, dst2, dst4, dst3, dst32_r, dst43_r);
+    DUP2_ARG2(__lsx_vilvh_h, dst3, dst2, dst4, dst3, dst32_l, dst43_l);
+
+    DUP4_ARG2(__lsx_vdp2_w_h, dst10_r, filt_h0, dst10_l, filt_h0, dst21_r,
+              filt_h0, dst21_l, filt_h0, dst0_r, dst0_l, dst1_r, dst1_l);
+    DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst32_r, filt_h1, dst0_l, dst32_l,
+              filt_h1, dst1_r, dst43_r, filt_h1, dst1_l, dst43_l, filt_h1,
+              dst0_r, dst0_l, dst1_r, dst1_l);
+    DUP4_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst1_r, 6, dst1_l, 6,
+              dst0_r, dst0_l, dst1_r, dst1_l);
+    DUP2_ARG2(__lsx_vpickev_h, dst0_l, dst0_r, dst1_l, dst1_r, dst0_r, dst1_r);
+    __lsx_vst(dst0_r, dst, 0);
+    __lsx_vst(dst1_r, dst + dst_stride, 0);
+}
+
+static void hevc_hv_4t_8multx4_lsx(uint8_t *src, int32_t src_stride,
+                                   int16_t *dst, int32_t dst_stride,
+                                   const int8_t *filter_x,
+                                   const int8_t *filter_y, int32_t width8mult)
+{
+    int32_t cnt;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t dst_stride_x  = (dst_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t dst_stride_2x = (dst_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride_x;
+
+    __m128i src0, src1, src2, src3, src4, src5, src6, mask0, mask1;
+    __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
+    __m128i filt0, filt1, filt_h0, filt_h1, filter_vec;
+    __m128i dst0, dst1, dst2, dst3, dst4, dst5, dst6;
+    __m128i dst10_r, dst32_r, dst54_r, dst21_r, dst43_r, dst65_r;
+    __m128i dst10_l, dst32_l, dst54_l, dst21_l, dst43_l, dst65_l;
+    __m128i dst0_r, dst0_l, dst1_r, dst1_l, dst2_r, dst2_l, dst3_r, dst3_l;
+
+    src -= (src_stride + 1);
+    DUP2_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filt0, filt1);
+
+    filter_vec = __lsx_vld(filter_y, 0);
+    filter_vec = __lsx_vsllwil_h_b(filter_vec, 0);
+    DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
+
+    mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
+    mask1 = __lsx_vaddi_bu(mask0, 2);
+
+    for (cnt = width8mult; cnt--;) {
+        src0 = __lsx_vld(src, 0);
+        DUP4_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src,
+                  src_stride_3x, src, src_stride_4x, src1, src2, src3, src4);
+        src += src_stride_4x;
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src5, src6);
+        src += (8 - src_stride_4x);
+
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1,
+                  vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1,
+                  vec4, vec5);
+
+        DUP2_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, dst0, dst1);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec1, filt1, dst1, vec3, filt1,
+                  dst0, dst1);
+        dst2 = __lsx_vdp2_h_bu_b(vec4, filt0);
+        dst2 = __lsx_vdp2add_h_bu_b(dst2, vec5, filt1);
+
+        DUP2_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst10_r, dst21_r);
+        DUP2_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst10_l, dst21_l);
+
+        DUP2_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1,
+                  vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1,
+                  vec4, vec5);
+        DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1,
+                  vec6, vec7);
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, vec4, filt0,
+                  vec6, filt0, dst3, dst4, dst5, dst6);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst3, vec1, filt1, dst4, vec3, filt1,
+                  dst5, vec5, filt1, dst6, vec7, filt1, dst3, dst4, dst5, dst6);
+        DUP2_ARG2(__lsx_vilvl_h, dst3, dst2, dst4, dst3, dst32_r, dst43_r);
+        DUP2_ARG2(__lsx_vilvh_h, dst3, dst2, dst4, dst3, dst32_l, dst43_l);
+        DUP2_ARG2(__lsx_vilvl_h, dst5, dst4, dst6, dst5, dst54_r, dst65_r);
+        DUP2_ARG2(__lsx_vilvh_h, dst5, dst4, dst6, dst5, dst54_l, dst65_l);
+
+        DUP4_ARG2(__lsx_vdp2_w_h, dst10_r, filt_h0, dst10_l, filt_h0, dst21_r,
+                  filt_h0, dst21_l, filt_h0, dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst32_r, filt_h1, dst0_l, dst32_l,
+                  filt_h1, dst1_r, dst43_r, filt_h1, dst1_l, dst43_l, filt_h1,
+                  dst0_r, dst0_l, dst1_r, dst1_l);
+
+        DUP4_ARG2(__lsx_vdp2_w_h, dst32_r, filt_h0, dst32_l, filt_h0, dst43_r,
+                  filt_h0, dst43_l, filt_h0, dst2_r, dst2_l, dst3_r, dst3_l);
+        DUP4_ARG3(__lsx_vdp2add_w_h, dst2_r, dst54_r, filt_h1, dst2_l, dst54_l,
+                  filt_h1, dst3_r, dst65_r, filt_h1, dst3_l, dst65_l, filt_h1,
+                  dst2_r, dst2_l, dst3_r, dst3_l);
+        DUP4_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst1_r, 6, dst1_l, 6,
+                  dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP4_ARG2(__lsx_vsrai_w, dst2_r, 6, dst2_l, 6, dst3_r, 6, dst3_l, 6,
+                  dst2_r, dst2_l, dst3_r, dst3_l);
+        DUP2_ARG2(__lsx_vpickev_h, dst0_l, dst0_r, dst1_l, dst1_r,
+                  dst0_r, dst1_r);
+        DUP2_ARG2(__lsx_vpickev_h, dst2_l, dst2_r, dst3_l, dst3_r,
+                  dst2_r, dst3_r);
+
+        __lsx_vst(dst0_r, dst, 0);
+        __lsx_vstx(dst1_r, dst, dst_stride_x);
+        __lsx_vstx(dst2_r, dst, dst_stride_2x);
+        __lsx_vstx(dst3_r, dst, dst_stride_3x);
+        dst += 8;
+    }
+}
+
+static void hevc_hv_4t_8x6_lsx(uint8_t *src,
+                               int32_t src_stride,
+                               int16_t *dst,
+                               int32_t dst_stride,
+                               const int8_t *filter_x,
+                               const int8_t *filter_y)
+{
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t dst_stride_2x = (dst_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8;
+    __m128i filt0, filt1;
+    __m128i filt_h0, filt_h1;
+    __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
+    __m128i mask1, filter_vec;
+    __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7, vec8, vec9;
+    __m128i vec10, vec11, vec12, vec13, vec14, vec15, vec16, vec17;
+    __m128i dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7, dst8;
+    __m128i dst0_r, dst0_l, dst1_r, dst1_l, dst2_r, dst2_l, dst3_r, dst3_l;
+    __m128i dst4_r, dst4_l, dst5_r, dst5_l;
+    __m128i dst10_r, dst32_r, dst10_l, dst32_l;
+    __m128i dst21_r, dst43_r, dst21_l, dst43_l;
+    __m128i dst54_r, dst54_l, dst65_r, dst65_l;
+    __m128i dst76_r, dst76_l, dst87_r, dst87_l;
+
+    src -= (src_stride + 1);
+    DUP2_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filt0, filt1);
+
+    filter_vec = __lsx_vld(filter_y, 0);
+    filter_vec = __lsx_vsllwil_h_b(filter_vec, 0);
+    DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
+
+    mask1 = __lsx_vaddi_bu(mask0, 2);
+
+    src0 = __lsx_vld(src, 0);
+    DUP4_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src,
+              src_stride_3x, src, src_stride_4x, src1, src2, src3, src4);
+    src += src_stride_4x;
+    DUP4_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src,
+              src_stride_3x, src, src_stride_4x, src5, src6, src7, src8);
+
+    DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, src1, src1,
+              mask0, src1, src1, mask1, vec0, vec1, vec2, vec3);
+    DUP4_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1,src3, src3,
+              mask0, src3, src3, mask1, vec4, vec5, vec6, vec7);
+    DUP4_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1, src5, src5,
+              mask0, src5, src5, mask1, vec8, vec9, vec10, vec11);
+    DUP4_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1, src7, src7,
+              mask0, src7, src7, mask1, vec12, vec13, vec14, vec15);
+    DUP2_ARG3(__lsx_vshuf_b, src8, src8, mask0, src8, src8, mask1,
+              vec16, vec17);
+
+    DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, vec4, filt0, vec6,
+              filt0, dst0, dst1, dst2, dst3);
+    DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec1, filt1, dst1, vec3, filt1,
+              dst2, vec5, filt1, dst3, vec7, filt1, dst0, dst1, dst2, dst3);
+    DUP4_ARG2(__lsx_vdp2_h_bu_b,  vec8, filt0, vec10, filt0, vec12, filt0,
+              vec14, filt0, dst4, dst5, dst6, dst7);
+    DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst4, vec9, filt1, dst5, vec11, filt1, dst6,
+              vec13, filt1, dst7, vec15, filt1, dst4, dst5, dst6, dst7);
+    dst8 = __lsx_vdp2_h_bu_b(vec16, filt0);
+    dst8 = __lsx_vdp2add_h_bu_b(dst8, vec17, filt1);
+
+    DUP4_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst3, dst2, dst4, dst3,
+              dst10_r, dst21_r, dst32_r, dst43_r);
+    DUP4_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst3, dst2, dst4, dst3,
+              dst10_l, dst21_l, dst32_l, dst43_l);
+    DUP4_ARG2(__lsx_vilvl_h, dst5, dst4, dst6, dst5, dst7, dst6, dst8, dst7,
+              dst54_r, dst65_r, dst76_r, dst87_r);
+    DUP4_ARG2(__lsx_vilvh_h, dst5, dst4, dst6, dst5, dst7, dst6, dst8, dst7,
+              dst54_l, dst65_l, dst76_l, dst87_l);
+
+    DUP4_ARG2(__lsx_vdp2_w_h, dst10_r, filt_h0, dst10_l, filt_h0, dst21_r,
+              filt_h0, dst21_l, filt_h0, dst0_r, dst0_l, dst1_r, dst1_l);
+    DUP4_ARG2(__lsx_vdp2_w_h, dst32_r, filt_h0, dst32_l, filt_h0, dst43_r,
+              filt_h0, dst43_l, filt_h0, dst2_r, dst2_l, dst3_r, dst3_l);
+    DUP4_ARG2(__lsx_vdp2_w_h, dst54_r, filt_h0, dst54_l, filt_h0, dst65_r,
+              filt_h0, dst65_l, filt_h0, dst4_r, dst4_l, dst5_r, dst5_l);
+    DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst32_r, filt_h1, dst0_l, dst32_l,
+              filt_h1, dst1_r, dst43_r, filt_h1, dst1_l, dst43_l, filt_h1,
+              dst0_r, dst0_l, dst1_r, dst1_l);
+    DUP4_ARG3(__lsx_vdp2add_w_h, dst2_r, dst54_r, filt_h1, dst2_l, dst54_l,
+              filt_h1, dst3_r, dst65_r, filt_h1, dst3_l, dst65_l, filt_h1,
+              dst2_r, dst2_l, dst3_r, dst3_l);
+    DUP4_ARG3(__lsx_vdp2add_w_h, dst4_r, dst76_r, filt_h1, dst4_l, dst76_l,
+              filt_h1, dst5_r, dst87_r, filt_h1, dst5_l, dst87_l, filt_h1,
+              dst4_r, dst4_l, dst5_r, dst5_l);
+    DUP4_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst1_r, 6, dst1_l, 6, dst0_r,
+              dst0_l, dst1_r, dst1_l);
+    DUP4_ARG2(__lsx_vsrai_w, dst2_r, 6, dst2_l, 6, dst3_r, 6, dst3_l, 6, dst2_r,
+              dst2_l, dst3_r, dst3_l);
+    DUP4_ARG2(__lsx_vsrai_w, dst4_r, 6, dst4_l, 6, dst5_r, 6, dst5_l, 6, dst4_r,
+              dst4_l, dst5_r, dst5_l);
+
+    DUP4_ARG2(__lsx_vpickev_h,dst0_l, dst0_r, dst1_l, dst1_r, dst2_l, dst2_r,
+              dst3_l, dst3_r, dst0_r, dst1_r, dst2_r, dst3_r);
+    DUP2_ARG2(__lsx_vpickev_h, dst4_l, dst4_r, dst5_l, dst5_r, dst4_r, dst5_r);
+
+    __lsx_vst(dst0_r, dst, 0);
+    __lsx_vstx(dst1_r, dst, dst_stride_2x);
+    dst += dst_stride_2x;
+    __lsx_vst(dst2_r, dst, 0);
+    __lsx_vstx(dst3_r, dst, dst_stride_2x);
+    dst += dst_stride_2x;
+    __lsx_vst(dst4_r, dst, 0);
+    __lsx_vstx(dst5_r, dst, dst_stride_2x);
+}
+
+static void hevc_hv_4t_8multx4mult_lsx(uint8_t *src,
+                                       int32_t src_stride,
+                                       int16_t *dst,
+                                       int32_t dst_stride,
+                                       const int8_t *filter_x,
+                                       const int8_t *filter_y,
+                                       int32_t height,
+                                       int32_t width8mult)
+{
+    uint32_t loop_cnt, cnt;
+    uint8_t *src_tmp;
+    int16_t *dst_tmp;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t dst_stride_x  = (dst_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t dst_stride_2x = (dst_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride_x;
+
+    __m128i src0, src1, src2, src3, src4, src5, src6;
+    __m128i filt0, filt1;
+    __m128i filt_h0, filt_h1;
+    __m128i mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
+    __m128i mask1, filter_vec;
+    __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
+    __m128i dst0, dst1, dst2, dst3, dst4, dst5, dst6;
+    __m128i dst0_r, dst0_l, dst1_r, dst1_l, dst2_r, dst2_l, dst3_r, dst3_l;
+    __m128i dst10_r, dst32_r, dst54_r, dst21_r, dst43_r, dst65_r;
+    __m128i dst10_l, dst32_l, dst54_l, dst21_l, dst43_l, dst65_l;
+
+    src -= (src_stride + 1);
+    DUP2_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filt0, filt1);
+
+    filter_vec = __lsx_vld(filter_y, 0);
+    filter_vec = __lsx_vsllwil_h_b(filter_vec, 0);
+    DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
+
+    mask1 = __lsx_vaddi_bu(mask0, 2);
+
+    for (cnt = width8mult; cnt--;) {
+        src_tmp = src;
+        dst_tmp = dst;
+
+        src0 = __lsx_vld(src_tmp, 0);
+        DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride_2x,
+                  src1, src2);
+        src_tmp += src_stride_3x;
+
+        DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1,
+                  vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1,
+                  vec4, vec5);
+
+        DUP2_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, dst0, dst1);
+        DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec1, filt1, dst1, vec3, filt1,
+                  dst0, dst1);
+        dst2 = __lsx_vdp2_h_bu_b(vec4, filt0);
+        dst2 = __lsx_vdp2add_h_bu_b(dst2, vec5, filt1);
+
+        DUP2_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst10_r, dst21_r);
+        DUP2_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst10_l, dst21_l);
+
+        for (loop_cnt = height >> 2; loop_cnt--;) {
+            src3 = __lsx_vld(src_tmp, 0);
+            DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride_2x,
+                      src4, src5);
+            src6 = __lsx_vldx(src_tmp, src_stride_3x);
+            src_tmp += src_stride_4x;
+
+            DUP2_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1,
+                      vec0, vec1);
+            DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1,
+                      vec2, vec3);
+            DUP2_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1,
+                      vec4, vec5);
+            DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1,
+                      vec6, vec7);
+
+            DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, vec4, filt0,
+                      vec6, filt0, dst3, dst4, dst5, dst6);
+            DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst3, vec1, filt1, dst4, vec3,
+                      filt1, dst5, vec5, filt1, dst6, vec7, filt1, dst3,
+                      dst4, dst5, dst6);
+
+            DUP2_ARG2(__lsx_vilvl_h, dst3, dst2, dst4, dst3, dst32_r, dst43_r);
+            DUP2_ARG2(__lsx_vilvh_h, dst3, dst2, dst4, dst3, dst32_l, dst43_l);
+            DUP2_ARG2(__lsx_vilvl_h, dst5, dst4, dst6, dst5, dst54_r, dst65_r);
+            DUP2_ARG2(__lsx_vilvh_h, dst5, dst4, dst6, dst5, dst54_l, dst65_l);
+
+            DUP4_ARG2(__lsx_vdp2_w_h, dst10_r, filt_h0, dst10_l, filt_h0, dst21_r,
+                      filt_h0, dst21_l, filt_h0, dst0_r, dst0_l, dst1_r, dst1_l);
+            DUP4_ARG2(__lsx_vdp2_w_h, dst32_r, filt_h0, dst32_l, filt_h0, dst43_r,
+                      filt_h0, dst43_l, filt_h0, dst2_r, dst2_l, dst3_r, dst3_l);
+            DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst32_r, filt_h1, dst0_l,
+                      dst32_l, filt_h1, dst1_r, dst43_r, filt_h1, dst1_l,
+                      dst43_l, filt_h1, dst0_r, dst0_l, dst1_r, dst1_l);
+            DUP4_ARG3(__lsx_vdp2add_w_h, dst2_r, dst54_r, filt_h1, dst2_l,
+                      dst54_l, filt_h1, dst3_r, dst65_r, filt_h1, dst3_l,
+                      dst65_l, filt_h1, dst2_r, dst2_l, dst3_r, dst3_l);
+
+            DUP4_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst1_r, 6, dst1_l, 6,
+                      dst0_r, dst0_l, dst1_r, dst1_l);
+            DUP4_ARG2(__lsx_vsrai_w, dst2_r, 6, dst2_l, 6, dst3_r, 6, dst3_l, 6,
+                      dst2_r, dst2_l, dst3_r, dst3_l);
+
+            DUP4_ARG2(__lsx_vpickev_h, dst0_l, dst0_r, dst1_l, dst1_r, dst2_l,
+                      dst2_r, dst3_l, dst3_r, dst0_r, dst1_r, dst2_r, dst3_r);
+
+            __lsx_vst(dst0_r, dst_tmp, 0);
+            __lsx_vstx(dst1_r, dst_tmp, dst_stride_x);
+            __lsx_vstx(dst2_r, dst_tmp, dst_stride_2x);
+            __lsx_vstx(dst3_r, dst_tmp, dst_stride_3x);
+            dst_tmp += dst_stride_2x;
+
+            dst10_r = dst54_r;
+            dst10_l = dst54_l;
+            dst21_r = dst65_r;
+            dst21_l = dst65_l;
+            dst2 = dst6;
+        }
+        src += 8;
+        dst += 8;
+    }
+}
+
+static void hevc_hv_4t_8w_lsx(uint8_t *src,
+                              int32_t src_stride,
+                              int16_t *dst,
+                              int32_t dst_stride,
+                              const int8_t *filter_x,
+                              const int8_t *filter_y,
+                              int32_t height)
+{
+
+    if (2 == height) {
+        hevc_hv_4t_8x2_lsx(src, src_stride, dst, dst_stride,
+                           filter_x, filter_y);
+    } else if (4 == height) {
+        hevc_hv_4t_8multx4_lsx(src, src_stride, dst, dst_stride,
+                               filter_x, filter_y, 1);
+    } else if (6 == height) {
+        hevc_hv_4t_8x6_lsx(src, src_stride, dst, dst_stride,
+                           filter_x, filter_y);
+    } else if (0 == (height & 0x03)) {
+        hevc_hv_4t_8multx4mult_lsx(src, src_stride, dst, dst_stride,
+                                   filter_x, filter_y, height, 1);
+    }
+}
+
+static void hevc_hv_4t_12w_lsx(uint8_t *src,
+                               int32_t src_stride,
+                               int16_t *dst,
+                               int32_t dst_stride,
+                               const int8_t *filter_x,
+                               const int8_t *filter_y,
+                               int32_t height)
+{
+    uint32_t loop_cnt;
+    uint8_t *src_tmp;
+    int16_t *dst_tmp;
+    int32_t src_stride_2x = (src_stride << 1);
+    int32_t dst_stride_x  = (dst_stride << 1);
+    int32_t src_stride_4x = (src_stride << 2);
+    int32_t dst_stride_2x = (dst_stride << 2);
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride_x;
+
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8, src9, src10;
+    __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
+    __m128i mask0, mask1, mask2, mask3;
+    __m128i filt0, filt1, filt_h0, filt_h1, filter_vec, dst0;
+    __m128i dst1, dst2, dst3, dst4, dst5, dst6, dst10, dst21, dst22, dst73;
+    __m128i dst84, dst95, dst106, dst76_r, dst98_r, dst87_r, dst109_r;
+    __m128i dst10_r, dst32_r, dst54_r, dst21_r, dst43_r, dst65_r;
+    __m128i dst10_l, dst32_l, dst54_l, dst21_l, dst43_l, dst65_l;
+    __m128i dst0_r, dst0_l, dst1_r, dst1_l, dst2_r, dst2_l, dst3_r, dst3_l;
+    __m128i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
+
+    src -= (src_stride + 1);
+    DUP2_ARG2(__lsx_vldrepl_h, filter_x, 0, filter_x, 2, filt0, filt1);
+
+    filter_vec = __lsx_vld(filter_y, 0);
+    filter_vec = __lsx_vsllwil_h_b(filter_vec, 0);
+    DUP2_ARG2(__lsx_vreplvei_w, filter_vec, 0, filter_vec, 1, filt_h0, filt_h1);
+
+    mask0 = __lsx_vld(ff_hevc_mask_arr, 0);
+    mask1 = __lsx_vaddi_bu(mask0, 2);
+
+    src_tmp = src;
+    dst_tmp = dst;
+
+    src0 = __lsx_vld(src_tmp, 0);
+    DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride_2x,
+              src1, src2);
+    src_tmp += src_stride_3x;
+
+    DUP2_ARG3(__lsx_vshuf_b, src0, src0, mask0, src0, src0, mask1, vec0, vec1);
+    DUP2_ARG3(__lsx_vshuf_b, src1, src1, mask0, src1, src1, mask1, vec2, vec3);
+    DUP2_ARG3(__lsx_vshuf_b, src2, src2, mask0, src2, src2, mask1, vec4, vec5);
+
+    DUP2_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, dst0, dst1);
+    dst2 = __lsx_vdp2_h_bu_b(vec4, filt0);
+    DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst0, vec1, filt1, dst1, vec3, filt1,
+              dst0, dst1);
+    dst2 = __lsx_vdp2add_h_bu_b(dst2, vec5, filt1);
+
+    DUP2_ARG2(__lsx_vilvl_h, dst1, dst0, dst2, dst1, dst10_r, dst21_r);
+    DUP2_ARG2(__lsx_vilvh_h, dst1, dst0, dst2, dst1, dst10_l, dst21_l);
+
+    for (loop_cnt = 4; loop_cnt--;) {
+        src3 = __lsx_vld(src_tmp, 0);
+        DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride_2x,
+                  src4, src5);
+        src6 = __lsx_vldx(src_tmp, src_stride_3x);
+        src_tmp += src_stride_4x;
+
+        DUP2_ARG3(__lsx_vshuf_b, src3, src3, mask0, src3, src3, mask1,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src4, src4, mask0, src4, src4, mask1,
+                  vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src5, src5, mask0, src5, src5, mask1,
+                  vec4, vec5);
+        DUP2_ARG3(__lsx_vshuf_b, src6, src6, mask0, src6, src6, mask1,
+                  vec6, vec7);
+
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, vec4, filt0,
+                  vec6, filt0, dst3, dst4, dst5, dst6);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst3, vec1, filt1, dst4, vec3,
+                  filt1, dst5, vec5, filt1, dst6, vec7, filt1, dst3,
+                  dst4, dst5, dst6);
+
+        DUP2_ARG2(__lsx_vilvl_h, dst3, dst2, dst4, dst3, dst32_r, dst43_r);
+        DUP2_ARG2(__lsx_vilvh_h, dst3, dst2, dst4, dst3, dst32_l, dst43_l);
+        DUP2_ARG2(__lsx_vilvl_h, dst5, dst4, dst6, dst5, dst54_r, dst65_r);
+        DUP2_ARG2(__lsx_vilvh_h, dst5, dst4, dst6, dst5, dst54_l, dst65_l);
+
+        DUP4_ARG2(__lsx_vdp2_w_h, dst10_r, filt_h0, dst10_l, filt_h0, dst21_r,
+                  filt_h0, dst21_l, filt_h0, dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP4_ARG2(__lsx_vdp2_w_h, dst32_r, filt_h0, dst32_l, filt_h0, dst43_r,
+                  filt_h0, dst43_l, filt_h0, dst2_r, dst2_l, dst3_r, dst3_l);
+        DUP4_ARG3(__lsx_vdp2add_w_h, dst0_r, dst32_r, filt_h1, dst0_l, dst32_l,
+                  filt_h1, dst1_r, dst43_r, filt_h1, dst1_l, dst43_l, filt_h1,
+                  dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP4_ARG3(__lsx_vdp2add_w_h, dst2_r, dst54_r, filt_h1, dst2_l, dst54_l,
+                  filt_h1, dst3_r, dst65_r, filt_h1, dst3_l, dst65_l, filt_h1,
+                  dst2_r, dst2_l, dst3_r, dst3_l);
+        DUP4_ARG2(__lsx_vsrai_w, dst0_r, 6, dst0_l, 6, dst1_r, 6, dst1_l, 6,
+                  dst0_r, dst0_l, dst1_r, dst1_l);
+        DUP4_ARG2(__lsx_vsrai_w, dst2_r, 6, dst2_l, 6, dst3_r, 6, dst3_l, 6,
+                  dst2_r, dst2_l, dst3_r, dst3_l);
+        DUP4_ARG2(__lsx_vpickev_h, dst0_l, dst0_r, dst1_l, dst1_r, dst2_l, dst2_r,
+                  dst3_l, dst3_r, dst0_r, dst1_r, dst2_r, dst3_r);
+        __lsx_vst(dst0_r, dst_tmp, 0);
+        __lsx_vstx(dst1_r, dst_tmp, dst_stride_x);
+        __lsx_vstx(dst2_r, dst_tmp, dst_stride_2x);
+        __lsx_vstx(dst3_r, dst_tmp, dst_stride_3x);
+        dst_tmp += dst_stride_2x;
+
+        dst10_r = dst54_r;
+        dst10_l = dst54_l;
+        dst21_r = dst65_r;
+        dst21_l = dst65_l;
+        dst2 = dst6;
+    }
+
+    src += 8;
+    dst += 8;
+
+    mask2 = __lsx_vld(ff_hevc_mask_arr, 16);
+    mask3 = __lsx_vaddi_bu(mask2, 2);
+
+    src0 = __lsx_vld(src, 0);
+    DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src1, src2);
+    src += src_stride_3x;
+    DUP2_ARG3(__lsx_vshuf_b, src1, src0, mask2, src1, src0, mask3, vec0, vec1);
+    DUP2_ARG3(__lsx_vshuf_b, src2, src1, mask2, src2, src1, mask3, vec2, vec3);
+    DUP2_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, dst10, dst21);
+    DUP2_ARG3(__lsx_vdp2add_h_bu_b, dst10, vec1, filt1, dst21, vec3, filt1,
+              dst10, dst21);
+    dst10_r = __lsx_vilvl_h(dst21, dst10);
+    dst21_r = __lsx_vilvh_h(dst21, dst10);
+    dst22 = __lsx_vreplvei_d(dst21, 1);
+
+    for (loop_cnt = 2; loop_cnt--;) {
+        src3 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src4, src5);
+        src6 = __lsx_vldx(src, src_stride_3x);
+        src += src_stride_4x;
+        src7 = __lsx_vld(src, 0);
+        DUP2_ARG2(__lsx_vldx, src, src_stride, src, src_stride_2x, src8, src9);
+        src10 = __lsx_vldx(src, src_stride_3x);
+        src += src_stride_4x;
+        DUP2_ARG3(__lsx_vshuf_b, src7, src3, mask2, src7, src3, mask3,
+                  vec0, vec1);
+        DUP2_ARG3(__lsx_vshuf_b, src8, src4, mask2, src8, src4, mask3,
+                  vec2, vec3);
+        DUP2_ARG3(__lsx_vshuf_b, src9, src5, mask2, src9, src5, mask3,
+                  vec4, vec5);
+        DUP2_ARG3(__lsx_vshuf_b, src10, src6, mask2, src10, src6, mask3,
+                  vec6, vec7);
+
+        DUP4_ARG2(__lsx_vdp2_h_bu_b, vec0, filt0, vec2, filt0, vec4, filt0,
+                  vec6, filt0, dst73, dst84, dst95, dst106);
+        DUP4_ARG3(__lsx_vdp2add_h_bu_b, dst73, vec1, filt1, dst84, vec3,
+                  filt1, dst95, vec5, filt1, dst106, vec7, filt1, dst73,
+                  dst84, dst95, dst106);
+
+        DUP2_ARG2(__lsx_vilvl_h, dst73, dst22, dst84, dst73, dst32_r, dst43_r);
+        DUP2_ARG2(__lsx_vilvh_h, dst84, dst73, dst95, dst84, dst87_r, dst98_r);
+        DUP2_ARG2(__lsx_vilvl_h, dst95, dst84, dst106, dst95, dst54_r, dst65_r);
+        dst109_r = __lsx_vilvh_h(dst106, dst95);
+        dst22 = __lsx_vreplvei_d(dst73, 1);
+        dst76_r = __lsx_vilvl_h(dst22, dst106);
+
+        DUP4_ARG2(__lsx_vdp2_w_h, dst10_r, filt_h0, dst21_r, filt_h0, dst32_r,
+                  filt_h0, dst43_r, filt_h0, tmp0, tmp1, tmp2, tmp3);
+        DUP4_ARG2(__lsx_vdp2_w_h, dst54_r, filt_h0, dst65_r, filt_h0, dst76_r,
+                  filt_h0, dst87_r, filt_h0, tmp4, tmp5, tmp6, tmp7);
+        DUP4_ARG3(__lsx_vdp2add_w_h, tmp0, dst32_r, filt_h1, tmp1, dst43_r,
+                  filt_h1, tmp2, dst54_r, filt_h1, tmp3, dst65_r, filt_h1,
+                  tmp0, tmp1, tmp2, tmp3);
+        DUP4_ARG3(__lsx_vdp2add_w_h, tmp4, dst76_r, filt_h1, tmp5, dst87_r,
+                  filt_h1, tmp6, dst98_r, filt_h1, tmp7, dst109_r, filt_h1,
+                  tmp4, tmp5, tmp6, tmp7);
+        DUP4_ARG2(__lsx_vsrai_w, tmp0, 6, tmp1, 6, tmp2, 6, tmp3, 6,
+                  tmp0, tmp1, tmp2, tmp3);
+        DUP4_ARG2(__lsx_vsrai_w, tmp4, 6, tmp5, 6, tmp6, 6, tmp7, 6,
+                  tmp4, tmp5, tmp6, tmp7);
+        DUP4_ARG2(__lsx_vpickev_h, tmp1, tmp0, tmp3, tmp2, tmp5, tmp4,
+                  tmp7, tmp6, tmp0, tmp1, tmp2, tmp3);
+
+        __lsx_vstelm_d(tmp0, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(tmp0, dst, 0, 1);
+        dst += dst_stride;
+        __lsx_vstelm_d(tmp1, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(tmp1, dst, 0, 1);
+        dst += dst_stride;
+        __lsx_vstelm_d(tmp2, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(tmp2, dst, 0, 1);
+        dst += dst_stride;
+        __lsx_vstelm_d(tmp3, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(tmp3, dst, 0, 1);
+        dst += dst_stride;
+
+        dst10_r = dst98_r;
+        dst21_r = dst109_r;
+        dst22 = __lsx_vreplvei_d(dst106, 1);
+    }
+}
+
+static void hevc_hv_4t_16w_lsx(uint8_t *src,
+                               int32_t src_stride,
+                               int16_t *dst,
+                               int32_t dst_stride,
+                               const int8_t *filter_x,
+                               const int8_t *filter_y,
+                               int32_t height)
+{
+    if (4 == height) {
+        hevc_hv_4t_8multx4_lsx(src, src_stride, dst, dst_stride,
+                               filter_x, filter_y, 2);
+    } else {
+        hevc_hv_4t_8multx4mult_lsx(src, src_stride, dst, dst_stride,
+                                   filter_x, filter_y, height, 2);
+    }
+}
+
+static void hevc_hv_4t_24w_lsx(uint8_t *src,
+                               int32_t src_stride,
+                               int16_t *dst,
+                               int32_t dst_stride,
+                               const int8_t *filter_x,
+                               const int8_t *filter_y,
+                               int32_t height)
+{
+    hevc_hv_4t_8multx4mult_lsx(src, src_stride, dst, dst_stride,
+                               filter_x, filter_y, height, 3);
+}
+
+static void hevc_hv_4t_32w_lsx(uint8_t *src,
+                               int32_t src_stride,
+                               int16_t *dst,
+                               int32_t dst_stride,
+                               const int8_t *filter_x,
+                               const int8_t *filter_y,
+                               int32_t height)
+{
+    hevc_hv_4t_8multx4mult_lsx(src, src_stride, dst, dst_stride,
+                               filter_x, filter_y, height, 4);
+}
+
+#define MC_COPY(WIDTH)                                                    \
+void ff_hevc_put_hevc_pel_pixels##WIDTH##_8_lsx(int16_t *dst,             \
+                                                uint8_t *src,             \
+                                                ptrdiff_t src_stride,     \
+                                                int height,               \
+                                                intptr_t mx,              \
+                                                intptr_t my,              \
+                                                int width)                \
+{                                                                         \
+    hevc_copy_##WIDTH##w_lsx(src, src_stride, dst, MAX_PB_SIZE, height);  \
+}
+
+MC_COPY(4);
+MC_COPY(6);
+MC_COPY(8);
+MC_COPY(12);
+MC_COPY(16);
+MC_COPY(24);
+MC_COPY(32);
+MC_COPY(48);
+MC_COPY(64);
+
+#undef MC_COPY
+
+#define MC(PEL, DIR, WIDTH, TAP, DIR1, FILT_DIR)                          \
+void ff_hevc_put_hevc_##PEL##_##DIR##WIDTH##_8_lsx(int16_t *dst,          \
+                                                   uint8_t *src,          \
+                                                   ptrdiff_t src_stride,  \
+                                                   int height,            \
+                                                   intptr_t mx,           \
+                                                   intptr_t my,           \
+                                                   int width)             \
+{                                                                         \
+    const int8_t *filter = ff_hevc_##PEL##_filters[FILT_DIR - 1];         \
+                                                                          \
+    hevc_##DIR1##_##TAP##t_##WIDTH##w_lsx(src, src_stride, dst,           \
+                                          MAX_PB_SIZE, filter, height);   \
+}
+
+MC(qpel, h, 4, 8, hz, mx);
+MC(qpel, h, 8, 8, hz, mx);
+MC(qpel, h, 12, 8, hz, mx);
+MC(qpel, h, 16, 8, hz, mx);
+MC(qpel, h, 24, 8, hz, mx);
+MC(qpel, h, 32, 8, hz, mx);
+MC(qpel, h, 48, 8, hz, mx);
+MC(qpel, h, 64, 8, hz, mx);
+
+MC(qpel, v, 4, 8, vt, my);
+MC(qpel, v, 8, 8, vt, my);
+MC(qpel, v, 12, 8, vt, my);
+MC(qpel, v, 16, 8, vt, my);
+MC(qpel, v, 24, 8, vt, my);
+MC(qpel, v, 32, 8, vt, my);
+MC(qpel, v, 48, 8, vt, my);
+MC(qpel, v, 64, 8, vt, my);
+
+MC(epel, h, 32, 4, hz, mx);
+
+MC(epel, v, 16, 4, vt, my);
+MC(epel, v, 24, 4, vt, my);
+MC(epel, v, 32, 4, vt, my);
+
+#undef MC
+
+#define MC_HV(PEL, WIDTH, TAP)                                          \
+void ff_hevc_put_hevc_##PEL##_hv##WIDTH##_8_lsx(int16_t *dst,           \
+                                                uint8_t *src,           \
+                                                ptrdiff_t src_stride,   \
+                                                int height,             \
+                                                intptr_t mx,            \
+                                                intptr_t my,            \
+                                                int width)              \
+{                                                                       \
+    const int8_t *filter_x = ff_hevc_##PEL##_filters[mx - 1];           \
+    const int8_t *filter_y = ff_hevc_##PEL##_filters[my - 1];           \
+                                                                        \
+    hevc_hv_##TAP##t_##WIDTH##w_lsx(src, src_stride, dst, MAX_PB_SIZE,  \
+                                          filter_x, filter_y, height);  \
+}
+
+MC_HV(qpel, 4, 8);
+MC_HV(qpel, 8, 8);
+MC_HV(qpel, 12, 8);
+MC_HV(qpel, 16, 8);
+MC_HV(qpel, 24, 8);
+MC_HV(qpel, 32, 8);
+MC_HV(qpel, 48, 8);
+MC_HV(qpel, 64, 8);
+
+MC_HV(epel, 8, 4);
+MC_HV(epel, 12, 4);
+MC_HV(epel, 16, 4);
+MC_HV(epel, 24, 4);
+MC_HV(epel, 32, 4);
+
+#undef MC_HV
diff --git a/libavcodec/loongarch/hevcdsp_lsx.h b/libavcodec/loongarch/hevcdsp_lsx.h
new file mode 100644
index 0000000000..0c517af887
--- /dev/null
+++ b/libavcodec/loongarch/hevcdsp_lsx.h
@@ -0,0 +1,230 @@
+/*
+ * Copyright (c) 2022 Loongson Technology Corporation Limited
+ * Contributed by Lu Wang <wanglu@loongson.cn>
+ *                Hao Chen <chenhao@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef AVCODEC_LOONGARCH_HEVCDSP_LSX_H
+#define AVCODEC_LOONGARCH_HEVCDSP_LSX_H
+
+#include "libavcodec/hevcdsp.h"
+
+#define MC(PEL, DIR, WIDTH)                                               \
+void ff_hevc_put_hevc_##PEL##_##DIR##WIDTH##_8_lsx(int16_t *dst,          \
+                                                   uint8_t *src,          \
+                                                   ptrdiff_t src_stride,  \
+                                                   int height,            \
+                                                   intptr_t mx,           \
+                                                   intptr_t my,           \
+                                                   int width)
+
+MC(pel, pixels, 4);
+MC(pel, pixels, 6);
+MC(pel, pixels, 8);
+MC(pel, pixels, 12);
+MC(pel, pixels, 16);
+MC(pel, pixels, 24);
+MC(pel, pixels, 32);
+MC(pel, pixels, 48);
+MC(pel, pixels, 64);
+
+MC(qpel, h, 4);
+MC(qpel, h, 8);
+MC(qpel, h, 12);
+MC(qpel, h, 16);
+MC(qpel, h, 24);
+MC(qpel, h, 32);
+MC(qpel, h, 48);
+MC(qpel, h, 64);
+
+MC(qpel, v, 4);
+MC(qpel, v, 8);
+MC(qpel, v, 12);
+MC(qpel, v, 16);
+MC(qpel, v, 24);
+MC(qpel, v, 32);
+MC(qpel, v, 48);
+MC(qpel, v, 64);
+
+MC(qpel, hv, 4);
+MC(qpel, hv, 8);
+MC(qpel, hv, 12);
+MC(qpel, hv, 16);
+MC(qpel, hv, 24);
+MC(qpel, hv, 32);
+MC(qpel, hv, 48);
+MC(qpel, hv, 64);
+
+MC(epel, h, 32);
+
+MC(epel, v, 16);
+MC(epel, v, 24);
+MC(epel, v, 32);
+
+MC(epel, hv, 8);
+MC(epel, hv, 12);
+MC(epel, hv, 16);
+MC(epel, hv, 24);
+MC(epel, hv, 32);
+
+#undef MC
+
+#define BI_MC(PEL, DIR, WIDTH)                                               \
+void ff_hevc_put_hevc_bi_##PEL##_##DIR##WIDTH##_8_lsx(uint8_t *dst,          \
+                                                      ptrdiff_t dst_stride,  \
+                                                      uint8_t *src,          \
+                                                      ptrdiff_t src_stride,  \
+                                                      int16_t *src_16bit,    \
+                                                      int height,            \
+                                                      intptr_t mx,           \
+                                                      intptr_t my,           \
+                                                      int width)
+
+BI_MC(pel, pixels, 4);
+BI_MC(pel, pixels, 6);
+BI_MC(pel, pixels, 8);
+BI_MC(pel, pixels, 12);
+BI_MC(pel, pixels, 16);
+BI_MC(pel, pixels, 24);
+BI_MC(pel, pixels, 32);
+BI_MC(pel, pixels, 48);
+BI_MC(pel, pixels, 64);
+
+BI_MC(qpel, h, 16);
+BI_MC(qpel, h, 24);
+BI_MC(qpel, h, 32);
+BI_MC(qpel, h, 48);
+BI_MC(qpel, h, 64);
+
+BI_MC(qpel, v, 8);
+BI_MC(qpel, v, 16);
+BI_MC(qpel, v, 24);
+BI_MC(qpel, v, 32);
+BI_MC(qpel, v, 48);
+BI_MC(qpel, v, 64);
+
+BI_MC(qpel, hv, 8);
+BI_MC(qpel, hv, 16);
+BI_MC(qpel, hv, 24);
+BI_MC(qpel, hv, 32);
+BI_MC(qpel, hv, 48);
+BI_MC(qpel, hv, 64);
+
+BI_MC(epel, h, 24);
+BI_MC(epel, h, 32);
+
+BI_MC(epel, v, 12);
+BI_MC(epel, v, 16);
+BI_MC(epel, v, 24);
+BI_MC(epel, v, 32);
+
+BI_MC(epel, hv, 6);
+BI_MC(epel, hv, 8);
+BI_MC(epel, hv, 16);
+BI_MC(epel, hv, 24);
+BI_MC(epel, hv, 32);
+
+#undef BI_MC
+
+#define UNI_MC(PEL, DIR, WIDTH)                                              \
+void ff_hevc_put_hevc_uni_##PEL##_##DIR##WIDTH##_8_lsx(uint8_t *dst,         \
+                                                       ptrdiff_t dst_stride, \
+                                                       uint8_t *src,         \
+                                                       ptrdiff_t src_stride, \
+                                                       int height,           \
+                                                       intptr_t mx,          \
+                                                       intptr_t my,          \
+                                                       int width)
+
+UNI_MC(qpel, h, 64);
+
+UNI_MC(qpel, v, 24);
+UNI_MC(qpel, v, 32);
+UNI_MC(qpel, v, 48);
+UNI_MC(qpel, v, 64);
+
+UNI_MC(qpel, hv, 8);
+UNI_MC(qpel, hv, 16);
+UNI_MC(qpel, hv, 24);
+UNI_MC(qpel, hv, 32);
+UNI_MC(qpel, hv, 48);
+UNI_MC(qpel, hv, 64);
+
+UNI_MC(epel, v, 24);
+UNI_MC(epel, v, 32);
+
+UNI_MC(epel, hv, 8);
+UNI_MC(epel, hv, 12);
+UNI_MC(epel, hv, 16);
+UNI_MC(epel, hv, 24);
+UNI_MC(epel, hv, 32);
+
+#undef UNI_MC
+
+#define UNI_W_MC(PEL, DIR, WIDTH)                                       \
+void ff_hevc_put_hevc_uni_w_##PEL##_##DIR##WIDTH##_8_lsx(uint8_t *dst,  \
+                                                         ptrdiff_t      \
+                                                         dst_stride,    \
+                                                         uint8_t *src,  \
+                                                         ptrdiff_t      \
+                                                         src_stride,    \
+                                                         int height,    \
+                                                         int denom,     \
+                                                         int weight,    \
+                                                         int offset,    \
+                                                         intptr_t mx,   \
+                                                         intptr_t my,   \
+                                                         int width)
+
+UNI_W_MC(qpel, hv, 8);
+UNI_W_MC(qpel, hv, 16);
+UNI_W_MC(qpel, hv, 24);
+UNI_W_MC(qpel, hv, 32);
+UNI_W_MC(qpel, hv, 48);
+UNI_W_MC(qpel, hv, 64);
+
+#undef UNI_W_MC
+
+void ff_hevc_loop_filter_luma_h_8_lsx(uint8_t *src, ptrdiff_t stride,
+                                      int32_t beta, int32_t *tc,
+                                      uint8_t *p_is_pcm, uint8_t *q_is_pcm);
+
+void ff_hevc_loop_filter_luma_v_8_lsx(uint8_t *src, ptrdiff_t stride,
+                                      int32_t beta, int32_t *tc,
+                                      uint8_t *p_is_pcm, uint8_t *q_is_pcm);
+
+void ff_hevc_loop_filter_chroma_h_8_lsx(uint8_t *src, ptrdiff_t stride,
+                                        int32_t *tc, uint8_t *p_is_pcm,
+                                        uint8_t *q_is_pcm);
+
+void ff_hevc_loop_filter_chroma_v_8_lsx(uint8_t *src, ptrdiff_t stride,
+                                        int32_t *tc, uint8_t *p_is_pcm,
+                                        uint8_t *q_is_pcm);
+
+void ff_hevc_sao_edge_filter_8_lsx(uint8_t *dst, uint8_t *src,
+                                   ptrdiff_t stride_dst,
+                                   int16_t *sao_offset_val,
+                                   int eo, int width, int height);
+
+void ff_hevc_idct_4x4_lsx(int16_t *coeffs, int col_limit);
+void ff_hevc_idct_8x8_lsx(int16_t *coeffs, int col_limit);
+void ff_hevc_idct_16x16_lsx(int16_t *coeffs, int col_limit);
+void ff_hevc_idct_32x32_lsx(int16_t *coeffs, int col_limit);
+
+#endif  // #ifndef AVCODEC_LOONGARCH_HEVCDSP_LSX_H
diff --git a/libavcodec/loongarch/hpeldsp_init_loongarch.c b/libavcodec/loongarch/hpeldsp_init_loongarch.c
new file mode 100644
index 0000000000..1690be5438
--- /dev/null
+++ b/libavcodec/loongarch/hpeldsp_init_loongarch.c
@@ -0,0 +1,50 @@
+/*
+ * Copyright (c) 2021 Loongson Technology Corporation Limited
+ * Contributed by Shiyou Yin <yinshiyou-hf@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "libavutil/loongarch/cpu.h"
+#include "libavcodec/hpeldsp.h"
+#include "libavcodec/loongarch/hpeldsp_lasx.h"
+
+void ff_hpeldsp_init_loongarch(HpelDSPContext *c, int flags)
+{
+    int cpu_flags = av_get_cpu_flags();
+
+    if (have_lasx(cpu_flags)) {
+        c->put_pixels_tab[0][0] = ff_put_pixels16_8_lsx;
+        c->put_pixels_tab[0][1] = ff_put_pixels16_x2_8_lasx;
+        c->put_pixels_tab[0][2] = ff_put_pixels16_y2_8_lasx;
+        c->put_pixels_tab[0][3] = ff_put_pixels16_xy2_8_lasx;
+
+        c->put_pixels_tab[1][0] = ff_put_pixels8_8_lasx;
+        c->put_pixels_tab[1][1] = ff_put_pixels8_x2_8_lasx;
+        c->put_pixels_tab[1][2] = ff_put_pixels8_y2_8_lasx;
+        c->put_pixels_tab[1][3] = ff_put_pixels8_xy2_8_lasx;
+        c->put_no_rnd_pixels_tab[0][0] = ff_put_pixels16_8_lsx;
+        c->put_no_rnd_pixels_tab[0][1] = ff_put_no_rnd_pixels16_x2_8_lasx;
+        c->put_no_rnd_pixels_tab[0][2] = ff_put_no_rnd_pixels16_y2_8_lasx;
+        c->put_no_rnd_pixels_tab[0][3] = ff_put_no_rnd_pixels16_xy2_8_lasx;
+
+        c->put_no_rnd_pixels_tab[1][0] = ff_put_pixels8_8_lasx;
+        c->put_no_rnd_pixels_tab[1][1] = ff_put_no_rnd_pixels8_x2_8_lasx;
+        c->put_no_rnd_pixels_tab[1][2] = ff_put_no_rnd_pixels8_y2_8_lasx;
+        c->put_no_rnd_pixels_tab[1][3] = ff_put_no_rnd_pixels8_xy2_8_lasx;
+    }
+}
diff --git a/libavcodec/loongarch/hpeldsp_lasx.c b/libavcodec/loongarch/hpeldsp_lasx.c
new file mode 100644
index 0000000000..dd2ae173da
--- /dev/null
+++ b/libavcodec/loongarch/hpeldsp_lasx.c
@@ -0,0 +1,1287 @@
+/*
+ * Copyright (c) 2021 Loongson Technology Corporation Limited
+ * Contributed by Shiyou Yin <yinshiyou-hf@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "libavutil/loongarch/loongson_intrinsics.h"
+#include "hpeldsp_lasx.h"
+
+static av_always_inline void
+put_pixels8_l2_8_lsx(uint8_t *dst, const uint8_t *src1, const uint8_t *src2,
+                     int dst_stride, int src_stride1, int src_stride2, int h)
+{
+    int stride1_2, stride1_3, stride1_4;
+    int stride2_2, stride2_3, stride2_4;
+    __asm__ volatile (
+        "slli.d   %[stride1_2],  %[srcStride1],     1             \n\t"
+        "slli.d   %[stride2_2],  %[srcStride2],     1             \n\t"
+        "add.d    %[stride1_3],  %[stride1_2],      %[srcStride1] \n\t"
+        "add.d    %[stride2_3],  %[stride2_2],      %[srcStride2] \n\t"
+        "slli.d   %[stride1_4],  %[stride1_2],      1             \n\t"
+        "slli.d   %[stride2_4],  %[stride2_2],      1             \n\t"
+        "1:                                                       \n\t"
+        "vld      $vr0,          %[src1],           0             \n\t"
+        "vldx     $vr1,          %[src1],           %[srcStride1] \n\t"
+        "vldx     $vr2,          %[src1],           %[stride1_2]  \n\t"
+        "vldx     $vr3,          %[src1],           %[stride1_3]  \n\t"
+        "add.d    %[src1],       %[src1],           %[stride1_4]  \n\t"
+
+        "vld      $vr4,          %[src2],           0             \n\t"
+        "vldx     $vr5,          %[src2],           %[srcStride2] \n\t"
+        "vldx     $vr6,          %[src2],           %[stride2_2]  \n\t"
+        "vldx     $vr7,          %[src2],           %[stride2_3]  \n\t"
+        "add.d    %[src2],       %[src2],           %[stride2_4]  \n\t"
+
+        "addi.d   %[h],          %[h],              -4            \n\t"
+
+        "vavgr.bu $vr0,          $vr4,              $vr0          \n\t"
+        "vavgr.bu $vr1,          $vr5,              $vr1          \n\t"
+        "vavgr.bu $vr2,          $vr6,              $vr2          \n\t"
+        "vavgr.bu $vr3,          $vr7,              $vr3          \n\t"
+        "vstelm.d $vr0,          %[dst],            0,  0         \n\t"
+        "add.d    %[dst],        %[dst],            %[dstStride]  \n\t"
+        "vstelm.d $vr1,          %[dst],            0,  0         \n\t"
+        "add.d    %[dst],        %[dst],            %[dstStride]  \n\t"
+        "vstelm.d $vr2,          %[dst],            0,  0         \n\t"
+        "add.d    %[dst],        %[dst],            %[dstStride]  \n\t"
+        "vstelm.d $vr3,          %[dst],            0,  0         \n\t"
+        "add.d    %[dst],        %[dst],            %[dstStride]  \n\t"
+        "bnez     %[h],                             1b            \n\t"
+
+        : [dst]"+&r"(dst), [src2]"+&r"(src2), [src1]"+&r"(src1),
+          [h]"+&r"(h), [stride1_2]"=&r"(stride1_2),
+          [stride1_3]"=&r"(stride1_3), [stride1_4]"=&r"(stride1_4),
+          [stride2_2]"=&r"(stride2_2), [stride2_3]"=&r"(stride2_3),
+          [stride2_4]"=&r"(stride2_4)
+        : [dstStride]"r"(dst_stride), [srcStride1]"r"(src_stride1),
+          [srcStride2]"r"(src_stride2)
+        : "memory"
+    );
+}
+
+static av_always_inline void
+put_pixels16_l2_8_lsx(uint8_t *dst, const uint8_t *src1, const uint8_t *src2,
+                      int dst_stride, int src_stride1, int src_stride2, int h)
+{
+    int stride1_2, stride1_3, stride1_4;
+    int stride2_2, stride2_3, stride2_4;
+    int dststride2, dststride3, dststride4;
+    __asm__ volatile (
+        "slli.d   %[stride1_2],  %[srcStride1],     1             \n\t"
+        "slli.d   %[stride2_2],  %[srcStride2],     1             \n\t"
+        "slli.d   %[dststride2], %[dstStride],      1             \n\t"
+        "add.d    %[stride1_3],  %[stride1_2],      %[srcStride1] \n\t"
+        "add.d    %[stride2_3],  %[stride2_2],      %[srcStride2] \n\t"
+        "add.d    %[dststride3], %[dststride2],     %[dstStride]  \n\t"
+        "slli.d   %[stride1_4],  %[stride1_2],      1             \n\t"
+        "slli.d   %[stride2_4],  %[stride2_2],      1             \n\t"
+        "slli.d   %[dststride4], %[dststride2],     1             \n\t"
+        "1:                                                       \n\t"
+        "vld      $vr0,          %[src1],           0             \n\t"
+        "vldx     $vr1,          %[src1],           %[srcStride1] \n\t"
+        "vldx     $vr2,          %[src1],           %[stride1_2]  \n\t"
+        "vldx     $vr3,          %[src1],           %[stride1_3]  \n\t"
+        "add.d    %[src1],       %[src1],           %[stride1_4]  \n\t"
+
+        "vld      $vr4,          %[src2],           0             \n\t"
+        "vldx     $vr5,          %[src2],           %[srcStride2] \n\t"
+        "vldx     $vr6,          %[src2],           %[stride2_2]  \n\t"
+        "vldx     $vr7,          %[src2],           %[stride2_3]  \n\t"
+        "add.d    %[src2],       %[src2],           %[stride2_4]  \n\t"
+
+        "addi.d   %[h],          %[h],              -4            \n\t"
+
+        "vavgr.bu $vr0,          $vr4,              $vr0          \n\t"
+        "vavgr.bu $vr1,          $vr5,              $vr1          \n\t"
+        "vavgr.bu $vr2,          $vr6,              $vr2          \n\t"
+        "vavgr.bu $vr3,          $vr7,              $vr3          \n\t"
+        "vst      $vr0,          %[dst],            0             \n\t"
+        "vstx     $vr1,          %[dst],            %[dstStride]  \n\t"
+        "vstx     $vr2,          %[dst],            %[dststride2] \n\t"
+        "vstx     $vr3,          %[dst],            %[dststride3] \n\t"
+        "add.d    %[dst],        %[dst],            %[dststride4] \n\t"
+        "bnez     %[h],                             1b            \n\t"
+
+        : [dst]"+&r"(dst), [src2]"+&r"(src2), [src1]"+&r"(src1),
+          [h]"+&r"(h), [stride1_2]"=&r"(stride1_2),
+          [stride1_3]"=&r"(stride1_3), [stride1_4]"=&r"(stride1_4),
+          [stride2_2]"=&r"(stride2_2), [stride2_3]"=&r"(stride2_3),
+          [stride2_4]"=&r"(stride2_4), [dststride2]"=&r"(dststride2),
+          [dststride3]"=&r"(dststride3), [dststride4]"=&r"(dststride4)
+        : [dstStride]"r"(dst_stride), [srcStride1]"r"(src_stride1),
+          [srcStride2]"r"(src_stride2)
+        : "memory"
+    );
+}
+
+void ff_put_pixels8_8_lasx(uint8_t *block, const uint8_t *pixels,
+                           ptrdiff_t line_size, int h)
+{
+    uint64_t tmp[8];
+    int h_8 = h >> 3;
+    int res = h & 7;
+    ptrdiff_t stride2, stride3, stride4;
+
+    __asm__ volatile (
+        "beqz     %[h_8],                           2f            \n\t"
+        "slli.d   %[stride2],    %[stride],         1             \n\t"
+        "add.d    %[stride3],    %[stride2],        %[stride]     \n\t"
+        "slli.d   %[stride4],    %[stride2],        1             \n\t"
+        "1:                                                       \n\t"
+        "ld.d     %[tmp0],       %[src],            0x0           \n\t"
+        "ldx.d    %[tmp1],       %[src],            %[stride]     \n\t"
+        "ldx.d    %[tmp2],       %[src],            %[stride2]    \n\t"
+        "ldx.d    %[tmp3],       %[src],            %[stride3]    \n\t"
+        "add.d    %[src],        %[src],            %[stride4]    \n\t"
+        "ld.d     %[tmp4],       %[src],            0x0           \n\t"
+        "ldx.d    %[tmp5],       %[src],            %[stride]     \n\t"
+        "ldx.d    %[tmp6],       %[src],            %[stride2]    \n\t"
+        "ldx.d    %[tmp7],       %[src],            %[stride3]    \n\t"
+        "add.d    %[src],        %[src],            %[stride4]    \n\t"
+
+        "addi.d   %[h_8],        %[h_8],            -1            \n\t"
+
+        "st.d     %[tmp0],       %[dst],            0x0           \n\t"
+        "stx.d    %[tmp1],       %[dst],            %[stride]     \n\t"
+        "stx.d    %[tmp2],       %[dst],            %[stride2]    \n\t"
+        "stx.d    %[tmp3],       %[dst],            %[stride3]    \n\t"
+        "add.d    %[dst],        %[dst],            %[stride4]    \n\t"
+        "st.d     %[tmp4],       %[dst],            0x0           \n\t"
+        "stx.d    %[tmp5],       %[dst],            %[stride]     \n\t"
+        "stx.d    %[tmp6],       %[dst],            %[stride2]    \n\t"
+        "stx.d    %[tmp7],       %[dst],            %[stride3]    \n\t"
+        "add.d    %[dst],        %[dst],            %[stride4]    \n\t"
+        "bnez     %[h_8],        1b                               \n\t"
+
+        "2:                                                       \n\t"
+        "beqz     %[res],        4f                               \n\t"
+        "3:                                                       \n\t"
+        "ld.d     %[tmp0],       %[src],            0x0           \n\t"
+        "add.d    %[src],        %[src],            %[stride]     \n\t"
+        "addi.d   %[res],        %[res],            -1            \n\t"
+        "st.d     %[tmp0],       %[dst],            0x0           \n\t"
+        "add.d    %[dst],        %[dst],            %[stride]     \n\t"
+        "bnez     %[res],        3b                               \n\t"
+        "4:                                                       \n\t"
+        : [tmp0]"=&r"(tmp[0]),        [tmp1]"=&r"(tmp[1]),
+          [tmp2]"=&r"(tmp[2]),        [tmp3]"=&r"(tmp[3]),
+          [tmp4]"=&r"(tmp[4]),        [tmp5]"=&r"(tmp[5]),
+          [tmp6]"=&r"(tmp[6]),        [tmp7]"=&r"(tmp[7]),
+          [dst]"+&r"(block),          [src]"+&r"(pixels),
+          [h_8]"+&r"(h_8),            [res]"+&r"(res),
+          [stride2]"=&r"(stride2),    [stride3]"=&r"(stride3),
+          [stride4]"=&r"(stride4)
+        : [stride]"r"(line_size)
+        : "memory"
+    );
+}
+
+void ff_put_pixels16_8_lsx(uint8_t *block, const uint8_t *pixels,
+                           ptrdiff_t line_size, int h)
+{
+    int h_8 = h >> 3;
+    int res = h & 7;
+    ptrdiff_t stride2, stride3, stride4;
+
+    __asm__ volatile (
+        "beqz     %[h_8],                           2f            \n\t"
+        "slli.d   %[stride2],    %[stride],         1             \n\t"
+        "add.d    %[stride3],    %[stride2],        %[stride]     \n\t"
+        "slli.d   %[stride4],    %[stride2],        1             \n\t"
+        "1:                                                       \n\t"
+        "vld      $vr0,          %[src],            0x0           \n\t"
+        "vldx     $vr1,          %[src],            %[stride]     \n\t"
+        "vldx     $vr2,          %[src],            %[stride2]    \n\t"
+        "vldx     $vr3,          %[src],            %[stride3]    \n\t"
+        "add.d    %[src],        %[src],            %[stride4]    \n\t"
+        "vld      $vr4,          %[src],            0x0           \n\t"
+        "vldx     $vr5,          %[src],            %[stride]     \n\t"
+        "vldx     $vr6,          %[src],            %[stride2]    \n\t"
+        "vldx     $vr7,          %[src],            %[stride3]    \n\t"
+        "add.d    %[src],        %[src],            %[stride4]    \n\t"
+
+        "addi.d   %[h_8],        %[h_8],            -1            \n\t"
+
+        "vst      $vr0,          %[dst],            0x0           \n\t"
+        "vstx     $vr1,          %[dst],            %[stride]     \n\t"
+        "vstx     $vr2,          %[dst],            %[stride2]    \n\t"
+        "vstx     $vr3,          %[dst],            %[stride3]    \n\t"
+        "add.d    %[dst],        %[dst],            %[stride4]    \n\t"
+        "vst      $vr4,          %[dst],            0x0           \n\t"
+        "vstx     $vr5,          %[dst],            %[stride]     \n\t"
+        "vstx     $vr6,          %[dst],            %[stride2]    \n\t"
+        "vstx     $vr7,          %[dst],            %[stride3]    \n\t"
+        "add.d    %[dst],        %[dst],            %[stride4]    \n\t"
+        "bnez     %[h_8],        1b                               \n\t"
+
+        "2:                                                       \n\t"
+        "beqz     %[res],        4f                               \n\t"
+        "3:                                                       \n\t"
+        "vld      $vr0,          %[src],            0x0           \n\t"
+        "add.d    %[src],        %[src],            %[stride]     \n\t"
+        "addi.d   %[res],        %[res],            -1            \n\t"
+        "vst      $vr0,          %[dst],            0x0           \n\t"
+        "add.d    %[dst],        %[dst],            %[stride]     \n\t"
+        "bnez     %[res],        3b                               \n\t"
+        "4:                                                       \n\t"
+        : [dst]"+&r"(block),          [src]"+&r"(pixels),
+          [h_8]"+&r"(h_8),            [res]"+&r"(res),
+          [stride2]"=&r"(stride2),    [stride3]"=&r"(stride3),
+          [stride4]"=&r"(stride4)
+        : [stride]"r"(line_size)
+        : "memory"
+    );
+}
+
+void ff_put_pixels8_x2_8_lasx(uint8_t *block, const uint8_t *pixels,
+                              ptrdiff_t line_size, int h)
+{
+    put_pixels8_l2_8_lsx(block, pixels, pixels + 1, line_size, line_size,
+                         line_size, h);
+}
+
+void ff_put_pixels8_y2_8_lasx(uint8_t *block, const uint8_t *pixels,
+                              ptrdiff_t line_size, int h)
+{
+    put_pixels8_l2_8_lsx(block, pixels, pixels + line_size, line_size,
+                         line_size, line_size, h);
+}
+
+void ff_put_pixels16_x2_8_lasx(uint8_t *block, const uint8_t *pixels,
+                               ptrdiff_t line_size, int h)
+{
+    put_pixels16_l2_8_lsx(block, pixels, pixels + 1, line_size, line_size,
+                          line_size, h);
+}
+
+void ff_put_pixels16_y2_8_lasx(uint8_t *block, const uint8_t *pixels,
+                               ptrdiff_t line_size, int h)
+{
+    put_pixels16_l2_8_lsx(block, pixels, pixels + line_size, line_size,
+                          line_size, line_size, h);
+}
+
+static void common_hz_bil_no_rnd_16x16_lasx(const uint8_t *src,
+                                            int32_t src_stride,
+                                            uint8_t *dst, int32_t dst_stride)
+{
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    int32_t src_stride_2x = src_stride << 1;
+    int32_t src_stride_4x = src_stride << 2;
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    uint8_t *_src = (uint8_t*)src;
+
+    src0 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src1, src2);
+    src3 = __lasx_xvldx(_src, src_stride_3x);
+    _src += 1;
+    src4 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src5, src6);
+    src7 = __lasx_xvldx(_src, src_stride_3x);
+    _src += (src_stride_4x -1);
+    DUP4_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src3, src2, 0x20, src5,
+              src4, 0x20, src7, src6, 0x20, src0, src1, src2, src3);
+    src0 = __lasx_xvavg_bu(src0, src2);
+    src1 = __lasx_xvavg_bu(src1, src3);
+    __lasx_xvstelm_d(src0, dst, 0, 0);
+    __lasx_xvstelm_d(src0, dst, 8, 1);
+    dst += dst_stride;
+    __lasx_xvstelm_d(src0, dst, 0, 2);
+    __lasx_xvstelm_d(src0, dst, 8, 3);
+    dst += dst_stride;
+    __lasx_xvstelm_d(src1, dst, 0, 0);
+    __lasx_xvstelm_d(src1, dst, 8, 1);
+    dst += dst_stride;
+    __lasx_xvstelm_d(src1, dst, 0, 2);
+    __lasx_xvstelm_d(src1, dst, 8, 3);
+    dst += dst_stride;
+
+    src0 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src1, src2);
+    src3 = __lasx_xvldx(_src, src_stride_3x);
+    _src += 1;
+    src4 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src5, src6);
+    src7 = __lasx_xvldx(_src, src_stride_3x);
+    _src += (src_stride_4x - 1);
+    DUP4_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src3, src2, 0x20, src5, src4,
+              0x20, src7, src6, 0x20, src0, src1, src2, src3);
+    src0 = __lasx_xvavg_bu(src0, src2);
+    src1 = __lasx_xvavg_bu(src1, src3);
+    __lasx_xvstelm_d(src0, dst, 0, 0);
+    __lasx_xvstelm_d(src0, dst, 8, 1);
+    dst += dst_stride;
+    __lasx_xvstelm_d(src0, dst, 0, 2);
+    __lasx_xvstelm_d(src0, dst, 8, 3);
+    dst += dst_stride;
+    __lasx_xvstelm_d(src1, dst, 0, 0);
+    __lasx_xvstelm_d(src1, dst, 8, 1);
+    dst += dst_stride;
+    __lasx_xvstelm_d(src1, dst, 0, 2);
+    __lasx_xvstelm_d(src1, dst, 8, 3);
+    dst += dst_stride;
+
+    src0 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src1, src2);
+    src3 = __lasx_xvldx(_src, src_stride_3x);
+    _src += 1;
+    src4 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src5, src6);
+    src7 = __lasx_xvldx(_src, src_stride_3x);
+    _src += (src_stride_4x - 1);
+    DUP4_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src3, src2, 0x20, src5, src4,
+              0x20, src7, src6, 0x20, src0, src1, src2, src3);
+    src0 = __lasx_xvavg_bu(src0, src2);
+    src1 = __lasx_xvavg_bu(src1, src3);
+    __lasx_xvstelm_d(src0, dst, 0, 0);
+    __lasx_xvstelm_d(src0, dst, 8, 1);
+    dst += dst_stride;
+    __lasx_xvstelm_d(src0, dst, 0, 2);
+    __lasx_xvstelm_d(src0, dst, 8, 3);
+    dst += dst_stride;
+    __lasx_xvstelm_d(src1, dst, 0, 0);
+    __lasx_xvstelm_d(src1, dst, 8, 1);
+    dst += dst_stride;
+    __lasx_xvstelm_d(src1, dst, 0, 2);
+    __lasx_xvstelm_d(src1, dst, 8, 3);
+    dst += dst_stride;
+
+    src0 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src1, src2);
+    src3 = __lasx_xvldx(_src, src_stride_3x);
+    _src += 1;
+    src4 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src5, src6);
+    src7 = __lasx_xvldx(_src, src_stride_3x);
+    DUP4_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src3, src2, 0x20, src5, src4,
+              0x20, src7, src6, 0x20, src0, src1, src2, src3);
+    src0 = __lasx_xvavg_bu(src0, src2);
+    src1 = __lasx_xvavg_bu(src1, src3);
+    __lasx_xvstelm_d(src0, dst, 0, 0);
+    __lasx_xvstelm_d(src0, dst, 8, 1);
+    dst += dst_stride;
+    __lasx_xvstelm_d(src0, dst, 0, 2);
+    __lasx_xvstelm_d(src0, dst, 8, 3);
+    dst += dst_stride;
+    __lasx_xvstelm_d(src1, dst, 0, 0);
+    __lasx_xvstelm_d(src1, dst, 8, 1);
+    dst += dst_stride;
+    __lasx_xvstelm_d(src1, dst, 0, 2);
+    __lasx_xvstelm_d(src1, dst, 8, 3);
+}
+
+static void common_hz_bil_no_rnd_8x16_lasx(const uint8_t *src,
+                                           int32_t src_stride,
+                                           uint8_t *dst, int32_t dst_stride)
+{
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    int32_t src_stride_2x = src_stride << 1;
+    int32_t src_stride_4x = src_stride << 2;
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    uint8_t* _src = (uint8_t*)src;
+
+    src0 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src1, src2);
+    src3 = __lasx_xvldx(_src, src_stride_3x);
+    _src += 1;
+    src4 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src5, src6);
+    src7 = __lasx_xvldx(_src, src_stride_3x);
+    _src += (src_stride_4x - 1);
+    DUP4_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src3, src2, 0x20, src5, src4,
+              0x20, src7, src6, 0x20, src0, src1, src2, src3);
+    src0 = __lasx_xvavg_bu(src0, src2);
+    src1 = __lasx_xvavg_bu(src1, src3);
+    __lasx_xvstelm_d(src0, dst, 0, 0);
+    __lasx_xvstelm_d(src0, dst, 8, 1);
+    dst += dst_stride;
+    __lasx_xvstelm_d(src0, dst, 0, 2);
+    __lasx_xvstelm_d(src0, dst, 8, 3);
+    dst += dst_stride;
+    __lasx_xvstelm_d(src1, dst, 0, 0);
+    __lasx_xvstelm_d(src1, dst, 8, 1);
+    dst += dst_stride;
+    __lasx_xvstelm_d(src1, dst, 0, 2);
+    __lasx_xvstelm_d(src1, dst, 8, 3);
+    dst += dst_stride;
+
+    src0 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src1, src2);
+    src3 = __lasx_xvldx(_src, src_stride_3x);
+    _src += 1;
+    src4 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src5, src6);
+    src7 = __lasx_xvldx(_src, src_stride_3x);
+    DUP4_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src3, src2, 0x20, src5, src4,
+              0x20, src7, src6, 0x20, src0, src1, src2, src3);
+    src0 = __lasx_xvavg_bu(src0, src2);
+    src1 = __lasx_xvavg_bu(src1, src3);
+    __lasx_xvstelm_d(src0, dst, 0, 0);
+    __lasx_xvstelm_d(src0, dst, 8, 1);
+    dst += dst_stride;
+    __lasx_xvstelm_d(src0, dst, 0, 2);
+    __lasx_xvstelm_d(src0, dst, 8, 3);
+    dst += dst_stride;
+    __lasx_xvstelm_d(src1, dst, 0, 0);
+    __lasx_xvstelm_d(src1, dst, 8, 1);
+    dst += dst_stride;
+    __lasx_xvstelm_d(src1, dst, 0, 2);
+    __lasx_xvstelm_d(src1, dst, 8, 3);
+}
+
+void ff_put_no_rnd_pixels16_x2_8_lasx(uint8_t *block, const uint8_t *pixels,
+                                      ptrdiff_t line_size, int h)
+{
+    if (h == 16) {
+        common_hz_bil_no_rnd_16x16_lasx(pixels, line_size, block, line_size);
+    } else if (h == 8) {
+        common_hz_bil_no_rnd_8x16_lasx(pixels, line_size, block, line_size);
+    }
+}
+
+static void common_vt_bil_no_rnd_16x16_lasx(const uint8_t *src,
+                                            int32_t src_stride,
+                                            uint8_t *dst, int32_t dst_stride)
+{
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7, src8;
+    __m256i src9, src10, src11, src12, src13, src14, src15, src16;
+    int32_t src_stride_2x = src_stride << 1;
+    int32_t src_stride_4x = src_stride << 2;
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    uint8_t* _src = (uint8_t*)src;
+
+    src0 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src1, src2);
+    src3 = __lasx_xvldx(_src, src_stride_3x);
+    _src += src_stride_4x;
+    src4 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src5, src6);
+    src7 = __lasx_xvldx(_src, src_stride_3x);
+    _src += src_stride_4x;
+    src8 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src9, src10);
+    src11 = __lasx_xvldx(_src, src_stride_3x);
+    _src += src_stride_4x;
+    src12 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x,
+              src13, src14);
+    src15 = __lasx_xvldx(_src, src_stride_3x);
+    _src += src_stride_4x;
+    src16 = __lasx_xvld(_src, 0);
+
+    DUP4_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src2, src1, 0x20, src3, src2,
+              0x20, src4, src3, 0x20, src0, src1, src2, src3);
+    DUP4_ARG3(__lasx_xvpermi_q, src5, src4, 0x20, src6, src5, 0x20, src7, src6,
+              0x20, src8, src7, 0x20, src4, src5, src6, src7);
+    DUP4_ARG3(__lasx_xvpermi_q, src9, src8, 0x20, src10, src9, 0x20, src11,
+              src10, 0x20, src12, src11, 0x20, src8, src9, src10, src11);
+    DUP4_ARG3(__lasx_xvpermi_q, src13, src12, 0x20, src14, src13, 0x20, src15,
+              src14, 0x20, src16, src15, 0x20, src12, src13, src14, src15);
+    DUP4_ARG2(__lasx_xvavg_bu, src0, src1, src2, src3, src4, src5, src6, src7,
+              src0, src2, src4, src6);
+    DUP4_ARG2(__lasx_xvavg_bu, src8, src9, src10, src11, src12, src13, src14,
+              src15, src8, src10, src12, src14);
+
+    __lasx_xvstelm_d(src0, dst, 0, 0);
+    __lasx_xvstelm_d(src0, dst, 8, 1);
+    dst += dst_stride;
+    __lasx_xvstelm_d(src0, dst, 0, 2);
+    __lasx_xvstelm_d(src0, dst, 8, 3);
+    dst += dst_stride;
+    __lasx_xvstelm_d(src2, dst, 0, 0);
+    __lasx_xvstelm_d(src2, dst, 8, 1);
+    dst += dst_stride;
+    __lasx_xvstelm_d(src2, dst, 0, 2);
+    __lasx_xvstelm_d(src2, dst, 8, 3);
+    dst += dst_stride;
+    __lasx_xvstelm_d(src4, dst, 0, 0);
+    __lasx_xvstelm_d(src4, dst, 8, 1);
+    dst += dst_stride;
+    __lasx_xvstelm_d(src4, dst, 0, 2);
+    __lasx_xvstelm_d(src4, dst, 8, 3);
+    dst += dst_stride;
+    __lasx_xvstelm_d(src6, dst, 0, 0);
+    __lasx_xvstelm_d(src6, dst, 8, 1);
+    dst += dst_stride;
+    __lasx_xvstelm_d(src6, dst, 0, 2);
+    __lasx_xvstelm_d(src6, dst, 8, 3);
+    dst += dst_stride;
+    __lasx_xvstelm_d(src8, dst, 0, 0);
+    __lasx_xvstelm_d(src8, dst, 8, 1);
+    dst += dst_stride;
+    __lasx_xvstelm_d(src8, dst, 0, 2);
+    __lasx_xvstelm_d(src8, dst, 8, 3);
+    dst += dst_stride;
+    __lasx_xvstelm_d(src10, dst, 0, 0);
+    __lasx_xvstelm_d(src10, dst, 8, 1);
+    dst += dst_stride;
+    __lasx_xvstelm_d(src10, dst, 0, 2);
+    __lasx_xvstelm_d(src10, dst, 8, 3);
+    dst += dst_stride;
+    __lasx_xvstelm_d(src12, dst, 0, 0);
+    __lasx_xvstelm_d(src12, dst, 8, 1);
+    dst += dst_stride;
+    __lasx_xvstelm_d(src12, dst, 0, 2);
+    __lasx_xvstelm_d(src12, dst, 8, 3);
+    dst += dst_stride;
+    __lasx_xvstelm_d(src14, dst, 0, 0);
+    __lasx_xvstelm_d(src14, dst, 8, 1);
+    dst += dst_stride;
+    __lasx_xvstelm_d(src14, dst, 0, 2);
+    __lasx_xvstelm_d(src14, dst, 8, 3);
+}
+
+static void common_vt_bil_no_rnd_8x16_lasx(const uint8_t *src,
+                                           int32_t src_stride,
+                                           uint8_t *dst, int32_t dst_stride)
+{
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7, src8;
+    int32_t src_stride_2x = src_stride << 1;
+    int32_t src_stride_4x = src_stride << 2;
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    uint8_t* _src = (uint8_t*)src;
+
+    src0 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src1, src2);
+    src3 = __lasx_xvldx(_src, src_stride_3x);
+    _src += src_stride_4x;
+    src4 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src5, src6);
+    src7 = __lasx_xvldx(_src, src_stride_3x);
+    _src += src_stride_4x;
+    src8 = __lasx_xvld(_src, 0);
+
+    DUP4_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src2, src1, 0x20, src3, src2,
+              0x20, src4, src3, 0x20, src0, src1, src2, src3);
+    DUP4_ARG3(__lasx_xvpermi_q, src5, src4, 0x20, src6, src5, 0x20, src7, src6,
+              0x20, src8, src7, 0x20, src4, src5, src6, src7);
+    DUP4_ARG2(__lasx_xvavg_bu, src0, src1, src2, src3, src4, src5, src6, src7,
+              src0, src2, src4, src6);
+
+    __lasx_xvstelm_d(src0, dst, 0, 0);
+    __lasx_xvstelm_d(src0, dst, 8, 1);
+    dst += dst_stride;
+    __lasx_xvstelm_d(src0, dst, 0, 2);
+    __lasx_xvstelm_d(src0, dst, 8, 3);
+    dst += dst_stride;
+    __lasx_xvstelm_d(src2, dst, 0, 0);
+    __lasx_xvstelm_d(src2, dst, 8, 1);
+    dst += dst_stride;
+    __lasx_xvstelm_d(src2, dst, 0, 2);
+    __lasx_xvstelm_d(src2, dst, 8, 3);
+    dst += dst_stride;
+    __lasx_xvstelm_d(src4, dst, 0, 0);
+    __lasx_xvstelm_d(src4, dst, 8, 1);
+    dst += dst_stride;
+    __lasx_xvstelm_d(src4, dst, 0, 2);
+    __lasx_xvstelm_d(src4, dst, 8, 3);
+    dst += dst_stride;
+    __lasx_xvstelm_d(src6, dst, 0, 0);
+    __lasx_xvstelm_d(src6, dst, 8, 1);
+    dst += dst_stride;
+    __lasx_xvstelm_d(src6, dst, 0, 2);
+    __lasx_xvstelm_d(src6, dst, 8, 3);
+}
+
+void ff_put_no_rnd_pixels16_y2_8_lasx(uint8_t *block, const uint8_t *pixels,
+                                      ptrdiff_t line_size, int h)
+{
+    if (h == 16) {
+        common_vt_bil_no_rnd_16x16_lasx(pixels, line_size, block, line_size);
+    } else if (h == 8) {
+        common_vt_bil_no_rnd_8x16_lasx(pixels, line_size, block, line_size);
+    }
+}
+
+static void common_hv_bil_no_rnd_16x16_lasx(const uint8_t *src,
+                                            int32_t src_stride,
+                                            uint8_t *dst, int32_t dst_stride)
+{
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7, src8, src9;
+    __m256i src10, src11, src12, src13, src14, src15, src16, src17;
+    __m256i sum0, sum1, sum2, sum3, sum4, sum5, sum6, sum7;
+    int32_t src_stride_2x = src_stride << 1;
+    int32_t src_stride_4x = src_stride << 2;
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    uint8_t* _src = (uint8_t*)src;
+
+    src0 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src1, src2);
+    src3 = __lasx_xvldx(_src, src_stride_3x);
+    _src += src_stride_4x;
+    src4 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src5, src6);
+    src7 = __lasx_xvldx(_src, src_stride_3x);
+    _src += (1 - src_stride_4x);
+    src9 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x,
+              src10, src11);
+    src12 = __lasx_xvldx(_src, src_stride_3x);
+    _src += src_stride_4x;
+    src13 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x,
+              src14, src15);
+    src16 = __lasx_xvldx(_src, src_stride_3x);
+    _src += (src_stride_4x - 1);
+    DUP2_ARG2(__lasx_xvld, _src, 0, _src, 1, src8, src17);
+
+    DUP4_ARG3(__lasx_xvpermi_q, src0, src4, 0x02, src1, src5, 0x02, src2,
+              src6, 0x02, src3, src7, 0x02, src0, src1, src2, src3);
+    DUP4_ARG3(__lasx_xvpermi_q, src4, src8, 0x02, src9, src13, 0x02, src10,
+              src14, 0x02, src11, src15, 0x02, src4, src5, src6, src7);
+    DUP2_ARG3(__lasx_xvpermi_q, src12, src16, 0x02, src13, src17, 0x02,
+              src8, src9);
+    DUP4_ARG2(__lasx_xvilvl_h, src5, src0, src6, src1, src7, src2, src8, src3,
+              sum0, sum2, sum4, sum6);
+    DUP4_ARG2(__lasx_xvilvh_h, src5, src0, src6, src1, src7, src2, src8, src3,
+              sum1, sum3, sum5, sum7);
+    src8 = __lasx_xvilvl_h(src9, src4);
+    src9 = __lasx_xvilvh_h(src9, src4);
+
+    DUP4_ARG2(__lasx_xvhaddw_hu_bu, sum0, sum0, sum1, sum1, sum2, sum2,
+              sum3, sum3, src0, src1, src2, src3);
+    DUP4_ARG2(__lasx_xvhaddw_hu_bu, sum4, sum4, sum5, sum5, sum6, sum6,
+              sum7, sum7, src4, src5, src6, src7);
+    DUP2_ARG2(__lasx_xvhaddw_hu_bu, src8, src8, src9, src9, src8, src9);
+
+    DUP4_ARG2(__lasx_xvadd_h, src0, src2, src1, src3, src2, src4, src3, src5,
+              sum0, sum1, sum2, sum3);
+    DUP4_ARG2(__lasx_xvadd_h, src4, src6, src5, src7, src6, src8, src7, src9,
+              sum4, sum5, sum6, sum7);
+    DUP4_ARG2(__lasx_xvaddi_hu, sum0, 1, sum1, 1, sum2, 1, sum3, 1,
+              sum0, sum1, sum2, sum3);
+    DUP4_ARG2(__lasx_xvaddi_hu, sum4, 1, sum5, 1, sum6, 1, sum7, 1,
+              sum4, sum5, sum6, sum7);
+    DUP4_ARG3(__lasx_xvsrani_b_h, sum1, sum0, 2, sum3, sum2, 2, sum5, sum4, 2,
+              sum7, sum6, 2, sum0, sum1, sum2, sum3);
+    __lasx_xvstelm_d(sum0, dst, 0, 0);
+    __lasx_xvstelm_d(sum0, dst, 8, 1);
+    dst += dst_stride;
+    __lasx_xvstelm_d(sum1, dst, 0, 0);
+    __lasx_xvstelm_d(sum1, dst, 8, 1);
+    dst += dst_stride;
+    __lasx_xvstelm_d(sum2, dst, 0, 0);
+    __lasx_xvstelm_d(sum2, dst, 8, 1);
+    dst += dst_stride;
+    __lasx_xvstelm_d(sum3, dst, 0, 0);
+    __lasx_xvstelm_d(sum3, dst, 8, 1);
+    dst += dst_stride;
+    __lasx_xvstelm_d(sum0, dst, 0, 2);
+    __lasx_xvstelm_d(sum0, dst, 8, 3);
+    dst += dst_stride;
+    __lasx_xvstelm_d(sum1, dst, 0, 2);
+    __lasx_xvstelm_d(sum1, dst, 8, 3);
+    dst += dst_stride;
+    __lasx_xvstelm_d(sum2, dst, 0, 2);
+    __lasx_xvstelm_d(sum2, dst, 8, 3);
+    dst += dst_stride;
+    __lasx_xvstelm_d(sum3, dst, 0, 2);
+    __lasx_xvstelm_d(sum3, dst, 8, 3);
+    dst += dst_stride;
+
+    src0 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src1, src2);
+    src3 = __lasx_xvldx(_src, src_stride_3x);
+    _src += src_stride_4x;
+    src4 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src5, src6);
+    src7 = __lasx_xvldx(_src, src_stride_3x);
+    _src += (1 - src_stride_4x);
+    src9 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x,
+              src10, src11);
+    src12 = __lasx_xvldx(_src, src_stride_3x);
+    _src += src_stride_4x;
+    src13 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x,
+              src14, src15);
+    src16 = __lasx_xvldx(_src, src_stride_3x);
+    _src += (src_stride_4x - 1);
+    DUP2_ARG2(__lasx_xvld, _src, 0, _src, 1, src8, src17);
+
+    DUP4_ARG3(__lasx_xvpermi_q, src0, src4, 0x02, src1, src5, 0x02, src2, src6, 0x02,
+              src3, src7, 0x02, src0, src1, src2, src3);
+    DUP4_ARG3(__lasx_xvpermi_q, src4, src8, 0x02, src9, src13, 0x02, src10, src14, 0x02,
+              src11, src15, 0x02, src4, src5, src6, src7);
+    DUP2_ARG3(__lasx_xvpermi_q, src12, src16, 0x02, src13, src17, 0x02, src8, src9);
+
+    DUP4_ARG2(__lasx_xvilvl_h, src5, src0, src6, src1, src7, src2, src8, src3,
+              sum0, sum2, sum4, sum6);
+    DUP4_ARG2(__lasx_xvilvh_h, src5, src0, src6, src1, src7, src2, src8, src3,
+              sum1, sum3, sum5, sum7);
+    src8 = __lasx_xvilvl_h(src9, src4);
+    src9 = __lasx_xvilvh_h(src9, src4);
+
+    DUP4_ARG2(__lasx_xvhaddw_hu_bu, sum0, sum0, sum1, sum1, sum2, sum2,
+              sum3, sum3, src0, src1, src2, src3);
+    DUP4_ARG2(__lasx_xvhaddw_hu_bu, sum4, sum4, sum5, sum5, sum6, sum6,
+              sum7, sum7, src4, src5, src6, src7);
+    DUP2_ARG2(__lasx_xvhaddw_hu_bu, src8, src8, src9, src9, src8, src9);
+
+    DUP4_ARG2(__lasx_xvadd_h, src0, src2, src1, src3, src2, src4, src3, src5,
+              sum0, sum1, sum2, sum3);
+    DUP4_ARG2(__lasx_xvadd_h, src4, src6, src5, src7, src6, src8, src7, src9,
+              sum4, sum5, sum6, sum7);
+    DUP4_ARG2(__lasx_xvaddi_hu, sum0, 1, sum1, 1, sum2, 1, sum3, 1,
+              sum0, sum1, sum2, sum3);
+    DUP4_ARG2(__lasx_xvaddi_hu, sum4, 1, sum5, 1, sum6, 1, sum7, 1,
+              sum4, sum5, sum6, sum7);
+    DUP4_ARG3(__lasx_xvsrani_b_h, sum1, sum0, 2, sum3, sum2, 2, sum5, sum4, 2,
+              sum7, sum6, 2, sum0, sum1, sum2, sum3);
+    __lasx_xvstelm_d(sum0, dst, 0, 0);
+    __lasx_xvstelm_d(sum0, dst, 8, 1);
+    dst += dst_stride;
+    __lasx_xvstelm_d(sum1, dst, 0, 0);
+    __lasx_xvstelm_d(sum1, dst, 8, 1);
+    dst += dst_stride;
+    __lasx_xvstelm_d(sum2, dst, 0, 0);
+    __lasx_xvstelm_d(sum2, dst, 8, 1);
+    dst += dst_stride;
+    __lasx_xvstelm_d(sum3, dst, 0, 0);
+    __lasx_xvstelm_d(sum3, dst, 8, 1);
+    dst += dst_stride;
+    __lasx_xvstelm_d(sum0, dst, 0, 2);
+    __lasx_xvstelm_d(sum0, dst, 8, 3);
+    dst += dst_stride;
+    __lasx_xvstelm_d(sum1, dst, 0, 2);
+    __lasx_xvstelm_d(sum1, dst, 8, 3);
+    dst += dst_stride;
+    __lasx_xvstelm_d(sum2, dst, 0, 2);
+    __lasx_xvstelm_d(sum2, dst, 8, 3);
+    dst += dst_stride;
+    __lasx_xvstelm_d(sum3, dst, 0, 2);
+    __lasx_xvstelm_d(sum3, dst, 8, 3);
+}
+
+static void common_hv_bil_no_rnd_8x16_lasx(const uint8_t *src,
+                                           int32_t src_stride,
+                                           uint8_t *dst, int32_t dst_stride)
+{
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7, src8, src9;
+    __m256i src10, src11, src12, src13, src14, src15, src16, src17;
+    __m256i sum0, sum1, sum2, sum3, sum4, sum5, sum6, sum7;
+    int32_t src_stride_2x = src_stride << 1;
+    int32_t src_stride_4x = src_stride << 2;
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    uint8_t* _src = (uint8_t*)src;
+
+    src0 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src1, src2);
+    src3 = __lasx_xvldx(_src, src_stride_3x);
+    _src += src_stride_4x;
+    src4 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src5, src6);
+    src7 = __lasx_xvldx(_src, src_stride_3x);
+    _src += (1 - src_stride_4x);
+    src9 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x,
+              src10, src11);
+    src12 = __lasx_xvldx(_src, src_stride_3x);
+    _src += src_stride_4x;
+    src13 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x,
+              src14, src15);
+    src16 = __lasx_xvldx(_src, src_stride_3x);
+    _src += (src_stride_4x - 1);
+    DUP2_ARG2(__lasx_xvld, _src, 0, _src, 1, src8, src17);
+
+    DUP4_ARG3(__lasx_xvpermi_q, src0, src4, 0x02, src1, src5, 0x02, src2,
+              src6, 0x02, src3, src7, 0x02, src0, src1, src2, src3);
+    DUP4_ARG3(__lasx_xvpermi_q, src4, src8, 0x02, src9, src13, 0x02, src10,
+              src14, 0x02, src11, src15, 0x02, src4, src5, src6, src7);
+    DUP2_ARG3(__lasx_xvpermi_q, src12, src16, 0x02, src13, src17, 0x02, src8, src9);
+
+    DUP4_ARG2(__lasx_xvilvl_h, src5, src0, src6, src1, src7, src2, src8, src3,
+              sum0, sum2, sum4, sum6);
+    DUP4_ARG2(__lasx_xvilvh_h, src5, src0, src6, src1, src7, src2, src8, src3,
+              sum1, sum3, sum5, sum7);
+    src8 = __lasx_xvilvl_h(src9, src4);
+    src9 = __lasx_xvilvh_h(src9, src4);
+
+    DUP4_ARG2(__lasx_xvhaddw_hu_bu, sum0, sum0, sum1, sum1, sum2, sum2,
+              sum3, sum3, src0, src1, src2, src3);
+    DUP4_ARG2(__lasx_xvhaddw_hu_bu, sum4, sum4, sum5, sum5, sum6, sum6,
+              sum7, sum7, src4, src5, src6, src7);
+    DUP2_ARG2(__lasx_xvhaddw_hu_bu, src8, src8, src9, src9, src8, src9);
+
+    DUP4_ARG2(__lasx_xvadd_h, src0, src2, src1, src3, src2, src4, src3, src5,
+              sum0, sum1, sum2, sum3);
+    DUP4_ARG2(__lasx_xvadd_h, src4, src6, src5, src7, src6, src8, src7, src9,
+              sum4, sum5, sum6, sum7);
+    DUP4_ARG2(__lasx_xvaddi_hu, sum0, 1, sum1, 1, sum2, 1, sum3, 1,
+              sum0, sum1, sum2, sum3);
+    DUP4_ARG2(__lasx_xvaddi_hu, sum4, 1, sum5, 1, sum6, 1, sum7, 1,
+              sum4, sum5, sum6, sum7);
+    DUP4_ARG3(__lasx_xvsrani_b_h, sum1, sum0, 2, sum3, sum2, 2, sum5, sum4, 2,
+              sum7, sum6, 2, sum0, sum1, sum2, sum3);
+    __lasx_xvstelm_d(sum0, dst, 0, 0);
+    __lasx_xvstelm_d(sum0, dst, 8, 1);
+    dst += dst_stride;
+    __lasx_xvstelm_d(sum1, dst, 0, 0);
+    __lasx_xvstelm_d(sum1, dst, 8, 1);
+    dst += dst_stride;
+    __lasx_xvstelm_d(sum2, dst, 0, 0);
+    __lasx_xvstelm_d(sum2, dst, 8, 1);
+    dst += dst_stride;
+    __lasx_xvstelm_d(sum3, dst, 0, 0);
+    __lasx_xvstelm_d(sum3, dst, 8, 1);
+    dst += dst_stride;
+    __lasx_xvstelm_d(sum0, dst, 0, 2);
+    __lasx_xvstelm_d(sum0, dst, 8, 3);
+    dst += dst_stride;
+    __lasx_xvstelm_d(sum1, dst, 0, 2);
+    __lasx_xvstelm_d(sum1, dst, 8, 3);
+    dst += dst_stride;
+    __lasx_xvstelm_d(sum2, dst, 0, 2);
+    __lasx_xvstelm_d(sum2, dst, 8, 3);
+    dst += dst_stride;
+    __lasx_xvstelm_d(sum3, dst, 0, 2);
+    __lasx_xvstelm_d(sum3, dst, 8, 3);
+}
+
+void ff_put_no_rnd_pixels16_xy2_8_lasx(uint8_t *block,
+                                       const uint8_t *pixels,
+                                       ptrdiff_t line_size, int h)
+{
+    if (h == 16) {
+        common_hv_bil_no_rnd_16x16_lasx(pixels, line_size, block, line_size);
+    } else if (h == 8) {
+        common_hv_bil_no_rnd_8x16_lasx(pixels, line_size, block, line_size);
+    }
+}
+
+static void common_hz_bil_no_rnd_8x8_lasx(const uint8_t *src,
+                                          int32_t src_stride,
+                                          uint8_t *dst, int32_t dst_stride)
+{
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i src8, src9, src10, src11, src12, src13, src14, src15;
+    int32_t src_stride_2x = src_stride << 1;
+    int32_t src_stride_4x = src_stride << 2;
+    int32_t dst_stride_2x = dst_stride << 1;
+    int32_t dst_stride_4x = dst_stride << 2;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    uint8_t* _src = (uint8_t*)src;
+
+    src0 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src1, src2);
+    src3 = __lasx_xvldx(_src, src_stride_3x);
+    _src += src_stride_4x;
+    src4 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src5, src6);
+    src7 = __lasx_xvldx(_src, src_stride_3x);
+    _src += (1 - src_stride_4x);
+    src8 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src9, src10);
+    src11 = __lasx_xvldx(_src, src_stride_3x);
+    _src += src_stride_4x;
+    src12 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x,
+              src13, src14);
+    src15 = __lasx_xvldx(_src, src_stride_3x);
+
+    DUP4_ARG2(__lasx_xvpickev_d, src1, src0, src3, src2, src5, src4, src7,
+              src6, src0, src1, src2, src3);
+    DUP4_ARG2(__lasx_xvpickev_d, src9, src8, src11, src10, src13, src12, src15,
+              src14, src4, src5, src6, src7);
+    DUP4_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src3, src2, 0x20, src5, src4,
+              0x20, src7, src6, 0x20, src0, src1, src2, src3);
+    src0 = __lasx_xvavg_bu(src0, src2);
+    src1 = __lasx_xvavg_bu(src1, src3);
+    __lasx_xvstelm_d(src0, dst, 0, 0);
+    __lasx_xvstelm_d(src0, dst + dst_stride, 0, 1);
+    __lasx_xvstelm_d(src0, dst + dst_stride_2x, 0, 2);
+    __lasx_xvstelm_d(src0, dst + dst_stride_3x, 0, 3);
+    dst += dst_stride_4x;
+    __lasx_xvstelm_d(src1, dst, 0, 0);
+    __lasx_xvstelm_d(src1, dst + dst_stride, 0, 1);
+    __lasx_xvstelm_d(src1, dst + dst_stride_2x, 0, 2);
+    __lasx_xvstelm_d(src1, dst + dst_stride_3x, 0, 3);
+}
+
+static void common_hz_bil_no_rnd_4x8_lasx(const uint8_t *src,
+                                          int32_t src_stride,
+                                          uint8_t *dst, int32_t dst_stride)
+{
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    int32_t src_stride_2x = src_stride << 1;
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    int32_t dst_stride_2x = dst_stride << 1;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    uint8_t *_src = (uint8_t*)src;
+
+    src0 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src1, src2);
+    src3 = __lasx_xvldx(_src, src_stride_3x);
+    _src += 1;
+    src4 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src5, src6);
+    src7 = __lasx_xvldx(_src, src_stride_3x);
+    DUP4_ARG2(__lasx_xvpickev_d, src1, src0, src3, src2, src5, src4, src7, src6,
+              src0, src1, src2, src3);
+    DUP2_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src3, src2, 0x20, src0, src1);
+    src0 = __lasx_xvavg_bu(src0, src1);
+    __lasx_xvstelm_d(src0, dst, 0, 0);
+    __lasx_xvstelm_d(src0, dst + dst_stride, 0, 1);
+    __lasx_xvstelm_d(src0, dst + dst_stride_2x, 0, 2);
+    __lasx_xvstelm_d(src0, dst + dst_stride_3x, 0, 3);
+}
+
+void ff_put_no_rnd_pixels8_x2_8_lasx(uint8_t *block, const uint8_t *pixels,
+                                     ptrdiff_t line_size, int h)
+{
+    if (h == 8) {
+        common_hz_bil_no_rnd_8x8_lasx(pixels, line_size, block, line_size);
+    } else if (h == 4) {
+        common_hz_bil_no_rnd_4x8_lasx(pixels, line_size, block, line_size);
+    }
+}
+
+static void common_vt_bil_no_rnd_8x8_lasx(const uint8_t *src, int32_t src_stride,
+                                          uint8_t *dst, int32_t dst_stride)
+{
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7, src8;
+    int32_t src_stride_2x = src_stride << 1;
+    int32_t src_stride_4x = src_stride << 2;
+    int32_t dst_stride_2x = dst_stride << 1;
+    int32_t dst_stride_4x = dst_stride << 2;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    uint8_t* _src = (uint8_t*)src;
+
+    src0 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src1, src2);
+    src3 = __lasx_xvldx(_src, src_stride_3x);
+    _src += src_stride_4x;
+    src4 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src5, src6);
+    src7 = __lasx_xvldx(_src, src_stride_3x);
+    _src += src_stride_4x;
+    src8 = __lasx_xvld(_src, 0);
+
+    DUP4_ARG2(__lasx_xvpickev_d, src1, src0, src2, src1, src3, src2, src4, src3,
+              src0, src1, src2, src3);
+    DUP4_ARG2(__lasx_xvpickev_d, src5, src4, src6, src5, src7, src6, src8, src7,
+              src4, src5, src6, src7);
+    DUP4_ARG3(__lasx_xvpermi_q, src2, src0, 0x20, src3, src1, 0x20, src6, src4,
+              0x20, src7, src5, 0x20, src0, src1, src2, src3);
+    src0 = __lasx_xvavg_bu(src0, src1);
+    src1 = __lasx_xvavg_bu(src2, src3);
+    __lasx_xvstelm_d(src0, dst, 0, 0);
+    __lasx_xvstelm_d(src0, dst + dst_stride, 0, 1);
+    __lasx_xvstelm_d(src0, dst + dst_stride_2x, 0, 2);
+    __lasx_xvstelm_d(src0, dst + dst_stride_3x, 0, 3);
+    dst += dst_stride_4x;
+    __lasx_xvstelm_d(src1, dst, 0, 0);
+    __lasx_xvstelm_d(src1, dst + dst_stride, 0, 1);
+    __lasx_xvstelm_d(src1, dst + dst_stride_2x, 0, 2);
+    __lasx_xvstelm_d(src1, dst + dst_stride_3x, 0, 3);
+}
+
+static void common_vt_bil_no_rnd_4x8_lasx(const uint8_t *src, int32_t src_stride,
+                                          uint8_t *dst, int32_t dst_stride)
+{
+    __m256i src0, src1, src2, src3, src4;
+    int32_t src_stride_2x = src_stride << 1;
+    int32_t src_stride_4x = src_stride << 2;
+    int32_t dst_stride_2x = dst_stride << 1;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    uint8_t* _src = (uint8_t*)src;
+
+    src0 = __lasx_xvld(_src, 0);
+    DUP4_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, _src,
+              src_stride_3x, _src, src_stride_4x, src1, src2, src3, src4);
+    DUP4_ARG2(__lasx_xvpickev_d, src1, src0, src2, src1, src3, src2, src4, src3,
+              src0, src1, src2, src3);
+    DUP2_ARG3(__lasx_xvpermi_q, src2, src0, 0x20, src3, src1, 0x20, src0, src1);
+    src0 = __lasx_xvavg_bu(src0, src1);
+    __lasx_xvstelm_d(src0, dst, 0, 0);
+    __lasx_xvstelm_d(src0, dst + dst_stride, 0, 1);
+    __lasx_xvstelm_d(src0, dst + dst_stride_2x, 0, 2);
+    __lasx_xvstelm_d(src0, dst + dst_stride_3x, 0, 3);
+}
+
+void ff_put_no_rnd_pixels8_y2_8_lasx(uint8_t *block, const uint8_t *pixels,
+                                     ptrdiff_t line_size, int h)
+{
+    if (h == 8) {
+        common_vt_bil_no_rnd_8x8_lasx(pixels, line_size, block, line_size);
+    } else if (h == 4) {
+        common_vt_bil_no_rnd_4x8_lasx(pixels, line_size, block, line_size);
+    }
+}
+
+static void common_hv_bil_no_rnd_8x8_lasx(const uint8_t *src, int32_t src_stride,
+                                          uint8_t *dst, int32_t dst_stride)
+{
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i src8, src9, src10, src11, src12, src13, src14, src15, src16, src17;
+    __m256i sum0, sum1, sum2, sum3;
+    int32_t src_stride_2x = src_stride << 1;
+    int32_t src_stride_4x = src_stride << 2;
+    int32_t dst_stride_2x = dst_stride << 1;
+    int32_t dst_stride_4x = dst_stride << 2;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    uint8_t* _src = (uint8_t*)src;
+
+    src0 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src1, src2);
+    src3 = __lasx_xvldx(_src, src_stride_3x);
+    _src += src_stride_4x;
+    src4 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src5, src6);
+    src7 = __lasx_xvldx(_src, src_stride_3x);
+    _src += (1 - src_stride_4x);
+    src9 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x,
+              src10, src11);
+    src12 = __lasx_xvldx(_src, src_stride_3x);
+    _src += src_stride_4x;
+    src13 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x,
+              src14, src15);
+    src16 = __lasx_xvldx(_src, src_stride_3x);
+    _src += (src_stride_4x - 1);
+    DUP2_ARG2(__lasx_xvld, _src, 0, _src, 1, src8, src17);
+
+    DUP4_ARG2(__lasx_xvilvl_b, src9, src0, src10, src1, src11, src2, src12, src3,
+              src0, src1, src2, src3);
+    DUP4_ARG2(__lasx_xvilvl_b, src13, src4, src14, src5, src15, src6, src16, src7,
+              src4, src5, src6, src7);
+    src8 = __lasx_xvilvl_b(src17, src8);
+    DUP4_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src2, src1, 0x20, src3, src2,
+              0x20, src4, src3, 0x20, src0, src1, src2, src3);
+    DUP4_ARG3(__lasx_xvpermi_q, src5, src4, 0x20, src6, src5, 0x20, src7, src6,
+              0x20, src8, src7, 0x20, src4, src5, src6, src7);
+    DUP4_ARG2(__lasx_xvhaddw_hu_bu, src0, src0, src1, src1, src2, src2,
+              src3, src3, src0, src1, src2, src3);
+    DUP4_ARG2(__lasx_xvhaddw_hu_bu, src4, src4, src5, src5, src6, src6,
+              src7, src7, src4, src5, src6, src7);
+    DUP4_ARG2(__lasx_xvadd_h, src0, src1, src2, src3, src4, src5, src6, src7,
+              sum0, sum1, sum2, sum3);
+    DUP4_ARG2(__lasx_xvaddi_hu, sum0, 1, sum1, 1, sum2, 1, sum3, 1,
+              sum0, sum1, sum2, sum3);
+    DUP2_ARG3(__lasx_xvsrani_b_h, sum1, sum0, 2, sum3, sum2, 2, sum0, sum1);
+    __lasx_xvstelm_d(sum0, dst, 0, 0);
+    __lasx_xvstelm_d(sum0, dst + dst_stride, 0, 2);
+    __lasx_xvstelm_d(sum0, dst + dst_stride_2x, 0, 1);
+    __lasx_xvstelm_d(sum0, dst + dst_stride_3x, 0, 3);
+    dst += dst_stride_4x;
+    __lasx_xvstelm_d(sum1, dst, 0, 0);
+    __lasx_xvstelm_d(sum1, dst + dst_stride, 0, 2);
+    __lasx_xvstelm_d(sum1, dst + dst_stride_2x, 0, 1);
+    __lasx_xvstelm_d(sum1, dst + dst_stride_3x, 0, 3);
+}
+
+static void common_hv_bil_no_rnd_4x8_lasx(const uint8_t *src, int32_t src_stride,
+                                          uint8_t *dst, int32_t dst_stride)
+{
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i src8, src9, sum0, sum1;
+    int32_t src_stride_2x = src_stride << 1;
+    int32_t src_stride_4x = src_stride << 2;
+    int32_t dst_stride_2x = dst_stride << 1;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    uint8_t *_src = (uint8_t*)src;
+
+    src0 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src1, src2);
+    src3 = __lasx_xvldx(_src, src_stride_3x);
+    _src += 1;
+    src5 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src6, src7);
+    src8 = __lasx_xvldx(_src, src_stride_3x);
+    _src += (src_stride_4x - 1);
+    DUP2_ARG2(__lasx_xvld, _src, 0, _src, 1, src4, src9);
+
+    DUP4_ARG2(__lasx_xvilvl_b, src5, src0, src6, src1, src7, src2, src8, src3,
+              src0, src1, src2, src3);
+    src4 = __lasx_xvilvl_b(src9, src4);
+    DUP4_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src2, src1, 0x20, src3, src2,
+              0x20, src4, src3, 0x20, src0, src1, src2, src3);
+    DUP4_ARG2(__lasx_xvhaddw_hu_bu, src0, src0, src1, src1, src2, src2,
+              src3, src3, src0, src1, src2, src3);
+    DUP2_ARG2(__lasx_xvadd_h, src0, src1, src2, src3, sum0, sum1);
+    sum0 = __lasx_xvaddi_hu(sum0, 1);
+    sum1 = __lasx_xvaddi_hu(sum1, 1);
+    sum0 = __lasx_xvsrani_b_h(sum1, sum0, 2);
+    __lasx_xvstelm_d(sum0, dst, 0, 0);
+    __lasx_xvstelm_d(sum0, dst + dst_stride, 0, 2);
+    __lasx_xvstelm_d(sum0, dst + dst_stride_2x, 0, 1);
+    __lasx_xvstelm_d(sum0, dst + dst_stride_3x, 0, 3);
+}
+
+void ff_put_no_rnd_pixels8_xy2_8_lasx(uint8_t *block, const uint8_t *pixels,
+                                      ptrdiff_t line_size, int h)
+{
+    if (h == 8) {
+        common_hv_bil_no_rnd_8x8_lasx(pixels, line_size, block, line_size);
+    } else if (h == 4) {
+        common_hv_bil_no_rnd_4x8_lasx(pixels, line_size, block, line_size);
+    }
+}
+
+static void common_hv_bil_16w_lasx(const uint8_t *src, int32_t src_stride,
+                                   uint8_t *dst, int32_t dst_stride,
+                                   uint8_t height)
+{
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7, src8, src9;
+    __m256i src10, src11, src12, src13, src14, src15, src16, src17;
+    __m256i sum0, sum1, sum2, sum3, sum4, sum5, sum6, sum7;
+    uint8_t loop_cnt;
+    int32_t src_stride_2x = src_stride << 1;
+    int32_t src_stride_4x = src_stride << 2;
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    uint8_t* _src = (uint8_t*)src;
+
+    for (loop_cnt = (height >> 3); loop_cnt--;) {
+        src0 = __lasx_xvld(_src, 0);
+        DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src1, src2);
+        src3 = __lasx_xvldx(_src, src_stride_3x);
+        _src += src_stride_4x;
+        src4 = __lasx_xvld(_src, 0);
+        DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src5, src6);
+        src7 = __lasx_xvldx(_src, src_stride_3x);
+        _src += (1 - src_stride_4x);
+        src9 = __lasx_xvld(_src, 0);
+        DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x,
+                  src10, src11);
+        src12 = __lasx_xvldx(_src, src_stride_3x);
+        _src += src_stride_4x;
+        src13 = __lasx_xvld(_src, 0);
+        DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x,
+                  src14, src15);
+        src16 = __lasx_xvldx(_src, src_stride_3x);
+        _src += (src_stride_4x - 1);
+        DUP2_ARG2(__lasx_xvld, _src, 0, _src, 1, src8, src17);
+
+        DUP4_ARG3(__lasx_xvpermi_q, src0, src4, 0x02, src1, src5, 0x02, src2,
+                  src6, 0x02, src3, src7, 0x02, src0, src1, src2, src3);
+        DUP4_ARG3(__lasx_xvpermi_q, src4, src8, 0x02, src9, src13, 0x02, src10,
+                  src14, 0x02, src11, src15, 0x02, src4, src5, src6, src7);
+        DUP2_ARG3(__lasx_xvpermi_q, src12, src16, 0x02, src13, src17, 0x02,
+                   src8, src9);
+
+        DUP4_ARG2(__lasx_xvilvl_h, src5, src0, src6, src1, src7, src2, src8,
+                  src3, sum0, sum2, sum4, sum6);
+        DUP4_ARG2(__lasx_xvilvh_h, src5, src0, src6, src1, src7, src2, src8,
+                  src3, sum1, sum3, sum5, sum7);
+        src8 = __lasx_xvilvl_h(src9, src4);
+        src9 = __lasx_xvilvh_h(src9, src4);
+
+        DUP4_ARG2(__lasx_xvhaddw_hu_bu, sum0, sum0, sum1, sum1, sum2, sum2,
+                  sum3, sum3, src0, src1, src2, src3);
+        DUP4_ARG2(__lasx_xvhaddw_hu_bu, sum4, sum4, sum5, sum5, sum6, sum6,
+                  sum7, sum7, src4, src5, src6, src7);
+        DUP2_ARG2(__lasx_xvhaddw_hu_bu, src8, src8, src9, src9, src8, src9);
+
+        DUP4_ARG2(__lasx_xvadd_h, src0, src2, src1, src3, src2, src4, src3,
+                  src5, sum0, sum1, sum2, sum3);
+        DUP4_ARG2(__lasx_xvadd_h, src4, src6, src5, src7, src6, src8, src7,
+                  src9, sum4, sum5, sum6, sum7);
+        DUP4_ARG3(__lasx_xvsrarni_b_h, sum1, sum0, 2, sum3, sum2, 2, sum5,
+                  sum4, 2, sum7, sum6, 2, sum0, sum1, sum2, sum3);
+        __lasx_xvstelm_d(sum0, dst, 0, 0);
+        __lasx_xvstelm_d(sum0, dst, 8, 1);
+        dst += dst_stride;
+        __lasx_xvstelm_d(sum1, dst, 0, 0);
+        __lasx_xvstelm_d(sum1, dst, 8, 1);
+        dst += dst_stride;
+        __lasx_xvstelm_d(sum2, dst, 0, 0);
+        __lasx_xvstelm_d(sum2, dst, 8, 1);
+        dst += dst_stride;
+        __lasx_xvstelm_d(sum3, dst, 0, 0);
+        __lasx_xvstelm_d(sum3, dst, 8, 1);
+        dst += dst_stride;
+        __lasx_xvstelm_d(sum0, dst, 0, 2);
+        __lasx_xvstelm_d(sum0, dst, 8, 3);
+        dst += dst_stride;
+        __lasx_xvstelm_d(sum1, dst, 0, 2);
+        __lasx_xvstelm_d(sum1, dst, 8, 3);
+        dst += dst_stride;
+        __lasx_xvstelm_d(sum2, dst, 0, 2);
+        __lasx_xvstelm_d(sum2, dst, 8, 3);
+        dst += dst_stride;
+        __lasx_xvstelm_d(sum3, dst, 0, 2);
+        __lasx_xvstelm_d(sum3, dst, 8, 3);
+        dst += dst_stride;
+    }
+}
+
+void ff_put_pixels16_xy2_8_lasx(uint8_t *block, const uint8_t *pixels,
+                                ptrdiff_t line_size, int h)
+{
+    common_hv_bil_16w_lasx(pixels, line_size, block, line_size, h);
+}
+
+static void common_hv_bil_8w_lasx(const uint8_t *src, int32_t src_stride,
+                                  uint8_t *dst, int32_t dst_stride,
+                                  uint8_t height)
+{
+    __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m256i src8, src9, sum0, sum1;
+    uint8_t loop_cnt;
+    int32_t src_stride_2x = src_stride << 1;
+    int32_t src_stride_4x = src_stride << 2;
+    int32_t dst_stride_2x = dst_stride << 1;
+    int32_t dst_stride_4x = dst_stride << 2;
+    int32_t dst_stride_3x = dst_stride_2x + dst_stride;
+    int32_t src_stride_3x = src_stride_2x + src_stride;
+    uint8_t* _src = (uint8_t*)src;
+
+    DUP2_ARG2(__lasx_xvld, _src, 0, _src, 1, src0, src5);
+    _src += src_stride;
+
+    for (loop_cnt = (height >> 2); loop_cnt--;) {
+        src1 = __lasx_xvld(_src, 0);
+        DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src2, src3);
+        src4 = __lasx_xvldx(_src, src_stride_3x);
+        _src += 1;
+        src6 = __lasx_xvld(_src, 0);
+        DUP2_ARG2(__lasx_xvldx, _src, src_stride, _src, src_stride_2x, src7, src8);
+        src9 = __lasx_xvldx(_src, src_stride_3x);
+        _src += (src_stride_4x - 1);
+        DUP4_ARG2(__lasx_xvilvl_b, src5, src0, src6, src1, src7, src2, src8, src3,
+                  src0, src1, src2, src3);
+        src5 = __lasx_xvilvl_b(src9, src4);
+        DUP4_ARG3(__lasx_xvpermi_q, src1, src0, 0x20, src2, src1, 0x20, src3, src2,
+                  0x20, src5, src3, 0x20, src0, src1, src2, src3);
+        DUP4_ARG2(__lasx_xvhaddw_hu_bu, src0, src0, src1, src1, src2, src2,
+                  src3, src3, src0, src1, src2, src3);
+        DUP2_ARG2(__lasx_xvadd_h, src0, src1, src2, src3, sum0, sum1);
+        sum0 = __lasx_xvsrarni_b_h(sum1, sum0, 2);
+        __lasx_xvstelm_d(sum0, dst, 0, 0);
+        __lasx_xvstelm_d(sum0, dst + dst_stride, 0, 2);
+        __lasx_xvstelm_d(sum0, dst + dst_stride_2x, 0, 1);
+        __lasx_xvstelm_d(sum0, dst + dst_stride_3x, 0, 3);
+        dst += dst_stride_4x;
+        src0 = src4;
+        src5 = src9;
+    }
+}
+
+void ff_put_pixels8_xy2_8_lasx(uint8_t *block, const uint8_t *pixels,
+                               ptrdiff_t line_size, int h)
+{
+    common_hv_bil_8w_lasx(pixels, line_size, block, line_size, h);
+}
diff --git a/libavcodec/loongarch/hpeldsp_lasx.h b/libavcodec/loongarch/hpeldsp_lasx.h
new file mode 100644
index 0000000000..2e035eade8
--- /dev/null
+++ b/libavcodec/loongarch/hpeldsp_lasx.h
@@ -0,0 +1,58 @@
+/*
+ * Copyright (c) 2021 Loongson Technology Corporation Limited
+ * Contributed by Shiyou Yin <yinshiyou-hf@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef AVCODEC_LOONGARCH_HPELDSP_LASX_H
+#define AVCODEC_LOONGARCH_HPELDSP_LASX_H
+
+#include <stdint.h>
+#include <stddef.h>
+#include "libavutil/attributes.h"
+
+void ff_put_pixels8_8_lasx(uint8_t *block, const uint8_t *pixels,
+                           ptrdiff_t line_size, int h);
+void ff_put_pixels8_x2_8_lasx(uint8_t *block, const uint8_t *pixels,
+                              ptrdiff_t line_size, int32_t h);
+void ff_put_pixels8_y2_8_lasx(uint8_t *block, const uint8_t *pixels,
+                              ptrdiff_t line_size, int32_t h);
+void ff_put_pixels16_8_lsx(uint8_t *block, const uint8_t *pixels,
+                           ptrdiff_t line_size, int h);
+void ff_put_pixels16_x2_8_lasx(uint8_t *block, const uint8_t *pixels,
+                               ptrdiff_t line_size, int32_t h);
+void ff_put_pixels16_y2_8_lasx(uint8_t *block, const uint8_t *pixels,
+                               ptrdiff_t line_size, int32_t h);
+void ff_put_no_rnd_pixels16_x2_8_lasx(uint8_t *block, const uint8_t *pixels,
+                                      ptrdiff_t line_size, int h);
+void ff_put_no_rnd_pixels16_y2_8_lasx(uint8_t *block, const uint8_t *pixels,
+                                      ptrdiff_t line_size, int h);
+void ff_put_no_rnd_pixels16_xy2_8_lasx(uint8_t *block,
+                                       const uint8_t *pixels,
+                                       ptrdiff_t line_size, int h);
+void ff_put_no_rnd_pixels8_x2_8_lasx(uint8_t *block, const uint8_t *pixels,
+                                     ptrdiff_t line_size, int h);
+void ff_put_no_rnd_pixels8_y2_8_lasx(uint8_t *block, const uint8_t *pixels,
+                                     ptrdiff_t line_size, int h);
+void ff_put_no_rnd_pixels8_xy2_8_lasx(uint8_t *block, const uint8_t *pixels,
+                                      ptrdiff_t line_size, int h);
+void ff_put_pixels8_xy2_8_lasx(uint8_t *block, const uint8_t *pixels,
+                               ptrdiff_t line_size, int h);
+void ff_put_pixels16_xy2_8_lasx(uint8_t *block, const uint8_t *pixels,
+                                ptrdiff_t line_size, int h);
+#endif
diff --git a/libavcodec/loongarch/idctdsp_init_loongarch.c b/libavcodec/loongarch/idctdsp_init_loongarch.c
new file mode 100644
index 0000000000..9d1d21cc18
--- /dev/null
+++ b/libavcodec/loongarch/idctdsp_init_loongarch.c
@@ -0,0 +1,45 @@
+/*
+ * Copyright (c) 2021 Loongson Technology Corporation Limited
+ * Contributed by Hao Chen <chenhao@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "libavutil/loongarch/cpu.h"
+#include "idctdsp_loongarch.h"
+#include "libavcodec/xvididct.h"
+
+av_cold void ff_idctdsp_init_loongarch(IDCTDSPContext *c, AVCodecContext *avctx,
+                                       unsigned high_bit_depth)
+{
+    int cpu_flags = av_get_cpu_flags();
+
+    if (have_lasx(cpu_flags)) {
+        if ((avctx->lowres != 1) && (avctx->lowres != 2) && (avctx->lowres != 3) &&
+            (avctx->bits_per_raw_sample != 10) &&
+            (avctx->bits_per_raw_sample != 12) &&
+            (avctx->idct_algo == FF_IDCT_AUTO)) {
+                    c->idct_put = ff_simple_idct_put_lasx;
+                    c->idct_add = ff_simple_idct_add_lasx;
+                    c->idct = ff_simple_idct_lasx;
+                    c->perm_type = FF_IDCT_PERM_NONE;
+        }
+        c->put_pixels_clamped = ff_put_pixels_clamped_lasx;
+        c->put_signed_pixels_clamped = ff_put_signed_pixels_clamped_lasx;
+        c->add_pixels_clamped = ff_add_pixels_clamped_lasx;
+    }
+}
diff --git a/libavcodec/loongarch/idctdsp_lasx.c b/libavcodec/loongarch/idctdsp_lasx.c
new file mode 100644
index 0000000000..1cfab0e028
--- /dev/null
+++ b/libavcodec/loongarch/idctdsp_lasx.c
@@ -0,0 +1,124 @@
+/*
+ * Copyright (c) 2021 Loongson Technology Corporation Limited
+ * Contributed by Hao Chen <chenhao@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "idctdsp_loongarch.h"
+#include "libavutil/loongarch/loongson_intrinsics.h"
+
+void ff_put_pixels_clamped_lasx(const int16_t *block,
+                                uint8_t *av_restrict pixels,
+                                ptrdiff_t stride)
+{
+    __m256i b0, b1, b2, b3;
+    __m256i temp0, temp1;
+    ptrdiff_t stride_2x = stride << 1;
+    ptrdiff_t stride_4x = stride << 2;
+    ptrdiff_t stride_3x = stride_2x + stride;
+
+    DUP4_ARG2(__lasx_xvld, block, 0, block, 32, block, 64, block, 96,
+              b0, b1, b2, b3);
+    DUP4_ARG1(__lasx_xvclip255_h, b0, b1, b2, b3, b0, b1, b2, b3);
+    DUP2_ARG2(__lasx_xvpickev_b, b1, b0, b3, b2, temp0, temp1);
+    __lasx_xvstelm_d(temp0, pixels, 0, 0);
+    __lasx_xvstelm_d(temp0, pixels + stride, 0, 2);
+    __lasx_xvstelm_d(temp0, pixels + stride_2x, 0, 1);
+    __lasx_xvstelm_d(temp0, pixels + stride_3x, 0, 3);
+    pixels += stride_4x;
+    __lasx_xvstelm_d(temp1, pixels, 0, 0);
+    __lasx_xvstelm_d(temp1, pixels + stride, 0, 2);
+    __lasx_xvstelm_d(temp1, pixels + stride_2x, 0, 1);
+    __lasx_xvstelm_d(temp1, pixels + stride_3x, 0, 3);
+}
+
+void ff_put_signed_pixels_clamped_lasx(const int16_t *block,
+                                       uint8_t *av_restrict pixels,
+                                       ptrdiff_t stride)
+{
+    __m256i b0, b1, b2, b3;
+    __m256i temp0, temp1;
+    __m256i const_128 = {0x0080008000800080, 0x0080008000800080,
+                         0x0080008000800080, 0x0080008000800080};
+    ptrdiff_t stride_2x = stride << 1;
+    ptrdiff_t stride_4x = stride << 2;
+    ptrdiff_t stride_3x = stride_2x + stride;
+
+    DUP4_ARG2(__lasx_xvld, block, 0, block, 32, block, 64, block, 96,
+              b0, b1, b2, b3);
+    DUP4_ARG2(__lasx_xvadd_h, b0, const_128, b1, const_128, b2, const_128,
+              b3, const_128, b0, b1, b2, b3);
+    DUP4_ARG1(__lasx_xvclip255_h, b0, b1, b2, b3, b0, b1, b2, b3);
+    DUP2_ARG2(__lasx_xvpickev_b, b1, b0, b3, b2, temp0, temp1);
+    __lasx_xvstelm_d(temp0, pixels, 0, 0);
+    __lasx_xvstelm_d(temp0, pixels + stride, 0, 2);
+    __lasx_xvstelm_d(temp0, pixels + stride_2x, 0, 1);
+    __lasx_xvstelm_d(temp0, pixels + stride_3x, 0, 3);
+    pixels += stride_4x;
+    __lasx_xvstelm_d(temp1, pixels, 0, 0);
+    __lasx_xvstelm_d(temp1, pixels + stride, 0, 2);
+    __lasx_xvstelm_d(temp1, pixels + stride_2x, 0, 1);
+    __lasx_xvstelm_d(temp1, pixels + stride_3x, 0, 3);
+}
+
+void ff_add_pixels_clamped_lasx(const int16_t *block,
+                                uint8_t *av_restrict pixels,
+                                ptrdiff_t stride)
+{
+    __m256i b0, b1, b2, b3;
+    __m256i p0, p1, p2, p3, p4, p5, p6, p7;
+    __m256i temp0, temp1, temp2, temp3;
+    uint8_t *pix = pixels;
+    ptrdiff_t stride_2x = stride << 1;
+    ptrdiff_t stride_4x = stride << 2;
+    ptrdiff_t stride_3x = stride_2x + stride;
+
+    DUP4_ARG2(__lasx_xvld, block, 0, block, 32, block, 64, block, 96,
+              b0, b1, b2, b3);
+    p0   = __lasx_xvldrepl_d(pix, 0);
+    pix += stride;
+    p1   = __lasx_xvldrepl_d(pix, 0);
+    pix += stride;
+    p2   = __lasx_xvldrepl_d(pix, 0);
+    pix += stride;
+    p3   = __lasx_xvldrepl_d(pix, 0);
+    pix += stride;
+    p4   = __lasx_xvldrepl_d(pix, 0);
+    pix += stride;
+    p5   = __lasx_xvldrepl_d(pix, 0);
+    pix += stride;
+    p6   = __lasx_xvldrepl_d(pix, 0);
+    pix += stride;
+    p7   = __lasx_xvldrepl_d(pix, 0);
+    DUP4_ARG3(__lasx_xvpermi_q, p1, p0, 0x20, p3, p2, 0x20, p5, p4, 0x20,
+              p7, p6, 0x20, temp0, temp1, temp2, temp3);
+    DUP4_ARG2(__lasx_xvaddw_h_h_bu, b0, temp0, b1, temp1, b2, temp2, b3, temp3,
+              temp0, temp1, temp2, temp3);
+    DUP4_ARG1(__lasx_xvclip255_h, temp0, temp1, temp2, temp3,
+              temp0, temp1, temp2, temp3);
+    DUP2_ARG2(__lasx_xvpickev_b, temp1, temp0, temp3, temp2, temp0, temp1);
+    __lasx_xvstelm_d(temp0, pixels, 0, 0);
+    __lasx_xvstelm_d(temp0, pixels + stride, 0, 2);
+    __lasx_xvstelm_d(temp0, pixels + stride_2x, 0, 1);
+    __lasx_xvstelm_d(temp0, pixels + stride_3x, 0, 3);
+    pixels += stride_4x;
+    __lasx_xvstelm_d(temp1, pixels, 0, 0);
+    __lasx_xvstelm_d(temp1, pixels + stride, 0, 2);
+    __lasx_xvstelm_d(temp1, pixels + stride_2x, 0, 1);
+    __lasx_xvstelm_d(temp1, pixels + stride_3x, 0, 3);
+}
diff --git a/libavcodec/loongarch/idctdsp_loongarch.h b/libavcodec/loongarch/idctdsp_loongarch.h
new file mode 100644
index 0000000000..cae8e7af58
--- /dev/null
+++ b/libavcodec/loongarch/idctdsp_loongarch.h
@@ -0,0 +1,41 @@
+/*
+ * Copyright (c) 2021 Loongson Technology Corporation Limited
+ * Contributed by Hao Chen <chenhao@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef AVCODEC_LOONGARCH_IDCTDSP_LOONGARCH_H
+#define AVCODEC_LOONGARCH_IDCTDSP_LOONGARCH_H
+
+#include <stdint.h>
+#include "libavcodec/mpegvideo.h"
+
+void ff_simple_idct_lasx(int16_t *block);
+void ff_simple_idct_put_lasx(uint8_t *dest, ptrdiff_t stride_dst, int16_t *block);
+void ff_simple_idct_add_lasx(uint8_t *dest, ptrdiff_t stride_dst, int16_t *block);
+void ff_put_pixels_clamped_lasx(const int16_t *block,
+                                uint8_t *av_restrict pixels,
+                                ptrdiff_t line_size);
+void ff_put_signed_pixels_clamped_lasx(const int16_t *block,
+                                       uint8_t *av_restrict pixels,
+                                       ptrdiff_t line_size);
+void ff_add_pixels_clamped_lasx(const int16_t *block,
+                                uint8_t *av_restrict pixels,
+                                ptrdiff_t line_size);
+
+#endif /* AVCODEC_LOONGARCH_IDCTDSP_LOONGARCH_H */
diff --git a/libavcodec/loongarch/loongson_asm.S b/libavcodec/loongarch/loongson_asm.S
new file mode 100644
index 0000000000..767c7c0bb7
--- /dev/null
+++ b/libavcodec/loongarch/loongson_asm.S
@@ -0,0 +1,946 @@
+/*
+ * Loongson asm helper.
+ *
+ * Copyright (c) 2022 Loongson Technology Corporation Limited
+ * Contributed by Gu Xiwei(guxiwei-hf@loongson.cn)
+ *                Shiyou Yin(yinshiyou-hf@loongson.cn)
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ * MAJOR version: Macro usage changes.
+ * MINOR version: Add new functions, or bug fixes.
+ * MICRO version: Comment changes or implementation changes.
+ */
+#define LML_VERSION_MAJOR 0
+#define LML_VERSION_MINOR 2
+#define LML_VERSION_MICRO 0
+
+/*
+ *============================================================================
+ * macros for specific projetc, set them as needed.
+ * Following LoongML macros for your reference.
+ *============================================================================
+ */
+#define ASM_PREF
+#define DEFAULT_ALIGN    5
+
+.macro function name, align=DEFAULT_ALIGN
+.macro endfunc
+    jirl    $r0, $r1, 0x0
+    .size ASM_PREF\name, . - ASM_PREF\name
+    .purgem endfunc
+.endm
+.text ;
+.align \align ;
+.globl ASM_PREF\name ;
+.type  ASM_PREF\name, @function ;
+ASM_PREF\name: ;
+.endm
+
+/**
+ *  Attention: If align is not zero, the macro will use
+ *  t7 until the end of function
+ */
+.macro alloc_stack size, align=0
+.if \align
+    .macro clean_stack
+        add.d   sp, sp, t7
+    .endm
+    addi.d  sp, sp, - \size
+    andi.d  t7, sp, \align - 1
+    sub.d   sp, sp, t7
+    addi.d  t7, t7, \size
+.else
+    .macro clean_stack
+        addi.d  sp, sp, \size
+    .endm
+    addi.d  sp, sp, - \size
+.endif
+.endm
+
+.macro  const name, align=DEFAULT_ALIGN
+    .macro endconst
+    .size  \name, . - \name
+    .purgem endconst
+    .endm
+.section .rodata
+.align   \align
+\name:
+.endm
+
+/*
+ *============================================================================
+ * LoongArch register alias
+ *============================================================================
+ */
+
+#define a0 $a0
+#define a1 $a1
+#define a2 $a2
+#define a3 $a3
+#define a4 $a4
+#define a5 $a5
+#define a6 $a6
+#define a7 $a7
+
+#define t0 $t0
+#define t1 $t1
+#define t2 $t2
+#define t3 $t3
+#define t4 $t4
+#define t5 $t5
+#define t6 $t6
+#define t7 $t7
+#define t8 $t8
+
+#define s0 $s0
+#define s1 $s1
+#define s2 $s2
+#define s3 $s3
+#define s4 $s4
+#define s5 $s5
+#define s6 $s6
+#define s7 $s7
+#define s8 $s8
+
+#define zero $zero
+#define sp   $sp
+#define ra   $ra
+
+#define f0  $f0
+#define f1  $f1
+#define f2  $f2
+#define f3  $f3
+#define f4  $f4
+#define f5  $f5
+#define f6  $f6
+#define f7  $f7
+#define f8  $f8
+#define f9  $f9
+#define f10 $f10
+#define f11 $f11
+#define f12 $f12
+#define f13 $f13
+#define f14 $f14
+#define f15 $f15
+#define f16 $f16
+#define f17 $f17
+#define f18 $f18
+#define f19 $f19
+#define f20 $f20
+#define f21 $f21
+#define f22 $f22
+#define f23 $f23
+#define f24 $f24
+#define f25 $f25
+#define f26 $f26
+#define f27 $f27
+#define f28 $f28
+#define f29 $f29
+#define f30 $f30
+#define f31 $f31
+
+#define vr0 $vr0
+#define vr1 $vr1
+#define vr2 $vr2
+#define vr3 $vr3
+#define vr4 $vr4
+#define vr5 $vr5
+#define vr6 $vr6
+#define vr7 $vr7
+#define vr8 $vr8
+#define vr9 $vr9
+#define vr10 $vr10
+#define vr11 $vr11
+#define vr12 $vr12
+#define vr13 $vr13
+#define vr14 $vr14
+#define vr15 $vr15
+#define vr16 $vr16
+#define vr17 $vr17
+#define vr18 $vr18
+#define vr19 $vr19
+#define vr20 $vr20
+#define vr21 $vr21
+#define vr22 $vr22
+#define vr23 $vr23
+#define vr24 $vr24
+#define vr25 $vr25
+#define vr26 $vr26
+#define vr27 $vr27
+#define vr28 $vr28
+#define vr29 $vr29
+#define vr30 $vr30
+#define vr31 $vr31
+
+#define xr0 $xr0
+#define xr1 $xr1
+#define xr2 $xr2
+#define xr3 $xr3
+#define xr4 $xr4
+#define xr5 $xr5
+#define xr6 $xr6
+#define xr7 $xr7
+#define xr8 $xr8
+#define xr9 $xr9
+#define xr10 $xr10
+#define xr11 $xr11
+#define xr12 $xr12
+#define xr13 $xr13
+#define xr14 $xr14
+#define xr15 $xr15
+#define xr16 $xr16
+#define xr17 $xr17
+#define xr18 $xr18
+#define xr19 $xr19
+#define xr20 $xr20
+#define xr21 $xr21
+#define xr22 $xr22
+#define xr23 $xr23
+#define xr24 $xr24
+#define xr25 $xr25
+#define xr26 $xr26
+#define xr27 $xr27
+#define xr28 $xr28
+#define xr29 $xr29
+#define xr30 $xr30
+#define xr31 $xr31
+
+/*
+ *============================================================================
+ * LSX/LASX synthesize instructions
+ *============================================================================
+ */
+
+/*
+ * Description : Dot product of byte vector elements
+ * Arguments   : Inputs  - vj, vk
+ *               Outputs - vd
+ *               Return Type - halfword
+ */
+.macro vdp2.h.bu vd, vj, vk
+    vmulwev.h.bu      \vd,    \vj,    \vk
+    vmaddwod.h.bu     \vd,    \vj,    \vk
+.endm
+
+.macro vdp2.h.bu.b vd, vj, vk
+    vmulwev.h.bu.b    \vd,    \vj,    \vk
+    vmaddwod.h.bu.b   \vd,    \vj,    \vk
+.endm
+
+.macro vdp2.w.h vd, vj, vk
+    vmulwev.w.h       \vd,    \vj,    \vk
+    vmaddwod.w.h      \vd,    \vj,    \vk
+.endm
+
+.macro xvdp2.h.bu xd, xj, xk
+    xvmulwev.h.bu    \xd,    \xj,    \xk
+    xvmaddwod.h.bu   \xd,    \xj,    \xk
+.endm
+
+.macro xvdp2.h.bu.b xd, xj, xk
+    xvmulwev.h.bu.b    \xd,  \xj,    \xk
+    xvmaddwod.h.bu.b   \xd,  \xj,    \xk
+.endm
+
+.macro xvdp2.w.h xd, xj, xk
+    xvmulwev.w.h       \xd,  \xj,    \xk
+    xvmaddwod.w.h      \xd,  \xj,    \xk
+.endm
+
+/*
+ * Description : Dot product & addition of halfword vector elements
+ * Arguments   : Inputs  - vj, vk
+ *               Outputs - vd
+ *               Return Type - twice size of input
+ */
+.macro vdp2add.h.bu vd, vj, vk
+    vmaddwev.h.bu     \vd,    \vj,    \vk
+    vmaddwod.h.bu     \vd,    \vj,    \vk
+.endm
+
+.macro vdp2add.h.bu.b vd, vj, vk
+    vmaddwev.h.bu.b   \vd,    \vj,    \vk
+    vmaddwod.h.bu.b   \vd,    \vj,    \vk
+.endm
+
+.macro vdp2add.w.h vd, vj, vk
+    vmaddwev.w.h      \vd,    \vj,    \vk
+    vmaddwod.w.h      \vd,    \vj,    \vk
+.endm
+
+.macro xvdp2add.h.bu.b xd, xj, xk
+    xvmaddwev.h.bu.b   \xd,  \xj,    \xk
+    xvmaddwod.h.bu.b   \xd,  \xj,    \xk
+.endm
+
+.macro xvdp2add.w.h xd, xj, xk
+    xvmaddwev.w.h      \xd,  \xj,    \xk
+    xvmaddwod.w.h      \xd,  \xj,    \xk
+.endm
+
+/*
+ * Description : Range each element of vector
+ * clip: vj > vk ? vj : vk && vj < va ? vj : va
+ * clip255: vj < 255 ? vj : 255 && vj > 0 ? vj : 0
+ */
+.macro vclip.h  vd,  vj, vk, va
+    vmax.h    \vd,  \vj,   \vk
+    vmin.h    \vd,  \vd,   \va
+.endm
+
+.macro vclip255.w  vd, vj
+    vmaxi.w   \vd,   \vj,  0
+    vsat.wu   \vd,   \vd,  7
+.endm
+
+.macro vclip255.h  vd, vj
+    vmaxi.h   \vd,   \vj,  0
+    vsat.hu   \vd,   \vd,  7
+.endm
+
+.macro xvclip.h  xd,  xj, xk, xa
+    xvmax.h    \xd,  \xj,   \xk
+    xvmin.h    \xd,  \xd,   \xa
+.endm
+
+.macro xvclip255.h  xd, xj
+    xvmaxi.h   \xd,   \xj,  0
+    xvsat.hu   \xd,   \xd,  7
+.endm
+
+.macro xvclip255.w  xd, xj
+    xvmaxi.w   \xd,   \xj,  0
+    xvsat.wu   \xd,   \xd,  7
+.endm
+
+/*
+ * Description : Store elements of vector
+ * vd : Data vector to be stroed
+ * rk : Address of data storage
+ * ra : Offset of address
+ * si : Index of data in vd
+ */
+.macro vstelmx.b vd, rk, ra, si
+    add.d      \rk,  \rk,  \ra
+    vstelm.b   \vd,  \rk,  0, \si
+.endm
+
+.macro vstelmx.h vd, rk, ra, si
+    add.d      \rk,  \rk,  \ra
+    vstelm.h   \vd,  \rk,  0, \si
+.endm
+
+.macro vstelmx.w vd, rk, ra, si
+    add.d      \rk,  \rk,  \ra
+    vstelm.w   \vd,  \rk,  0, \si
+.endm
+
+.macro vstelmx.d  vd, rk, ra, si
+    add.d      \rk,  \rk,  \ra
+    vstelm.d   \vd,  \rk,  0, \si
+.endm
+
+.macro vmov xd, xj
+    vor.v  \xd,  \xj,  \xj
+.endm
+
+.macro xmov xd, xj
+    xvor.v  \xd,  \xj,  \xj
+.endm
+
+.macro xvstelmx.d  xd, rk, ra, si
+    add.d      \rk, \rk,  \ra
+    xvstelm.d  \xd, \rk,  0, \si
+.endm
+
+/*
+ *============================================================================
+ * LSX/LASX custom macros
+ *============================================================================
+ */
+
+/*
+ * Load 4 float, double, V128, v256 elements with stride.
+ */
+.macro FLDS_LOADX_4 src, stride, stride2, stride3, out0, out1, out2, out3
+    fld.s     \out0,    \src,    0
+    fldx.s    \out1,    \src,    \stride
+    fldx.s    \out2,    \src,    \stride2
+    fldx.s    \out3,    \src,    \stride3
+.endm
+
+.macro FLDD_LOADX_4 src, stride, stride2, stride3, out0, out1, out2, out3
+    fld.d     \out0,    \src,    0
+    fldx.d    \out1,    \src,    \stride
+    fldx.d    \out2,    \src,    \stride2
+    fldx.d    \out3,    \src,    \stride3
+.endm
+
+.macro LSX_LOADX_4 src, stride, stride2, stride3, out0, out1, out2, out3
+    vld     \out0,    \src,    0
+    vldx    \out1,    \src,    \stride
+    vldx    \out2,    \src,    \stride2
+    vldx    \out3,    \src,    \stride3
+.endm
+
+.macro LASX_LOADX_4 src, stride, stride2, stride3, out0, out1, out2, out3
+    xvld    \out0,    \src,    0
+    xvldx   \out1,    \src,    \stride
+    xvldx   \out2,    \src,    \stride2
+    xvldx   \out3,    \src,    \stride3
+.endm
+
+/*
+ * Description : Transpose 4x4 block with half-word elements in vectors
+ * Arguments   : Inputs  - in0, in1, in2, in3
+ *               Outputs - out0, out1, out2, out3
+ */
+.macro LSX_TRANSPOSE4x4_H in0, in1, in2, in3, out0, out1, out2, out3, \
+                          tmp0, tmp1
+    vilvl.h   \tmp0,  \in1,   \in0
+    vilvl.h   \tmp1,  \in3,   \in2
+    vilvl.w   \out0,  \tmp1,  \tmp0
+    vilvh.w   \out2,  \tmp1,  \tmp0
+    vilvh.d   \out1,  \out0,  \out0
+    vilvh.d   \out3,  \out0,  \out2
+.endm
+
+/*
+ * Description : Transpose 4x4 block with word elements in vectors
+ * Arguments   : Inputs  - in0, in1, in2, in3
+ *               Outputs - out0, out1, out2, out3
+ * Details     :
+ * Example     :
+ *               1, 2, 3, 4            1, 5, 9,13
+ *               5, 6, 7, 8    to      2, 6,10,14
+ *               9,10,11,12  =====>    3, 7,11,15
+ *              13,14,15,16            4, 8,12,16
+ */
+.macro LSX_TRANSPOSE4x4_W _in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3, \
+                          _tmp0, _tmp1
+
+    vilvl.w    \_tmp0,   \_in1,    \_in0
+    vilvh.w    \_out1,   \_in1,    \_in0
+    vilvl.w    \_tmp1,   \_in3,    \_in2
+    vilvh.w    \_out3,   \_in3,    \_in2
+
+    vilvl.d    \_out0,   \_tmp1,   \_tmp0
+    vilvl.d    \_out2,   \_out3,   \_out1
+    vilvh.d    \_out3,   \_out3,   \_out1
+    vilvh.d    \_out1,   \_tmp1,   \_tmp0
+.endm
+
+/*
+ * Description : Transpose 8x8 block with half-word elements in vectors
+ * Arguments   : Inputs  - in0, in1, in2, in3, in4, in5, in6, in7
+ *               Outputs - out0, out1, out2, out3, out4, out5, out6, out7
+ */
+.macro LSX_TRANSPOSE8x8_H in0, in1, in2, in3, in4, in5, in6, in7, out0, out1,   \
+                          out2, out3, out4, out5, out6, out7, tmp0, tmp1, tmp2, \
+                          tmp3, tmp4, tmp5, tmp6, tmp7
+    vilvl.h      \tmp0,    \in6,   \in4
+    vilvl.h      \tmp1,    \in7,   \in5
+    vilvl.h      \tmp2,    \in2,   \in0
+    vilvl.h      \tmp3,    \in3,   \in1
+
+    vilvl.h      \tmp4,    \tmp1,  \tmp0
+    vilvh.h      \tmp5,    \tmp1,  \tmp0
+    vilvl.h      \tmp6,    \tmp3,  \tmp2
+    vilvh.h      \tmp7,    \tmp3,  \tmp2
+
+    vilvh.h      \tmp0,    \in6,   \in4
+    vilvh.h      \tmp1,    \in7,   \in5
+    vilvh.h      \tmp2,    \in2,   \in0
+    vilvh.h      \tmp3,    \in3,   \in1
+
+    vpickev.d    \out0,    \tmp4,  \tmp6
+    vpickod.d    \out1,    \tmp4,  \tmp6
+    vpickev.d    \out2,    \tmp5,  \tmp7
+    vpickod.d    \out3,    \tmp5,  \tmp7
+
+    vilvl.h      \tmp4,    \tmp1,  \tmp0
+    vilvh.h      \tmp5,    \tmp1,  \tmp0
+    vilvl.h      \tmp6,    \tmp3,  \tmp2
+    vilvh.h      \tmp7,    \tmp3,  \tmp2
+
+    vpickev.d    \out4,    \tmp4,  \tmp6
+    vpickod.d    \out5,    \tmp4,  \tmp6
+    vpickev.d    \out6,    \tmp5,  \tmp7
+    vpickod.d    \out7,    \tmp5,  \tmp7
+.endm
+
+/*
+ * Description : Transpose 16x8 block with byte elements in vectors
+ * Arguments   : Inputs  - in0, in1, in2, in3, in4, in5, in6, in7
+ *               Outputs - out0, out1, out2, out3, out4, out5, out6, out7
+ */
+.macro LASX_TRANSPOSE16X8_B in0, in1, in2, in3, in4, in5, in6, in7,        \
+                            in8, in9, in10, in11, in12, in13, in14, in15,  \
+                            out0, out1, out2, out3, out4, out5, out6, out7,\
+                            tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7
+    xvilvl.b   \tmp0,    \in2,     \in0
+    xvilvl.b   \tmp1,    \in3,     \in1
+    xvilvl.b   \tmp2,    \in6,     \in4
+    xvilvl.b   \tmp3,    \in7,     \in5
+    xvilvl.b   \tmp4,    \in10,    \in8
+    xvilvl.b   \tmp5,    \in11,    \in9
+    xvilvl.b   \tmp6,    \in14,    \in12
+    xvilvl.b   \tmp7,    \in15,    \in13
+    xvilvl.b   \out0,    \tmp1,    \tmp0
+    xvilvh.b   \out1,    \tmp1,    \tmp0
+    xvilvl.b   \out2,    \tmp3,    \tmp2
+    xvilvh.b   \out3,    \tmp3,    \tmp2
+    xvilvl.b   \out4,    \tmp5,    \tmp4
+    xvilvh.b   \out5,    \tmp5,    \tmp4
+    xvilvl.b   \out6,    \tmp7,    \tmp6
+    xvilvh.b   \out7,    \tmp7,    \tmp6
+    xvilvl.w   \tmp0,    \out2,    \out0
+    xvilvh.w   \tmp2,    \out2,    \out0
+    xvilvl.w   \tmp4,    \out3,    \out1
+    xvilvh.w   \tmp6,    \out3,    \out1
+    xvilvl.w   \tmp1,    \out6,    \out4
+    xvilvh.w   \tmp3,    \out6,    \out4
+    xvilvl.w   \tmp5,    \out7,    \out5
+    xvilvh.w   \tmp7,    \out7,    \out5
+    xvilvl.d   \out0,    \tmp1,    \tmp0
+    xvilvh.d   \out1,    \tmp1,    \tmp0
+    xvilvl.d   \out2,    \tmp3,    \tmp2
+    xvilvh.d   \out3,    \tmp3,    \tmp2
+    xvilvl.d   \out4,    \tmp5,    \tmp4
+    xvilvh.d   \out5,    \tmp5,    \tmp4
+    xvilvl.d   \out6,    \tmp7,    \tmp6
+    xvilvh.d   \out7,    \tmp7,    \tmp6
+.endm
+
+/*
+ * Description : Transpose 16x8 block with byte elements in vectors
+ * Arguments   : Inputs  - in0, in1, in2, in3, in4, in5, in6, in7
+ *               Outputs - out0, out1, out2, out3, out4, out5, out6, out7
+ */
+.macro LSX_TRANSPOSE16X8_B in0, in1, in2, in3, in4, in5, in6, in7,        \
+                           in8, in9, in10, in11, in12, in13, in14, in15,  \
+                           out0, out1, out2, out3, out4, out5, out6, out7,\
+                           tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7
+    vilvl.b   \tmp0,    \in2,     \in0
+    vilvl.b   \tmp1,    \in3,     \in1
+    vilvl.b   \tmp2,    \in6,     \in4
+    vilvl.b   \tmp3,    \in7,     \in5
+    vilvl.b   \tmp4,    \in10,    \in8
+    vilvl.b   \tmp5,    \in11,    \in9
+    vilvl.b   \tmp6,    \in14,    \in12
+    vilvl.b   \tmp7,    \in15,    \in13
+
+    vilvl.b   \out0,    \tmp1,    \tmp0
+    vilvh.b   \out1,    \tmp1,    \tmp0
+    vilvl.b   \out2,    \tmp3,    \tmp2
+    vilvh.b   \out3,    \tmp3,    \tmp2
+    vilvl.b   \out4,    \tmp5,    \tmp4
+    vilvh.b   \out5,    \tmp5,    \tmp4
+    vilvl.b   \out6,    \tmp7,    \tmp6
+    vilvh.b   \out7,    \tmp7,    \tmp6
+    vilvl.w   \tmp0,    \out2,    \out0
+    vilvh.w   \tmp2,    \out2,    \out0
+    vilvl.w   \tmp4,    \out3,    \out1
+    vilvh.w   \tmp6,    \out3,    \out1
+    vilvl.w   \tmp1,    \out6,    \out4
+    vilvh.w   \tmp3,    \out6,    \out4
+    vilvl.w   \tmp5,    \out7,    \out5
+    vilvh.w   \tmp7,    \out7,    \out5
+    vilvl.d   \out0,    \tmp1,    \tmp0
+    vilvh.d   \out1,    \tmp1,    \tmp0
+    vilvl.d   \out2,    \tmp3,    \tmp2
+    vilvh.d   \out3,    \tmp3,    \tmp2
+    vilvl.d   \out4,    \tmp5,    \tmp4
+    vilvh.d   \out5,    \tmp5,    \tmp4
+    vilvl.d   \out6,    \tmp7,    \tmp6
+    vilvh.d   \out7,    \tmp7,    \tmp6
+.endm
+
+/*
+ * Description : Transpose 4x4 block with half-word elements in vectors
+ * Arguments   : Inputs  - in0, in1, in2, in3
+ *               Outputs - out0, out1, out2, out3
+ */
+.macro LASX_TRANSPOSE4x4_H in0, in1, in2, in3, out0, out1, out2, out3, \
+                           tmp0, tmp1
+    xvilvl.h   \tmp0,  \in1,   \in0
+    xvilvl.h   \tmp1,  \in3,   \in2
+    xvilvl.w   \out0,  \tmp1,  \tmp0
+    xvilvh.w   \out2,  \tmp1,  \tmp0
+    xvilvh.d   \out1,  \out0,  \out0
+    xvilvh.d   \out3,  \out0,  \out2
+.endm
+
+/*
+ * Description : Transpose 4x8 block with half-word elements in vectors
+ * Arguments   : Inputs  - in0, in1, in2, in3
+ *               Outputs - out0, out1, out2, out3
+ */
+.macro LASX_TRANSPOSE4x8_H in0, in1, in2, in3, out0, out1, out2, out3, \
+                           tmp0, tmp1
+    xvilvl.h      \tmp0,    \in2,   \in0
+    xvilvl.h      \tmp1,    \in3,   \in1
+    xvilvl.h      \out2,    \tmp1,  \tmp0
+    xvilvh.h      \out3,    \tmp1,  \tmp0
+
+    xvilvl.d      \out0,    \out2,  \out2
+    xvilvh.d      \out1,    \out2,  \out2
+    xvilvl.d      \out2,    \out3,  \out3
+    xvilvh.d      \out3,    \out3,  \out3
+.endm
+
+/*
+ * Description : Transpose 8x8 block with half-word elements in vectors
+ * Arguments   : Inputs  - in0, in1, in2, in3, in4, in5, in6, in7
+ *               Outputs - out0, out1, out2, out3, out4, out5, out6, out7
+ */
+.macro LASX_TRANSPOSE8x8_H in0, in1, in2, in3, in4, in5, in6, in7,         \
+                           out0, out1, out2, out3, out4, out5, out6, out7, \
+                           tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7
+    xvilvl.h     \tmp0,   \in6,     \in4
+    xvilvl.h     \tmp1,   \in7,     \in5
+    xvilvl.h     \tmp2,   \in2,     \in0
+    xvilvl.h     \tmp3,   \in3,     \in1
+
+    xvilvl.h     \tmp4,   \tmp1,    \tmp0
+    xvilvh.h     \tmp5,   \tmp1,    \tmp0
+    xvilvl.h     \tmp6,   \tmp3,    \tmp2
+    xvilvh.h     \tmp7,   \tmp3,    \tmp2
+
+    xvilvh.h     \tmp0,   \in6,     \in4
+    xvilvh.h     \tmp1,   \in7,     \in5
+    xvilvh.h     \tmp2,   \in2,     \in0
+    xvilvh.h     \tmp3,   \in3,     \in1
+
+    xvpickev.d   \out0,   \tmp4,    \tmp6
+    xvpickod.d   \out1,   \tmp4,    \tmp6
+    xvpickev.d   \out2,   \tmp5,    \tmp7
+    xvpickod.d   \out3,   \tmp5,    \tmp7
+
+    xvilvl.h     \tmp4,   \tmp1,    \tmp0
+    xvilvh.h     \tmp5,   \tmp1,    \tmp0
+    xvilvl.h     \tmp6,   \tmp3,    \tmp2
+    xvilvh.h     \tmp7,   \tmp3,    \tmp2
+
+    xvpickev.d   \out4,   \tmp4,    \tmp6
+    xvpickod.d   \out5,   \tmp4,    \tmp6
+    xvpickev.d   \out6,   \tmp5,    \tmp7
+    xvpickod.d   \out7,   \tmp5,    \tmp7
+.endm
+
+/*
+ * Description : Transpose 2x4x4 block with half-word elements in vectors
+ * Arguments   : Inputs  - in0, in1, in2, in3
+ *               Outputs - out0, out1, out2, out3
+ */
+.macro LASX_TRANSPOSE2x4x4_H in0, in1, in2, in3, out0, out1, out2, out3, \
+                             tmp0, tmp1, tmp2
+    xvilvh.h   \tmp1,    \in0,     \in1
+    xvilvl.h   \out1,    \in0,     \in1
+    xvilvh.h   \tmp0,    \in2,     \in3
+    xvilvl.h   \out3,    \in2,     \in3
+
+    xvilvh.w   \tmp2,    \out3,    \out1
+    xvilvl.w   \out3,    \out3,    \out1
+
+    xvilvl.w   \out2,    \tmp0,    \tmp1
+    xvilvh.w   \tmp1,    \tmp0,    \tmp1
+
+    xvilvh.d   \out0,    \out2,    \out3
+    xvilvl.d   \out2,    \out2,    \out3
+    xvilvh.d   \out1,    \tmp1,    \tmp2
+    xvilvl.d   \out3,    \tmp1,    \tmp2
+.endm
+
+/*
+ * Description : Transpose 4x4 block with word elements in vectors
+ * Arguments   : Inputs  - in0, in1, in2, in3
+ *               Outputs - out0, out1, out2, out3
+ * Details     :
+ * Example     :
+ *               1, 2, 3, 4,  1, 2, 3, 4        1,5, 9,13, 1,5, 9,13
+ *               5, 6, 7, 8,  5, 6, 7, 8   to   2,6,10,14, 2,6,10,14
+ *               9,10,11,12,  9,10,11,12 =====> 3,7,11,15, 3,7,11,15
+ *              13,14,15,16, 13,14,15,16        4,8,12,16, 4,8,12,16
+ */
+.macro LASX_TRANSPOSE4x4_W _in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3, \
+                           _tmp0, _tmp1
+
+    xvilvl.w    \_tmp0,   \_in1,    \_in0
+    xvilvh.w    \_out1,   \_in1,    \_in0
+    xvilvl.w    \_tmp1,   \_in3,    \_in2
+    xvilvh.w    \_out3,   \_in3,    \_in2
+
+    xvilvl.d    \_out0,   \_tmp1,   \_tmp0
+    xvilvl.d    \_out2,   \_out3,   \_out1
+    xvilvh.d    \_out3,   \_out3,   \_out1
+    xvilvh.d    \_out1,   \_tmp1,   \_tmp0
+.endm
+
+/*
+ * Description : Transpose 8x8 block with word elements in vectors
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7
+ *               Outputs - _out0, _out1, _out2, _out3, _out4, _out5, _out6,
+ *               _out7
+ * Example     : LASX_TRANSPOSE8x8_W
+ *        _in0 : 1,2,3,4,5,6,7,8
+ *        _in1 : 2,2,3,4,5,6,7,8
+ *        _in2 : 3,2,3,4,5,6,7,8
+ *        _in3 : 4,2,3,4,5,6,7,8
+ *        _in4 : 5,2,3,4,5,6,7,8
+ *        _in5 : 6,2,3,4,5,6,7,8
+ *        _in6 : 7,2,3,4,5,6,7,8
+ *        _in7 : 8,2,3,4,5,6,7,8
+ *
+ *       _out0 : 1,2,3,4,5,6,7,8
+ *       _out1 : 2,2,2,2,2,2,2,2
+ *       _out2 : 3,3,3,3,3,3,3,3
+ *       _out3 : 4,4,4,4,4,4,4,4
+ *       _out4 : 5,5,5,5,5,5,5,5
+ *       _out5 : 6,6,6,6,6,6,6,6
+ *       _out6 : 7,7,7,7,7,7,7,7
+ *       _out7 : 8,8,8,8,8,8,8,8
+ */
+.macro LASX_TRANSPOSE8x8_W _in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,\
+                           _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7,\
+                           _tmp0, _tmp1, _tmp2, _tmp3
+    xvilvl.w    \_tmp0,   \_in2,    \_in0
+    xvilvl.w    \_tmp1,   \_in3,    \_in1
+    xvilvh.w    \_tmp2,   \_in2,    \_in0
+    xvilvh.w    \_tmp3,   \_in3,    \_in1
+    xvilvl.w    \_out0,   \_tmp1,   \_tmp0
+    xvilvh.w    \_out1,   \_tmp1,   \_tmp0
+    xvilvl.w    \_out2,   \_tmp3,   \_tmp2
+    xvilvh.w    \_out3,   \_tmp3,   \_tmp2
+
+    xvilvl.w    \_tmp0,   \_in6,    \_in4
+    xvilvl.w    \_tmp1,   \_in7,    \_in5
+    xvilvh.w    \_tmp2,   \_in6,    \_in4
+    xvilvh.w    \_tmp3,   \_in7,    \_in5
+    xvilvl.w    \_out4,   \_tmp1,   \_tmp0
+    xvilvh.w    \_out5,   \_tmp1,   \_tmp0
+    xvilvl.w    \_out6,   \_tmp3,   \_tmp2
+    xvilvh.w    \_out7,   \_tmp3,   \_tmp2
+
+    xmov        \_tmp0,   \_out0
+    xmov        \_tmp1,   \_out1
+    xmov        \_tmp2,   \_out2
+    xmov        \_tmp3,   \_out3
+    xvpermi.q   \_out0,   \_out4,   0x02
+    xvpermi.q   \_out1,   \_out5,   0x02
+    xvpermi.q   \_out2,   \_out6,   0x02
+    xvpermi.q   \_out3,   \_out7,   0x02
+    xvpermi.q   \_out4,   \_tmp0,   0x31
+    xvpermi.q   \_out5,   \_tmp1,   0x31
+    xvpermi.q   \_out6,   \_tmp2,   0x31
+    xvpermi.q   \_out7,   \_tmp3,   0x31
+.endm
+
+/*
+ * Description : Transpose 4x4 block with double-word elements in vectors
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3
+ *               Outputs - _out0, _out1, _out2, _out3
+ * Example     : LASX_TRANSPOSE4x4_D
+ *        _in0 : 1,2,3,4
+ *        _in1 : 1,2,3,4
+ *        _in2 : 1,2,3,4
+ *        _in3 : 1,2,3,4
+ *
+ *       _out0 : 1,1,1,1
+ *       _out1 : 2,2,2,2
+ *       _out2 : 3,3,3,3
+ *       _out3 : 4,4,4,4
+ */
+.macro LASX_TRANSPOSE4x4_D _in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3, \
+                           _tmp0, _tmp1
+    xvilvl.d    \_tmp0,   \_in1,    \_in0
+    xvilvh.d    \_out1,   \_in1,    \_in0
+    xvilvh.d    \_tmp1,   \_in3,    \_in2
+    xvilvl.d    \_out2,   \_in3,    \_in2
+
+    xvor.v      \_out0,   \_tmp0,   \_tmp0
+    xvor.v      \_out3,   \_tmp1,   \_tmp1
+
+    xvpermi.q   \_out0,   \_out2,   0x02
+    xvpermi.q   \_out2,   \_tmp0,   0x31
+    xvpermi.q   \_out3,   \_out1,   0x31
+    xvpermi.q   \_out1,   \_tmp1,   0x02
+.endm
+
+/*
+ * Description : Butterfly of 4 input vectors
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3
+ *               Outputs - _out0, _out1, _out2, _out3
+ * Details     : Butterfly operation
+ * Example     : LSX_BUTTERFLY_4
+ *               _out0 = _in0 + _in3;
+ *               _out1 = _in1 + _in2;
+ *               _out2 = _in1 - _in2;
+ *               _out3 = _in0 - _in3;
+ */
+.macro LSX_BUTTERFLY_4_B _in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3
+    vadd.b   \_out0,   \_in0,   \_in3
+    vadd.b   \_out1,   \_in1,   \_in2
+    vsub.b   \_out2,   \_in1,   \_in2
+    vsub.b   \_out3,   \_in0,   \_in3
+.endm
+.macro LSX_BUTTERFLY_4_H _in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3
+    vadd.h   \_out0,   \_in0,   \_in3
+    vadd.h   \_out1,   \_in1,   \_in2
+    vsub.h   \_out2,   \_in1,   \_in2
+    vsub.h   \_out3,   \_in0,   \_in3
+.endm
+.macro LSX_BUTTERFLY_4_W _in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3
+    vadd.w   \_out0,   \_in0,   \_in3
+    vadd.w   \_out1,   \_in1,   \_in2
+    vsub.w   \_out2,   \_in1,   \_in2
+    vsub.w   \_out3,   \_in0,   \_in3
+.endm
+.macro LSX_BUTTERFLY_4_D _in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3
+    vadd.d   \_out0,   \_in0,   \_in3
+    vadd.d   \_out1,   \_in1,   \_in2
+    vsub.d   \_out2,   \_in1,   \_in2
+    vsub.d   \_out3,   \_in0,   \_in3
+.endm
+
+.macro LASX_BUTTERFLY_4_B _in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3
+    xvadd.b   \_out0,   \_in0,   \_in3
+    xvadd.b   \_out1,   \_in1,   \_in2
+    xvsub.b   \_out2,   \_in1,   \_in2
+    xvsub.b   \_out3,   \_in0,   \_in3
+.endm
+.macro LASX_BUTTERFLY_4_H _in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3
+    xvadd.h   \_out0,   \_in0,   \_in3
+    xvadd.h   \_out1,   \_in1,   \_in2
+    xvsub.h   \_out2,   \_in1,   \_in2
+    xvsub.h   \_out3,   \_in0,   \_in3
+.endm
+.macro LASX_BUTTERFLY_4_W _in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3
+    xvadd.w   \_out0,   \_in0,   \_in3
+    xvadd.w   \_out1,   \_in1,   \_in2
+    xvsub.w   \_out2,   \_in1,   \_in2
+    xvsub.w   \_out3,   \_in0,   \_in3
+.endm
+.macro LASX_BUTTERFLY_4_D _in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3
+    xvadd.d   \_out0,   \_in0,   \_in3
+    xvadd.d   \_out1,   \_in1,   \_in2
+    xvsub.d   \_out2,   \_in1,   \_in2
+    xvsub.d   \_out3,   \_in0,   \_in3
+.endm
+
+/*
+ * Description : Butterfly of 8 input vectors
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3, ~
+ *               Outputs - _out0, _out1, _out2, _out3, ~
+ * Details     : Butterfly operation
+ * Example     : LASX_BUTTERFLY_8
+ *               _out0 = _in0 + _in7;
+ *               _out1 = _in1 + _in6;
+ *               _out2 = _in2 + _in5;
+ *               _out3 = _in3 + _in4;
+ *               _out4 = _in3 - _in4;
+ *               _out5 = _in2 - _in5;
+ *               _out6 = _in1 - _in6;
+ *               _out7 = _in0 - _in7;
+ */
+.macro LSX_BUTTERFLY_8_B _in0,  _in1,  _in2,  _in3,  _in4,  _in5,  _in6,  _in7, \
+                         _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7
+    vadd.b    \_out0,    \_in0,    \_in7
+    vadd.b    \_out1,    \_in1,    \_in6
+    vadd.b    \_out2,    \_in2,    \_in5
+    vadd.b    \_out3,    \_in3,    \_in4
+    vsub.b    \_out4,    \_in3,    \_in4
+    vsub.b    \_out5,    \_in2,    \_in5
+    vsub.b    \_out6,    \_in1,    \_in6
+    vsub.b    \_out7,    \_in0,    \_in7
+.endm
+
+.macro LSX_BUTTERFLY_8_H _in0,  _in1,  _in2,  _in3,  _in4,  _in5,  _in6,  _in7, \
+                         _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7
+    vadd.h    \_out0,    \_in0,    \_in7
+    vadd.h    \_out1,    \_in1,    \_in6
+    vadd.h    \_out2,    \_in2,    \_in5
+    vadd.h    \_out3,    \_in3,    \_in4
+    vsub.h    \_out4,    \_in3,    \_in4
+    vsub.h    \_out5,    \_in2,    \_in5
+    vsub.h    \_out6,    \_in1,    \_in6
+    vsub.h    \_out7,    \_in0,    \_in7
+.endm
+
+.macro LSX_BUTTERFLY_8_W _in0,  _in1,  _in2,  _in3,  _in4,  _in5,  _in6,  _in7, \
+                         _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7
+    vadd.w    \_out0,    \_in0,    \_in7
+    vadd.w    \_out1,    \_in1,    \_in6
+    vadd.w    \_out2,    \_in2,    \_in5
+    vadd.w    \_out3,    \_in3,    \_in4
+    vsub.w    \_out4,    \_in3,    \_in4
+    vsub.w    \_out5,    \_in2,    \_in5
+    vsub.w    \_out6,    \_in1,    \_in6
+    vsub.w    \_out7,    \_in0,    \_in7
+.endm
+
+.macro LSX_BUTTERFLY_8_D _in0,  _in1,  _in2,  _in3,  _in4,  _in5,  _in6,  _in7, \
+                         _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7
+    vadd.d    \_out0,    \_in0,    \_in7
+    vadd.d    \_out1,    \_in1,    \_in6
+    vadd.d    \_out2,    \_in2,    \_in5
+    vadd.d    \_out3,    \_in3,    \_in4
+    vsub.d    \_out4,    \_in3,    \_in4
+    vsub.d    \_out5,    \_in2,    \_in5
+    vsub.d    \_out6,    \_in1,    \_in6
+    vsub.d    \_out7,    \_in0,    \_in7
+.endm
+
+.macro LASX_BUTTERFLY_8_B _in0,  _in1,  _in2,  _in3,  _in4,  _in5,  _in6,  _in7, \
+                          _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7
+    xvadd.b    \_out0,    \_in0,    \_in7
+    xvadd.b    \_out1,    \_in1,    \_in6
+    xvadd.b    \_out2,    \_in2,    \_in5
+    xvadd.b    \_out3,    \_in3,    \_in4
+    xvsub.b    \_out4,    \_in3,    \_in4
+    xvsub.b    \_out5,    \_in2,    \_in5
+    xvsub.b    \_out6,    \_in1,    \_in6
+    xvsub.b    \_out7,    \_in0,    \_in7
+.endm
+
+.macro LASX_BUTTERFLY_8_H _in0,  _in1,  _in2,  _in3,  _in4,  _in5,  _in6,  _in7, \
+                          _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7
+    xvadd.h    \_out0,    \_in0,    \_in7
+    xvadd.h    \_out1,    \_in1,    \_in6
+    xvadd.h    \_out2,    \_in2,    \_in5
+    xvadd.h    \_out3,    \_in3,    \_in4
+    xvsub.h    \_out4,    \_in3,    \_in4
+    xvsub.h    \_out5,    \_in2,    \_in5
+    xvsub.h    \_out6,    \_in1,    \_in6
+    xvsub.h    \_out7,    \_in0,    \_in7
+.endm
+
+.macro LASX_BUTTERFLY_8_W _in0,  _in1,  _in2,  _in3,  _in4,  _in5,  _in6,  _in7, \
+                          _out0, _out1, _out2, _out3, _out4, _out5, _out6, _out7
+    xvadd.w    \_out0,    \_in0,    \_in7
+    xvadd.w    \_out1,    \_in1,    \_in6
+    xvadd.w    \_out2,    \_in2,    \_in5
+    xvadd.w    \_out3,    \_in3,    \_in4
+    xvsub.w    \_out4,    \_in3,    \_in4
+    xvsub.w    \_out5,    \_in2,    \_in5
+    xvsub.w    \_out6,    \_in1,    \_in6
+    xvsub.w    \_out7,    \_in0,    \_in7
+.endm
+
diff --git a/libavcodec/loongarch/simple_idct_lasx.c b/libavcodec/loongarch/simple_idct_lasx.c
new file mode 100644
index 0000000000..a0d936b666
--- /dev/null
+++ b/libavcodec/loongarch/simple_idct_lasx.c
@@ -0,0 +1,297 @@
+/*
+ * Copyright (c) 2021 Loongson Technology Corporation Limited
+ * Contributed by Hao Chen <chenhao@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "libavutil/loongarch/loongson_intrinsics.h"
+#include "idctdsp_loongarch.h"
+
+#define LASX_TRANSPOSE4x16(in_0, in_1, in_2, in_3, out_0, out_1, out_2, out_3) \
+{                                                                              \
+    __m256i temp_0, temp_1, temp_2, temp_3;                                    \
+    __m256i temp_4, temp_5, temp_6, temp_7;                                    \
+    DUP4_ARG3(__lasx_xvpermi_q, in_2, in_0, 0x20, in_2, in_0, 0x31, in_3, in_1,\
+              0x20, in_3, in_1, 0x31, temp_0, temp_1, temp_2, temp_3);         \
+    DUP2_ARG2(__lasx_xvilvl_h, temp_1, temp_0, temp_3, temp_2, temp_4, temp_6);\
+    DUP2_ARG2(__lasx_xvilvh_h, temp_1, temp_0, temp_3, temp_2, temp_5, temp_7);\
+    DUP2_ARG2(__lasx_xvilvl_w, temp_6, temp_4, temp_7, temp_5, out_0, out_2);  \
+    DUP2_ARG2(__lasx_xvilvh_w, temp_6, temp_4, temp_7, temp_5, out_1, out_3);  \
+}
+
+#define LASX_IDCTROWCONDDC                                                     \
+    const_val  = 16383 * ((1 << 19) / 16383);                                  \
+    const_val1 = __lasx_xvreplgr2vr_w(const_val);                              \
+    DUP4_ARG2(__lasx_xvld, block, 0, block, 32, block, 64, block, 96,          \
+              in0, in1, in2, in3);                                             \
+    LASX_TRANSPOSE4x16(in0, in1, in2, in3, in0, in1, in2, in3);                \
+    a0 = __lasx_xvpermi_d(in0, 0xD8);                                          \
+    a0 = __lasx_vext2xv_w_h(a0);                                               \
+    temp  = __lasx_xvslli_w(a0, 3);                                            \
+    a1 = __lasx_xvpermi_d(in0, 0x8D);                                          \
+    a1 = __lasx_vext2xv_w_h(a1);                                               \
+    a2 = __lasx_xvpermi_d(in1, 0xD8);                                          \
+    a2 = __lasx_vext2xv_w_h(a2);                                               \
+    a3 = __lasx_xvpermi_d(in1, 0x8D);                                          \
+    a3 = __lasx_vext2xv_w_h(a3);                                               \
+    b0 = __lasx_xvpermi_d(in2, 0xD8);                                          \
+    b0 = __lasx_vext2xv_w_h(b0);                                               \
+    b1 = __lasx_xvpermi_d(in2, 0x8D);                                          \
+    b1 = __lasx_vext2xv_w_h(b1);                                               \
+    b2 = __lasx_xvpermi_d(in3, 0xD8);                                          \
+    b2 = __lasx_vext2xv_w_h(b2);                                               \
+    b3 = __lasx_xvpermi_d(in3, 0x8D);                                          \
+    b3 = __lasx_vext2xv_w_h(b3);                                               \
+    select_vec = a0 | a1 | a2 | a3 | b0 | b1 | b2 | b3;                        \
+    select_vec = __lasx_xvslti_wu(select_vec, 1);                              \
+                                                                               \
+    DUP4_ARG2(__lasx_xvrepl128vei_h, w1, 2, w1, 3, w1, 4, w1, 5,               \
+              w2, w3, w4, w5);                                                 \
+    DUP2_ARG2(__lasx_xvrepl128vei_h, w1, 6, w1, 7, w6, w7);                    \
+    w1 = __lasx_xvrepl128vei_h(w1, 1);                                         \
+                                                                               \
+    /* part of FUNC6(idctRowCondDC) */                                         \
+    temp0 = __lasx_xvmaddwl_w_h(const_val0, in0, w4);                          \
+    DUP2_ARG2(__lasx_xvmulwl_w_h, in1, w2, in1, w6, temp1, temp2);             \
+    a0    = __lasx_xvadd_w(temp0, temp1);                                      \
+    a1    = __lasx_xvadd_w(temp0, temp2);                                      \
+    a2    = __lasx_xvsub_w(temp0, temp2);                                      \
+    a3    = __lasx_xvsub_w(temp0, temp1);                                      \
+                                                                               \
+    DUP2_ARG2(__lasx_xvilvh_h, in1, in0, w3, w1, temp0, temp1);                \
+    b0 = __lasx_xvdp2_w_h(temp0, temp1);                                       \
+    temp1 = __lasx_xvneg_h(w7);                                                \
+    temp2 = __lasx_xvilvl_h(temp1, w3);                                        \
+    b1 = __lasx_xvdp2_w_h(temp0, temp2);                                       \
+    temp1 = __lasx_xvneg_h(w1);                                                \
+    temp2 = __lasx_xvilvl_h(temp1, w5);                                        \
+    b2 = __lasx_xvdp2_w_h(temp0, temp2);                                       \
+    temp1 = __lasx_xvneg_h(w5);                                                \
+    temp2 = __lasx_xvilvl_h(temp1, w7);                                        \
+    b3 = __lasx_xvdp2_w_h(temp0, temp2);                                       \
+                                                                               \
+    /* if (AV_RAN64A(row + 4)) */                                              \
+    DUP2_ARG2(__lasx_xvilvl_h, in3, in2, w6, w4, temp0, temp1);                \
+    a0 = __lasx_xvdp2add_w_h(a0, temp0, temp1);                                \
+    temp1 = __lasx_xvilvl_h(w2, w4);                                           \
+    a1 = __lasx_xvdp2sub_w_h(a1, temp0, temp1);                                \
+    temp1 = __lasx_xvneg_h(w4);                                                \
+    temp2 = __lasx_xvilvl_h(w2, temp1);                                        \
+    a2 = __lasx_xvdp2add_w_h(a2, temp0, temp2);                                \
+    temp1 = __lasx_xvneg_h(w6);                                                \
+    temp2 = __lasx_xvilvl_h(temp1, w4);                                        \
+    a3 = __lasx_xvdp2add_w_h(a3, temp0, temp2);                                \
+                                                                               \
+    DUP2_ARG2(__lasx_xvilvh_h, in3, in2, w7, w5, temp0, temp1);                \
+    b0 = __lasx_xvdp2add_w_h(b0, temp0, temp1);                                \
+    DUP2_ARG2(__lasx_xvilvl_h, w5, w1, w3, w7, temp1, temp2);                  \
+    b1 = __lasx_xvdp2sub_w_h(b1, temp0, temp1);                                \
+    b2 = __lasx_xvdp2add_w_h(b2, temp0, temp2);                                \
+    temp1 = __lasx_xvneg_h(w1);                                                \
+    temp2 = __lasx_xvilvl_h(temp1, w3);                                        \
+    b3 = __lasx_xvdp2add_w_h(b3, temp0, temp2);                                \
+                                                                               \
+    DUP4_ARG2(__lasx_xvadd_w, a0, b0, a1, b1, a2, b2, a3, b3,                  \
+              temp0, temp1, temp2, temp3);                                     \
+    DUP4_ARG2(__lasx_xvsub_w, a0, b0, a1, b1, a2, b2, a3, b3,                  \
+              a0, a1, a2, a3);                                                 \
+    DUP4_ARG2(__lasx_xvsrai_w, temp0, 11, temp1, 11, temp2, 11, temp3, 11,     \
+              temp0, temp1, temp2, temp3);                                     \
+    DUP4_ARG2(__lasx_xvsrai_w, a0, 11, a1, 11, a2, 11, a3, 11, a0, a1, a2, a3);\
+    DUP4_ARG3(__lasx_xvbitsel_v, temp0, temp, select_vec, temp1, temp,         \
+              select_vec, temp2, temp, select_vec, temp3, temp, select_vec,    \
+              in0, in1, in2, in3);                                             \
+    DUP4_ARG3(__lasx_xvbitsel_v, a0, temp, select_vec, a1, temp,               \
+              select_vec, a2, temp, select_vec, a3, temp, select_vec,          \
+              a0, a1, a2, a3);                                                 \
+    DUP4_ARG2(__lasx_xvpickev_h, in1, in0, in3, in2, a2, a3, a0, a1,           \
+              in0, in1, in2, in3);                                             \
+    DUP4_ARG2(__lasx_xvpermi_d, in0, 0xD8, in1, 0xD8, in2, 0xD8, in3, 0xD8,    \
+              in0, in1, in2, in3);                                             \
+
+#define LASX_IDCTCOLS                                                          \
+    /* part of FUNC6(idctSparaseCol) */                                        \
+    LASX_TRANSPOSE4x16(in0, in1, in2, in3, in0, in1, in2, in3);                \
+    temp0 = __lasx_xvmaddwl_w_h(const_val1, in0, w4);                          \
+    DUP2_ARG2(__lasx_xvmulwl_w_h, in1, w2, in1, w6, temp1, temp2);             \
+    a0    = __lasx_xvadd_w(temp0, temp1);                                      \
+    a1    = __lasx_xvadd_w(temp0, temp2);                                      \
+    a2    = __lasx_xvsub_w(temp0, temp2);                                      \
+    a3    = __lasx_xvsub_w(temp0, temp1);                                      \
+                                                                               \
+    DUP2_ARG2(__lasx_xvilvh_h, in1, in0, w3, w1, temp0, temp1);                \
+    b0 = __lasx_xvdp2_w_h(temp0, temp1);                                       \
+    temp1 = __lasx_xvneg_h(w7);                                                \
+    temp2 = __lasx_xvilvl_h(temp1, w3);                                        \
+    b1 = __lasx_xvdp2_w_h(temp0, temp2);                                       \
+    temp1 = __lasx_xvneg_h(w1);                                                \
+    temp2 = __lasx_xvilvl_h(temp1, w5);                                        \
+    b2 = __lasx_xvdp2_w_h(temp0, temp2);                                       \
+    temp1 = __lasx_xvneg_h(w5);                                                \
+    temp2 = __lasx_xvilvl_h(temp1, w7);                                        \
+    b3 = __lasx_xvdp2_w_h(temp0, temp2);                                       \
+                                                                               \
+    /* if (AV_RAN64A(row + 4)) */                                              \
+    DUP2_ARG2(__lasx_xvilvl_h, in3, in2, w6, w4, temp0, temp1);                \
+    a0 = __lasx_xvdp2add_w_h(a0, temp0, temp1);                                \
+    temp1 = __lasx_xvilvl_h(w2, w4);                                           \
+    a1 = __lasx_xvdp2sub_w_h(a1, temp0, temp1);                                \
+    temp1 = __lasx_xvneg_h(w4);                                                \
+    temp2 = __lasx_xvilvl_h(w2, temp1);                                        \
+    a2 = __lasx_xvdp2add_w_h(a2, temp0, temp2);                                \
+    temp1 = __lasx_xvneg_h(w6);                                                \
+    temp2 = __lasx_xvilvl_h(temp1, w4);                                        \
+    a3 = __lasx_xvdp2add_w_h(a3, temp0, temp2);                                \
+                                                                               \
+    DUP2_ARG2(__lasx_xvilvh_h, in3, in2, w7, w5, temp0, temp1);                \
+    b0 = __lasx_xvdp2add_w_h(b0, temp0, temp1);                                \
+    DUP2_ARG2(__lasx_xvilvl_h, w5, w1, w3, w7, temp1, temp2);                  \
+    b1 = __lasx_xvdp2sub_w_h(b1, temp0, temp1);                                \
+    b2 = __lasx_xvdp2add_w_h(b2, temp0, temp2);                                \
+    temp1 = __lasx_xvneg_h(w1);                                                \
+    temp2 = __lasx_xvilvl_h(temp1, w3);                                        \
+    b3 = __lasx_xvdp2add_w_h(b3, temp0, temp2);                                \
+                                                                               \
+    DUP4_ARG2(__lasx_xvadd_w, a0, b0, a1, b1, a2, b2, a3, b3,                  \
+              temp0, temp1, temp2, temp3);                                     \
+    DUP4_ARG2(__lasx_xvsub_w, a3, b3, a2, b2, a1, b1, a0, b0,                  \
+              a3, a2, a1, a0);                                                 \
+    DUP4_ARG3(__lasx_xvsrani_h_w, temp1, temp0, 20, temp3, temp2, 20, a2, a3,  \
+              20, a0, a1, 20, in0, in1, in2, in3);                             \
+
+void ff_simple_idct_lasx(int16_t *block)
+{
+    int32_t const_val = 1 << 10;
+    __m256i w1 = {0x4B42539F58C50000, 0x11A822A332493FFF,
+                  0x4B42539F58C50000, 0x11A822A332493FFF};
+    __m256i in0, in1, in2, in3;
+    __m256i w2, w3, w4, w5, w6, w7;
+    __m256i a0, a1, a2, a3;
+    __m256i b0, b1, b2, b3;
+    __m256i temp0, temp1, temp2, temp3;
+    __m256i const_val0 = __lasx_xvreplgr2vr_w(const_val);
+    __m256i const_val1, select_vec, temp;
+
+    LASX_IDCTROWCONDDC
+    LASX_IDCTCOLS
+    DUP4_ARG2(__lasx_xvpermi_d, in0, 0xD8, in1, 0xD8, in2, 0xD8, in3, 0xD8,
+              in0, in1, in2, in3);
+    __lasx_xvst(in0, block, 0);
+    __lasx_xvst(in1, block, 32);
+    __lasx_xvst(in2, block, 64);
+    __lasx_xvst(in3, block, 96);
+}
+
+void ff_simple_idct_put_lasx(uint8_t *dst, ptrdiff_t dst_stride,
+                             int16_t *block)
+{
+    int32_t const_val = 1 << 10;
+    ptrdiff_t dst_stride_2x = dst_stride << 1;
+    ptrdiff_t dst_stride_4x = dst_stride << 2;
+    ptrdiff_t dst_stride_3x = dst_stride_2x + dst_stride;
+    __m256i w1 = {0x4B42539F58C50000, 0x11A822A332493FFF,
+                  0x4B42539F58C50000, 0x11A822A332493FFF};
+    __m256i in0, in1, in2, in3;
+    __m256i w2, w3, w4, w5, w6, w7;
+    __m256i a0, a1, a2, a3;
+    __m256i b0, b1, b2, b3;
+    __m256i temp0, temp1, temp2, temp3;
+    __m256i const_val0 = __lasx_xvreplgr2vr_w(const_val);
+    __m256i const_val1, select_vec, temp;
+
+    LASX_IDCTROWCONDDC
+    LASX_IDCTCOLS
+    DUP4_ARG2(__lasx_xvpermi_d, in0, 0xD8, in1, 0xD8, in2, 0xD8, in3, 0xD8,
+              in0, in1, in2, in3);
+    DUP4_ARG1(__lasx_xvclip255_h, in0, in1, in2, in3, in0, in1, in2, in3);
+    DUP2_ARG2(__lasx_xvpickev_b, in1, in0, in3, in2, in0, in1);
+    __lasx_xvstelm_d(in0, dst, 0, 0);
+    __lasx_xvstelm_d(in0, dst + dst_stride, 0, 2);
+    __lasx_xvstelm_d(in0, dst + dst_stride_2x, 0, 1);
+    __lasx_xvstelm_d(in0, dst + dst_stride_3x, 0, 3);
+    dst += dst_stride_4x;
+    __lasx_xvstelm_d(in1, dst, 0, 0);
+    __lasx_xvstelm_d(in1, dst + dst_stride, 0, 2);
+    __lasx_xvstelm_d(in1, dst + dst_stride_2x, 0, 1);
+    __lasx_xvstelm_d(in1, dst + dst_stride_3x, 0, 3);
+}
+
+void ff_simple_idct_add_lasx(uint8_t *dst, ptrdiff_t dst_stride,
+                             int16_t *block)
+{
+    int32_t const_val = 1 << 10;
+    uint8_t *dst1 = dst;
+    ptrdiff_t dst_stride_2x = dst_stride << 1;
+    ptrdiff_t dst_stride_4x = dst_stride << 2;
+    ptrdiff_t dst_stride_3x = dst_stride_2x + dst_stride;
+
+    __m256i w1 = {0x4B42539F58C50000, 0x11A822A332493FFF,
+                  0x4B42539F58C50000, 0x11A822A332493FFF};
+    __m256i sh = {0x0003000200010000, 0x000B000A00090008,
+                  0x0007000600050004, 0x000F000E000D000C};
+    __m256i in0, in1, in2, in3;
+    __m256i w2, w3, w4, w5, w6, w7;
+    __m256i a0, a1, a2, a3;
+    __m256i b0, b1, b2, b3;
+    __m256i temp0, temp1, temp2, temp3;
+    __m256i const_val0 = __lasx_xvreplgr2vr_w(const_val);
+    __m256i const_val1, select_vec, temp;
+
+    LASX_IDCTROWCONDDC
+    LASX_IDCTCOLS
+    a0    = __lasx_xvldrepl_d(dst1, 0);
+    a0    = __lasx_vext2xv_hu_bu(a0);
+    dst1 += dst_stride;
+    a1    = __lasx_xvldrepl_d(dst1, 0);
+    a1    = __lasx_vext2xv_hu_bu(a1);
+    dst1 += dst_stride;
+    a2    = __lasx_xvldrepl_d(dst1, 0);
+    a2    = __lasx_vext2xv_hu_bu(a2);
+    dst1 += dst_stride;
+    a3    = __lasx_xvldrepl_d(dst1, 0);
+    a3    = __lasx_vext2xv_hu_bu(a3);
+    dst1 += dst_stride;
+    b0    = __lasx_xvldrepl_d(dst1, 0);
+    b0    = __lasx_vext2xv_hu_bu(b0);
+    dst1 += dst_stride;
+    b1    = __lasx_xvldrepl_d(dst1, 0);
+    b1    = __lasx_vext2xv_hu_bu(b1);
+    dst1 += dst_stride;
+    b2    = __lasx_xvldrepl_d(dst1, 0);
+    b2    = __lasx_vext2xv_hu_bu(b2);
+    dst1 += dst_stride;
+    b3    = __lasx_xvldrepl_d(dst1, 0);
+    b3    = __lasx_vext2xv_hu_bu(b3);
+    DUP4_ARG3(__lasx_xvshuf_h, sh, a1, a0, sh, a3, a2, sh, b1, b0, sh, b3, b2,
+              temp0, temp1, temp2, temp3);
+    DUP4_ARG2(__lasx_xvadd_h, temp0, in0, temp1, in1, temp2, in2, temp3, in3,
+              in0, in1, in2, in3);
+    DUP4_ARG2(__lasx_xvpermi_d, in0, 0xD8, in1, 0xD8, in2, 0xD8, in3, 0xD8,
+              in0, in1, in2, in3);
+    DUP4_ARG1(__lasx_xvclip255_h, in0, in1, in2, in3, in0, in1, in2, in3);
+    DUP2_ARG2(__lasx_xvpickev_b, in1, in0, in3, in2, in0, in1);
+    __lasx_xvstelm_d(in0, dst, 0, 0);
+    __lasx_xvstelm_d(in0, dst + dst_stride, 0, 2);
+    __lasx_xvstelm_d(in0, dst + dst_stride_2x, 0, 1);
+    __lasx_xvstelm_d(in0, dst + dst_stride_3x, 0, 3);
+    dst += dst_stride_4x;
+    __lasx_xvstelm_d(in1, dst, 0, 0);
+    __lasx_xvstelm_d(in1, dst + dst_stride, 0, 2);
+    __lasx_xvstelm_d(in1, dst + dst_stride_2x, 0, 1);
+    __lasx_xvstelm_d(in1, dst + dst_stride_3x, 0, 3);
+}
diff --git a/libavcodec/loongarch/vc1dsp_init_loongarch.c b/libavcodec/loongarch/vc1dsp_init_loongarch.c
new file mode 100644
index 0000000000..e72a4a3203
--- /dev/null
+++ b/libavcodec/loongarch/vc1dsp_init_loongarch.c
@@ -0,0 +1,67 @@
+/*
+ * Copyright (c) 2021 Loongson Technology Corporation Limited
+ * Contributed by Hao Chen <chenhao@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "libavutil/loongarch/cpu.h"
+#include "libavutil/attributes.h"
+#include "libavcodec/vc1dsp.h"
+#include "vc1dsp_loongarch.h"
+
+#define FN_ASSIGN(OP, X, Y, INSN) \
+    dsp->OP##vc1_mspel_pixels_tab[1][X+4*Y] = ff_##OP##vc1_mspel_mc##X##Y##INSN; \
+    dsp->OP##vc1_mspel_pixels_tab[0][X+4*Y] = ff_##OP##vc1_mspel_mc##X##Y##_16##INSN
+
+#define FN_ASSIGN_V(OP, Y, INSN) \
+    dsp->OP##vc1_mspel_pixels_tab[0][4*Y] = ff_##OP##vc1_mspel_mc0##Y##_16##INSN
+
+#define FN_ASSIGN_H(OP, X, INSN) \
+    dsp->OP##vc1_mspel_pixels_tab[0][X] = ff_##OP##vc1_mspel_mc##X##0_16##INSN
+
+av_cold void ff_vc1dsp_init_loongarch(VC1DSPContext *dsp)
+{
+    int cpu_flags = av_get_cpu_flags();
+
+    if (have_lasx(cpu_flags)) {
+        dsp->vc1_inv_trans_8x8    = ff_vc1_inv_trans_8x8_lasx;
+        dsp->vc1_inv_trans_4x8    = ff_vc1_inv_trans_4x8_lasx;
+        dsp->vc1_inv_trans_8x4    = ff_vc1_inv_trans_8x4_lasx;
+        dsp->vc1_inv_trans_4x4    = ff_vc1_inv_trans_4x4_lasx;
+        dsp->vc1_inv_trans_8x8_dc = ff_vc1_inv_trans_8x8_dc_lasx;
+        dsp->vc1_inv_trans_4x8_dc = ff_vc1_inv_trans_4x8_dc_lasx;
+        dsp->vc1_inv_trans_8x4_dc = ff_vc1_inv_trans_8x4_dc_lasx;
+        dsp->vc1_inv_trans_4x4_dc = ff_vc1_inv_trans_4x4_dc_lasx;
+        FN_ASSIGN(put_, 1, 1, _lasx);
+        FN_ASSIGN(put_, 1, 2, _lasx);
+        FN_ASSIGN(put_, 1, 3, _lasx);
+        FN_ASSIGN(put_, 2, 1, _lasx);
+        FN_ASSIGN(put_, 2, 2, _lasx);
+        FN_ASSIGN(put_, 2, 3, _lasx);
+        FN_ASSIGN(put_, 3, 1, _lasx);
+        FN_ASSIGN(put_, 3, 2, _lasx);
+        FN_ASSIGN(put_, 3, 3, _lasx);
+        FN_ASSIGN_V(put_, 1, _lasx);
+        FN_ASSIGN_V(put_, 2, _lasx);
+        FN_ASSIGN_V(put_, 3, _lasx);
+        FN_ASSIGN_H(put_, 1, _lasx);
+        FN_ASSIGN_H(put_, 2, _lasx);
+        FN_ASSIGN_H(put_, 3, _lasx);
+        dsp->put_no_rnd_vc1_chroma_pixels_tab[0] = ff_put_no_rnd_vc1_chroma_mc8_lasx;
+    }
+}
diff --git a/libavcodec/loongarch/vc1dsp_lasx.c b/libavcodec/loongarch/vc1dsp_lasx.c
new file mode 100644
index 0000000000..40b8668f2b
--- /dev/null
+++ b/libavcodec/loongarch/vc1dsp_lasx.c
@@ -0,0 +1,1005 @@
+/*
+ * Copyright (c) 2021 Loongson Technology Corporation Limited
+ * Contributed by Hao Chen <chenhao@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "vc1dsp_loongarch.h"
+#include "libavutil/loongarch/loongson_intrinsics.h"
+
+void ff_vc1_inv_trans_8x8_lasx(int16_t block[64])
+{
+    int32_t con_4    = 4;
+    int32_t con_64   = 64;
+    __m256i in0, in1, in2, in3;
+    __m256i temp0, temp1, temp2, temp3, t1, t2, t3, t4, t5, t6, t7, t8;
+    __m256i const_1  = {0x000c000c000c000c, 0x000c000c000c000c,
+                        0x000c000c000c000c, 0x000c000c000c000c};
+    __m256i const_2  = {0xfff4000cfff4000c, 0xfff4000cfff4000c,
+                        0xfff4000cfff4000c, 0xfff4000cfff4000c};
+    __m256i const_3  = {0x0006001000060010, 0x0006001000060010,
+                        0x0006001000060010, 0x0006001000060010};
+    __m256i const_4  = {0xfff00006fff00006, 0xfff00006fff00006,
+                        0xfff00006fff00006, 0xfff00006fff00006};
+    __m256i const_5  = {0x000f0010000f0010, 0x000f0010000f0010,
+                        0x000f0010000f0010, 0x000f0010000f0010};
+    __m256i const_6  = {0x0004000900040009, 0x0004000900040009,
+                        0x0004000900040009, 0x0004000900040009};
+    __m256i const_7  = {0xfffc000ffffc000f, 0xfffc000ffffc000f,
+                        0xfffc000ffffc000f, 0xfffc000ffffc000f};
+    __m256i const_8  = {0xfff7fff0fff7fff0, 0xfff7fff0fff7fff0,
+                        0xfff7fff0fff7fff0, 0xfff7fff0fff7fff0};
+    __m256i const_9  = {0xfff00009fff00009, 0xfff00009fff00009,
+                        0xfff00009fff00009, 0xfff00009fff00009};
+    __m256i const_10 = {0x000f0004000f0004, 0x000f0004000f0004,
+                        0x000f0004000f0004, 0x000f0004000f0004};
+    __m256i const_11 = {0xfff70004fff70004, 0xfff70004fff70004,
+                        0xfff70004fff70004, 0xfff70004fff70004};
+    __m256i const_12 = {0xfff0000ffff0000f, 0xfff0000ffff0000f,
+                        0xfff0000ffff0000f, 0xfff0000ffff0000f};
+
+    DUP4_ARG2(__lasx_xvld, block, 0, block, 32, block, 64, block, 96,
+              in0, in1, in2, in3);
+    DUP4_ARG2(__lasx_xvpermi_d, in0, 0xD8, in1, 0xD8, in2, 0xD8, in3, 0xD8,
+              in0, in1, in2, in3);
+    /* first loops */
+    DUP2_ARG2(__lasx_xvilvl_h, in2, in0, in3, in1, temp0, temp1);
+    t2 = __lasx_xvreplgr2vr_w(con_4);
+    DUP2_ARG3(__lasx_xvdp2add_w_h, t2, temp0, const_1, t2, temp0,
+              const_2, t1, t2);
+    DUP2_ARG2(__lasx_xvdp2_w_h, temp1, const_3, temp1, const_4, t3, t4);
+
+    t5 = __lasx_xvadd_w(t1, t3);
+    t6 = __lasx_xvadd_w(t2, t4);
+    t7 = __lasx_xvsub_w(t2, t4);
+    t8 = __lasx_xvsub_w(t1, t3);
+
+    DUP2_ARG2(__lasx_xvilvh_h, in1, in0, in3, in2, temp0, temp1);
+    temp2 = __lasx_xvdp2_w_h(const_5, temp0);
+    t1 = __lasx_xvdp2add_w_h(temp2, temp1, const_6);
+    temp2 = __lasx_xvdp2_w_h(const_7, temp0);
+    t2 = __lasx_xvdp2add_w_h(temp2, temp1, const_8);
+    temp2 = __lasx_xvdp2_w_h(const_9, temp0);
+    t3 = __lasx_xvdp2add_w_h(temp2, temp1, const_10);
+    temp2 = __lasx_xvdp2_w_h(const_11, temp0);
+    t4 = __lasx_xvdp2add_w_h(temp2, temp1, const_12);
+
+    DUP4_ARG2(__lasx_xvadd_w, t1, t5, t6, t2, t7, t3, t8, t4,
+              temp0, temp1, temp2, temp3);
+    DUP4_ARG2(__lasx_xvsub_w, t8, t4, t7, t3, t6, t2, t5, t1,
+              in0, in1, in2, in3);
+    DUP4_ARG2(__lasx_xvsrai_w, temp0, 3, temp1, 3, temp2, 3, temp3, 3,
+              temp0, temp1, temp2, temp3);
+    DUP4_ARG2(__lasx_xvsrai_w, in0, 3, in1, 3, in2, 3, in3, 3,
+              in0, in1, in2, in3);
+
+    /* second loops */
+    DUP4_ARG2(__lasx_xvpackev_h, temp1, temp0, temp3, temp2, in1, in0,
+              in3, in2, temp0, temp1, temp2, temp3);
+    DUP2_ARG2(__lasx_xvilvl_w, temp1, temp0, temp3, temp2, t1, t3);
+    DUP2_ARG2(__lasx_xvilvh_w, temp1, temp0, temp3, temp2, t2, t4);
+    DUP4_ARG3(__lasx_xvpermi_q, t3, t1, 0x20, t3, t1, 0x31, t4, t2, 0x20,
+              t4, t2, 0x31, in0, in1, in2, in3);
+    DUP2_ARG2(__lasx_xvilvl_h, in1, in0, in3, in2, temp0, temp1);
+    t3    = __lasx_xvreplgr2vr_w(con_64);
+    DUP2_ARG3(__lasx_xvdp2add_w_h, t3, temp0, const_1, t3, temp0,
+              const_2, t1, t2);
+    DUP2_ARG2(__lasx_xvdp2_w_h, temp1, const_3, temp1, const_4, t3, t4);
+
+    t5    = __lasx_xvadd_w(t1, t3);
+    t6    = __lasx_xvadd_w(t2, t4);
+    t7    = __lasx_xvsub_w(t2, t4);
+    t8    = __lasx_xvsub_w(t1, t3);
+
+    DUP2_ARG2(__lasx_xvilvh_h, in2, in0, in3, in1, temp0, temp1);
+    temp2 = __lasx_xvdp2_w_h(const_5, temp0);
+    t1 = __lasx_xvdp2add_w_h(temp2, temp1, const_6);
+    temp2 = __lasx_xvdp2_w_h(const_7, temp0);
+    t2 = __lasx_xvdp2add_w_h(temp2, temp1, const_8);
+    temp2 = __lasx_xvdp2_w_h(const_9, temp0);
+    t3 = __lasx_xvdp2add_w_h(temp2, temp1, const_10);
+    temp2 = __lasx_xvdp2_w_h(const_11, temp0);
+    t4 = __lasx_xvdp2add_w_h(temp2, temp1, const_12);
+
+    DUP4_ARG2(__lasx_xvadd_w, t5, t1, t6, t2, t7, t3, t8, t4,
+              temp0, temp1, temp2, temp3);
+    DUP4_ARG2(__lasx_xvsub_w, t8, t4, t7, t3, t6, t2, t5, t1,
+              in0, in1, in2, in3);
+    DUP4_ARG2(__lasx_xvaddi_wu, in0, 1, in1, 1, in2, 1, in3, 1,
+              in0, in1, in2, in3);
+    DUP4_ARG3(__lasx_xvsrani_h_w, temp1, temp0, 7, temp3, temp2, 7,
+              in1, in0, 7, in3, in2, 7, t1, t2, t3, t4);
+    DUP4_ARG2(__lasx_xvpermi_d, t1, 0xD8, t2, 0xD8, t3, 0xD8, t4, 0xD8,
+              in0, in1, in2, in3);
+    __lasx_xvst(in0, block, 0);
+    __lasx_xvst(in1, block, 32);
+    __lasx_xvst(in2, block, 64);
+    __lasx_xvst(in3, block, 96);
+}
+
+void ff_vc1_inv_trans_8x8_dc_lasx(uint8_t *dest, ptrdiff_t stride,
+                                  int16_t *block)
+{
+    int dc = block[0];
+    ptrdiff_t stride2 = stride << 1;
+    ptrdiff_t stride3 = stride2 + stride;
+    uint8_t *dst = dest + (stride2 << 1);
+    __m256i in0, in1, in2, in3, in4, in5, in6, in7;
+    __m256i const_dc, temp0, temp1, temp2, temp3;
+    __m256i reg0, reg1, reg2, reg3;
+
+    dc = (3 * dc +  1) >> 1;
+    dc = (3 * dc + 16) >> 5;
+
+    const_dc = __lasx_xvreplgr2vr_h(dc);
+    DUP4_ARG2(__lasx_xvldrepl_d, dest, 0, dest + stride, 0, dest + stride2,
+              0, dest + stride3, 0, in0, in1, in2, in3);
+    DUP4_ARG2(__lasx_xvldrepl_d, dst, 0, dst + stride, 0, dst + stride2,
+              0, dst + stride3, 0, in4, in5, in6, in7);
+
+    DUP4_ARG2(__lasx_xvilvl_d, in1, in0, in3, in2, in5, in4, in7, in6,
+              temp0, temp1, temp2, temp3);
+    DUP4_ARG1(__lasx_vext2xv_hu_bu, temp0, temp1, temp2, temp3,
+              temp0, temp1, temp2, temp3);
+
+    DUP4_ARG2(__lasx_xvadd_h, temp0, const_dc, temp1, const_dc, temp2,
+              const_dc, temp3, const_dc, reg0, reg1, reg2, reg3);
+    DUP2_ARG3(__lasx_xvssrarni_bu_h, reg1, reg0, 0, reg3, reg2, 0,
+              temp0, temp1);
+    __lasx_xvstelm_d(temp0, dest, 0, 0);
+    __lasx_xvstelm_d(temp0, dest + stride, 0, 2);
+    __lasx_xvstelm_d(temp0, dest + stride2, 0, 1);
+    __lasx_xvstelm_d(temp0, dest + stride3, 0, 3);
+    __lasx_xvstelm_d(temp1, dst, 0, 0);
+    __lasx_xvstelm_d(temp1, dst + stride, 0, 2);
+    __lasx_xvstelm_d(temp1, dst + stride2, 0, 1);
+    __lasx_xvstelm_d(temp1, dst + stride3, 0, 3);
+}
+
+void ff_vc1_inv_trans_8x4_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *block)
+{
+    ptrdiff_t stride2 = stride << 1;
+    ptrdiff_t stride3 = stride2 + stride;
+    __m256i shift    = {0x0000000400000000, 0x0000000500000001,
+                        0x0000000600000002, 0x0000000700000003};
+    __m256i const_64 = {0x0000004000000040, 0x0000004000000040,
+                        0x0000004000000040, 0x0000004000000040};
+    __m256i const_1  = {0x00060010000C000C, 0x00060010000C000C,
+                        0x00060010000C000C, 0x00060010000C000C};
+    __m256i const_2  = {0xFFF00006FFF4000C, 0xFFF00006FFF4000C,
+                        0xFFF00006FFF4000C, 0xFFF00006FFF4000C};
+    __m256i const_3  = {0x0004000F00090010, 0x0004000F00090010,
+                        0x0004000F00090010, 0x0004000F00090010};
+    __m256i const_4  = {0xFFF7FFFCFFF0000F, 0xFFF7FFFCFFF0000F,
+                        0xFFF7FFFCFFF0000F, 0xFFF7FFFCFFF0000F};
+    __m256i const_5  = {0x000FFFF000040009, 0x000FFFF000040009,
+                        0x000FFFF000040009, 0x000FFFF000040009};
+    __m256i const_6  = {0xFFF0FFF7000F0004, 0xFFF0FFF7000F0004,
+                        0xFFF0FFF7000F0004, 0xFFF0FFF7000F0004};
+    __m256i const_7  = {0x0000000000000004, 0x0000000000000004,
+                        0x0000000000000004, 0x0000000000000004};
+    __m256i const_8  = {0x0011001100110011, 0x0011001100110011,
+                        0x0011001100110011, 0x0011001100110011};
+    __m256i const_9  = {0xFFEF0011FFEF0011, 0xFFEF0011FFEF0011,
+                        0xFFEF0011FFEF0011, 0xFFEF0011FFEF0011};
+    __m256i const_10 = {0x000A0016000A0016, 0x000A0016000A0016,
+                        0x000A0016000A0016, 0x000A0016000A0016};
+    __m256i const_11 = {0x0016FFF60016FFF6, 0x0016FFF60016FFF6,
+                        0x0016FFF60016FFF6, 0x0016FFF60016FFF6};
+    __m256i in0, in1;
+    __m256i temp0, temp1, temp2, temp3, t1, t2, t3, t4;
+
+    DUP2_ARG2(__lasx_xvld, block, 0, block, 32, in0, in1);
+    /* first loops */
+    temp0 = __lasx_xvpermi_d(in0, 0xB1);
+    temp1 = __lasx_xvpermi_d(in1, 0xB1);
+    DUP2_ARG2(__lasx_xvilvl_h, temp0, in0, temp1, in1, temp0, temp1);
+    temp2 = __lasx_xvpickev_w(temp1, temp0);
+    temp3 = __lasx_xvpickod_w(temp1, temp0);
+
+    DUP2_ARG2(__lasx_xvdp2_w_h, temp2, const_1, temp2, const_2, temp0, temp1);
+    t1    = __lasx_xvadd_w(temp0, const_7);
+    t2    = __lasx_xvadd_w(temp1, const_7);
+    temp0 = __lasx_xvpickev_w(t2, t1);
+    temp1 = __lasx_xvpickod_w(t2, t1);
+    t3    = __lasx_xvadd_w(temp0, temp1);
+    t4    = __lasx_xvsub_w(temp0, temp1);
+    t4    = __lasx_xvpermi_d(t4, 0xB1);
+
+    DUP4_ARG2(__lasx_xvdp4_d_h, temp3, const_3, temp3, const_4, temp3,
+              const_5, temp3, const_6, t1, t2, temp0, temp1);
+    temp2 = __lasx_xvpickev_w(t2, t1);
+    temp3 = __lasx_xvpickev_w(temp1, temp0);
+
+    t1    = __lasx_xvadd_w(temp2, t3);
+    t2    = __lasx_xvadd_w(temp3, t4);
+    temp0 = __lasx_xvsub_w(t4, temp3);
+    temp1 = __lasx_xvsub_w(t3, temp2);
+    /* second loops */
+    DUP2_ARG3(__lasx_xvsrani_h_w, t2, t1, 3, temp1, temp0, 3, temp2, temp3);
+    temp3 = __lasx_xvshuf4i_h(temp3, 0x4E);
+    temp0 = __lasx_xvpermi_q(temp3, temp2, 0x20);
+    temp1 = __lasx_xvpermi_q(temp3, temp2, 0x31);
+    DUP2_ARG3(__lasx_xvdp2add_w_h, const_64, temp0, const_8, const_64, temp0,
+              const_9, t1, t2);
+    DUP2_ARG2(__lasx_xvdp2_w_h, temp1, const_10, temp1, const_11, t3, t4);
+    temp0 = __lasx_xvadd_w(t1, t3);
+    temp1 = __lasx_xvsub_w(t2, t4);
+    temp2 = __lasx_xvadd_w(t2, t4);
+    temp3 = __lasx_xvsub_w(t1, t3);
+    DUP4_ARG2(__lasx_xvsrai_w, temp0, 7, temp1, 7, temp2, 7, temp3, 7,
+              t1, t2, t3, t4);
+
+    temp0 = __lasx_xvldrepl_d(dest, 0);
+    DUP4_ARG2(__lasx_xvldrepl_d, dest, 0, dest + stride, 0, dest + stride2, 0,
+              dest + stride3, 0, temp0, temp1, temp2, temp3);
+    DUP4_ARG1(__lasx_vext2xv_wu_bu, temp0, temp1, temp2, temp3,
+              temp0, temp1, temp2, temp3);
+    DUP4_ARG2(__lasx_xvadd_w, temp0, t1, temp1, t2, temp2, t3, temp3, t4,
+              t1, t2, t3, t4);
+    DUP4_ARG1(__lasx_xvclip255_w, t1, t2, t3, t4, t1, t2, t3, t4);
+    DUP2_ARG2(__lasx_xvpickev_h, t2, t1, t4, t3, temp0, temp1);
+    temp2 = __lasx_xvpickev_b(temp1, temp0);
+    temp0 = __lasx_xvperm_w(temp2, shift);
+    __lasx_xvstelm_d(temp0, dest, 0, 0);
+    __lasx_xvstelm_d(temp0, dest + stride, 0, 1);
+    __lasx_xvstelm_d(temp0, dest + stride2, 0, 2);
+    __lasx_xvstelm_d(temp0, dest + stride3, 0, 3);
+}
+
+void ff_vc1_inv_trans_8x4_dc_lasx(uint8_t *dest, ptrdiff_t stride,
+                                  int16_t *block)
+{
+    int dc = block[0];
+    ptrdiff_t stride2 = stride << 1;
+    ptrdiff_t stride3 = stride2 + stride;
+    __m256i in0, in1, in2, in3;
+    __m256i const_dc, temp0, temp1, reg0, reg1;
+
+    dc = (3  * dc + 1) >> 1;
+    dc = (17 * dc + 64) >> 7;
+    const_dc = __lasx_xvreplgr2vr_h(dc);
+
+    DUP4_ARG2(__lasx_xvldrepl_d, dest, 0, dest + stride, 0, dest + stride2,
+              0, dest + stride3, 0, in0, in1, in2, in3);
+    DUP2_ARG2(__lasx_xvilvl_d, in1, in0, in3, in2, temp0, temp1);
+    DUP2_ARG1(__lasx_vext2xv_hu_bu, temp0, temp1, temp0, temp1);
+    DUP2_ARG2(__lasx_xvadd_h, temp0, const_dc, temp1, const_dc, reg0, reg1);
+    temp0 = __lasx_xvssrarni_bu_h(reg1, reg0, 0);
+    __lasx_xvstelm_d(temp0, dest, 0, 0);
+    __lasx_xvstelm_d(temp0, dest + stride, 0, 2);
+    __lasx_xvstelm_d(temp0, dest + stride2, 0, 1);
+    __lasx_xvstelm_d(temp0, dest + stride3, 0, 3);
+}
+
+void ff_vc1_inv_trans_4x8_dc_lasx(uint8_t *dest, ptrdiff_t stride,
+                                  int16_t *block)
+{
+    int dc = block[0];
+    ptrdiff_t stride2 = stride << 1;
+    ptrdiff_t stride3 = stride2 + stride;
+    uint8_t *dst = dest + (stride2 << 1);
+    __m256i in0, in1, in2, in3, in4, in5, in6, in7;
+    __m256i const_dc, temp0, temp1, temp2, temp3, reg0, reg1;
+
+    dc = (17 * dc +  4) >> 3;
+    dc = (12 * dc + 64) >> 7;
+    const_dc = __lasx_xvreplgr2vr_h(dc);
+
+    DUP4_ARG2(__lasx_xvldrepl_w, dest, 0, dest + stride, 0, dest + stride2,
+              0, dest + stride3, 0, in0, in1, in2, in3);
+    DUP4_ARG2(__lasx_xvldrepl_w, dst, 0, dst + stride, 0, dst + stride2,
+              0, dst + stride3, 0, in4, in5, in6, in7);
+
+    DUP4_ARG2(__lasx_xvilvl_w, in1, in0, in3, in2, in5, in4, in7, in6,
+              temp0, temp1, temp2, temp3);
+    DUP2_ARG2(__lasx_xvilvl_d, temp1, temp0, temp3, temp2, reg0, reg1);
+    DUP2_ARG1(__lasx_vext2xv_hu_bu, reg0, reg1, temp0, temp1);
+    DUP2_ARG2(__lasx_xvadd_h, temp0, const_dc, temp1, const_dc, reg0, reg1);
+    temp0 = __lasx_xvssrarni_bu_h(reg1, reg0, 0);
+    __lasx_xvstelm_w(temp0, dest, 0, 0);
+    __lasx_xvstelm_w(temp0, dest + stride, 0, 1);
+    __lasx_xvstelm_w(temp0, dest + stride2, 0, 4);
+    __lasx_xvstelm_w(temp0, dest + stride3, 0, 5);
+    __lasx_xvstelm_w(temp0, dst, 0, 2);
+    __lasx_xvstelm_w(temp0, dst + stride, 0, 3);
+    __lasx_xvstelm_w(temp0, dst + stride2, 0, 6);
+    __lasx_xvstelm_w(temp0, dst + stride3, 0, 7);
+}
+
+void ff_vc1_inv_trans_4x8_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *block)
+{
+    ptrdiff_t stride2 = stride << 1;
+    ptrdiff_t stride3 = stride2 + stride;
+    uint8_t *dst = dest + (stride2 << 1);
+    __m256i in0, in1, in2, in3;
+    __m256i temp0, temp1, temp2, temp3, t1, t2, t3, t4;
+
+    __m256i const_1  = {0x0011001100110011, 0x0011001100110011,
+                        0x0011001100110011, 0x0011001100110011};
+    __m256i const_2  = {0xFFEF0011FFEF0011, 0xFFEF0011FFEF0011,
+                        0xFFEF0011FFEF0011, 0xFFEF0011FFEF0011};
+    __m256i const_3  = {0x000A0016000A0016, 0x000A0016000A0016,
+                        0x000A0016000A0016, 0x000A0016000A0016};
+    __m256i const_4  = {0x0016FFF60016FFF6, 0x0016FFF60016FFF6,
+                        0x0016FFF60016FFF6, 0x0016FFF60016FFF6};
+    __m256i const_5  = {0x0000000400000004, 0x0000000400000004,
+                        0x0000000400000004, 0x0000000400000004};
+    __m256i const_6  = {0x0000004000000040, 0x0000004000000040,
+                        0x0000004000000040, 0x0000004000000040};
+    __m256i const_7  = {0x000C000C000C000C, 0X000C000C000C000C,
+                        0xFFF4000CFFF4000C, 0xFFF4000CFFF4000C};
+    __m256i const_8  = {0x0006001000060010, 0x0006001000060010,
+                        0xFFF00006FFF00006, 0xFFF00006FFF00006};
+    __m256i const_9  = {0x0009001000090010, 0x0009001000090010,
+                        0x0004000F0004000F, 0x0004000F0004000F};
+    __m256i const_10 = {0xFFF0000FFFF0000F, 0xFFF0000FFFF0000F,
+                        0xFFF7FFFCFFF7FFFC, 0xFFF7FFFCFFF7FFFC};
+    __m256i const_11 = {0x0004000900040009, 0x0004000900040009,
+                        0x000FFFF0000FFFF0, 0x000FFFF0000FFFF0};
+    __m256i const_12 = {0x000F0004000F0004, 0x000F0004000F0004,
+                        0xFFF0FFF7FFF0FFF7, 0xFFF0FFF7FFF0FFF7};
+    __m256i shift    = {0x0000000400000000, 0x0000000600000002,
+                        0x0000000500000001, 0x0000000700000003};
+
+    /* first loops */
+    DUP4_ARG2(__lasx_xvld, block, 0, block, 32, block, 64, block, 96,
+              in0, in1, in2, in3);
+    in0   = __lasx_xvilvl_d(in1, in0);
+    in1   = __lasx_xvilvl_d(in3, in2);
+    temp0 = __lasx_xvpickev_h(in1, in0);
+    temp1 = __lasx_xvpickod_h(in1, in0);
+    temp0 = __lasx_xvperm_w(temp0, shift);
+    temp1 = __lasx_xvperm_w(temp1, shift);
+
+    DUP2_ARG3(__lasx_xvdp2add_w_h, const_5, temp0, const_1, const_5, temp0,
+              const_2, t1, t2);
+    DUP2_ARG2(__lasx_xvdp2_w_h, temp1, const_3, temp1, const_4, t3, t4);
+
+    temp0 = __lasx_xvadd_w(t1, t3);
+    temp1 = __lasx_xvsub_w(t2, t4);
+    temp2 = __lasx_xvadd_w(t2, t4);
+    temp3 = __lasx_xvsub_w(t1, t3);
+    DUP4_ARG2(__lasx_xvsrai_w, temp0, 3, temp1, 3, temp2, 3, temp3, 3,
+              temp0, temp1, temp2, temp3);
+
+    /* second loops */
+    t1    = __lasx_xvpickev_w(temp1, temp0);
+    t2    = __lasx_xvpickev_w(temp3, temp2);
+    t1    = __lasx_xvpickev_h(t2, t1);
+    t3    = __lasx_xvpickod_w(temp1, temp0);
+    t4    = __lasx_xvpickod_w(temp3, temp2);
+    temp1 = __lasx_xvpickev_h(t4, t3);
+    temp2 = __lasx_xvpermi_q(t1, t1, 0x00);
+    temp3 = __lasx_xvpermi_q(t1, t1, 0x11);
+    t1 = __lasx_xvdp2add_w_h(const_6, temp2, const_7);
+    t2 = __lasx_xvdp2_w_h(temp3, const_8);
+    t3    = __lasx_xvadd_w(t1, t2);
+    t4    = __lasx_xvsub_w(t1, t2);
+    t4    = __lasx_xvpermi_d(t4, 0x4E);
+
+    DUP4_ARG2(__lasx_xvdp2_w_h, temp1, const_9, temp1, const_10, temp1,
+              const_11, temp1, const_12, t1, t2, temp2, temp3);
+
+    temp0 = __lasx_xvpermi_q(t2, t1, 0x20);
+    temp1 = __lasx_xvpermi_q(t2, t1, 0x31);
+    t1    = __lasx_xvadd_w(temp0, temp1);
+    temp0 = __lasx_xvpermi_q(temp3, temp2, 0x20);
+    temp1 = __lasx_xvpermi_q(temp3, temp2, 0x31);
+    t2    = __lasx_xvadd_w(temp1, temp0);
+    temp0 = __lasx_xvadd_w(t1, t3);
+    temp1 = __lasx_xvadd_w(t2, t4);
+    temp2 = __lasx_xvsub_w(t4, t2);
+    temp3 = __lasx_xvsub_w(t3, t1);
+    temp2 = __lasx_xvaddi_wu(temp2, 1);
+    temp3 = __lasx_xvaddi_wu(temp3, 1);
+    DUP4_ARG2(__lasx_xvsrai_w, temp0, 7, temp1, 7, temp2, 7, temp3, 7,
+              temp0, temp1, temp2, temp3);
+
+    DUP4_ARG2(__lasx_xvldrepl_w, dest, 0, dest + stride, 0, dest + stride2, 0,
+              dest + stride3, 0, const_1, const_2, const_3, const_4);
+    DUP4_ARG2(__lasx_xvldrepl_w, dst, 0, dst + stride, 0, dst + stride2, 0,
+              dst + stride3, 0, const_5, const_6, const_7, const_8);
+
+    DUP4_ARG2(__lasx_xvilvl_w, const_2, const_1, const_4, const_3, const_5,
+              const_6, const_7, const_8, const_1, const_2, const_3, const_4);
+    DUP4_ARG1(__lasx_vext2xv_wu_bu, const_1, const_2, const_3, const_4,
+              const_1, const_2, const_3, const_4);
+    DUP4_ARG2(__lasx_xvadd_w, temp0, const_1, temp1, const_2, temp2, const_3,
+              temp3, const_4, temp0, temp1, temp2, temp3);
+    DUP4_ARG1(__lasx_xvclip255_w, temp0, temp1, temp2, temp3,
+              temp0, temp1, temp2, temp3);
+    DUP2_ARG2(__lasx_xvpickev_h, temp1, temp0, temp3, temp2, temp0, temp1);
+    temp0   = __lasx_xvpickev_b(temp1, temp0);
+    __lasx_xvstelm_w(temp0, dest, 0, 0);
+    __lasx_xvstelm_w(temp0, dest + stride, 0, 4);
+    __lasx_xvstelm_w(temp0, dest + stride2, 0, 1);
+    __lasx_xvstelm_w(temp0, dest + stride3, 0, 5);
+    __lasx_xvstelm_w(temp0, dst, 0, 6);
+    __lasx_xvstelm_w(temp0, dst + stride, 0, 2);
+    __lasx_xvstelm_w(temp0, dst + stride2, 0, 7);
+    __lasx_xvstelm_w(temp0, dst + stride3, 0, 3);
+}
+
+void ff_vc1_inv_trans_4x4_dc_lasx(uint8_t *dest, ptrdiff_t stride,
+                                  int16_t *block)
+{
+    int dc = block[0];
+    uint8_t *dst1 = dest + stride;
+    uint8_t *dst2 = dst1 + stride;
+    uint8_t *dst3 = dst2 + stride;
+    __m256i in0, in1, in2, in3, temp0, temp1, const_dc;
+    __m256i zero  = {0};
+
+    dc = (17 * dc +  4) >> 3;
+    dc = (17 * dc + 64) >> 7;
+    const_dc = __lasx_xvreplgr2vr_h(dc);
+
+    DUP4_ARG2(__lasx_xvldrepl_w, dest, 0, dst1, 0, dst2, 0, dst3, 0,
+              in0, in1, in2, in3);
+    DUP2_ARG2(__lasx_xvilvl_w, in1, in0, in3, in2, temp0, temp1);
+    in0   = __lasx_xvpermi_q(temp1, temp0, 0x20);
+    temp0 = __lasx_xvilvl_b(zero, in0);
+    in0   = __lasx_xvadd_h(temp0, const_dc);
+    temp0 = __lasx_xvssrarni_bu_h(in0, in0, 0);
+    __lasx_xvstelm_w(temp0, dest, 0, 0);
+    __lasx_xvstelm_w(temp0, dst1, 0, 1);
+    __lasx_xvstelm_w(temp0, dst2, 0, 4);
+    __lasx_xvstelm_w(temp0, dst3, 0, 5);
+}
+
+void ff_vc1_inv_trans_4x4_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *block)
+{
+    uint8_t *dst1 = dest + stride;
+    uint8_t *dst2 = dst1 + stride;
+    uint8_t *dst3 = dst2 + stride;
+    __m256i in0, in1, in2, in3;
+    __m256i temp0, temp1, temp2, temp3, t1, t2;
+
+    __m256i const_1  = {0x0011001100110011, 0xFFEF0011FFEF0011,
+                        0x0011001100110011, 0xFFEF0011FFEF0011};
+    __m256i const_2  = {0x000A0016000A0016, 0x0016FFF60016FFF6,
+                        0x000A0016000A0016, 0x0016FFF60016FFF6};
+    __m256i const_64 = {0x0000004000000040, 0x0000004000000040,
+                        0x0000004000000040, 0x0000004000000040};
+
+    DUP2_ARG2(__lasx_xvld, block, 0, block, 32, in0, in1);
+    /* first loops */
+    temp0 = __lasx_xvilvl_d(in1, in0);
+    temp1 = __lasx_xvpickev_h(temp0, temp0);
+    temp2 = __lasx_xvpickod_h(temp0, temp0);
+    DUP2_ARG2(__lasx_xvdp2_w_h, temp1, const_1, temp2, const_2, t1, t2);
+    t1    = __lasx_xvaddi_wu(t1, 4);
+    in0   = __lasx_xvadd_w(t1, t2);
+    in1   = __lasx_xvsub_w(t1, t2);
+    DUP2_ARG2(__lasx_xvsrai_w, in0, 3, in1, 3, in0, in1);
+    /* second loops */
+    temp0   = __lasx_xvpickev_h(in1, in0);
+    temp1   = __lasx_xvpermi_q(temp0, temp0, 0x00);
+    temp2   = __lasx_xvpermi_q(temp0, temp0, 0x11);
+    const_1 = __lasx_xvpermi_d(const_1, 0xD8);
+    const_2 = __lasx_xvpermi_d(const_2, 0xD8);
+    t1 = __lasx_xvdp2add_w_h(const_64, temp1, const_1);
+    t2 = __lasx_xvdp2_w_h(temp2, const_2);
+    in0     = __lasx_xvadd_w(t1, t2);
+    in1     = __lasx_xvsub_w(t1, t2);
+    DUP2_ARG2(__lasx_xvsrai_w, in0, 7, in1, 7, in0, in1);
+    temp0   = __lasx_xvshuf4i_w(in0, 0x9C);
+    temp1   = __lasx_xvshuf4i_w(in1, 0x9C);
+
+    DUP4_ARG2(__lasx_xvldrepl_w, dest, 0, dst1, 0, dst2, 0, dst3, 0,
+              in0, in1, in2, in3);
+    temp2   = __lasx_xvilvl_w(in2, in0);
+    temp2   = __lasx_vext2xv_wu_bu(temp2);
+    temp3   = __lasx_xvilvl_w(in1, in3);
+    temp3   = __lasx_vext2xv_wu_bu(temp3);
+    temp0   = __lasx_xvadd_w(temp0, temp2);
+    temp1   = __lasx_xvadd_w(temp1, temp3);
+    DUP2_ARG1(__lasx_xvclip255_w, temp0, temp1, temp0, temp1);
+    temp1   = __lasx_xvpickev_h(temp1, temp0);
+    temp0   = __lasx_xvpickev_b(temp1, temp1);
+    __lasx_xvstelm_w(temp0, dest, 0, 0);
+    __lasx_xvstelm_w(temp0, dst1, 0, 5);
+    __lasx_xvstelm_w(temp0, dst2, 0, 4);
+    __lasx_xvstelm_w(temp0, dst3, 0, 1);
+}
+
+static void put_vc1_mspel_mc_h_v_lasx(uint8_t *dst, const uint8_t *src,
+                                      ptrdiff_t stride, int hmode, int vmode,
+                                      int rnd)
+{
+    __m256i in0, in1, in2, in3;
+    __m256i t0, t1, t2, t3, t4, t5, t6, t7;
+    __m256i temp0, temp1, const_para1_2, const_para0_3;
+    __m256i const_r, const_sh;
+    __m256i sh = {0x0000000400000000, 0x0000000500000001,
+                  0x0000000600000002, 0x0000000700000003};
+    static const uint8_t para_value[][4] = {{4, 3, 53, 18},
+                                            {1, 1, 9, 9},
+                                            {3, 4, 18, 53}};
+    static const int shift_value[] = {0, 5, 1, 5};
+    int shift = (shift_value[hmode] + shift_value[vmode]) >> 1;
+    int r     = (1 << (shift - 1)) + rnd - 1;
+    const uint8_t *para_v = para_value[vmode - 1];
+    ptrdiff_t stride2 = stride << 1;
+    ptrdiff_t stride4 = stride << 2;
+    ptrdiff_t stride3 = stride2 + stride;
+
+    const_r  = __lasx_xvreplgr2vr_h(r);
+    const_sh = __lasx_xvreplgr2vr_h(shift);
+    src -= 1, src -= stride;
+    const_para0_3 = __lasx_xvldrepl_h(para_v, 0);
+    const_para1_2 = __lasx_xvldrepl_h(para_v, 2);
+    DUP4_ARG2(__lasx_xvld, src, 0, src + stride, 0, src + stride2, 0,
+              src + stride3, 0, in0, in1, in2, in3);
+    DUP4_ARG2(__lasx_xvpermi_d, in0, 0xD8, in1, 0xD8, in2, 0xD8, in3, 0xD8,
+              in0, in1, in2, in3);
+    DUP2_ARG2(__lasx_xvilvl_b, in2, in1, in3, in0, temp0, temp1);
+    t0 = __lasx_xvdp2_h_bu(temp0, const_para1_2);
+    t0 = __lasx_xvdp2sub_h_bu(t0, temp1, const_para0_3);
+    src  += stride4;
+    in0   = __lasx_xvld(src, 0);
+    in0   = __lasx_xvpermi_d(in0, 0xD8);
+    DUP2_ARG2(__lasx_xvilvl_b, in3, in2, in0, in1, temp0, temp1);
+    t1 = __lasx_xvdp2_h_bu(temp0, const_para1_2);
+    t1 = __lasx_xvdp2sub_h_bu(t1, temp1, const_para0_3);
+    src  += stride;
+    in1   = __lasx_xvld(src, 0);
+    in1   = __lasx_xvpermi_d(in1, 0xD8);
+    DUP2_ARG2(__lasx_xvilvl_b, in0, in3, in1, in2, temp0, temp1);
+    t2 = __lasx_xvdp2_h_bu(temp0, const_para1_2);
+    t2 = __lasx_xvdp2sub_h_bu(t2, temp1, const_para0_3);
+    src  += stride;
+    in2   = __lasx_xvld(src, 0);
+    in2   = __lasx_xvpermi_d(in2, 0xD8);
+    DUP2_ARG2(__lasx_xvilvl_b, in1, in0, in2, in3, temp0, temp1);
+    t3 = __lasx_xvdp2_h_bu(temp0, const_para1_2);
+    t3 = __lasx_xvdp2sub_h_bu(t3, temp1, const_para0_3);
+    src  += stride;
+    in3   = __lasx_xvld(src, 0);
+    in3   = __lasx_xvpermi_d(in3, 0xD8);
+    DUP2_ARG2(__lasx_xvilvl_b, in2, in1, in3, in0, temp0, temp1);
+    t4 = __lasx_xvdp2_h_bu(temp0, const_para1_2);
+    t4 = __lasx_xvdp2sub_h_bu(t4, temp1, const_para0_3);
+    src  += stride;
+    in0   = __lasx_xvld(src, 0);
+    in0   = __lasx_xvpermi_d(in0, 0xD8);
+    DUP2_ARG2(__lasx_xvilvl_b, in3, in2, in0, in1, temp0, temp1);
+    t5 = __lasx_xvdp2_h_bu(temp0, const_para1_2);
+    t5 = __lasx_xvdp2sub_h_bu(t5, temp1, const_para0_3);
+    src  += stride;
+    in1   = __lasx_xvld(src, 0);
+    in1   = __lasx_xvpermi_d(in1, 0xD8);
+    DUP2_ARG2(__lasx_xvilvl_b, in0, in3, in1, in2, temp0, temp1);
+    t6 = __lasx_xvdp2_h_bu(temp0, const_para1_2);
+    t6 = __lasx_xvdp2sub_h_bu(t6, temp1, const_para0_3);
+    src  += stride;
+    in2   = __lasx_xvld(src, 0);
+    in2   = __lasx_xvpermi_d(in2, 0xD8);
+    DUP2_ARG2(__lasx_xvilvl_b, in1, in0, in2, in3, temp0, temp1);
+    t7 = __lasx_xvdp2_h_bu(temp0, const_para1_2);
+    t7 = __lasx_xvdp2sub_h_bu(t7, temp1, const_para0_3);
+    DUP4_ARG2(__lasx_xvadd_h, t0, const_r, t1, const_r, t2, const_r, t3,
+              const_r, t0, t1, t2, t3);
+    DUP4_ARG2(__lasx_xvadd_h, t4, const_r, t5, const_r, t6, const_r, t7,
+              const_r, t4, t5, t6, t7);
+    DUP4_ARG2(__lasx_xvsra_h, t0, const_sh, t1, const_sh, t2, const_sh,
+              t3, const_sh, t0, t1, t2, t3);
+    DUP4_ARG2(__lasx_xvsra_h, t4, const_sh, t5, const_sh, t6, const_sh,
+              t7, const_sh, t4, t5, t6, t7);
+    LASX_TRANSPOSE8x8_H(t0, t1, t2, t3, t4, t5, t6, t7, t0,
+                        t1, t2, t3, t4, t5, t6, t7);
+    para_v  = para_value[hmode - 1];
+    const_para0_3 = __lasx_xvldrepl_h(para_v, 0);
+    const_para1_2 = __lasx_xvldrepl_h(para_v, 2);
+    const_para0_3 = __lasx_vext2xv_h_b(const_para0_3);
+    const_para1_2 = __lasx_vext2xv_h_b(const_para1_2);
+    r       = 64 - rnd;
+    const_r = __lasx_xvreplgr2vr_w(r);
+    DUP4_ARG2(__lasx_xvpermi_d, t0, 0x72, t1, 0x72, t2, 0x72, t0, 0xD8,
+              in0, in1, in2, t0);
+    DUP4_ARG2(__lasx_xvpermi_d, t1, 0xD8, t2, 0xD8, t3, 0xD8, t4, 0xD8,
+              t1, t2, t3, t4);
+    DUP2_ARG2(__lasx_xvpermi_d, t5, 0xD8, t6, 0xD8, t5, t6);
+    t7      = __lasx_xvpermi_d(t7, 0xD8);
+    DUP2_ARG2(__lasx_xvilvl_h, t2, t1, t3, t0, temp0, temp1);
+    t0 = __lasx_xvdp2_w_h(temp0, const_para1_2);
+    t0 = __lasx_xvdp2sub_w_h(t0, temp1, const_para0_3);
+    DUP2_ARG2(__lasx_xvilvl_h, t3, t2, t4, t1, temp0, temp1);
+    t1 = __lasx_xvdp2_w_h(temp0, const_para1_2);
+    t1 = __lasx_xvdp2sub_w_h(t1, temp1, const_para0_3);
+    DUP2_ARG2(__lasx_xvilvl_h, t4, t3, t5, t2, temp0, temp1);
+    t2 = __lasx_xvdp2_w_h(temp0, const_para1_2);
+    t2 = __lasx_xvdp2sub_w_h(t2, temp1, const_para0_3);
+    DUP2_ARG2(__lasx_xvilvl_h, t5, t4, t6, t3, temp0, temp1);
+    t3 = __lasx_xvdp2_w_h(temp0, const_para1_2);
+    t3 = __lasx_xvdp2sub_w_h(t3, temp1, const_para0_3);
+    DUP2_ARG2(__lasx_xvilvl_h, t6, t5, t7, t4, temp0, temp1);
+    t4 = __lasx_xvdp2_w_h(temp0, const_para1_2);
+    t4 = __lasx_xvdp2sub_w_h(t4, temp1, const_para0_3);
+    DUP2_ARG2(__lasx_xvilvl_h, t7, t6, in0, t5, temp0, temp1);
+    t5 = __lasx_xvdp2_w_h(temp0, const_para1_2);
+    t5 = __lasx_xvdp2sub_w_h(t5, temp1, const_para0_3);
+    DUP2_ARG2(__lasx_xvilvl_h, in0, t7, in1, t6, temp0, temp1);
+    t6 = __lasx_xvdp2_w_h(temp0, const_para1_2);
+    t6 = __lasx_xvdp2sub_w_h(t6, temp1, const_para0_3);
+    DUP2_ARG2(__lasx_xvilvl_h, in1, in0, in2, t7, temp0, temp1);
+    t7 = __lasx_xvdp2_w_h(temp0, const_para1_2);
+    t7 = __lasx_xvdp2sub_w_h(t7, temp1, const_para0_3);
+    DUP4_ARG2(__lasx_xvadd_w, t0, const_r, t1, const_r, t2, const_r,
+              t3, const_r, t0, t1, t2, t3);
+    DUP4_ARG2(__lasx_xvadd_w, t4, const_r, t5, const_r, t6, const_r,
+              t7, const_r, t4, t5, t6, t7);
+    DUP4_ARG2(__lasx_xvsrai_w, t0, 7, t1, 7, t2, 7, t3, 7, t0, t1, t2, t3);
+    DUP4_ARG2(__lasx_xvsrai_w, t4, 7, t5, 7, t6, 7, t7, 7, t4, t5, t6, t7);
+    LASX_TRANSPOSE8x8_W(t0, t1, t2, t3, t4, t5, t6, t7,
+                        t0, t1, t2, t3, t4, t5, t6, t7);
+    DUP4_ARG1(__lasx_xvclip255_w, t0, t1, t2, t3, t0, t1, t2, t3);
+    DUP4_ARG1(__lasx_xvclip255_w, t4, t5, t6, t7, t4, t5, t6, t7);
+    DUP4_ARG2(__lasx_xvpickev_h, t1, t0, t3, t2, t5, t4, t7, t6,
+              t0, t1, t2, t3);
+    DUP2_ARG2(__lasx_xvpickev_b, t1, t0, t3, t2, t0, t1);
+    t0 = __lasx_xvperm_w(t0, sh);
+    t1 = __lasx_xvperm_w(t1, sh);
+    __lasx_xvstelm_d(t0, dst, 0, 0);
+    __lasx_xvstelm_d(t0, dst + stride, 0, 1);
+    __lasx_xvstelm_d(t0, dst + stride2, 0, 2);
+    __lasx_xvstelm_d(t0, dst + stride3, 0, 3);
+    dst += stride4;
+    __lasx_xvstelm_d(t1, dst, 0, 0);
+    __lasx_xvstelm_d(t1, dst + stride, 0, 1);
+    __lasx_xvstelm_d(t1, dst + stride2, 0, 2);
+    __lasx_xvstelm_d(t1, dst + stride3, 0, 3);
+}
+
+#define PUT_VC1_MSPEL_MC_LASX(hmode, vmode)                                   \
+void ff_put_vc1_mspel_mc ## hmode ## vmode ## _lasx(uint8_t *dst,             \
+                                                const uint8_t *src,           \
+                                                ptrdiff_t stride, int rnd)    \
+{                                                                             \
+    put_vc1_mspel_mc_h_v_lasx(dst, src, stride, hmode, vmode, rnd);           \
+}                                                                             \
+void ff_put_vc1_mspel_mc ## hmode ## vmode ## _16_lasx(uint8_t *dst,          \
+                                                   const uint8_t *src,        \
+                                                   ptrdiff_t stride, int rnd) \
+{                                                                             \
+    put_vc1_mspel_mc_h_v_lasx(dst, src, stride, hmode, vmode, rnd);           \
+    put_vc1_mspel_mc_h_v_lasx(dst + 8, src + 8, stride, hmode, vmode, rnd);   \
+    dst += 8 * stride, src += 8 * stride;                                     \
+    put_vc1_mspel_mc_h_v_lasx(dst, src, stride, hmode, vmode, rnd);           \
+    put_vc1_mspel_mc_h_v_lasx(dst + 8, src + 8, stride, hmode, vmode, rnd);   \
+}
+
+PUT_VC1_MSPEL_MC_LASX(1, 1);
+PUT_VC1_MSPEL_MC_LASX(1, 2);
+PUT_VC1_MSPEL_MC_LASX(1, 3);
+
+PUT_VC1_MSPEL_MC_LASX(2, 1);
+PUT_VC1_MSPEL_MC_LASX(2, 2);
+PUT_VC1_MSPEL_MC_LASX(2, 3);
+
+PUT_VC1_MSPEL_MC_LASX(3, 1);
+PUT_VC1_MSPEL_MC_LASX(3, 2);
+PUT_VC1_MSPEL_MC_LASX(3, 3);
+
+void ff_put_no_rnd_vc1_chroma_mc8_lasx(uint8_t *dst /* align 8 */,
+                                       uint8_t *src /* align 1 */,
+                                       ptrdiff_t stride, int h, int x, int y)
+{
+    const int intA = (8 - x) * (8 - y);
+    const int intB =     (x) * (8 - y);
+    const int intC = (8 - x) *     (y);
+    const int intD =     (x) *     (y);
+    __m256i src00, src01, src10, src11;
+    __m256i A, B, C, D;
+    int i;
+
+    av_assert2(x < 8 && y < 8 && x >= 0 && y >= 0);
+
+    A = __lasx_xvreplgr2vr_h(intA);
+    B = __lasx_xvreplgr2vr_h(intB);
+    C = __lasx_xvreplgr2vr_h(intC);
+    D = __lasx_xvreplgr2vr_h(intD);
+    for(i = 0; i < h; i++){
+        DUP2_ARG2(__lasx_xvld, src, 0, src, 1, src00, src01);
+        src += stride;
+        DUP2_ARG2(__lasx_xvld, src, 0, src, 1, src10, src11);
+
+        DUP4_ARG1(__lasx_vext2xv_hu_bu, src00, src01, src10, src11,
+                  src00, src01, src10, src11);
+        DUP4_ARG2(__lasx_xvmul_h, src00, A, src01, B, src10, C, src11, D,
+                  src00, src01, src10, src11);
+        src00 = __lasx_xvadd_h(src00, src01);
+        src10 = __lasx_xvadd_h(src10, src11);
+        src00 = __lasx_xvadd_h(src00, src10);
+        src00 = __lasx_xvaddi_hu(src00, 28);
+        src00 = __lasx_xvsrli_h(src00, 6);
+        src00 = __lasx_xvpickev_b(src00, src00);
+        __lasx_xvstelm_d(src00, dst, 0, 0);
+        dst += stride;
+    }
+}
+
+static void put_vc1_mspel_mc_v_lasx(uint8_t *dst, const uint8_t *src,
+                                    ptrdiff_t stride, int vmode, int rnd)
+{
+    __m256i in0, in1, in2, in3, temp0, temp1, t0;
+    __m256i const_para0_3, const_para1_2, const_r, const_sh;
+    static const uint16_t para_value[][2] = {{0x0304, 0x1235},
+                                            {0x0101, 0x0909},
+                                            {0x0403, 0x3512}};
+    const uint16_t *para_v = para_value[vmode - 1];
+    static const int shift_value[] = {0, 6, 4, 6};
+    static int add_value[3];
+    ptrdiff_t stride_2x = stride << 1;
+    int i = 0;
+    add_value[2] = add_value[0] = 31 + rnd, add_value[1] = 7 + rnd;
+
+    const_r  = __lasx_xvreplgr2vr_h(add_value[vmode - 1]);
+    const_sh = __lasx_xvreplgr2vr_h(shift_value[vmode]);
+    const_para0_3 = __lasx_xvreplgr2vr_h(*para_v);
+    const_para1_2 = __lasx_xvreplgr2vr_h(*(para_v + 1));
+
+    DUP2_ARG2(__lasx_xvld, src - stride, 0, src, 0, in0, in1);
+    in2 = __lasx_xvld(src + stride, 0);
+    in0   = __lasx_xvpermi_d(in0, 0xD8);
+    in1   = __lasx_xvpermi_d(in1, 0xD8);
+    in2   = __lasx_xvpermi_d(in2, 0xD8);
+    for (; i < 16; i++) {
+        in3 = __lasx_xvld(src + stride_2x, 0);
+        in3 = __lasx_xvpermi_d(in3, 0xD8);
+        DUP2_ARG2(__lasx_xvilvl_b, in2, in1, in3, in0, temp0, temp1);
+        t0 = __lasx_xvdp2_h_bu(temp0, const_para1_2);
+        t0 = __lasx_xvdp2sub_h_bu(t0, temp1, const_para0_3);
+        t0 = __lasx_xvadd_h(t0, const_r);
+        t0 = __lasx_xvsra_h(t0, const_sh);
+        t0 = __lasx_xvclip255_h(t0);
+        t0 = __lasx_xvpickev_b(t0, t0);
+        __lasx_xvstelm_d(t0, dst, 0, 0);
+        __lasx_xvstelm_d(t0, dst, 8, 2);
+        dst += stride;
+        src += stride;
+        in0 = in1;
+        in1 = in2;
+        in2 = in3;
+    }
+}
+
+#define PUT_VC1_MSPEL_MC_V_LASX(vmode)                                    \
+void ff_put_vc1_mspel_mc0 ## vmode ## _16_lasx(uint8_t *dst,              \
+                                               const uint8_t *src,        \
+                                               ptrdiff_t stride, int rnd) \
+{                                                                         \
+    put_vc1_mspel_mc_v_lasx(dst, src, stride, vmode, rnd);                \
+}
+
+PUT_VC1_MSPEL_MC_V_LASX(1);
+PUT_VC1_MSPEL_MC_V_LASX(2);
+PUT_VC1_MSPEL_MC_V_LASX(3);
+
+#define ROW_LASX(in0, in1, in2, in3, out0)                                \
+    DUP2_ARG2(__lasx_xvilvl_b, in2, in1, in3, in0, tmp0_m, tmp1_m);       \
+    out0 = __lasx_xvdp2_h_bu(tmp0_m, const_para1_2);                      \
+    out0 = __lasx_xvdp2sub_h_bu(out0, tmp1_m, const_para0_3);             \
+    out0 = __lasx_xvadd_h(out0, const_r);                                 \
+    out0 = __lasx_xvsra_h(out0, const_sh);                                \
+    out0 = __lasx_xvclip255_h(out0);                                      \
+    out0 = __lasx_xvpickev_b(out0, out0);                                 \
+    out0 = __lasx_xvpermi_d(out0, 0xd8);                                  \
+
+static void put_vc1_mspel_mc_h_lasx(uint8_t *dst, const uint8_t *src,
+                                    ptrdiff_t stride, int hmode, int rnd)
+{
+    __m256i in0, in1, in2, in3, in4, in5, in6, in7,
+            in8, in9, in10, in11, in12, in13, in14, in15;
+    __m256i out0, out1, out2, out3, out4, out5, out6, out7, out8, out9,
+            out10, out11, out12, out13, out14, out15, out16, out17, out18;
+    __m256i const_para0_3, const_para1_2, const_r, const_sh;
+    __m256i tmp0_m, tmp1_m, tmp2_m, tmp3_m;
+    __m256i tmp4_m, tmp5_m, tmp6_m, tmp7_m;
+    __m256i t0, t1, t2, t3, t4, t5, t6, t7;
+    ptrdiff_t stride2 = stride << 1;
+    ptrdiff_t stride4 = stride << 2;
+    ptrdiff_t stride3 = stride2 + stride;
+    static const uint16_t para_value[][2] = {{0x0304, 0x1235},
+                                            {0x0101, 0x0909},
+                                            {0x0403, 0x3512}};
+    const uint16_t *para_v = para_value[hmode - 1];
+    static const int shift_value[] = {0, 6, 4, 6};
+    static int add_value[3];
+    uint8_t *_src = (uint8_t*)src - 1;
+    add_value[2] = add_value[0] = 32 - rnd, add_value[1] = 8 - rnd;
+
+    const_r  = __lasx_xvreplgr2vr_h(add_value[hmode - 1]);
+    const_sh = __lasx_xvreplgr2vr_h(shift_value[hmode]);
+    const_para0_3 = __lasx_xvreplgr2vr_h(*para_v);
+    const_para1_2 = __lasx_xvreplgr2vr_h(*(para_v + 1));
+
+    in0 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, stride, _src, stride2, in1, in2);
+    in3 = __lasx_xvldx(_src, stride3);
+    _src += stride4;
+    in4 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, stride, _src, stride2, in5, in6);
+    in7 = __lasx_xvldx(_src, stride3);
+    _src += stride4;
+    in8 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, stride, _src, stride2, in9, in10);
+    in11 = __lasx_xvldx(_src, stride3);
+    _src += stride4;
+    in12 = __lasx_xvld(_src, 0);
+    DUP2_ARG2(__lasx_xvldx, _src, stride, _src, stride2, in13, in14);
+    in15 = __lasx_xvldx(_src, stride3);
+    DUP4_ARG2(__lasx_xvilvl_b, in2, in0, in3, in1, in6, in4, in7, in5,
+              tmp0_m, tmp1_m, tmp2_m, tmp3_m);
+    DUP4_ARG2(__lasx_xvilvl_b, in10, in8, in11, in9, in14, in12, in15, in13,
+              tmp4_m, tmp5_m, tmp6_m, tmp7_m);
+    DUP4_ARG2(__lasx_xvilvl_b, tmp1_m, tmp0_m, tmp3_m, tmp2_m, tmp5_m, tmp4_m,
+              tmp7_m, tmp6_m, t0, t2, t4, t6);
+    DUP4_ARG2(__lasx_xvilvh_b, tmp1_m, tmp0_m, tmp3_m, tmp2_m, tmp5_m, tmp4_m,
+              tmp7_m, tmp6_m, t1, t3, t5, t7);
+    DUP4_ARG2(__lasx_xvilvl_w, t2, t0, t3, t1, t6, t4, t7, t5, tmp0_m, tmp4_m,
+              tmp1_m, tmp5_m);
+    DUP4_ARG2(__lasx_xvilvh_w, t2, t0, t3, t1, t6, t4, t7, t5, tmp2_m, tmp6_m,
+              tmp3_m, tmp7_m);
+    DUP4_ARG2(__lasx_xvilvl_d, tmp1_m, tmp0_m, tmp3_m, tmp2_m, tmp5_m, tmp4_m,
+              tmp7_m, tmp6_m, out0, out2, out4, out6);
+    DUP4_ARG2(__lasx_xvilvh_d, tmp1_m, tmp0_m, tmp3_m, tmp2_m, tmp5_m, tmp4_m,
+              tmp7_m, tmp6_m, out1, out3, out5, out7);
+
+    DUP4_ARG2(__lasx_xvilvh_b, in2, in0, in3, in1, in6, in4, in7, in5,
+              tmp0_m, tmp1_m, tmp2_m, tmp3_m);
+    DUP4_ARG2(__lasx_xvilvh_b, in10, in8, in11, in9, in14, in12, in15, in13,
+              tmp4_m, tmp5_m, tmp6_m, tmp7_m);
+    DUP4_ARG2(__lasx_xvilvl_b, tmp1_m, tmp0_m, tmp3_m, tmp2_m, tmp5_m, tmp4_m,
+              tmp7_m, tmp6_m, t0, t2, t4, t6);
+    DUP4_ARG2(__lasx_xvilvh_b, tmp1_m, tmp0_m, tmp3_m, tmp2_m, tmp5_m, tmp4_m,
+              tmp7_m, tmp6_m, t1, t3, t5, t7);
+    DUP4_ARG2(__lasx_xvilvl_w, t2, t0, t3, t1, t6, t4, t7, t5, tmp0_m, tmp4_m,
+              tmp1_m, tmp5_m);
+    DUP4_ARG2(__lasx_xvilvh_w, t2, t0, t3, t1, t6, t4, t7, t5, tmp2_m, tmp6_m,
+              tmp3_m, tmp7_m);
+    DUP4_ARG2(__lasx_xvilvl_d, tmp1_m, tmp0_m, tmp3_m, tmp2_m, tmp5_m, tmp4_m,
+              tmp7_m, tmp6_m, out8, out10, out12, out14);
+    DUP4_ARG2(__lasx_xvilvh_d, tmp1_m, tmp0_m, tmp3_m, tmp2_m, tmp5_m, tmp4_m,
+              tmp7_m, tmp6_m, out9, out11, out13, out15);
+    DUP2_ARG3(__lasx_xvpermi_q, out0, out0, 0x31, out1, out1, 0x31, out16, out17);
+    out18 = __lasx_xvpermi_q(out2, out2, 0x31);
+
+    DUP4_ARG2(__lasx_xvpermi_d, out0, 0xD8, out1, 0xD8, out2, 0xD8, out3, 0xD8,
+              out0, out1, out2, out3);
+    DUP4_ARG2(__lasx_xvpermi_d, out4, 0xD8, out5, 0xD8, out6, 0xD8, out7, 0xD8,
+              out4, out5, out6, out7);
+    DUP4_ARG2(__lasx_xvpermi_d, out8, 0xD8, out9, 0xD8, out10, 0xD8, out11,
+              0xD8, out8, out9, out10, out11);
+    DUP4_ARG2(__lasx_xvpermi_d, out12, 0xD8, out13, 0xD8, out14, 0xD8, out15,
+              0xD8, out12, out13, out14, out15);
+    out16 = __lasx_xvpermi_d(out16, 0xD8);
+    out17 = __lasx_xvpermi_d(out17, 0xD8);
+    out18 = __lasx_xvpermi_d(out18, 0xD8);
+
+    ROW_LASX(out0,  out1,  out2,  out3,  in0);
+    ROW_LASX(out1,  out2,  out3,  out4,  in1);
+    ROW_LASX(out2,  out3,  out4,  out5,  in2);
+    ROW_LASX(out3,  out4,  out5,  out6,  in3);
+    ROW_LASX(out4,  out5,  out6,  out7,  in4);
+    ROW_LASX(out5,  out6,  out7,  out8,  in5);
+    ROW_LASX(out6,  out7,  out8,  out9,  in6);
+    ROW_LASX(out7,  out8,  out9,  out10, in7);
+    ROW_LASX(out8,  out9,  out10, out11, in8);
+    ROW_LASX(out9,  out10, out11, out12, in9);
+    ROW_LASX(out10, out11, out12, out13, in10);
+    ROW_LASX(out11, out12, out13, out14, in11);
+    ROW_LASX(out12, out13, out14, out15, in12);
+    ROW_LASX(out13, out14, out15, out16, in13);
+    ROW_LASX(out14, out15, out16, out17, in14);
+    ROW_LASX(out15, out16, out17, out18, in15);
+
+    DUP4_ARG2(__lasx_xvilvl_b, in2, in0, in3, in1, in6, in4, in7, in5,
+              tmp0_m, tmp1_m, tmp2_m, tmp3_m);
+    DUP4_ARG2(__lasx_xvilvl_b, in10, in8, in11, in9, in14, in12, in15, in13,
+              tmp4_m, tmp5_m, tmp6_m, tmp7_m);
+    DUP4_ARG2(__lasx_xvilvl_b, tmp1_m, tmp0_m, tmp3_m, tmp2_m, tmp5_m, tmp4_m,
+              tmp7_m, tmp6_m, t0, t2, t4, t6);
+    DUP4_ARG2(__lasx_xvilvh_b, tmp1_m, tmp0_m, tmp3_m, tmp2_m, tmp5_m, tmp4_m,
+              tmp7_m, tmp6_m, t1, t3, t5, t7);
+    DUP4_ARG2(__lasx_xvilvl_w, t2, t0, t3, t1, t6, t4, t7, t5, tmp0_m, tmp4_m,
+              tmp1_m, tmp5_m);
+    DUP4_ARG2(__lasx_xvilvh_w, t2, t0, t3, t1, t6, t4, t7, t5, tmp2_m, tmp6_m,
+              tmp3_m, tmp7_m);
+    DUP4_ARG2(__lasx_xvilvl_d, tmp1_m, tmp0_m, tmp3_m, tmp2_m, tmp5_m, tmp4_m,
+              tmp7_m, tmp6_m, out0, out2, out4, out6);
+    DUP4_ARG2(__lasx_xvilvh_d, tmp1_m, tmp0_m, tmp3_m, tmp2_m, tmp5_m, tmp4_m,
+              tmp7_m, tmp6_m, out1, out3, out5, out7);
+
+    DUP4_ARG2(__lasx_xvilvh_b, in2, in0, in3, in1, in6, in4, in7, in5,
+              tmp0_m, tmp1_m, tmp2_m, tmp3_m);
+    DUP4_ARG2(__lasx_xvilvh_b, in10, in8, in11, in9, in14, in12, in15, in13,
+              tmp4_m, tmp5_m, tmp6_m, tmp7_m);
+    DUP4_ARG2(__lasx_xvilvl_b, tmp1_m, tmp0_m, tmp3_m, tmp2_m, tmp5_m, tmp4_m,
+              tmp7_m, tmp6_m, t0, t2, t4, t6);
+    DUP4_ARG2(__lasx_xvilvh_b, tmp1_m, tmp0_m, tmp3_m, tmp2_m, tmp5_m, tmp4_m,
+              tmp7_m, tmp6_m, t1, t3, t5, t7);
+    DUP4_ARG2(__lasx_xvilvl_w, t2, t0, t3, t1, t6, t4, t7, t5, tmp0_m, tmp4_m,
+              tmp1_m, tmp5_m);
+    DUP4_ARG2(__lasx_xvilvh_w, t2, t0, t3, t1, t6, t4, t7, t5, tmp2_m, tmp6_m,
+              tmp3_m, tmp7_m);
+    DUP4_ARG2(__lasx_xvilvl_d, tmp1_m, tmp0_m, tmp3_m, tmp2_m, tmp5_m, tmp4_m,
+              tmp7_m, tmp6_m, out8, out10, out12, out14);
+    DUP4_ARG2(__lasx_xvilvh_d, tmp1_m, tmp0_m, tmp3_m, tmp2_m, tmp5_m, tmp4_m,
+              tmp7_m, tmp6_m, out9, out11, out13, out15);
+    __lasx_xvstelm_d(out0, dst, 0, 0);
+    __lasx_xvstelm_d(out0, dst, 8, 1);
+    dst += stride;
+    __lasx_xvstelm_d(out1, dst, 0, 0);
+    __lasx_xvstelm_d(out1, dst, 8, 1);
+    dst += stride;
+    __lasx_xvstelm_d(out2, dst, 0, 0);
+    __lasx_xvstelm_d(out2, dst, 8, 1);
+    dst += stride;
+    __lasx_xvstelm_d(out3, dst, 0, 0);
+    __lasx_xvstelm_d(out3, dst, 8, 1);
+    dst += stride;
+    __lasx_xvstelm_d(out4, dst, 0, 0);
+    __lasx_xvstelm_d(out4, dst, 8, 1);
+    dst += stride;
+    __lasx_xvstelm_d(out5, dst, 0, 0);
+    __lasx_xvstelm_d(out5, dst, 8, 1);
+    dst += stride;
+    __lasx_xvstelm_d(out6, dst, 0, 0);
+    __lasx_xvstelm_d(out6, dst, 8, 1);
+    dst += stride;
+    __lasx_xvstelm_d(out7, dst, 0, 0);
+    __lasx_xvstelm_d(out7, dst, 8, 1);
+    dst += stride;
+    __lasx_xvstelm_d(out8, dst, 0, 0);
+    __lasx_xvstelm_d(out8, dst, 8, 1);
+    dst += stride;
+    __lasx_xvstelm_d(out9, dst, 0, 0);
+    __lasx_xvstelm_d(out9, dst, 8, 1);
+    dst += stride;
+    __lasx_xvstelm_d(out10, dst, 0, 0);
+    __lasx_xvstelm_d(out10, dst, 8, 1);
+    dst += stride;
+    __lasx_xvstelm_d(out11, dst, 0, 0);
+    __lasx_xvstelm_d(out11, dst, 8, 1);
+    dst += stride;
+    __lasx_xvstelm_d(out12, dst, 0, 0);
+    __lasx_xvstelm_d(out12, dst, 8, 1);
+    dst += stride;
+    __lasx_xvstelm_d(out13, dst, 0, 0);
+    __lasx_xvstelm_d(out13, dst, 8, 1);
+    dst += stride;
+    __lasx_xvstelm_d(out14, dst, 0, 0);
+    __lasx_xvstelm_d(out14, dst, 8, 1);
+    dst += stride;
+    __lasx_xvstelm_d(out15, dst, 0, 0);
+    __lasx_xvstelm_d(out15, dst, 8, 1);
+}
+
+#define PUT_VC1_MSPEL_MC_H_LASX(hmode)                                    \
+void ff_put_vc1_mspel_mc ## hmode ## 0_16_lasx(uint8_t *dst,              \
+                                               const uint8_t *src,        \
+                                               ptrdiff_t stride, int rnd) \
+{                                                                         \
+    put_vc1_mspel_mc_h_lasx(dst, src, stride, hmode, rnd);                \
+}
+
+PUT_VC1_MSPEL_MC_H_LASX(1);
+PUT_VC1_MSPEL_MC_H_LASX(2);
+PUT_VC1_MSPEL_MC_H_LASX(3);
diff --git a/libavcodec/loongarch/vc1dsp_loongarch.h b/libavcodec/loongarch/vc1dsp_loongarch.h
new file mode 100644
index 0000000000..398631aecc
--- /dev/null
+++ b/libavcodec/loongarch/vc1dsp_loongarch.h
@@ -0,0 +1,79 @@
+/*
+ * Copyright (c) 2021 Loongson Technology Corporation Limited
+ * Contributed by Hao Chen <chenhao@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef AVCODEC_LOONGARCH_VC1DSP_LOONGARCH_H
+#define AVCODEC_LOONGARCH_VC1DSP_LOONGARCH_H
+
+#include "libavcodec/vc1dsp.h"
+#include "libavutil/avassert.h"
+
+void ff_vc1_inv_trans_8x8_lasx(int16_t block[64]);
+void ff_vc1_inv_trans_8x8_dc_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *block);
+void ff_vc1_inv_trans_8x4_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *block);
+void ff_vc1_inv_trans_8x4_dc_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *block);
+void ff_vc1_inv_trans_4x8_dc_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *block);
+void ff_vc1_inv_trans_4x8_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *blokc);
+void ff_vc1_inv_trans_4x4_dc_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *block);
+void ff_vc1_inv_trans_4x4_lasx(uint8_t *dest, ptrdiff_t stride, int16_t *block);
+
+#define FF_PUT_VC1_MSPEL_MC_LASX(hmode, vmode)                                \
+void ff_put_vc1_mspel_mc ## hmode ## vmode ## _lasx(uint8_t *dst,             \
+                                                  const uint8_t *src,         \
+                                                  ptrdiff_t stride, int rnd); \
+void ff_put_vc1_mspel_mc ## hmode ## vmode ## _16_lasx(uint8_t *dst,          \
+                                                  const uint8_t *src,         \
+                                                  ptrdiff_t stride, int rnd);
+
+FF_PUT_VC1_MSPEL_MC_LASX(1, 1);
+FF_PUT_VC1_MSPEL_MC_LASX(1, 2);
+FF_PUT_VC1_MSPEL_MC_LASX(1, 3);
+
+FF_PUT_VC1_MSPEL_MC_LASX(2, 1);
+FF_PUT_VC1_MSPEL_MC_LASX(2, 2);
+FF_PUT_VC1_MSPEL_MC_LASX(2, 3);
+
+FF_PUT_VC1_MSPEL_MC_LASX(3, 1);
+FF_PUT_VC1_MSPEL_MC_LASX(3, 2);
+FF_PUT_VC1_MSPEL_MC_LASX(3, 3);
+
+#define FF_PUT_VC1_MSPEL_MC_V_LASX(vmode)                                 \
+void ff_put_vc1_mspel_mc0 ## vmode ## _16_lasx(uint8_t *dst,              \
+                                               const uint8_t *src,        \
+                                               ptrdiff_t stride, int rnd);
+
+FF_PUT_VC1_MSPEL_MC_V_LASX(1);
+FF_PUT_VC1_MSPEL_MC_V_LASX(2);
+FF_PUT_VC1_MSPEL_MC_V_LASX(3);
+
+#define FF_PUT_VC1_MSPEL_MC_H_LASX(hmode)                                 \
+void ff_put_vc1_mspel_mc ## hmode ## 0_16_lasx(uint8_t *dst,              \
+                                               const uint8_t *src,        \
+                                               ptrdiff_t stride, int rnd);
+
+FF_PUT_VC1_MSPEL_MC_H_LASX(1);
+FF_PUT_VC1_MSPEL_MC_H_LASX(2);
+FF_PUT_VC1_MSPEL_MC_H_LASX(3);
+
+void ff_put_no_rnd_vc1_chroma_mc8_lasx(uint8_t *dst /* align 8 */,
+                                       uint8_t *src /* align 1 */,
+                                       ptrdiff_t stride, int h, int x, int y);
+
+#endif /* AVCODEC_LOONGARCH_VC1DSP_LOONGARCH_H */
diff --git a/libavcodec/loongarch/videodsp_init.c b/libavcodec/loongarch/videodsp_init.c
new file mode 100644
index 0000000000..6cbb7763ff
--- /dev/null
+++ b/libavcodec/loongarch/videodsp_init.c
@@ -0,0 +1,45 @@
+/*
+ * Copyright (c) 2021 Loongson Technology Corporation Limited
+ * Contributed by Xiwei Gu <guxiwei-hf@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "libavcodec/videodsp.h"
+#include "libavutil/attributes.h"
+
+static void prefetch_loongarch(uint8_t *mem, ptrdiff_t stride, int h)
+{
+    register const uint8_t *p = mem;
+
+    __asm__ volatile (
+        "1:                                     \n\t"
+        "preld      0,     %[p],     0          \n\t"
+        "preld      0,     %[p],     32         \n\t"
+        "addi.d     %[h],  %[h],     -1         \n\t"
+        "add.d      %[p],  %[p],     %[stride]  \n\t"
+
+        "blt        $r0,   %[h],     1b         \n\t"
+        : [p] "+r" (p), [h] "+r" (h)
+        : [stride] "r" (stride)
+    );
+}
+
+av_cold void ff_videodsp_init_loongarch(VideoDSPContext *ctx, int bpc)
+{
+    ctx->prefetch = prefetch_loongarch;
+}
diff --git a/libavcodec/loongarch/vp8_lpf_lsx.c b/libavcodec/loongarch/vp8_lpf_lsx.c
new file mode 100644
index 0000000000..f0fc3f3a5b
--- /dev/null
+++ b/libavcodec/loongarch/vp8_lpf_lsx.c
@@ -0,0 +1,591 @@
+/*
+ * Copyright (c) 2021 Loongson Technology Corporation Limited
+ * Contributed by Hecai Yuan <yuanhecai@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "libavcodec/vp8dsp.h"
+#include "vp8dsp_loongarch.h"
+#include "libavutil/loongarch/loongson_intrinsics.h"
+
+#define VP8_LPF_FILTER4_4W(p1_in_out, p0_in_out, q0_in_out, q1_in_out,  \
+                           mask_in, hev_in)                             \
+{                                                                       \
+    __m128i p1_m, p0_m, q0_m, q1_m, q0_sub_p0, filt_sign;               \
+    __m128i filt, filt1, filt2, cnst4b, cnst3b;                         \
+    __m128i q0_sub_p0_l, q0_sub_p0_h, filt_h, filt_l, cnst3h;           \
+                                                                        \
+    p1_m = __lsx_vxori_b(p1_in_out, 0x80);                              \
+    p0_m = __lsx_vxori_b(p0_in_out, 0x80);                              \
+    q0_m = __lsx_vxori_b(q0_in_out, 0x80);                              \
+    q1_m = __lsx_vxori_b(q1_in_out, 0x80);                              \
+    filt = __lsx_vssub_b(p1_m, q1_m);                                   \
+    filt = filt & hev_in;                                               \
+                                                                        \
+    q0_sub_p0 = __lsx_vsub_b(q0_m, p0_m);                               \
+    filt_sign = __lsx_vslti_b(filt, 0);                                 \
+                                                                        \
+    cnst3h = __lsx_vreplgr2vr_h(3);                                     \
+    q0_sub_p0_l = __lsx_vilvl_b(q0_sub_p0, q0_sub_p0);                  \
+    q0_sub_p0_l = __lsx_vdp2_h_b(q0_sub_p0_l, cnst3h);                  \
+    filt_l = __lsx_vilvl_b(filt_sign, filt);                            \
+    filt_l = __lsx_vadd_h(filt_l, q0_sub_p0_l);                         \
+    filt_l = __lsx_vsat_h(filt_l, 7);                                   \
+                                                                        \
+    q0_sub_p0_h = __lsx_vilvh_b(q0_sub_p0, q0_sub_p0);                  \
+    q0_sub_p0_h = __lsx_vdp2_h_b(q0_sub_p0_h, cnst3h);                  \
+    filt_h = __lsx_vilvh_b(filt_sign, filt);                            \
+    filt_h = __lsx_vadd_h(filt_h, q0_sub_p0_h);                         \
+    filt_h = __lsx_vsat_h(filt_h, 7);                                   \
+                                                                        \
+    filt = __lsx_vpickev_b(filt_h, filt_l);                             \
+    filt = filt & mask_in;                                              \
+    cnst4b = __lsx_vreplgr2vr_b(4);                                     \
+    filt1 = __lsx_vsadd_b(filt, cnst4b);                                \
+    filt1 = __lsx_vsrai_b(filt1, 3);                                    \
+                                                                        \
+    cnst3b = __lsx_vreplgr2vr_b(3);                                     \
+    filt2 = __lsx_vsadd_b(filt, cnst3b);                                \
+    filt2 = __lsx_vsrai_b(filt2, 3);                                    \
+                                                                        \
+    q0_m = __lsx_vssub_b(q0_m, filt1);                                  \
+    q0_in_out = __lsx_vxori_b(q0_m, 0x80);                              \
+    p0_m = __lsx_vsadd_b(p0_m, filt2);                                  \
+    p0_in_out = __lsx_vxori_b(p0_m, 0x80);                              \
+                                                                        \
+    filt = __lsx_vsrari_b(filt1, 1);                                    \
+    hev_in = __lsx_vxori_b(hev_in, 0xff);                               \
+    filt = filt & hev_in;                                               \
+                                                                        \
+    q1_m = __lsx_vssub_b(q1_m, filt);                                   \
+    q1_in_out = __lsx_vxori_b(q1_m, 0x80);                              \
+    p1_m = __lsx_vsadd_b(p1_m, filt);                                   \
+    p1_in_out = __lsx_vxori_b(p1_m, 0x80);                              \
+}
+
+#define VP8_MBFILTER(p2, p1, p0, q0, q1, q2, mask, hev)             \
+{                                                                   \
+    __m128i p2_m, p1_m, p0_m, q2_m, q1_m, q0_m;                     \
+    __m128i filt, q0_sub_p0, cnst4b, cnst3b;                        \
+    __m128i u, filt1, filt2, filt_sign, q0_sub_p0_sign;             \
+    __m128i q0_sub_p0_l, q0_sub_p0_h, filt_l, u_l, u_h, filt_h;     \
+    __m128i cnst3h, cnst27h, cnst18h, cnst63h;                      \
+                                                                    \
+    cnst3h = __lsx_vreplgr2vr_h(3);                                 \
+                                                                    \
+    p2_m = __lsx_vxori_b(p2, 0x80);                                 \
+    p1_m = __lsx_vxori_b(p1, 0x80);                                 \
+    p0_m = __lsx_vxori_b(p0, 0x80);                                 \
+    q0_m = __lsx_vxori_b(q0, 0x80);                                 \
+    q1_m = __lsx_vxori_b(q1, 0x80);                                 \
+    q2_m = __lsx_vxori_b(q2, 0x80);                                 \
+                                                                    \
+    filt = __lsx_vssub_b(p1_m, q1_m);                               \
+    q0_sub_p0 = __lsx_vsub_b(q0_m, p0_m);                           \
+    q0_sub_p0_sign = __lsx_vslti_b(q0_sub_p0, 0);                   \
+    filt_sign = __lsx_vslti_b(filt, 0);                             \
+                                                                    \
+    /* right part */                                                \
+    q0_sub_p0_l = __lsx_vilvl_b(q0_sub_p0_sign, q0_sub_p0);         \
+    q0_sub_p0_l = __lsx_vmul_h(q0_sub_p0_l, cnst3h);                \
+    filt_l = __lsx_vilvl_b(filt_sign, filt);                        \
+    filt_l = __lsx_vadd_h(filt_l, q0_sub_p0_l);                     \
+    filt_l = __lsx_vsat_h(filt_l, 7);                               \
+                                                                    \
+    /* left part */                                                 \
+    q0_sub_p0_h = __lsx_vilvh_b(q0_sub_p0_sign, q0_sub_p0);         \
+    q0_sub_p0_h = __lsx_vmul_h(q0_sub_p0_h, cnst3h);                \
+    filt_h = __lsx_vilvh_b(filt_sign, filt);                        \
+    filt_h = __lsx_vadd_h(filt_h,  q0_sub_p0_h);                    \
+    filt_h = __lsx_vsat_h(filt_h, 7);                               \
+                                                                    \
+    /* combine left and right part */                               \
+    filt = __lsx_vpickev_b(filt_h, filt_l);                         \
+    filt = filt & mask;                                             \
+    filt2 = filt & hev;                                             \
+    /* filt_val &= ~hev */                                          \
+    hev = __lsx_vxori_b(hev, 0xff);                                 \
+    filt = filt & hev;                                              \
+    cnst4b = __lsx_vreplgr2vr_b(4);                                 \
+    filt1 = __lsx_vsadd_b(filt2, cnst4b);                           \
+    filt1 = __lsx_vsrai_b(filt1, 3);                                \
+    cnst3b = __lsx_vreplgr2vr_b(3);                                 \
+    filt2 = __lsx_vsadd_b(filt2, cnst3b);                           \
+    filt2 = __lsx_vsrai_b(filt2, 3);                                \
+    q0_m = __lsx_vssub_b(q0_m, filt1);                              \
+    p0_m = __lsx_vsadd_b(p0_m, filt2);                              \
+                                                                    \
+    filt_sign = __lsx_vslti_b(filt, 0);                             \
+    filt_l = __lsx_vilvl_b(filt_sign, filt);                        \
+    filt_h = __lsx_vilvh_b(filt_sign, filt);                        \
+                                                                    \
+    cnst27h = __lsx_vreplgr2vr_h(27);                               \
+    cnst63h = __lsx_vreplgr2vr_h(63);                               \
+                                                                    \
+    /* right part */                                                \
+    u_l = __lsx_vmul_h(filt_l, cnst27h);                            \
+    u_l = __lsx_vadd_h(u_l, cnst63h);                               \
+    u_l = __lsx_vsrai_h(u_l, 7);                                    \
+    u_l = __lsx_vsat_h(u_l, 7);                                     \
+    /* left part */                                                 \
+    u_h = __lsx_vmul_h(filt_h, cnst27h);                            \
+    u_h = __lsx_vadd_h(u_h, cnst63h);                               \
+    u_h = __lsx_vsrai_h(u_h, 7);                                    \
+    u_h = __lsx_vsat_h(u_h, 7);                                     \
+    /* combine left and right part */                               \
+    u = __lsx_vpickev_b(u_h, u_l);                                  \
+    q0_m = __lsx_vssub_b(q0_m, u);                                  \
+    q0 = __lsx_vxori_b(q0_m, 0x80);                                 \
+    p0_m = __lsx_vsadd_b(p0_m, u);                                  \
+    p0 = __lsx_vxori_b(p0_m, 0x80);                                 \
+    cnst18h = __lsx_vreplgr2vr_h(18);                               \
+    u_l = __lsx_vmul_h(filt_l, cnst18h);                            \
+    u_l = __lsx_vadd_h(u_l, cnst63h);                               \
+    u_l = __lsx_vsrai_h(u_l, 7);                                    \
+    u_l = __lsx_vsat_h(u_l, 7);                                     \
+                                                                    \
+    /* left part */                                                 \
+    u_h = __lsx_vmul_h(filt_h, cnst18h);                            \
+    u_h = __lsx_vadd_h(u_h, cnst63h);                               \
+    u_h = __lsx_vsrai_h(u_h, 7);                                    \
+    u_h = __lsx_vsat_h(u_h, 7);                                     \
+    /* combine left and right part */                               \
+    u = __lsx_vpickev_b(u_h, u_l);                                  \
+    q1_m = __lsx_vssub_b(q1_m, u);                                  \
+    q1 = __lsx_vxori_b(q1_m, 0x80);                                 \
+    p1_m = __lsx_vsadd_b(p1_m, u);                                  \
+    p1 = __lsx_vxori_b(p1_m, 0x80);                                 \
+    u_l = __lsx_vslli_h(filt_l, 3);                                 \
+    u_l = __lsx_vadd_h(u_l, filt_l);                                \
+    u_l = __lsx_vadd_h(u_l, cnst63h);                               \
+    u_l = __lsx_vsrai_h(u_l, 7);                                    \
+    u_l = __lsx_vsat_h(u_l, 7);                                     \
+                                                                    \
+    /* left part */                                                 \
+    u_h = __lsx_vslli_h(filt_h, 3);                                 \
+    u_h = __lsx_vadd_h(u_h, filt_h);                                \
+    u_h = __lsx_vadd_h(u_h, cnst63h);                               \
+    u_h = __lsx_vsrai_h(u_h, 7);                                    \
+    u_h = __lsx_vsat_h(u_h, 7);                                     \
+    /* combine left and right part */                               \
+    u = __lsx_vpickev_b(u_h, u_l);                                  \
+    q2_m = __lsx_vssub_b(q2_m, u);                                  \
+    q2 = __lsx_vxori_b(q2_m, 0x80);                                 \
+    p2_m = __lsx_vsadd_b(p2_m, u);                                  \
+    p2 = __lsx_vxori_b(p2_m, 0x80);                                 \
+}
+
+#define LPF_MASK_HEV(p3_src, p2_src, p1_src, p0_src,                \
+                     q0_src, q1_src, q2_src, q3_src,                \
+                     limit_src, b_limit_src, thresh_src,            \
+                     hev_dst, mask_dst, flat_dst)                   \
+{                                                                   \
+    __m128i p3_asub_p2_m, p2_asub_p1_m, p1_asub_p0_m, q1_asub_q0_m; \
+    __m128i p1_asub_q1_m, p0_asub_q0_m, q3_asub_q2_m, q2_asub_q1_m; \
+                                                                    \
+    /* absolute subtraction of pixel values */                      \
+    p3_asub_p2_m = __lsx_vabsd_bu(p3_src, p2_src);                  \
+    p2_asub_p1_m = __lsx_vabsd_bu(p2_src, p1_src);                  \
+    p1_asub_p0_m = __lsx_vabsd_bu(p1_src, p0_src);                  \
+    q1_asub_q0_m = __lsx_vabsd_bu(q1_src, q0_src);                  \
+    q2_asub_q1_m = __lsx_vabsd_bu(q2_src, q1_src);                  \
+    q3_asub_q2_m = __lsx_vabsd_bu(q3_src, q2_src);                  \
+    p0_asub_q0_m = __lsx_vabsd_bu(p0_src, q0_src);                  \
+    p1_asub_q1_m = __lsx_vabsd_bu(p1_src, q1_src);                  \
+                                                                    \
+    /* calculation of hev */                                        \
+    flat_dst = __lsx_vmax_bu(p1_asub_p0_m, q1_asub_q0_m);           \
+    hev_dst = __lsx_vslt_bu(thresh_src, flat_dst);                  \
+    /* calculation of mask */                                       \
+    p0_asub_q0_m = __lsx_vsadd_bu(p0_asub_q0_m, p0_asub_q0_m);      \
+    p1_asub_q1_m = __lsx_vsrli_b(p1_asub_q1_m, 1);                  \
+    p0_asub_q0_m = __lsx_vsadd_bu(p0_asub_q0_m, p1_asub_q1_m);      \
+    mask_dst = __lsx_vslt_bu(b_limit_src, p0_asub_q0_m);            \
+    mask_dst = __lsx_vmax_bu(flat_dst, mask_dst);                   \
+    p3_asub_p2_m = __lsx_vmax_bu(p3_asub_p2_m, p2_asub_p1_m);       \
+    mask_dst = __lsx_vmax_bu(p3_asub_p2_m, mask_dst);               \
+    q2_asub_q1_m = __lsx_vmax_bu(q2_asub_q1_m, q3_asub_q2_m);       \
+    mask_dst = __lsx_vmax_bu(q2_asub_q1_m, mask_dst);               \
+    mask_dst = __lsx_vslt_bu(limit_src, mask_dst);                  \
+    mask_dst = __lsx_vxori_b(mask_dst, 0xff);                       \
+}
+
+#define VP8_ST6x1_UB(in0, in0_idx, in1, in1_idx, pdst, stride)      \
+{                                                                   \
+    __lsx_vstelm_w(in0, pdst, 0, in0_idx);                          \
+    __lsx_vstelm_h(in1, pdst + stride, 0, in1_idx);                 \
+}
+
+#define ST_W4(in, idx0, idx1, idx2, idx3, pdst, stride)     \
+{                                                           \
+    __lsx_vstelm_w(in, pdst, 0, idx0);                      \
+    pdst += stride;                                         \
+    __lsx_vstelm_w(in, pdst, 0, idx1);                      \
+    pdst += stride;                                         \
+    __lsx_vstelm_w(in, pdst, 0, idx2);                      \
+    pdst += stride;                                         \
+    __lsx_vstelm_w(in, pdst, 0, idx3);                      \
+    pdst += stride;                                         \
+}
+
+void ff_vp8_v_loop_filter16_lsx(uint8_t *dst, ptrdiff_t stride, int b_limit_in,
+                                int limit_in, int thresh_in)
+{
+    __m128i p3, p2, p1, p0, q3, q2, q1, q0;
+    __m128i mask, hev, flat, thresh, limit, b_limit;
+
+    ptrdiff_t stride2 = stride << 1;
+    ptrdiff_t stride3 = stride2 + stride;
+    ptrdiff_t stride4 = stride2 << 1;
+
+    b_limit = __lsx_vreplgr2vr_b(b_limit_in);
+    limit = __lsx_vreplgr2vr_b(limit_in);
+    thresh = __lsx_vreplgr2vr_b(thresh_in);
+
+    /*load vector elements*/
+    DUP4_ARG2(__lsx_vld, dst - stride4, 0, dst - stride3, 0, dst - stride2, 0,
+              dst - stride, 0, p3, p2, p1, p0);
+    DUP4_ARG2(__lsx_vld, dst, 0, dst + stride, 0, dst + stride2, 0, dst + stride3, 0,
+              q0, q1, q2, q3);
+    LPF_MASK_HEV(p3, p2, p1, p0, q0, q1, q2, q3, limit, b_limit, thresh, hev, mask, flat);
+    VP8_MBFILTER(p2, p1, p0, q0, q1, q2, mask, hev);
+
+    /*store vector elements*/
+    __lsx_vst(p2, dst - stride3, 0);
+    __lsx_vst(p1, dst - stride2, 0);
+    __lsx_vst(p0, dst - stride,  0);
+    __lsx_vst(q0, dst,           0);
+
+    __lsx_vst(q1, dst + stride,  0);
+    __lsx_vst(q2, dst + stride2, 0);
+}
+
+void ff_vp8_v_loop_filter8uv_lsx(uint8_t *dst_u, uint8_t *dst_v,
+                                 ptrdiff_t stride, int b_limit_in,
+                                 int limit_in, int thresh_in)
+{
+    __m128i p3, p2, p1, p0, q3, q2, q1, q0;
+    __m128i mask, hev, flat, thresh, limit, b_limit;
+    __m128i p3_u, p2_u, p1_u, p0_u, q3_u, q2_u, q1_u, q0_u;
+    __m128i p3_v, p2_v, p1_v, p0_v, q3_v, q2_v, q1_v, q0_v;
+
+    ptrdiff_t stride2 = stride << 1;
+    ptrdiff_t stride3 = stride2 + stride;
+    ptrdiff_t stride4 = stride2 << 1;
+
+    b_limit = __lsx_vreplgr2vr_b(b_limit_in);
+    limit = __lsx_vreplgr2vr_b(limit_in);
+    thresh = __lsx_vreplgr2vr_b(thresh_in);
+
+    DUP4_ARG2(__lsx_vld, dst_u - stride4, 0, dst_u - stride3, 0, dst_u - stride2, 0,
+              dst_u - stride, 0, p3_u, p2_u, p1_u, p0_u);
+    DUP4_ARG2(__lsx_vld, dst_u, 0, dst_u + stride, 0, dst_u + stride2, 0,
+              dst_u + stride3, 0, q0_u, q1_u, q2_u, q3_u);
+
+    DUP4_ARG2(__lsx_vld, dst_v - stride4, 0, dst_v - stride3, 0, dst_v - stride2, 0,
+              dst_v - stride, 0, p3_v, p2_v, p1_v, p0_v);
+    DUP4_ARG2(__lsx_vld, dst_v, 0, dst_v + stride, 0, dst_v + stride2, 0,
+              dst_v + stride3, 0, q0_v, q1_v, q2_v, q3_v);
+
+    /* rht 8 element of p3 are u pixel and left 8 element of p3 are v pixei */
+    DUP4_ARG2(__lsx_vilvl_d, p3_v, p3_u, p2_v, p2_u, p1_v, p1_u, p0_v, p0_u, p3, p2, p1, p0);
+    DUP4_ARG2(__lsx_vilvl_d, q0_v, q0_u, q1_v, q1_u, q2_v, q2_u, q3_v, q3_u, q0, q1, q2, q3);
+    LPF_MASK_HEV(p3, p2, p1, p0, q0, q1, q2, q3, limit, b_limit, thresh,
+                 hev, mask, flat);
+    VP8_MBFILTER(p2, p1, p0, q0, q1, q2, mask, hev);
+
+    __lsx_vstelm_d(p2, dst_u - stride3, 0, 0);
+    __lsx_vstelm_d(p1, dst_u - stride2, 0, 0);
+    __lsx_vstelm_d(p0, dst_u - stride , 0, 0);
+    __lsx_vstelm_d(q0, dst_u,           0, 0);
+
+    __lsx_vstelm_d(q1, dst_u + stride,  0, 0);
+    __lsx_vstelm_d(q2, dst_u + stride2, 0, 0);
+
+    __lsx_vstelm_d(p2, dst_v - stride3, 0, 1);
+    __lsx_vstelm_d(p1, dst_v - stride2, 0, 1);
+    __lsx_vstelm_d(p0, dst_v - stride , 0, 1);
+    __lsx_vstelm_d(q0, dst_v,           0, 1);
+
+    __lsx_vstelm_d(q1, dst_v + stride,  0, 1);
+    __lsx_vstelm_d(q2, dst_v + stride2, 0, 1);
+}
+
+void ff_vp8_h_loop_filter16_lsx(uint8_t *dst, ptrdiff_t stride, int b_limit_in,
+                                int limit_in, int thresh_in)
+{
+    uint8_t *temp_src;
+    __m128i p3, p2, p1, p0, q3, q2, q1, q0;
+    __m128i mask, hev, flat, thresh, limit, b_limit;
+    __m128i row0, row1, row2, row3, row4, row5, row6, row7, row8;
+    __m128i row9, row10, row11, row12, row13, row14, row15;
+    __m128i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
+
+    ptrdiff_t stride2 = stride << 1;
+    ptrdiff_t stride3 = stride2 + stride;
+    ptrdiff_t stride4 = stride2 << 1;
+
+    b_limit = __lsx_vreplgr2vr_b(b_limit_in);
+    limit = __lsx_vreplgr2vr_b(limit_in);
+    thresh = __lsx_vreplgr2vr_b(thresh_in);
+
+    temp_src = dst - 4;
+    DUP4_ARG2(__lsx_vld, temp_src, 0, temp_src + stride, 0, temp_src + stride2, 0,
+              temp_src + stride3, 0, row0, row1, row2, row3);
+    temp_src += stride4;
+    DUP4_ARG2(__lsx_vld, temp_src, 0, temp_src + stride, 0, temp_src + stride2, 0,
+              temp_src + stride3, 0, row4, row5, row6, row7);
+
+    temp_src += stride4;
+    DUP4_ARG2(__lsx_vld, temp_src, 0, temp_src + stride, 0, temp_src + stride2, 0,
+              temp_src + stride3, 0, row8, row9, row10, row11);
+    temp_src += stride4;
+    DUP4_ARG2(__lsx_vld, temp_src, 0, temp_src + stride, 0, temp_src + stride2, 0,
+              temp_src + stride3, 0, row12, row13, row14, row15);
+    LSX_TRANSPOSE16x8_B(row0, row1, row2, row3, row4, row5, row6, row7, row8, row9, row10,
+                        row11, row12, row13, row14, row15, p3, p2, p1, p0, q0, q1, q2, q3);
+
+    LPF_MASK_HEV(p3, p2, p1, p0, q0, q1, q2, q3, limit, b_limit, thresh, hev, mask, flat);
+    VP8_MBFILTER(p2, p1, p0, q0, q1, q2, mask, hev);
+
+    tmp0 = __lsx_vilvl_b(p1, p2);
+    tmp1 = __lsx_vilvl_b(q0, p0);
+
+    tmp3 = __lsx_vilvl_h(tmp1, tmp0);
+    tmp4 = __lsx_vilvh_h(tmp1, tmp0);
+
+    tmp0 = __lsx_vilvh_b(p1, p2);
+    tmp1 = __lsx_vilvh_b(q0, p0);
+
+    tmp6 = __lsx_vilvl_h(tmp1, tmp0);
+    tmp7 = __lsx_vilvh_h(tmp1, tmp0);
+
+    tmp2 = __lsx_vilvl_b(q2, q1);
+    tmp5 = __lsx_vilvh_b(q2, q1);
+
+    temp_src = dst - 3;
+    VP8_ST6x1_UB(tmp3, 0, tmp2, 0, temp_src, 4);
+    temp_src += stride;
+    VP8_ST6x1_UB(tmp3, 1, tmp2, 1, temp_src, 4);
+    temp_src += stride;
+    VP8_ST6x1_UB(tmp3, 2, tmp2, 2, temp_src, 4);
+    temp_src += stride;
+    VP8_ST6x1_UB(tmp3, 3, tmp2, 3, temp_src, 4);
+    temp_src += stride;
+    VP8_ST6x1_UB(tmp4, 0, tmp2, 4, temp_src, 4);
+    temp_src += stride;
+    VP8_ST6x1_UB(tmp4, 1, tmp2, 5, temp_src, 4);
+    temp_src += stride;
+    VP8_ST6x1_UB(tmp4, 2, tmp2, 6, temp_src, 4);
+    temp_src += stride;
+    VP8_ST6x1_UB(tmp4, 3, tmp2, 7, temp_src, 4);
+    temp_src += stride;
+    VP8_ST6x1_UB(tmp6, 0, tmp5, 0, temp_src, 4);
+    temp_src += stride;
+    VP8_ST6x1_UB(tmp6, 1, tmp5, 1, temp_src, 4);
+    temp_src += stride;
+    VP8_ST6x1_UB(tmp6, 2, tmp5, 2, temp_src, 4);
+    temp_src += stride;
+    VP8_ST6x1_UB(tmp6, 3, tmp5, 3, temp_src, 4);
+    temp_src += stride;
+    VP8_ST6x1_UB(tmp7, 0, tmp5, 4, temp_src, 4);
+    temp_src += stride;
+    VP8_ST6x1_UB(tmp7, 1, tmp5, 5, temp_src, 4);
+    temp_src += stride;
+    VP8_ST6x1_UB(tmp7, 2, tmp5, 6, temp_src, 4);
+    temp_src += stride;
+    VP8_ST6x1_UB(tmp7, 3, tmp5, 7, temp_src, 4);
+}
+
+void ff_vp8_h_loop_filter8uv_lsx(uint8_t *dst_u, uint8_t *dst_v,
+                                 ptrdiff_t stride, int b_limit_in,
+                                 int limit_in, int thresh_in)
+{
+    uint8_t *temp_src;
+    __m128i p3, p2, p1, p0, q3, q2, q1, q0;
+    __m128i mask, hev, flat, thresh, limit, b_limit;
+    __m128i row0, row1, row2, row3, row4, row5, row6, row7, row8;
+    __m128i row9, row10, row11, row12, row13, row14, row15;
+    __m128i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
+
+    ptrdiff_t stride2 = stride << 1;
+    ptrdiff_t stride3 = stride2 + stride;
+    ptrdiff_t stride4 = stride2 << 1;
+
+    b_limit = __lsx_vreplgr2vr_b(b_limit_in);
+    limit = __lsx_vreplgr2vr_b(limit_in);
+    thresh = __lsx_vreplgr2vr_b(thresh_in);
+
+    temp_src = dst_u - 4;
+    DUP4_ARG2(__lsx_vld, temp_src, 0, temp_src + stride, 0, temp_src + stride2, 0,
+              temp_src + stride3, 0, row0, row1, row2, row3);
+    temp_src += stride4;
+    DUP4_ARG2(__lsx_vld, temp_src, 0, temp_src + stride, 0, temp_src + stride2, 0,
+              temp_src + stride3, 0, row4, row5, row6, row7);
+
+    temp_src = dst_v - 4;
+    DUP4_ARG2(__lsx_vld, temp_src, 0, temp_src + stride, 0, temp_src + stride2, 0,
+              temp_src + stride3, 0, row8, row9, row10, row11);
+    temp_src += stride4;
+    DUP4_ARG2(__lsx_vld, temp_src, 0, temp_src + stride, 0, temp_src + stride2, 0,
+              temp_src + stride3, 0, row12, row13, row14, row15);
+
+    LSX_TRANSPOSE16x8_B(row0, row1, row2, row3, row4, row5, row6, row7,
+                        row8, row9, row10, row11, row12, row13, row14, row15,
+                        p3, p2, p1, p0, q0, q1, q2, q3);
+
+    LPF_MASK_HEV(p3, p2, p1, p0, q0, q1, q2, q3, limit, b_limit, thresh, hev, mask, flat);
+    VP8_MBFILTER(p2, p1, p0, q0, q1, q2, mask, hev);
+
+    tmp0 = __lsx_vilvl_b(p1, p2);
+    tmp1 = __lsx_vilvl_b(q0, p0);
+
+    tmp3 = __lsx_vilvl_h(tmp1, tmp0);
+    tmp4 = __lsx_vilvh_h(tmp1, tmp0);
+
+    tmp0 = __lsx_vilvh_b(p1, p2);
+    tmp1 = __lsx_vilvh_b(q0, p0);
+
+    tmp6 = __lsx_vilvl_h(tmp1, tmp0);
+    tmp7 = __lsx_vilvh_h(tmp1, tmp0);
+
+    tmp2 = __lsx_vilvl_b(q2, q1);
+    tmp5 = __lsx_vilvh_b(q2, q1);
+
+    dst_u -= 3;
+    VP8_ST6x1_UB(tmp3, 0, tmp2, 0, dst_u, 4);
+    dst_u += stride;
+    VP8_ST6x1_UB(tmp3, 1, tmp2, 1, dst_u, 4);
+    dst_u += stride;
+    VP8_ST6x1_UB(tmp3, 2, tmp2, 2, dst_u, 4);
+    dst_u += stride;
+    VP8_ST6x1_UB(tmp3, 3, tmp2, 3, dst_u, 4);
+    dst_u += stride;
+    VP8_ST6x1_UB(tmp4, 0, tmp2, 4, dst_u, 4);
+    dst_u += stride;
+    VP8_ST6x1_UB(tmp4, 1, tmp2, 5, dst_u, 4);
+    dst_u += stride;
+    VP8_ST6x1_UB(tmp4, 2, tmp2, 6, dst_u, 4);
+    dst_u += stride;
+    VP8_ST6x1_UB(tmp4, 3, tmp2, 7, dst_u, 4);
+
+    dst_v -= 3;
+    VP8_ST6x1_UB(tmp6, 0, tmp5, 0, dst_v, 4);
+    dst_v += stride;
+    VP8_ST6x1_UB(tmp6, 1, tmp5, 1, dst_v, 4);
+    dst_v += stride;
+    VP8_ST6x1_UB(tmp6, 2, tmp5, 2, dst_v, 4);
+    dst_v += stride;
+    VP8_ST6x1_UB(tmp6, 3, tmp5, 3, dst_v, 4);
+    dst_v += stride;
+    VP8_ST6x1_UB(tmp7, 0, tmp5, 4, dst_v, 4);
+    dst_v += stride;
+    VP8_ST6x1_UB(tmp7, 1, tmp5, 5, dst_v, 4);
+    dst_v += stride;
+    VP8_ST6x1_UB(tmp7, 2, tmp5, 6, dst_v, 4);
+    dst_v += stride;
+    VP8_ST6x1_UB(tmp7, 3, tmp5, 7, dst_v, 4);
+}
+
+void ff_vp8_v_loop_filter16_inner_lsx(uint8_t *src, ptrdiff_t stride,
+                                      int32_t e, int32_t i, int32_t h)
+{
+    __m128i mask, hev, flat;
+    __m128i thresh, b_limit, limit;
+    __m128i p3, p2, p1, p0, q3, q2, q1, q0;
+
+    ptrdiff_t stride2 = stride << 1;
+    ptrdiff_t stride3 = stride2 + stride;
+    ptrdiff_t stride4 = stride2 << 1;
+
+    /* load vector elements */
+    src -= stride4;
+    DUP4_ARG2(__lsx_vld, src, 0, src + stride, 0, src + stride2, 0,
+              src + stride3, 0, p3, p2, p1, p0);
+    src += stride4;
+    DUP4_ARG2(__lsx_vld, src, 0, src + stride, 0, src + stride2, 0,
+              src + stride3, 0, q0, q1, q2, q3);
+    thresh = __lsx_vreplgr2vr_b(h);
+    b_limit = __lsx_vreplgr2vr_b(e);
+    limit = __lsx_vreplgr2vr_b(i);
+
+    LPF_MASK_HEV(p3, p2, p1, p0, q0, q1, q2, q3, limit, b_limit, thresh,
+                 hev, mask, flat);
+    VP8_LPF_FILTER4_4W(p1, p0, q0, q1, mask, hev);
+
+    __lsx_vst(p1, src - stride2, 0);
+    __lsx_vst(p0, src - stride,  0);
+    __lsx_vst(q0, src,           0);
+    __lsx_vst(q1, src + stride,  0);
+}
+
+void ff_vp8_h_loop_filter16_inner_lsx(uint8_t *src, ptrdiff_t stride,
+                                      int32_t e, int32_t i, int32_t h)
+{
+    __m128i mask, hev, flat;
+    __m128i thresh, b_limit, limit;
+    __m128i p3, p2, p1, p0, q3, q2, q1, q0;
+    __m128i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
+    __m128i tmp8, tmp9, tmp10, tmp11, tmp12, tmp13, tmp14, tmp15;
+
+    ptrdiff_t stride2 = stride << 1;
+    ptrdiff_t stride3 = stride2 + stride;
+    ptrdiff_t stride4 = stride2 << 1;
+
+    src -= 4;
+    DUP4_ARG2(__lsx_vld, src, 0, src + stride, 0, src + stride2, 0,
+              src + stride3, 0, tmp0, tmp1, tmp2, tmp3);
+    src += stride4;
+    DUP4_ARG2(__lsx_vld, src, 0, src + stride, 0, src + stride2, 0,
+              src + stride3, 0, tmp4, tmp5, tmp6, tmp7);
+    src += stride4;
+    DUP4_ARG2(__lsx_vld, src, 0, src + stride, 0, src + stride2, 0,
+              src + stride3, 0, tmp8, tmp9, tmp10, tmp11);
+    src += stride4;
+    DUP4_ARG2(__lsx_vld, src, 0, src + stride, 0, src + stride2, 0,
+              src + stride3, 0, tmp12, tmp13, tmp14, tmp15);
+    src -= 3 * stride4;
+
+    LSX_TRANSPOSE16x8_B(tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7,
+                        tmp8, tmp9, tmp10, tmp11, tmp12, tmp13, tmp14, tmp15,
+                        p3, p2, p1, p0, q0, q1, q2, q3);
+
+    thresh = __lsx_vreplgr2vr_b(h);
+    b_limit = __lsx_vreplgr2vr_b(e);
+    limit = __lsx_vreplgr2vr_b(i);
+
+    LPF_MASK_HEV(p3, p2, p1, p0, q0, q1, q2, q3, limit, b_limit, thresh,
+                 hev, mask, flat);
+    VP8_LPF_FILTER4_4W(p1, p0, q0, q1, mask, hev);
+
+    DUP2_ARG2(__lsx_vilvl_b, p0, p1, q1, q0, tmp0, tmp1);
+    tmp2 = __lsx_vilvl_h(tmp1, tmp0);
+    tmp3 = __lsx_vilvh_h(tmp1, tmp0);
+
+    src += 2;
+    ST_W4(tmp2, 0, 1, 2, 3, src, stride);
+    ST_W4(tmp3, 0, 1, 2, 3, src, stride);
+
+    DUP2_ARG2(__lsx_vilvh_b, p0, p1, q1, q0, tmp0, tmp1);
+    tmp2 = __lsx_vilvl_h(tmp1, tmp0);
+    tmp3 = __lsx_vilvh_h(tmp1, tmp0);
+
+    ST_W4(tmp2, 0, 1, 2, 3, src, stride);
+    ST_W4(tmp3, 0, 1, 2, 3, src, stride);
+    src -= 4 * stride4;
+}
diff --git a/libavcodec/loongarch/vp8_mc_lsx.c b/libavcodec/loongarch/vp8_mc_lsx.c
new file mode 100644
index 0000000000..80c4f87e80
--- /dev/null
+++ b/libavcodec/loongarch/vp8_mc_lsx.c
@@ -0,0 +1,951 @@
+/*
+ * Copyright (c) 2021 Loongson Technology Corporation Limited
+ * Contributed by Hecai Yuan <yuanhecai@loongson.cn>
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+#include "libavcodec/vp8dsp.h"
+#include "libavutil/loongarch/loongson_intrinsics.h"
+#include "vp8dsp_loongarch.h"
+
+static const uint8_t mc_filt_mask_arr[16 * 3] = {
+    /* 8 width cases */
+    0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8,
+    /* 4 width cases */
+    0, 1, 1, 2, 2, 3, 3, 4, 16, 17, 17, 18, 18, 19, 19, 20,
+    /* 4 width cases */
+    8, 9, 9, 10, 10, 11, 11, 12, 24, 25, 25, 26, 26, 27, 27, 28
+};
+
+static const int8_t subpel_filters_lsx[7][8] = {
+    {-6, 123, 12, -1, 0, 0, 0, 0},
+    {2, -11, 108, 36, -8, 1, 0, 0},     /* New 1/4 pel 6 tap filter */
+    {-9, 93, 50, -6, 0, 0, 0, 0},
+    {3, -16, 77, 77, -16, 3, 0, 0},     /* New 1/2 pel 6 tap filter */
+    {-6, 50, 93, -9, 0, 0, 0, 0},
+    {1, -8, 36, 108, -11, 2, 0, 0},     /* New 1/4 pel 6 tap filter */
+    {-1, 12, 123, -6, 0, 0, 0, 0},
+};
+
+#define DPADD_SH3_SH(in0, in1, in2, coeff0, coeff1, coeff2)         \
+( {                                                                 \
+    __m128i out0_m;                                                 \
+                                                                    \
+    out0_m = __lsx_vdp2_h_b(in0, coeff0);                           \
+    out0_m = __lsx_vdp2add_h_b(out0_m, in1, coeff1);                \
+    out0_m = __lsx_vdp2add_h_b(out0_m, in2, coeff2);                \
+                                                                    \
+    out0_m;                                                         \
+} )
+
+#define VSHF_B3_SB(in0, in1, in2, in3, in4, in5, mask0, mask1, mask2,  \
+                out0, out1, out2)                                      \
+{                                                                      \
+    DUP2_ARG3(__lsx_vshuf_b, in1, in0, mask0, in3, in2, mask1,         \
+              out0, out1);                                             \
+    out2 = __lsx_vshuf_b(in5, in4, mask2);                             \
+}
+
+#define HORIZ_6TAP_FILT(src0, src1, mask0, mask1, mask2,                 \
+                        filt_h0, filt_h1, filt_h2)                       \
+( {                                                                      \
+    __m128i vec0_m, vec1_m, vec2_m;                                      \
+    __m128i hz_out_m;                                                    \
+                                                                         \
+    VSHF_B3_SB(src0, src1, src0, src1, src0, src1, mask0, mask1, mask2,  \
+               vec0_m, vec1_m, vec2_m);                                  \
+    hz_out_m = DPADD_SH3_SH(vec0_m, vec1_m, vec2_m,                      \
+                            filt_h0, filt_h1, filt_h2);                  \
+                                                                         \
+    hz_out_m = __lsx_vsrari_h(hz_out_m, 7);                              \
+    hz_out_m = __lsx_vsat_h(hz_out_m, 7);                                \
+                                                                         \
+    hz_out_m;                                                            \
+} )
+
+#define HORIZ_6TAP_8WID_4VECS_FILT(src0, src1, src2, src3,                            \
+                                   mask0, mask1, mask2,                               \
+                                   filt0, filt1, filt2,                               \
+                                   out0, out1, out2, out3)                            \
+{                                                                                     \
+    __m128i vec0_m, vec1_m, vec2_m, vec3_m, vec4_m, vec5_m, vec6_m, vec7_m;           \
+                                                                                      \
+    DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src1, mask0, src2, src2,        \
+              mask0, src3, src3, mask0, vec0_m, vec1_m, vec2_m, vec3_m);              \
+    DUP4_ARG2(__lsx_vdp2_h_b, vec0_m, filt0, vec1_m, filt0, vec2_m, filt0,            \
+              vec3_m, filt0, out0, out1, out2, out3);                                 \
+    DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src1, mask1, src2, src2,        \
+              mask1, src3, src3, mask1, vec0_m, vec1_m, vec2_m, vec3_m);              \
+    DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src1, mask2, src2, src2,        \
+              mask2, src3, src3, mask2, vec4_m, vec5_m, vec6_m, vec7_m);              \
+    DUP4_ARG3(__lsx_vdp2add_h_b, out0, vec0_m, filt1, out1, vec1_m, filt1,            \
+              out2, vec2_m, filt1, out3, vec3_m, filt1, out0, out1, out2, out3);      \
+    DUP4_ARG3(__lsx_vdp2add_h_b, out0, vec4_m, filt2, out1, vec5_m, filt2,            \
+              out2, vec6_m, filt2, out3, vec7_m, filt2, out0, out1, out2, out3);      \
+}
+
+#define FILT_4TAP_DPADD_S_H(vec0, vec1, filt0, filt1)           \
+( {                                                             \
+    __m128i tmp0;                                               \
+                                                                \
+    tmp0 = __lsx_vdp2_h_b(vec0, filt0);                         \
+    tmp0 = __lsx_vdp2add_h_b(tmp0, vec1, filt1);                \
+                                                                \
+    tmp0;                                                       \
+} )
+
+#define HORIZ_4TAP_FILT(src0, src1, mask0, mask1, filt_h0, filt_h1)    \
+( {                                                                    \
+    __m128i vec0_m, vec1_m;                                            \
+    __m128i hz_out_m;                                                  \
+    DUP2_ARG3(__lsx_vshuf_b, src1, src0, mask0, src1, src0, mask1,     \
+              vec0_m, vec1_m);                                         \
+    hz_out_m = FILT_4TAP_DPADD_S_H(vec0_m, vec1_m, filt_h0, filt_h1);  \
+                                                                       \
+    hz_out_m = __lsx_vsrari_h(hz_out_m, 7);                            \
+    hz_out_m = __lsx_vsat_h(hz_out_m, 7);                              \
+                                                                       \
+    hz_out_m;                                                          \
+} )
+
+void ff_put_vp8_epel8_h6_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                             uint8_t *src, ptrdiff_t src_stride,
+                             int height, int mx, int my)
+{
+    uint32_t loop_cnt;
+    const int8_t *filter = subpel_filters_lsx[mx - 1];
+    __m128i src0, src1, src2, src3, filt0, filt1, filt2;
+    __m128i mask0, mask1, mask2;
+    __m128i out0, out1, out2, out3;
+
+    ptrdiff_t src_stride2 = src_stride << 1;
+    ptrdiff_t src_stride3 = src_stride2 + src_stride;
+    ptrdiff_t src_stride4 = src_stride2 << 1;
+
+    mask0 = __lsx_vld(mc_filt_mask_arr, 0);
+    src -= 2;
+
+    /* rearranging filter */
+    DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
+    filt2 = __lsx_vldrepl_h(filter, 4);
+
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+
+    DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride2, 0,
+              src + src_stride3, 0, src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+              src0, src1, src2, src3);
+    src += src_stride4;
+    HORIZ_6TAP_8WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2,
+                               filt0, filt1, filt2, out0, out1, out2, out3);
+
+    DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
+    DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
+    __lsx_vstelm_d(out0, dst, 0, 0);
+    dst += dst_stride;
+    __lsx_vstelm_d(out0, dst, 0, 1);
+    dst += dst_stride;
+    __lsx_vstelm_d(out1, dst, 0, 0);
+    dst += dst_stride;
+    __lsx_vstelm_d(out1, dst, 0, 1);
+    dst += dst_stride;
+
+    for (loop_cnt = (height >> 2) - 1; loop_cnt--;) {
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride2, 0,
+                  src + src_stride3, 0, src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+                  src0, src1, src2, src3);
+        src += src_stride4;
+        HORIZ_6TAP_8WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2,
+                                   filt0, filt1, filt2, out0, out1, out2, out3);
+
+        DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
+        DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
+
+        __lsx_vstelm_d(out0, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(out0, dst, 0, 1);
+        dst += dst_stride;
+        __lsx_vstelm_d(out1, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(out1, dst, 0, 1);
+        dst += dst_stride;
+    }
+}
+
+void ff_put_vp8_epel16_h6_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                              uint8_t *src, ptrdiff_t src_stride,
+                              int height, int mx, int my)
+{
+    uint32_t loop_cnt;
+    const int8_t *filter = subpel_filters_lsx[mx - 1];
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7, filt0, filt1;
+    __m128i filt2, mask0, mask1, mask2;
+    __m128i out0, out1, out2, out3, out4, out5, out6, out7;
+
+    ptrdiff_t src_stride2 = src_stride << 1;
+    ptrdiff_t src_stride3 = src_stride2 + src_stride;
+    ptrdiff_t src_stride4 = src_stride2 << 1;
+
+    mask0 = __lsx_vld(mc_filt_mask_arr, 0);
+    src -= 2;
+    /* rearranging filter */
+    DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
+    filt2 = __lsx_vldrepl_h(filter, 4);
+
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+
+    for (loop_cnt = (height >> 2); loop_cnt--;) {
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride2,
+                  0, src + src_stride3, 0, src0 ,src2, src4, src6);
+        DUP4_ARG2(__lsx_vld, src, 8, src + src_stride, 8, src + src_stride2,
+                  8, src + src_stride3, 8, src1, src3, src5, src7);
+
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+                  src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src6, 128, src7, 128,
+                  src4, src5, src6, src7);
+        src += src_stride4;
+
+        HORIZ_6TAP_8WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2,
+                                   filt0, filt1, filt2, out0, out1, out2, out3);
+        HORIZ_6TAP_8WID_4VECS_FILT(src4, src5, src6, src7, mask0, mask1, mask2,
+                                   filt0, filt1, filt2, out4, out5, out6, out7);
+        DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
+        DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
+        __lsx_vst(out0, dst, 0);
+        dst += dst_stride;
+        __lsx_vst(out1, dst, 0);
+        dst += dst_stride;
+
+        DUP2_ARG3(__lsx_vssrarni_b_h, out5, out4, 7, out7, out6, 7, out4, out5);
+        DUP2_ARG2(__lsx_vxori_b, out4, 128, out5, 128, out4, out5);
+        __lsx_vst(out4, dst, 0);
+        dst += dst_stride;
+        __lsx_vst(out5, dst, 0);
+        dst += dst_stride;
+    }
+}
+
+void ff_put_vp8_epel8_v6_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                             uint8_t *src, ptrdiff_t src_stride,
+                             int height, int mx, int my)
+{
+    uint32_t loop_cnt;
+    const int8_t *filter = subpel_filters_lsx[my - 1];
+    __m128i src0, src1, src2, src3, src4, src7, src8, src9, src10;
+    __m128i src10_l, src32_l, src76_l, src98_l, src21_l, src43_l, src87_l;
+    __m128i src109_l, filt0, filt1, filt2;
+    __m128i out0_l, out1_l, out2_l, out3_l;
+
+    ptrdiff_t src_stride2 = src_stride << 1;
+    ptrdiff_t src_stride3 = src_stride2 + src_stride;
+    ptrdiff_t src_stride4 = src_stride2 << 1;
+
+    src -= src_stride2;
+    DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
+    filt2 = __lsx_vldrepl_h(filter, 4);
+
+    DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride2, 0,
+              src + src_stride3, 0, src0, src1, src2, src3);
+    src += src_stride4;
+    src4 = __lsx_vld(src, 0);
+    src += src_stride;
+
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+              src0, src1, src2, src3);
+    src4 = __lsx_vxori_b(src4, 128);
+
+    DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src2, src1, src4,
+              src3, src10_l, src32_l, src21_l, src43_l);
+    for (loop_cnt = (height >> 2); loop_cnt--;) {
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride2,
+                  0, src + src_stride3, 0, src7, src8, src9, src10);
+        DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10,
+                  128, src7, src8, src9, src10);
+        src += src_stride4;
+
+        DUP4_ARG2(__lsx_vilvl_b, src7, src4, src8, src7, src9, src8, src10,
+                  src9, src76_l, src87_l, src98_l, src109_l);
+
+        out0_l = DPADD_SH3_SH(src10_l, src32_l, src76_l, filt0, filt1, filt2);
+        out1_l = DPADD_SH3_SH(src21_l, src43_l, src87_l, filt0, filt1, filt2);
+        out2_l = DPADD_SH3_SH(src32_l, src76_l, src98_l, filt0, filt1, filt2);
+        out3_l = DPADD_SH3_SH(src43_l, src87_l, src109_l, filt0, filt1, filt2);
+
+        DUP2_ARG3(__lsx_vssrarni_b_h, out1_l, out0_l, 7, out3_l, out2_l, 7,
+                  out0_l, out1_l);
+        DUP2_ARG2(__lsx_vxori_b, out0_l, 128, out1_l, 128, out0_l, out1_l);
+
+        __lsx_vstelm_d(out0_l, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(out0_l, dst, 0, 1);
+        dst += dst_stride;
+        __lsx_vstelm_d(out1_l, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(out1_l, dst, 0, 1);
+        dst += dst_stride;
+
+        src10_l = src76_l;
+        src32_l = src98_l;
+        src21_l = src87_l;
+        src43_l = src109_l;
+        src4 = src10;
+    }
+}
+
+void ff_put_vp8_epel16_v6_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                              uint8_t *src, ptrdiff_t src_stride,
+                              int height, int mx, int my)
+{
+    uint32_t loop_cnt;
+    const int8_t *filter = subpel_filters_lsx[my - 1];
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8;
+    __m128i src10_l, src32_l, src54_l, src76_l, src21_l, src43_l, src65_l, src87_l;
+    __m128i src10_h, src32_h, src54_h, src76_h, src21_h, src43_h, src65_h, src87_h;
+    __m128i filt0, filt1, filt2;
+    __m128i tmp0, tmp1, tmp2, tmp3;
+
+    ptrdiff_t src_stride2 = src_stride << 1;
+    ptrdiff_t src_stride3 = src_stride2 + src_stride;
+    ptrdiff_t src_stride4 = src_stride2 << 1;
+
+    DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
+    filt2 = __lsx_vldrepl_h(filter, 4);
+
+    DUP4_ARG2(__lsx_vld, src - src_stride2, 0, src - src_stride, 0, src, 0,
+              src + src_stride, 0, src0, src1, src2, src3);
+    src4 = __lsx_vld(src + src_stride2, 0);
+    src += src_stride3;
+
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128, src0,
+              src1, src2, src3);
+    src4 = __lsx_vxori_b(src4, 128);
+
+    DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src4, src3, src2, src1,
+              src10_l, src32_l, src43_l, src21_l);
+    DUP4_ARG2(__lsx_vilvh_b, src1, src0, src3, src2, src4, src3, src2, src1,
+              src10_h, src32_h, src43_h, src21_h);
+
+    for (loop_cnt = (height >> 2); loop_cnt--;) {
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride2, 0,
+                  src + src_stride3, 0, src5, src6, src7, src8);
+        src += src_stride4;
+        DUP4_ARG2(__lsx_vxori_b, src5, 128, src6, 128, src7, 128, src8, 128,
+                  src5, src6, src7, src8);
+
+        DUP4_ARG2(__lsx_vilvl_b, src5, src4, src6, src5, src7, src6, src8, src7,
+                  src54_l, src65_l, src76_l, src87_l);
+        DUP4_ARG2(__lsx_vilvh_b, src5, src4, src6, src5, src7, src6, src8, src7,
+                  src54_h, src65_h, src76_h, src87_h);
+
+        tmp0 = DPADD_SH3_SH(src10_l, src32_l, src54_l, filt0, filt1, filt2);
+        tmp1 = DPADD_SH3_SH(src21_l, src43_l, src65_l, filt0, filt1, filt2);
+        tmp2 = DPADD_SH3_SH(src10_h, src32_h, src54_h, filt0, filt1, filt2);
+        tmp3 = DPADD_SH3_SH(src21_h, src43_h, src65_h, filt0, filt1, filt2);
+
+        DUP2_ARG3(__lsx_vssrarni_b_h, tmp2, tmp0, 7, tmp3, tmp1, 7, tmp0, tmp1);
+        DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
+        __lsx_vst(tmp0, dst, 0);
+        dst += dst_stride;
+        __lsx_vst(tmp1, dst, 0);
+        dst += dst_stride;
+
+        tmp0 = DPADD_SH3_SH(src32_l, src54_l, src76_l, filt0, filt1, filt2);
+        tmp1 = DPADD_SH3_SH(src43_l, src65_l, src87_l, filt0, filt1, filt2);
+        tmp2 = DPADD_SH3_SH(src32_h, src54_h, src76_h, filt0, filt1, filt2);
+        tmp3 = DPADD_SH3_SH(src43_h, src65_h, src87_h, filt0, filt1, filt2);
+
+        DUP2_ARG3(__lsx_vssrarni_b_h, tmp2, tmp0, 7, tmp3, tmp1, 7, tmp0, tmp1);
+        DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
+        __lsx_vst(tmp0, dst, 0);
+        dst += dst_stride;
+        __lsx_vst(tmp1, dst, 0);
+        dst += dst_stride;
+
+        src10_l = src54_l;
+        src32_l = src76_l;
+        src21_l = src65_l;
+        src43_l = src87_l;
+        src10_h = src54_h;
+        src32_h = src76_h;
+        src21_h = src65_h;
+        src43_h = src87_h;
+        src4 = src8;
+    }
+}
+
+void ff_put_vp8_epel8_h6v6_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                               uint8_t *src, ptrdiff_t src_stride,
+                               int height, int mx, int my)
+{
+    uint32_t loop_cnt;
+    const int8_t *filter_horiz = subpel_filters_lsx[mx - 1];
+    const int8_t *filter_vert = subpel_filters_lsx[my - 1];
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8;
+    __m128i filt_hz0, filt_hz1, filt_hz2;
+    __m128i mask0, mask1, mask2, filt_vt0, filt_vt1, filt_vt2;
+    __m128i hz_out0, hz_out1, hz_out2, hz_out3, hz_out4, hz_out5, hz_out6;
+    __m128i hz_out7, hz_out8, out0, out1, out2, out3, out4, out5, out6, out7;
+    __m128i tmp0, tmp1, tmp2, tmp3;
+
+    ptrdiff_t src_stride2 = src_stride << 1;
+    ptrdiff_t src_stride3 = src_stride2 + src_stride;
+    ptrdiff_t src_stride4 = src_stride2 << 1;
+
+    mask0 = __lsx_vld(mc_filt_mask_arr, 0);
+    src -= (2 + src_stride2);
+
+    /* rearranging filter */
+    DUP2_ARG2(__lsx_vldrepl_h, filter_horiz, 0, filter_horiz, 2, filt_hz0, filt_hz1);
+    filt_hz2 = __lsx_vldrepl_h(filter_horiz, 4);
+
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+
+    DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride2, 0,
+              src + src_stride3, 0, src0, src1, src2, src3);
+    src += src_stride4;
+    src4 = __lsx_vld(src, 0);
+    src +=  src_stride;
+
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+              src0 ,src1, src2, src3);
+    src4 = __lsx_vxori_b(src4, 128);
+
+    hz_out0 = HORIZ_6TAP_FILT(src0, src0, mask0, mask1, mask2, filt_hz0,
+                              filt_hz1, filt_hz2);
+    hz_out1 = HORIZ_6TAP_FILT(src1, src1, mask0, mask1, mask2, filt_hz0,
+                              filt_hz1, filt_hz2);
+    hz_out2 = HORIZ_6TAP_FILT(src2, src2, mask0, mask1, mask2, filt_hz0,
+                              filt_hz1, filt_hz2);
+    hz_out3 = HORIZ_6TAP_FILT(src3, src3, mask0, mask1, mask2, filt_hz0,
+                              filt_hz1, filt_hz2);
+    hz_out4 = HORIZ_6TAP_FILT(src4, src4, mask0, mask1, mask2, filt_hz0,
+                              filt_hz1, filt_hz2);
+
+    DUP2_ARG2(__lsx_vldrepl_h, filter_vert, 0, filter_vert, 2, filt_vt0, filt_vt1);
+    filt_vt2 = __lsx_vldrepl_h(filter_vert, 4);
+
+    DUP2_ARG2(__lsx_vpackev_b, hz_out1, hz_out0, hz_out3, hz_out2, out0, out1);
+    DUP2_ARG2(__lsx_vpackev_b, hz_out2, hz_out1, hz_out4, hz_out3, out3, out4);
+    for (loop_cnt = (height >> 2); loop_cnt--;) {
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride2, 0,
+                  src + src_stride3, 0, src5, src6, src7, src8);
+        src += src_stride4;
+
+        DUP4_ARG2(__lsx_vxori_b, src5, 128, src6, 128, src7, 128, src8, 128,
+                  src5, src6, src7, src8);
+
+        hz_out5 = HORIZ_6TAP_FILT(src5, src5, mask0, mask1, mask2, filt_hz0,
+                                  filt_hz1, filt_hz2);
+        out2 = __lsx_vpackev_b(hz_out5, hz_out4);
+        tmp0 = DPADD_SH3_SH(out0, out1, out2,filt_vt0, filt_vt1, filt_vt2);
+
+        hz_out6 = HORIZ_6TAP_FILT(src6, src6, mask0, mask1, mask2, filt_hz0,
+                                  filt_hz1, filt_hz2);
+        out5 = __lsx_vpackev_b(hz_out6, hz_out5);
+        tmp1 = DPADD_SH3_SH(out3, out4, out5, filt_vt0, filt_vt1, filt_vt2);
+
+        hz_out7 = HORIZ_6TAP_FILT(src7, src7, mask0, mask1, mask2, filt_hz0,
+                                  filt_hz1, filt_hz2);
+
+        out7 = __lsx_vpackev_b(hz_out7, hz_out6);
+        tmp2 = DPADD_SH3_SH(out1, out2, out7, filt_vt0, filt_vt1, filt_vt2);
+
+        hz_out8 = HORIZ_6TAP_FILT(src8, src8, mask0, mask1, mask2, filt_hz0,
+                                  filt_hz1, filt_hz2);
+        out6 = __lsx_vpackev_b(hz_out8, hz_out7);
+        tmp3 = DPADD_SH3_SH(out4, out5, out6, filt_vt0, filt_vt1, filt_vt2);
+
+        DUP2_ARG3(__lsx_vssrarni_b_h, tmp1, tmp0, 7, tmp3, tmp2, 7, tmp0, tmp1);
+        DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
+        __lsx_vstelm_d(tmp0, dst, 0, 0);
+
+        dst += dst_stride;
+        __lsx_vstelm_d(tmp0, dst, 0, 1);
+        dst += dst_stride;
+        __lsx_vstelm_d(tmp1, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(tmp1, dst, 0, 1);
+        dst += dst_stride;
+
+        hz_out4 = hz_out8;
+        out0 = out2;
+        out1 = out7;
+        out3 = out5;
+        out4 = out6;
+    }
+}
+
+void ff_put_vp8_epel16_h6v6_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                                uint8_t *src, ptrdiff_t src_stride,
+                                int height, int mx, int my)
+{
+    int32_t multiple8_cnt;
+
+    for (multiple8_cnt = 2; multiple8_cnt--;) {
+        ff_put_vp8_epel8_h6v6_lsx(dst, dst_stride, src, src_stride, height, mx, my);
+        src += 8;
+        dst += 8;
+    }
+}
+
+void ff_put_vp8_epel8_v4_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                             uint8_t *src, ptrdiff_t src_stride,
+                             int height, int mx, int my)
+{
+    uint32_t loop_cnt;
+    const int8_t *filter = subpel_filters_lsx[my - 1];
+    __m128i src0, src1, src2, src7, src8, src9, src10;
+    __m128i src10_l, src72_l, src98_l, src21_l, src87_l, src109_l, filt0, filt1;
+    __m128i out0, out1, out2, out3;
+
+    ptrdiff_t src_stride2 = src_stride << 1;
+    ptrdiff_t src_stride3 = src_stride2 + src_stride;
+    ptrdiff_t src_stride4 = src_stride2 << 1;
+
+    src -= src_stride;
+
+    DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
+    DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src1);
+    src2 = __lsx_vld(src + src_stride2, 0);
+    src += src_stride3;
+
+    DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
+    src2 = __lsx_vxori_b(src2, 128);
+    DUP2_ARG2(__lsx_vilvl_b, src1, src0, src2, src1, src10_l, src21_l);
+
+    for (loop_cnt = (height >> 2); loop_cnt--;) {
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride2, 0,
+                  src + src_stride3, 0, src7, src8, src9, src10);
+        src += src_stride4;
+
+        DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
+                  src7, src8, src9, src10);
+        DUP4_ARG2(__lsx_vilvl_b, src7, src2, src8, src7, src9, src8, src10, src9,
+                  src72_l, src87_l, src98_l, src109_l);
+
+        out0 = FILT_4TAP_DPADD_S_H(src10_l, src72_l, filt0, filt1);
+        out1 = FILT_4TAP_DPADD_S_H(src21_l, src87_l, filt0, filt1);
+        out2 = FILT_4TAP_DPADD_S_H(src72_l, src98_l, filt0, filt1);
+        out3 = FILT_4TAP_DPADD_S_H(src87_l, src109_l, filt0, filt1);
+        DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
+        DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
+
+        __lsx_vstelm_d(out0, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(out0, dst, 0, 1);
+        dst += dst_stride;
+        __lsx_vstelm_d(out1, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(out1, dst, 0, 1);
+        dst += dst_stride;
+
+        src10_l = src98_l;
+        src21_l = src109_l;
+        src2 = src10;
+    }
+}
+
+void ff_put_vp8_epel16_v4_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                              uint8_t *src, ptrdiff_t src_stride,
+                              int height, int mx, int my)
+{
+    uint32_t loop_cnt;
+    const int8_t *filter = subpel_filters_lsx[my - 1];
+    __m128i src0, src1, src2, src3, src4, src5, src6;
+    __m128i src10_l, src32_l, src54_l, src21_l, src43_l, src65_l, src10_h;
+    __m128i src32_h, src54_h, src21_h, src43_h, src65_h, filt0, filt1;
+    __m128i tmp0, tmp1, tmp2, tmp3;
+
+    ptrdiff_t src_stride2 = src_stride << 1;
+    ptrdiff_t src_stride3 = src_stride2 + src_stride;
+    ptrdiff_t src_stride4 = src_stride2 << 1;
+
+    src -= src_stride;
+    DUP2_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filt0, filt1);
+    DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src1);
+    src2 = __lsx_vld(src + src_stride2, 0);
+    src += src_stride3;
+
+    DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
+    src2 = __lsx_vxori_b(src2, 128);
+    DUP2_ARG2(__lsx_vilvl_b, src1, src0, src2, src1, src10_l, src21_l);
+    DUP2_ARG2(__lsx_vilvh_b, src1, src0, src2, src1, src10_h, src21_h);
+
+    for (loop_cnt = (height >> 2); loop_cnt--;) {
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride2,
+                  0, src + src_stride3, 0, src3, src4, src5, src6);
+        src += src_stride4;
+
+        DUP4_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src5, 128, src6, 128,
+                  src3, src4, src5, src6);
+        DUP4_ARG2(__lsx_vilvl_b, src3, src2, src4, src3, src5, src4, src6,
+                  src5, src32_l, src43_l, src54_l, src65_l);
+        DUP4_ARG2(__lsx_vilvh_b, src3, src2, src4, src3, src5, src4, src6,
+                  src5, src32_h, src43_h, src54_h, src65_h);
+
+        tmp0 = FILT_4TAP_DPADD_S_H(src10_l, src32_l, filt0, filt1);
+        tmp1 = FILT_4TAP_DPADD_S_H(src21_l, src43_l, filt0, filt1);
+        tmp2 = FILT_4TAP_DPADD_S_H(src10_h, src32_h, filt0, filt1);
+        tmp3 = FILT_4TAP_DPADD_S_H(src21_h, src43_h, filt0, filt1);
+        DUP2_ARG3(__lsx_vssrarni_b_h, tmp2, tmp0, 7, tmp3, tmp1, 7, tmp0, tmp1);
+        DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
+
+        __lsx_vst(tmp0, dst, 0);
+        dst += dst_stride;
+        __lsx_vst(tmp1, dst, 0);
+        dst += dst_stride;
+
+        tmp0 = FILT_4TAP_DPADD_S_H(src32_l, src54_l, filt0, filt1);
+        tmp1 = FILT_4TAP_DPADD_S_H(src43_l, src65_l, filt0, filt1);
+        tmp2 = FILT_4TAP_DPADD_S_H(src32_h, src54_h, filt0, filt1);
+        tmp3 = FILT_4TAP_DPADD_S_H(src43_h, src65_h, filt0, filt1);
+        DUP2_ARG3(__lsx_vssrarni_b_h, tmp2, tmp0, 7, tmp3, tmp1, 7, tmp0, tmp1);
+        DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
+
+        __lsx_vst(tmp0, dst, 0);
+        dst += dst_stride;
+        __lsx_vst(tmp1, dst, 0);
+        dst += dst_stride;
+
+        src10_l = src54_l;
+        src21_l = src65_l;
+        src10_h = src54_h;
+        src21_h = src65_h;
+        src2 = src6;
+    }
+}
+
+void ff_put_vp8_epel8_h6v4_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                               uint8_t *src, ptrdiff_t src_stride,
+                               int height, int mx, int my)
+{
+    uint32_t loop_cnt;
+    const int8_t *filter_horiz = subpel_filters_lsx[mx - 1];
+    const int8_t *filter_vert = subpel_filters_lsx[my - 1];
+    __m128i src0, src1, src2, src3, src4, src5, src6;
+    __m128i filt_hz0, filt_hz1, filt_hz2, mask0, mask1, mask2;
+    __m128i filt_vt0, filt_vt1, hz_out0, hz_out1, hz_out2, hz_out3;
+    __m128i tmp0, tmp1, tmp2, tmp3, vec0, vec1, vec2, vec3;
+
+    ptrdiff_t src_stride2 = src_stride << 1;
+    ptrdiff_t src_stride3 = src_stride2 + src_stride;
+    ptrdiff_t src_stride4 = src_stride2 << 1;
+
+    mask0 = __lsx_vld(mc_filt_mask_arr, 0);
+    src -= (2 + src_stride);
+
+    /* rearranging filter */
+    DUP2_ARG2(__lsx_vldrepl_h, filter_horiz, 0, filter_horiz, 2, filt_hz0, filt_hz1);
+    filt_hz2 = __lsx_vldrepl_h(filter_horiz, 4);
+
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+
+    DUP2_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src0, src1);
+    src2 = __lsx_vld(src + src_stride2, 0);
+    src += src_stride3;
+
+    DUP2_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src0, src1);
+    src2 = __lsx_vxori_b(src2, 128);
+    hz_out0 = HORIZ_6TAP_FILT(src0, src0, mask0, mask1, mask2, filt_hz0,
+                              filt_hz1, filt_hz2);
+    hz_out1 = HORIZ_6TAP_FILT(src1, src1, mask0, mask1, mask2, filt_hz0,
+                              filt_hz1, filt_hz2);
+    hz_out2 = HORIZ_6TAP_FILT(src2, src2, mask0, mask1, mask2, filt_hz0,
+                              filt_hz1, filt_hz2);
+    DUP2_ARG2(__lsx_vpackev_b, hz_out1, hz_out0, hz_out2, hz_out1, vec0, vec2);
+
+    DUP2_ARG2(__lsx_vldrepl_h, filter_vert, 0, filter_vert, 2, filt_vt0, filt_vt1);
+
+    for (loop_cnt = (height >> 2); loop_cnt--;) {
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride2, 0,
+                  src + src_stride3, 0, src3, src4, src5, src6);
+        src += src_stride4;
+
+        DUP4_ARG2(__lsx_vxori_b, src3, 128, src4, 128, src5, 128, src6, 128,
+                  src3, src4, src5, src6);
+
+        hz_out3 = HORIZ_6TAP_FILT(src3, src3, mask0, mask1, mask2, filt_hz0,
+                                  filt_hz1, filt_hz2);
+        vec1 = __lsx_vpackev_b(hz_out3, hz_out2);
+        tmp0 = FILT_4TAP_DPADD_S_H(vec0, vec1, filt_vt0, filt_vt1);
+
+        hz_out0 = HORIZ_6TAP_FILT(src4, src4, mask0, mask1, mask2, filt_hz0,
+                                  filt_hz1, filt_hz2);
+        vec3 = __lsx_vpackev_b(hz_out0, hz_out3);
+        tmp1 = FILT_4TAP_DPADD_S_H(vec2, vec3, filt_vt0, filt_vt1);
+
+        hz_out1 = HORIZ_6TAP_FILT(src5, src5, mask0, mask1, mask2, filt_hz0,
+                                  filt_hz1, filt_hz2);
+        vec0 = __lsx_vpackev_b(hz_out1, hz_out0);
+        tmp2 = FILT_4TAP_DPADD_S_H(vec1, vec0, filt_vt0, filt_vt1);
+
+        hz_out2 = HORIZ_6TAP_FILT(src6, src6, mask0, mask1, mask2, filt_hz0,
+                                  filt_hz1, filt_hz2);
+        DUP2_ARG2(__lsx_vpackev_b, hz_out0, hz_out3, hz_out2, hz_out1, vec1, vec2);
+        tmp3 = FILT_4TAP_DPADD_S_H(vec1, vec2, filt_vt0, filt_vt1);
+
+        DUP2_ARG3(__lsx_vssrarni_b_h, tmp1, tmp0, 7, tmp3, tmp2, 7, tmp0, tmp1);
+        DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
+
+        __lsx_vstelm_d(tmp0, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(tmp0, dst, 0, 1);
+        dst += dst_stride;
+        __lsx_vstelm_d(tmp1, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(tmp1, dst, 0, 1);
+        dst += dst_stride;
+    }
+}
+
+void ff_put_vp8_epel16_h6v4_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                                uint8_t *src, ptrdiff_t src_stride,
+                                int height, int mx, int my)
+{
+    int32_t multiple8_cnt;
+
+    for (multiple8_cnt = 2; multiple8_cnt--;) {
+        ff_put_vp8_epel8_h6v4_lsx(dst, dst_stride, src, src_stride, height,
+                                  mx, my);
+        src += 8;
+        dst += 8;
+    }
+}
+
+void ff_put_vp8_epel8_h4v6_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                               uint8_t *src, ptrdiff_t src_stride,
+                               int height, int mx, int my)
+{
+    uint32_t loop_cnt;
+    const int8_t *filter_horiz = subpel_filters_lsx[mx - 1];
+    const int8_t *filter_vert = subpel_filters_lsx[my - 1];
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8;
+    __m128i filt_hz0, filt_hz1, mask0, mask1;
+    __m128i filt_vt0, filt_vt1, filt_vt2;
+    __m128i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7, tmp8;
+    __m128i out0, out1, out2, out3, out4, out5, out6, out7;
+
+    ptrdiff_t src_stride2 = src_stride << 1;
+    ptrdiff_t src_stride3 = src_stride2 + src_stride;
+    ptrdiff_t src_stride4 = src_stride2 << 1;
+
+    mask0 = __lsx_vld(mc_filt_mask_arr, 0);
+    src -= (1 + src_stride2);
+
+    /* rearranging filter */
+    DUP2_ARG2(__lsx_vldrepl_h, filter_horiz, 0, filter_horiz, 2, filt_hz0, filt_hz1);
+    mask1 = __lsx_vaddi_bu(mask0, 2);
+
+    DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride2, 0,
+              src + src_stride3, 0, src0, src1, src2, src3);
+    src += src_stride4;
+    src4 = __lsx_vld(src, 0);
+    src += src_stride;
+
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+              src0, src1, src2, src3);
+    src4 = __lsx_vxori_b(src4, 128);
+
+    tmp0 = HORIZ_4TAP_FILT(src0, src0, mask0, mask1, filt_hz0, filt_hz1);
+    tmp1 = HORIZ_4TAP_FILT(src1, src1, mask0, mask1, filt_hz0, filt_hz1);
+    tmp2 = HORIZ_4TAP_FILT(src2, src2, mask0, mask1, filt_hz0, filt_hz1);
+    tmp3 = HORIZ_4TAP_FILT(src3, src3, mask0, mask1, filt_hz0, filt_hz1);
+    tmp4 = HORIZ_4TAP_FILT(src4, src4, mask0, mask1, filt_hz0, filt_hz1);
+
+    DUP4_ARG2(__lsx_vpackev_b, tmp1, tmp0, tmp3, tmp2, tmp2, tmp1,
+              tmp4, tmp3, out0, out1, out3, out4);
+
+    DUP2_ARG2(__lsx_vldrepl_h, filter_vert, 0, filter_vert, 2, filt_vt0, filt_vt1);
+    filt_vt2 = __lsx_vldrepl_h(filter_vert, 4);
+
+    for (loop_cnt = (height >> 2); loop_cnt--;) {
+        DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride2, 0,
+                  src + src_stride3, 0, src5, src6, src7, src8);
+        src += src_stride4;
+
+        DUP4_ARG2(__lsx_vxori_b, src5, 128, src6, 128, src7, 128, src8, 128,
+                  src5, src6, src7, src8);
+
+        tmp5 = HORIZ_4TAP_FILT(src5, src5, mask0, mask1, filt_hz0, filt_hz1);
+        out2 = __lsx_vpackev_b(tmp5, tmp4);
+        tmp0 = DPADD_SH3_SH(out0, out1, out2, filt_vt0, filt_vt1, filt_vt2);
+
+        tmp6 = HORIZ_4TAP_FILT(src6, src6, mask0, mask1, filt_hz0, filt_hz1);
+        out5 = __lsx_vpackev_b(tmp6, tmp5);
+        tmp1 = DPADD_SH3_SH(out3, out4, out5, filt_vt0, filt_vt1, filt_vt2);
+
+        tmp7 = HORIZ_4TAP_FILT(src7, src7, mask0, mask1, filt_hz0, filt_hz1);
+        out6 = __lsx_vpackev_b(tmp7, tmp6);
+        tmp2 = DPADD_SH3_SH(out1, out2, out6, filt_vt0, filt_vt1, filt_vt2);
+
+        tmp8 = HORIZ_4TAP_FILT(src8, src8, mask0, mask1, filt_hz0, filt_hz1);
+        out7 = __lsx_vpackev_b(tmp8, tmp7);
+        tmp3 = DPADD_SH3_SH(out4, out5, out7, filt_vt0, filt_vt1, filt_vt2);
+
+        DUP2_ARG3(__lsx_vssrarni_b_h, tmp1, tmp0, 7, tmp3, tmp2, 7, tmp0, tmp1);
+        DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
+
+        __lsx_vstelm_d(tmp0, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(tmp0, dst, 0, 1);
+        dst += dst_stride;
+        __lsx_vstelm_d(tmp1, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(tmp1, dst, 0, 1);
+        dst += dst_stride;
+
+        tmp4 = tmp8;
+        out0 = out2;
+        out1 = out6;
+        out3 = out5;
+        out4 = out7;
+    }
+}
+
+void ff_put_vp8_epel16_h4v6_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                                uint8_t *src, ptrdiff_t src_stride,
+                                int height, int mx, int my)
+{
+    int32_t multiple8_cnt;
+
+    for (multiple8_cnt = 2; multiple8_cnt--;) {
+        ff_put_vp8_epel8_h4v6_lsx(dst, dst_stride, src, src_stride, height,
+                                  mx, my);
+        src += 8;
+        dst += 8;
+    }
+}
+
+void ff_put_vp8_pixels8_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                            uint8_t *src, ptrdiff_t src_stride,
+                            int height, int mx, int my)
+{
+    int32_t cnt;
+    __m128i src0, src1, src2, src3;
+
+    ptrdiff_t src_stride2 = src_stride << 1;
+    ptrdiff_t src_stride3 = src_stride2 + src_stride;
+    ptrdiff_t src_stride4 = src_stride2 << 1;
+
+    if (0 == height % 8) {
+        for (cnt = height >> 3; cnt--;) {
+            DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride2, 0,
+                      src + src_stride3, 0, src0, src1, src2, src3);
+            src += src_stride4;
+
+            __lsx_vstelm_d(src0, dst, 0, 0);
+            dst += dst_stride;
+            __lsx_vstelm_d(src1, dst, 0, 0);
+            dst += dst_stride;
+            __lsx_vstelm_d(src2, dst, 0, 0);
+            dst += dst_stride;
+            __lsx_vstelm_d(src3, dst, 0, 0);
+            dst += dst_stride;
+
+            DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride2, 0,
+                      src + src_stride3, 0, src0, src1, src2, src3);
+            src += src_stride4;
+
+            __lsx_vstelm_d(src0, dst, 0, 0);
+            dst += dst_stride;
+            __lsx_vstelm_d(src1, dst, 0, 0);
+            dst += dst_stride;
+            __lsx_vstelm_d(src2, dst, 0, 0);
+            dst += dst_stride;
+            __lsx_vstelm_d(src3, dst, 0, 0);
+            dst += dst_stride;
+        }
+    } else if( 0 == height % 4) {
+        for (cnt = (height >> 2); cnt--;) {
+            DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride2, 0,
+                      src + src_stride3, 0, src0, src1, src2, src3);
+            src += src_stride4;
+
+            __lsx_vstelm_d(src0, dst, 0, 0);
+            dst += dst_stride;
+            __lsx_vstelm_d(src1, dst, 0, 0);
+            dst += dst_stride;
+            __lsx_vstelm_d(src2, dst, 0, 0);
+            dst += dst_stride;
+            __lsx_vstelm_d(src3, dst, 0, 0);
+            dst += dst_stride;
+        }
+    }
+}
+
+void ff_put_vp8_pixels16_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                             uint8_t *src, ptrdiff_t src_stride,
+                             int height, int mx, int my)
+{
+    int32_t width = 16;
+    int32_t cnt, loop_cnt;
+    uint8_t *src_tmp, *dst_tmp;
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7;
+
+    ptrdiff_t src_stride2 = src_stride << 1;
+    ptrdiff_t src_stride3 = src_stride2 + src_stride;
+    ptrdiff_t src_stride4 = src_stride2 << 1;
+
+    ptrdiff_t dst_stride2 = dst_stride << 1;
+    ptrdiff_t dst_stride3 = dst_stride2 + dst_stride;
+    ptrdiff_t dst_stride4 = dst_stride2 << 1;
+
+    if (0 == height % 8) {
+        for (cnt = (width >> 4); cnt--;) {
+            src_tmp = src;
+            dst_tmp = dst;
+            for (loop_cnt = (height >> 3); loop_cnt--;) {
+                DUP4_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0,
+                          src_tmp + src_stride2, 0, src_tmp + src_stride3, 0,
+                          src4, src5, src6, src7);
+                src_tmp += src_stride4;
+
+                __lsx_vst(src4, dst_tmp,               0);
+                __lsx_vst(src5, dst_tmp + dst_stride,  0);
+                __lsx_vst(src6, dst_tmp + dst_stride2, 0);
+                __lsx_vst(src7, dst_tmp + dst_stride3, 0);
+                dst_tmp += dst_stride4;
+
+                DUP4_ARG2(__lsx_vld, src_tmp, 0, src_tmp + src_stride, 0,
+                          src_tmp + src_stride2, 0, src_tmp + src_stride3, 0,
+                          src4, src5, src6, src7);
+                src_tmp += src_stride4;
+
+                __lsx_vst(src4, dst_tmp,               0);
+                __lsx_vst(src5, dst_tmp + dst_stride,  0);
+                __lsx_vst(src6, dst_tmp + dst_stride2, 0);
+                __lsx_vst(src7, dst_tmp + dst_stride3, 0);
+                dst_tmp += dst_stride4;
+            }
+            src += 16;
+            dst += 16;
+        }
+    } else if (0 == height % 4) {
+        for (cnt = (height >> 2); cnt--;) {
+            DUP4_ARG2(__lsx_vld, src, 0, src + src_stride, 0, src + src_stride2, 0,
+                      src + src_stride3, 0, src0, src1, src2, src3);
+            src += 4 * src_stride4;
+
+            __lsx_vst(src0, dst,               0);
+            __lsx_vst(src1, dst + dst_stride,  0);
+            __lsx_vst(src2, dst + dst_stride2, 0);
+            __lsx_vst(src3, dst + dst_stride3, 0);
+            dst += dst_stride4;
+       }
+    }
+}
diff --git a/libavcodec/loongarch/vp8dsp_init_loongarch.c b/libavcodec/loongarch/vp8dsp_init_loongarch.c
new file mode 100644
index 0000000000..63da15b198
--- /dev/null
+++ b/libavcodec/loongarch/vp8dsp_init_loongarch.c
@@ -0,0 +1,63 @@
+/*
+ * Copyright (c) 2021 Loongson Technology Corporation Limited
+ * Contributed by Hecai Yuan <yuanhecai@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+/**
+ * @file
+ * VP8 compatible video decoder
+ */
+
+#include "libavutil/loongarch/cpu.h"
+#include "libavcodec/vp8dsp.h"
+#include "libavutil/attributes.h"
+#include "vp8dsp_loongarch.h"
+
+#define VP8_MC_LOONGARCH_FUNC(IDX, SIZE)                                          \
+    dsp->put_vp8_epel_pixels_tab[IDX][0][2] = ff_put_vp8_epel##SIZE##_h6_lsx;     \
+    dsp->put_vp8_epel_pixels_tab[IDX][1][0] = ff_put_vp8_epel##SIZE##_v4_lsx;     \
+    dsp->put_vp8_epel_pixels_tab[IDX][1][2] = ff_put_vp8_epel##SIZE##_h6v4_lsx;   \
+    dsp->put_vp8_epel_pixels_tab[IDX][2][0] = ff_put_vp8_epel##SIZE##_v6_lsx;     \
+    dsp->put_vp8_epel_pixels_tab[IDX][2][1] = ff_put_vp8_epel##SIZE##_h4v6_lsx;   \
+    dsp->put_vp8_epel_pixels_tab[IDX][2][2] = ff_put_vp8_epel##SIZE##_h6v6_lsx;
+
+#define VP8_MC_LOONGARCH_COPY(IDX, SIZE)                                          \
+    dsp->put_vp8_epel_pixels_tab[IDX][0][0] = ff_put_vp8_pixels##SIZE##_lsx;      \
+    dsp->put_vp8_bilinear_pixels_tab[IDX][0][0] = ff_put_vp8_pixels##SIZE##_lsx;
+
+av_cold void ff_vp8dsp_init_loongarch(VP8DSPContext *dsp)
+{
+    int cpu_flags = av_get_cpu_flags();
+
+    if (have_lsx(cpu_flags)) {
+        VP8_MC_LOONGARCH_FUNC(0, 16);
+        VP8_MC_LOONGARCH_FUNC(1, 8);
+
+        VP8_MC_LOONGARCH_COPY(0, 16);
+        VP8_MC_LOONGARCH_COPY(1, 8);
+
+        dsp->vp8_v_loop_filter16y = ff_vp8_v_loop_filter16_lsx;
+        dsp->vp8_h_loop_filter16y = ff_vp8_h_loop_filter16_lsx;
+        dsp->vp8_v_loop_filter8uv = ff_vp8_v_loop_filter8uv_lsx;
+        dsp->vp8_h_loop_filter8uv = ff_vp8_h_loop_filter8uv_lsx;
+
+        dsp->vp8_v_loop_filter16y_inner = ff_vp8_v_loop_filter16_inner_lsx;
+        dsp->vp8_h_loop_filter16y_inner = ff_vp8_h_loop_filter16_inner_lsx;
+    }
+}
diff --git a/libavcodec/loongarch/vp8dsp_loongarch.h b/libavcodec/loongarch/vp8dsp_loongarch.h
new file mode 100644
index 0000000000..87e9509db9
--- /dev/null
+++ b/libavcodec/loongarch/vp8dsp_loongarch.h
@@ -0,0 +1,90 @@
+/*
+ * Copyright (c) 2021 Loongson Technology Corporation Limited
+ * Contributed by Hecai Yuan <yuanhecai@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef AVCODEC_LOONGARCH_VP8DSP_LOONGARCH_H
+#define AVCODEC_LOONGARCH_VP8DSP_LOONGARCH_H
+
+#include "libavcodec/vp8dsp.h"
+
+void ff_put_vp8_pixels8_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                            uint8_t *src, ptrdiff_t src_stride,
+                            int h, int x, int y);
+void ff_put_vp8_pixels16_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                             uint8_t *src, ptrdiff_t src_stride,
+                             int h, int x, int y);
+
+void ff_put_vp8_epel16_h6_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                              uint8_t *src, ptrdiff_t src_stride,
+                              int h, int mx, int my);
+void ff_put_vp8_epel16_v4_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                              uint8_t *src, ptrdiff_t src_stride,
+                              int h, int mx, int my);
+void ff_put_vp8_epel16_v6_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                              uint8_t *src, ptrdiff_t src_stride,
+                              int h, int mx, int my);
+void ff_put_vp8_epel16_h6v4_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                                uint8_t *src, ptrdiff_t src_stride,
+                                int h, int mx, int my);
+void ff_put_vp8_epel16_h4v6_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                                uint8_t *src, ptrdiff_t src_stride,
+                                int h, int mx, int my);
+void ff_put_vp8_epel16_h6v6_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                                uint8_t *src, ptrdiff_t src_stride,
+                                int h, int mx, int my);
+
+void ff_put_vp8_epel8_v4_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                             uint8_t *src, ptrdiff_t src_stride,
+                             int h, int mx, int my);
+void ff_put_vp8_epel8_v6_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                             uint8_t *src, ptrdiff_t src_stride,
+                             int h, int mx, int my);
+void ff_put_vp8_epel8_h6v4_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                               uint8_t *src, ptrdiff_t src_stride,
+                               int h, int mx, int my);
+void ff_put_vp8_epel8_h4v6_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                               uint8_t *src, ptrdiff_t src_stride,
+                               int h, int mx, int my);
+void ff_put_vp8_epel8_h6v6_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                               uint8_t *src, ptrdiff_t src_stride,
+                               int h, int mx, int my);
+
+void ff_put_vp8_epel8_h6_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                             uint8_t *src, ptrdiff_t src_stride,
+                             int h, int mx, int my);
+
+/* loop filter */
+void ff_vp8_v_loop_filter16_inner_lsx(uint8_t *dst, ptrdiff_t stride,
+                                      int32_t e, int32_t i, int32_t h);
+void ff_vp8_h_loop_filter16_inner_lsx(uint8_t *src, ptrdiff_t stride,
+                                      int32_t e, int32_t i, int32_t h);
+
+void ff_vp8_v_loop_filter16_lsx(uint8_t *dst, ptrdiff_t stride,
+                                int flim_e, int flim_i, int hev_thresh);
+void ff_vp8_h_loop_filter16_lsx(uint8_t *dst, ptrdiff_t stride,
+                                int flim_e, int flim_i, int hev_thresh);
+void ff_vp8_h_loop_filter8uv_lsx(uint8_t *dst_u, uint8_t *dst_v,
+                                 ptrdiff_t stride,
+                                 int flim_e, int flim_i, int hev_thresh);
+void ff_vp8_v_loop_filter8uv_lsx(uint8_t *dst_u, uint8_t *dst_v,
+                                 ptrdiff_t stride,
+                                 int flim_e, int flim_i, int hev_thresh);
+
+#endif  // #ifndef AVCODEC_LOONGARCH_VP8DSP_LOONGARCH_H
diff --git a/libavcodec/loongarch/vp9_idct_lsx.c b/libavcodec/loongarch/vp9_idct_lsx.c
new file mode 100644
index 0000000000..88805814c6
--- /dev/null
+++ b/libavcodec/loongarch/vp9_idct_lsx.c
@@ -0,0 +1,1411 @@
+/*
+ * Copyright (c) 2021 Loongson Technology Corporation Limited
+ * Contributed by Jin Bo <jinbo@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "libavcodec/vp9dsp.h"
+#include "libavutil/loongarch/loongson_intrinsics.h"
+#include "vp9dsp_loongarch.h"
+#include "libavutil/attributes.h"
+
+#define VP9_DCT_CONST_BITS   14
+#define ALLOC_ALIGNED(align) __attribute__ ((aligned(align)))
+#define ROUND_POWER_OF_TWO(value, n) (((value) + (1 << ((n) - 1))) >> (n))
+
+const int32_t cospi_1_64 = 16364;
+const int32_t cospi_2_64 = 16305;
+const int32_t cospi_3_64 = 16207;
+const int32_t cospi_4_64 = 16069;
+const int32_t cospi_5_64 = 15893;
+const int32_t cospi_6_64 = 15679;
+const int32_t cospi_7_64 = 15426;
+const int32_t cospi_8_64 = 15137;
+const int32_t cospi_9_64 = 14811;
+const int32_t cospi_10_64 = 14449;
+const int32_t cospi_11_64 = 14053;
+const int32_t cospi_12_64 = 13623;
+const int32_t cospi_13_64 = 13160;
+const int32_t cospi_14_64 = 12665;
+const int32_t cospi_15_64 = 12140;
+const int32_t cospi_16_64 = 11585;
+const int32_t cospi_17_64 = 11003;
+const int32_t cospi_18_64 = 10394;
+const int32_t cospi_19_64 = 9760;
+const int32_t cospi_20_64 = 9102;
+const int32_t cospi_21_64 = 8423;
+const int32_t cospi_22_64 = 7723;
+const int32_t cospi_23_64 = 7005;
+const int32_t cospi_24_64 = 6270;
+const int32_t cospi_25_64 = 5520;
+const int32_t cospi_26_64 = 4756;
+const int32_t cospi_27_64 = 3981;
+const int32_t cospi_28_64 = 3196;
+const int32_t cospi_29_64 = 2404;
+const int32_t cospi_30_64 = 1606;
+const int32_t cospi_31_64 = 804;
+
+const int32_t sinpi_1_9 = 5283;
+const int32_t sinpi_2_9 = 9929;
+const int32_t sinpi_3_9 = 13377;
+const int32_t sinpi_4_9 = 15212;
+
+#define VP9_DOTP_CONST_PAIR(reg0, reg1, cnst0, cnst1, out0, out1)  \
+{                                                                  \
+    __m128i k0_m = __lsx_vreplgr2vr_h(cnst0);                      \
+    __m128i s0_m, s1_m, s2_m, s3_m;                                \
+                                                                   \
+    s0_m = __lsx_vreplgr2vr_h(cnst1);                              \
+    k0_m = __lsx_vpackev_h(s0_m, k0_m);                            \
+                                                                   \
+    s1_m = __lsx_vilvl_h(__lsx_vneg_h(reg1), reg0);                \
+    s0_m = __lsx_vilvh_h(__lsx_vneg_h(reg1), reg0);                \
+    s3_m = __lsx_vilvl_h(reg0, reg1);                              \
+    s2_m = __lsx_vilvh_h(reg0, reg1);                              \
+    DUP2_ARG2(__lsx_vdp2_w_h, s1_m, k0_m, s0_m, k0_m, s1_m, s0_m); \
+    DUP2_ARG2(__lsx_vsrari_w, s1_m, VP9_DCT_CONST_BITS,            \
+              s0_m, VP9_DCT_CONST_BITS, s1_m, s0_m);               \
+    out0 = __lsx_vpickev_h(s0_m, s1_m);                            \
+    DUP2_ARG2(__lsx_vdp2_w_h, s3_m, k0_m, s2_m, k0_m, s1_m, s0_m); \
+    DUP2_ARG2(__lsx_vsrari_w, s1_m, VP9_DCT_CONST_BITS,            \
+              s0_m, VP9_DCT_CONST_BITS, s1_m, s0_m);               \
+    out1 = __lsx_vpickev_h(s0_m, s1_m);                            \
+}
+
+#define VP9_SET_COSPI_PAIR(c0_h, c1_h)    \
+( {                                       \
+    __m128i out0_m, r0_m, r1_m;           \
+                                          \
+    r0_m = __lsx_vreplgr2vr_h(c0_h);      \
+    r1_m = __lsx_vreplgr2vr_h(c1_h);      \
+    out0_m = __lsx_vpackev_h(r1_m, r0_m); \
+                                          \
+    out0_m;                               \
+} )
+
+#define VP9_ADDBLK_ST8x4_UB(dst, dst_stride, in0, in1, in2, in3)      \
+{                                                                     \
+    uint8_t *dst_m = (uint8_t *) (dst);                               \
+    __m128i dst0_m, dst1_m, dst2_m, dst3_m;                           \
+    __m128i tmp0_m, tmp1_m;                                           \
+    __m128i res0_m, res1_m, res2_m, res3_m;                           \
+    __m128i zero_m = __lsx_vldi(0);                                   \
+    DUP4_ARG2(__lsx_vld, dst_m, 0, dst_m + dst_stride, 0,             \
+              dst_m + 2 * dst_stride, 0, dst_m + 3 * dst_stride, 0,   \
+              dst0_m, dst1_m, dst2_m, dst3_m);                        \
+    DUP4_ARG2(__lsx_vilvl_b, zero_m, dst0_m, zero_m, dst1_m, zero_m,  \
+              dst2_m, zero_m, dst3_m, res0_m, res1_m, res2_m, res3_m);\
+    DUP4_ARG2(__lsx_vadd_h, res0_m, in0, res1_m, in1, res2_m, in2,    \
+              res3_m, in3, res0_m, res1_m, res2_m, res3_m);           \
+    DUP4_ARG1(__lsx_vclip255_h, res0_m, res1_m, res2_m, res3_m,       \
+              res0_m, res1_m, res2_m, res3_m);                        \
+    DUP2_ARG2(__lsx_vpickev_b, res1_m, res0_m, res3_m, res2_m,        \
+              tmp0_m, tmp1_m);                                        \
+    __lsx_vstelm_d(tmp0_m, dst_m, 0, 0);                              \
+    __lsx_vstelm_d(tmp0_m, dst_m + dst_stride, 0, 1);                 \
+    __lsx_vstelm_d(tmp1_m, dst_m + 2 * dst_stride, 0, 0);             \
+    __lsx_vstelm_d(tmp1_m, dst_m + 3 * dst_stride, 0, 1);             \
+}
+
+#define VP9_UNPCK_UB_SH(in, out_h, out_l) \
+{                                         \
+    __m128i zero = __lsx_vldi(0);         \
+    out_l = __lsx_vilvl_b(zero, in);      \
+    out_h = __lsx_vilvh_b(zero, in);      \
+}
+
+#define VP9_ILVLTRANS4x8_H(in0, in1, in2, in3, in4, in5, in6, in7,          \
+                           out0, out1, out2, out3, out4, out5, out6, out7)  \
+{                                                                           \
+    __m128i tmp0_m, tmp1_m, tmp2_m, tmp3_m;                                 \
+    __m128i tmp0_n, tmp1_n, tmp2_n, tmp3_n;                                 \
+    __m128i zero_m = __lsx_vldi(0);                                         \
+                                                                            \
+    DUP4_ARG2(__lsx_vilvl_h, in1, in0, in3, in2, in5, in4, in7, in6,        \
+              tmp0_n, tmp1_n, tmp2_n, tmp3_n);                              \
+    tmp0_m = __lsx_vilvl_w(tmp1_n, tmp0_n);                                 \
+    tmp2_m = __lsx_vilvh_w(tmp1_n, tmp0_n);                                 \
+    tmp1_m = __lsx_vilvl_w(tmp3_n, tmp2_n);                                 \
+    tmp3_m = __lsx_vilvh_w(tmp3_n, tmp2_n);                                 \
+                                                                            \
+    out0 = __lsx_vilvl_d(tmp1_m, tmp0_m);                                   \
+    out1 = __lsx_vilvh_d(tmp1_m, tmp0_m);                                   \
+    out2 = __lsx_vilvl_d(tmp3_m, tmp2_m);                                   \
+    out3 = __lsx_vilvh_d(tmp3_m, tmp2_m);                                   \
+                                                                            \
+    out4 = zero_m;                                                          \
+    out5 = zero_m;                                                          \
+    out6 = zero_m;                                                          \
+    out7 = zero_m;                                                          \
+}
+
+/* multiply and add macro */
+#define VP9_MADD(inp0, inp1, inp2, inp3, cst0, cst1, cst2, cst3,            \
+                 out0, out1, out2, out3)                                    \
+{                                                                           \
+    __m128i madd_s0_m, madd_s1_m, madd_s2_m, madd_s3_m;                     \
+    __m128i tmp0_m, tmp1_m, tmp2_m, tmp3_m;                                 \
+                                                                            \
+    madd_s1_m = __lsx_vilvl_h(inp1, inp0);                                  \
+    madd_s0_m = __lsx_vilvh_h(inp1, inp0);                                  \
+    madd_s3_m = __lsx_vilvl_h(inp3, inp2);                                  \
+    madd_s2_m = __lsx_vilvh_h(inp3, inp2);                                  \
+    DUP4_ARG2(__lsx_vdp2_w_h, madd_s1_m, cst0, madd_s0_m, cst0,             \
+              madd_s1_m, cst1, madd_s0_m, cst1, tmp0_m, tmp1_m,             \
+              tmp2_m, tmp3_m);                                              \
+    DUP4_ARG2(__lsx_vsrari_w, tmp0_m, VP9_DCT_CONST_BITS, tmp1_m,           \
+              VP9_DCT_CONST_BITS, tmp2_m, VP9_DCT_CONST_BITS, tmp3_m,       \
+              VP9_DCT_CONST_BITS, tmp0_m, tmp1_m, tmp2_m, tmp3_m);          \
+    DUP2_ARG2(__lsx_vpickev_h, tmp1_m, tmp0_m, tmp3_m, tmp2_m, out0, out1); \
+    DUP4_ARG2(__lsx_vdp2_w_h, madd_s3_m, cst2, madd_s2_m, cst2, madd_s3_m,  \
+              cst3, madd_s2_m, cst3, tmp0_m, tmp1_m, tmp2_m, tmp3_m);       \
+    DUP4_ARG2(__lsx_vsrari_w, tmp0_m, VP9_DCT_CONST_BITS,                   \
+              tmp1_m, VP9_DCT_CONST_BITS, tmp2_m, VP9_DCT_CONST_BITS,       \
+              tmp3_m, VP9_DCT_CONST_BITS, tmp0_m, tmp1_m, tmp2_m, tmp3_m);  \
+    DUP2_ARG2(__lsx_vpickev_h, tmp1_m, tmp0_m, tmp3_m, tmp2_m, out2, out3); \
+}
+
+#define VP9_SET_CONST_PAIR(mask_h, idx1_h, idx2_h)                           \
+( {                                                                          \
+    __m128i c0_m, c1_m;                                                      \
+                                                                             \
+    DUP2_ARG2(__lsx_vreplvei_h, mask_h, idx1_h, mask_h, idx2_h, c0_m, c1_m); \
+    c0_m = __lsx_vpackev_h(c1_m, c0_m);                                      \
+                                                                             \
+    c0_m;                                                                    \
+} )
+
+/* idct 8x8 macro */
+#define VP9_IDCT8x8_1D(in0, in1, in2, in3, in4, in5, in6, in7,                 \
+                       out0, out1, out2, out3, out4, out5, out6, out7)         \
+{                                                                              \
+    __m128i tp0_m, tp1_m, tp2_m, tp3_m, tp4_m, tp5_m, tp6_m, tp7_m;            \
+    __m128i k0_m, k1_m, k2_m, k3_m, res0_m, res1_m, res2_m, res3_m;            \
+    __m128i tmp0_m, tmp1_m, tmp2_m, tmp3_m;                                    \
+    v8i16 mask_m = { cospi_28_64, cospi_4_64, cospi_20_64, cospi_12_64,        \
+          cospi_16_64, -cospi_4_64, -cospi_20_64, -cospi_16_64 };              \
+                                                                               \
+    k0_m = VP9_SET_CONST_PAIR(mask_m, 0, 5);                                   \
+    k1_m = VP9_SET_CONST_PAIR(mask_m, 1, 0);                                   \
+    k2_m = VP9_SET_CONST_PAIR(mask_m, 6, 3);                                   \
+    k3_m = VP9_SET_CONST_PAIR(mask_m, 3, 2);                                   \
+    VP9_MADD(in1, in7, in3, in5, k0_m, k1_m, k2_m, k3_m, in1, in7, in3, in5);  \
+    DUP2_ARG2(__lsx_vsub_h, in1, in3, in7, in5, res0_m, res1_m);               \
+    k0_m = VP9_SET_CONST_PAIR(mask_m, 4, 7);                                   \
+    k1_m = __lsx_vreplvei_h(mask_m, 4);                                        \
+                                                                               \
+    res2_m = __lsx_vilvl_h(res0_m, res1_m);                                    \
+    res3_m = __lsx_vilvh_h(res0_m, res1_m);                                    \
+    DUP4_ARG2(__lsx_vdp2_w_h, res2_m, k0_m, res3_m, k0_m, res2_m, k1_m,        \
+              res3_m, k1_m, tmp0_m, tmp1_m, tmp2_m, tmp3_m);                   \
+    DUP4_ARG2(__lsx_vsrari_w, tmp0_m, VP9_DCT_CONST_BITS,                      \
+              tmp1_m, VP9_DCT_CONST_BITS, tmp2_m, VP9_DCT_CONST_BITS,          \
+              tmp3_m, VP9_DCT_CONST_BITS, tmp0_m, tmp1_m, tmp2_m, tmp3_m);     \
+    tp4_m = __lsx_vadd_h(in1, in3);                                            \
+    DUP2_ARG2(__lsx_vpickev_h, tmp1_m, tmp0_m, tmp3_m, tmp2_m, tp5_m, tp6_m);  \
+    tp7_m = __lsx_vadd_h(in7, in5);                                            \
+    k2_m = VP9_SET_COSPI_PAIR(cospi_24_64, -cospi_8_64);                       \
+    k3_m = VP9_SET_COSPI_PAIR(cospi_8_64, cospi_24_64);                        \
+    VP9_MADD(in0, in4, in2, in6, k1_m, k0_m, k2_m, k3_m,                       \
+             in0, in4, in2, in6);                                              \
+    LSX_BUTTERFLY_4_H(in0, in4, in2, in6, tp0_m, tp1_m, tp2_m, tp3_m);         \
+    LSX_BUTTERFLY_8_H(tp0_m, tp1_m, tp2_m, tp3_m, tp4_m, tp5_m, tp6_m, tp7_m,  \
+                  out0, out1, out2, out3, out4, out5, out6, out7);             \
+}
+
+static av_always_inline
+void vp9_idct8x8_1_add_lsx(int16_t *input, uint8_t *dst,
+                                  int32_t dst_stride)
+{
+    int16_t out;
+    int32_t val;
+    __m128i vec;
+
+    out = ROUND_POWER_OF_TWO((input[0] * cospi_16_64), VP9_DCT_CONST_BITS);
+    out = ROUND_POWER_OF_TWO((out * cospi_16_64), VP9_DCT_CONST_BITS);
+    val = ROUND_POWER_OF_TWO(out, 5);
+    vec = __lsx_vreplgr2vr_h(val);
+    input[0] = 0;
+
+    VP9_ADDBLK_ST8x4_UB(dst, dst_stride, vec, vec, vec, vec);
+    dst += (4 * dst_stride);
+    VP9_ADDBLK_ST8x4_UB(dst, dst_stride, vec, vec, vec, vec);
+}
+
+static void vp9_idct8x8_12_colcol_addblk_lsx(int16_t *input, uint8_t *dst,
+                                             int32_t dst_stride)
+{
+    __m128i in0, in1, in2, in3, in4, in5, in6, in7;
+    __m128i s0, s1, s2, s3, s4, s5, s6, s7, k0, k1, k2, k3, m0, m1, m2, m3;
+    __m128i tmp0, tmp1, tmp2, tmp3;
+    __m128i zero = __lsx_vldi(0);
+
+    /* load vector elements of 8x8 block */
+    DUP4_ARG2(__lsx_vld, input, 0, input, 16, input, 32, input, 48,
+              in0, in1, in2, in3);
+    DUP4_ARG2(__lsx_vld, input, 64, input, 80, input, 96, input, 112,
+              in4, in5, in6, in7);
+    __lsx_vst(zero, input, 0);
+    __lsx_vst(zero, input, 16);
+    __lsx_vst(zero, input, 32);
+    __lsx_vst(zero, input, 48);
+    __lsx_vst(zero, input, 64);
+    __lsx_vst(zero, input, 80);
+    __lsx_vst(zero, input, 96);
+    __lsx_vst(zero, input, 112);
+    DUP4_ARG2(__lsx_vilvl_d,in1, in0, in3, in2, in5, in4, in7,
+              in6, in0, in1, in2, in3);
+
+    /* stage1 */
+    DUP2_ARG2(__lsx_vilvh_h, in3, in0, in2, in1, s0, s1);
+    k0 = VP9_SET_COSPI_PAIR(cospi_28_64, -cospi_4_64);
+    k1 = VP9_SET_COSPI_PAIR(cospi_4_64, cospi_28_64);
+    k2 = VP9_SET_COSPI_PAIR(-cospi_20_64, cospi_12_64);
+    k3 = VP9_SET_COSPI_PAIR(cospi_12_64, cospi_20_64);
+    DUP4_ARG2(__lsx_vdp2_w_h, s0, k0, s0, k1, s1, k2, s1, k3,
+              tmp0, tmp1, tmp2, tmp3);
+    DUP4_ARG2(__lsx_vsrari_w, tmp0, VP9_DCT_CONST_BITS, tmp1,
+              VP9_DCT_CONST_BITS, tmp2, VP9_DCT_CONST_BITS, tmp3,
+              VP9_DCT_CONST_BITS, tmp0, tmp1, tmp2, tmp3);
+    DUP4_ARG2(__lsx_vpickev_h, zero, tmp0, zero, tmp1, zero, tmp2, zero, tmp3,
+              s0, s1, s2, s3);
+    LSX_BUTTERFLY_4_H(s0, s1, s3, s2, s4, s7, s6, s5);
+
+    /* stage2 */
+    DUP2_ARG2(__lsx_vilvl_h, in3, in1, in2, in0, s1, s0);
+    k0 = VP9_SET_COSPI_PAIR(cospi_16_64, cospi_16_64);
+    k1 = VP9_SET_COSPI_PAIR(cospi_16_64, -cospi_16_64);
+    k2 = VP9_SET_COSPI_PAIR(cospi_24_64, -cospi_8_64);
+    k3 = VP9_SET_COSPI_PAIR(cospi_8_64, cospi_24_64);
+    DUP4_ARG2(__lsx_vdp2_w_h, s0, k0, s0, k1, s1, k2, s1, k3,
+                  tmp0, tmp1, tmp2, tmp3);
+    DUP4_ARG2(__lsx_vsrari_w, tmp0, VP9_DCT_CONST_BITS, tmp1,
+              VP9_DCT_CONST_BITS, tmp2, VP9_DCT_CONST_BITS, tmp3,
+              VP9_DCT_CONST_BITS, tmp0, tmp1, tmp2, tmp3);
+    DUP4_ARG2(__lsx_vpickev_h, zero, tmp0, zero, tmp1, zero, tmp2, zero, tmp3,
+              s0, s1, s2, s3);
+    LSX_BUTTERFLY_4_H(s0, s1, s2, s3, m0, m1, m2, m3);
+
+    /* stage3 */
+    s0 = __lsx_vilvl_h(s6, s5);
+
+    k1 = VP9_SET_COSPI_PAIR(-cospi_16_64, cospi_16_64);
+    DUP2_ARG2(__lsx_vdp2_w_h, s0, k1, s0, k0, tmp0, tmp1);
+    DUP2_ARG2(__lsx_vsrari_w, tmp0, VP9_DCT_CONST_BITS, tmp1,
+              VP9_DCT_CONST_BITS, tmp0, tmp1);
+    DUP2_ARG2(__lsx_vpickev_h, zero, tmp0, zero, tmp1, s2, s3);
+
+    /* stage4 */
+    LSX_BUTTERFLY_8_H(m0, m1, m2, m3, s4, s2, s3, s7,
+                      in0, in1, in2, in3, in4, in5, in6, in7);
+    VP9_ILVLTRANS4x8_H(in0, in1, in2, in3, in4, in5, in6, in7,
+                       in0, in1, in2, in3, in4, in5, in6, in7);
+    VP9_IDCT8x8_1D(in0, in1, in2, in3, in4, in5, in6, in7,
+                   in0, in1, in2, in3, in4, in5, in6, in7);
+
+    /* final rounding (add 2^4, divide by 2^5) and shift */
+    DUP4_ARG2(__lsx_vsrari_h, in0 , 5, in1, 5, in2, 5, in3, 5,
+              in0, in1, in2, in3);
+    DUP4_ARG2(__lsx_vsrari_h, in4 , 5, in5, 5, in6, 5, in7, 5,
+              in4, in5, in6, in7);
+
+    /* add block and store 8x8 */
+    VP9_ADDBLK_ST8x4_UB(dst, dst_stride, in0, in1, in2, in3);
+    dst += (4 * dst_stride);
+    VP9_ADDBLK_ST8x4_UB(dst, dst_stride, in4, in5, in6, in7);
+}
+
+static void vp9_idct8x8_colcol_addblk_lsx(int16_t *input, uint8_t *dst,
+                                          int32_t dst_stride)
+{
+    __m128i in0, in1, in2, in3, in4, in5, in6, in7;
+    __m128i zero = __lsx_vldi(0);
+
+    /* load vector elements of 8x8 block */
+    DUP4_ARG2(__lsx_vld, input, 0, input, 16, input, 32, input, 48,
+              in0, in1, in2, in3);
+    DUP4_ARG2(__lsx_vld, input, 64, input, 80, input, 96, input, 112,
+              in4, in5, in6, in7);
+    __lsx_vst(zero, input, 0);
+    __lsx_vst(zero, input, 16);
+    __lsx_vst(zero, input, 32);
+    __lsx_vst(zero, input, 48);
+    __lsx_vst(zero, input, 64);
+    __lsx_vst(zero, input, 80);
+    __lsx_vst(zero, input, 96);
+    __lsx_vst(zero, input, 112);
+    /* 1D idct8x8 */
+    VP9_IDCT8x8_1D(in0, in1, in2, in3, in4, in5, in6, in7,
+                   in0, in1, in2, in3, in4, in5, in6, in7);
+    /* columns transform */
+    LSX_TRANSPOSE8x8_H(in0, in1, in2, in3, in4, in5, in6, in7,
+                       in0, in1, in2, in3, in4, in5, in6, in7);
+    /* 1D idct8x8 */
+    VP9_IDCT8x8_1D(in0, in1, in2, in3, in4, in5, in6, in7,
+                   in0, in1, in2, in3, in4, in5, in6, in7);
+    /* final rounding (add 2^4, divide by 2^5) and shift */
+    DUP4_ARG2(__lsx_vsrari_h, in0, 5, in1, 5, in2, 5, in3, 5,
+              in0, in1, in2, in3);
+    DUP4_ARG2(__lsx_vsrari_h, in4, 5, in5, 5, in6, 5, in7, 5,
+              in4, in5, in6, in7);
+    /* add block and store 8x8 */
+    VP9_ADDBLK_ST8x4_UB(dst, dst_stride, in0, in1, in2, in3);
+    dst += (4 * dst_stride);
+    VP9_ADDBLK_ST8x4_UB(dst, dst_stride, in4, in5, in6, in7);
+}
+
+static void vp9_idct16_1d_columns_addblk_lsx(int16_t *input, uint8_t *dst,
+                                             int32_t dst_stride)
+{
+    __m128i loc0, loc1, loc2, loc3;
+    __m128i reg0, reg2, reg4, reg6, reg8, reg10, reg12, reg14;
+    __m128i reg1, reg3, reg5, reg7, reg9, reg11, reg13, reg15;
+    __m128i tmp5, tmp6, tmp7;
+    __m128i zero = __lsx_vldi(0);
+    int32_t offset = dst_stride << 2;
+
+    DUP4_ARG2(__lsx_vld, input, 32*0, input, 32*1, input, 32*2, input, 32*3,
+              reg0, reg1, reg2, reg3);
+    DUP4_ARG2(__lsx_vld, input, 32*4, input, 32*5, input, 32*6, input, 32*7,
+              reg4, reg5, reg6, reg7);
+    DUP4_ARG2(__lsx_vld, input, 32*8, input, 32*9, input, 32*10, input, 32*11,
+              reg8, reg9, reg10, reg11);
+    DUP4_ARG2(__lsx_vld, input, 32*12, input, 32*13, input, 32*14, input,
+              32*15, reg12, reg13, reg14, reg15);
+
+    __lsx_vst(zero, input, 32*0);
+    __lsx_vst(zero, input, 32*1);
+    __lsx_vst(zero, input, 32*2);
+    __lsx_vst(zero, input, 32*3);
+    __lsx_vst(zero, input, 32*4);
+    __lsx_vst(zero, input, 32*5);
+    __lsx_vst(zero, input, 32*6);
+    __lsx_vst(zero, input, 32*7);
+    __lsx_vst(zero, input, 32*8);
+    __lsx_vst(zero, input, 32*9);
+    __lsx_vst(zero, input, 32*10);
+    __lsx_vst(zero, input, 32*11);
+    __lsx_vst(zero, input, 32*12);
+    __lsx_vst(zero, input, 32*13);
+    __lsx_vst(zero, input, 32*14);
+    __lsx_vst(zero, input, 32*15);
+
+    VP9_DOTP_CONST_PAIR(reg2, reg14, cospi_28_64, cospi_4_64, reg2, reg14);
+    VP9_DOTP_CONST_PAIR(reg10, reg6, cospi_12_64, cospi_20_64, reg10, reg6);
+    LSX_BUTTERFLY_4_H(reg2, reg14, reg6, reg10, loc0, loc1, reg14, reg2);
+    VP9_DOTP_CONST_PAIR(reg14, reg2, cospi_16_64, cospi_16_64, loc2, loc3);
+    VP9_DOTP_CONST_PAIR(reg0, reg8, cospi_16_64, cospi_16_64, reg0, reg8);
+    VP9_DOTP_CONST_PAIR(reg4, reg12, cospi_24_64, cospi_8_64, reg4, reg12);
+    LSX_BUTTERFLY_4_H(reg8, reg0, reg4, reg12, reg2, reg6, reg10, reg14);
+
+    reg0 = __lsx_vsub_h(reg2, loc1);
+    reg2 = __lsx_vadd_h(reg2, loc1);
+    reg12 = __lsx_vsub_h(reg14, loc0);
+    reg14 = __lsx_vadd_h(reg14, loc0);
+    reg4 = __lsx_vsub_h(reg6, loc3);
+    reg6 = __lsx_vadd_h(reg6, loc3);
+    reg8 = __lsx_vsub_h(reg10, loc2);
+    reg10 = __lsx_vadd_h(reg10, loc2);
+
+    /* stage2 */
+    VP9_DOTP_CONST_PAIR(reg1, reg15, cospi_30_64, cospi_2_64, reg1, reg15);
+    VP9_DOTP_CONST_PAIR(reg9, reg7, cospi_14_64, cospi_18_64, loc2, loc3);
+
+    reg9 = __lsx_vsub_h(reg1, loc2);
+    reg1 = __lsx_vadd_h(reg1, loc2);
+    reg7 = __lsx_vsub_h(reg15, loc3);
+    reg15 = __lsx_vadd_h(reg15, loc3);
+
+    VP9_DOTP_CONST_PAIR(reg5, reg11, cospi_22_64, cospi_10_64, reg5, reg11);
+    VP9_DOTP_CONST_PAIR(reg13, reg3, cospi_6_64, cospi_26_64, loc0, loc1);
+    LSX_BUTTERFLY_4_H(loc0, loc1, reg11, reg5, reg13, reg3, reg11, reg5);
+
+    loc1 = __lsx_vadd_h(reg15, reg3);
+    reg3 = __lsx_vsub_h(reg15, reg3);
+    loc2 = __lsx_vadd_h(reg2, loc1);
+    reg15 = __lsx_vsub_h(reg2, loc1);
+
+    loc1 = __lsx_vadd_h(reg1, reg13);
+    reg13 = __lsx_vsub_h(reg1, reg13);
+    loc0 = __lsx_vadd_h(reg0, loc1);
+    loc1 = __lsx_vsub_h(reg0, loc1);
+    tmp6 = loc0;
+    tmp7 = loc1;
+    reg0 = loc2;
+
+    VP9_DOTP_CONST_PAIR(reg7, reg9, cospi_24_64, cospi_8_64, reg7, reg9);
+    VP9_DOTP_CONST_PAIR(__lsx_vneg_h(reg5), __lsx_vneg_h(reg11), cospi_8_64,
+                        cospi_24_64, reg5, reg11);
+
+    loc0 = __lsx_vadd_h(reg9, reg5);
+    reg5 = __lsx_vsub_h(reg9, reg5);
+    reg2 = __lsx_vadd_h(reg6, loc0);
+    reg1 = __lsx_vsub_h(reg6, loc0);
+
+    loc0 = __lsx_vadd_h(reg7, reg11);
+    reg11 = __lsx_vsub_h(reg7, reg11);
+    loc1 = __lsx_vadd_h(reg4, loc0);
+    loc2 = __lsx_vsub_h(reg4, loc0);
+    tmp5 = loc1;
+
+    VP9_DOTP_CONST_PAIR(reg5, reg11, cospi_16_64, cospi_16_64, reg5, reg11);
+    LSX_BUTTERFLY_4_H(reg8, reg10, reg11, reg5, loc0, reg4, reg9, loc1);
+
+    reg10 = loc0;
+    reg11 = loc1;
+
+    VP9_DOTP_CONST_PAIR(reg3, reg13, cospi_16_64, cospi_16_64, reg3, reg13);
+    LSX_BUTTERFLY_4_H(reg12, reg14, reg13, reg3, reg8, reg6, reg7, reg5);
+    reg13 = loc2;
+
+    /* Transpose and store the output */
+    reg12 = tmp5;
+    reg14 = tmp6;
+    reg3 = tmp7;
+
+    DUP4_ARG2(__lsx_vsrari_h, reg0, 6, reg2, 6, reg4, 6, reg6, 6,
+              reg0, reg2, reg4, reg6);
+    VP9_ADDBLK_ST8x4_UB(dst, dst_stride, reg0, reg2, reg4, reg6);
+    dst += offset;
+    DUP4_ARG2(__lsx_vsrari_h, reg8, 6, reg10, 6, reg12, 6, reg14, 6,
+              reg8, reg10, reg12, reg14);
+    VP9_ADDBLK_ST8x4_UB(dst, dst_stride, reg8, reg10, reg12, reg14);
+    dst += offset;
+    DUP4_ARG2(__lsx_vsrari_h, reg3, 6, reg5, 6, reg11, 6, reg13, 6,
+              reg3, reg5, reg11, reg13);
+    VP9_ADDBLK_ST8x4_UB(dst, dst_stride, reg3, reg13, reg11, reg5);
+    dst += offset;
+    DUP4_ARG2(__lsx_vsrari_h, reg1, 6, reg7, 6, reg9, 6, reg15, 6,
+              reg1, reg7, reg9, reg15);
+    VP9_ADDBLK_ST8x4_UB(dst, dst_stride, reg7, reg9, reg1, reg15);
+}
+
+static void vp9_idct16_1d_columns_lsx(int16_t *input, int16_t *output)
+{
+    __m128i loc0, loc1, loc2, loc3;
+    __m128i reg1, reg3, reg5, reg7, reg9, reg11, reg13, reg15;
+    __m128i reg0, reg2, reg4, reg6, reg8, reg10, reg12, reg14;
+    __m128i tmp5, tmp6, tmp7;
+    __m128i zero = __lsx_vldi(0);
+    int16_t *offset;
+
+    DUP4_ARG2(__lsx_vld, input, 32*0, input, 32*1, input, 32*2, input, 32*3,
+              reg0, reg1, reg2, reg3);
+    DUP4_ARG2(__lsx_vld, input, 32*4, input, 32*5, input, 32*6, input, 32*7,
+              reg4, reg5, reg6, reg7);
+    DUP4_ARG2(__lsx_vld, input, 32*8, input, 32*9, input, 32*10, input, 32*11,
+              reg8, reg9, reg10, reg11);
+    DUP4_ARG2(__lsx_vld, input, 32*12, input, 32*13, input, 32*14, input,
+              32*15, reg12, reg13, reg14, reg15);
+
+    __lsx_vst(zero, input, 32*0);
+    __lsx_vst(zero, input, 32*1);
+    __lsx_vst(zero, input, 32*2);
+    __lsx_vst(zero, input, 32*3);
+    __lsx_vst(zero, input, 32*4);
+    __lsx_vst(zero, input, 32*5);
+    __lsx_vst(zero, input, 32*6);
+    __lsx_vst(zero, input, 32*7);
+    __lsx_vst(zero, input, 32*8);
+    __lsx_vst(zero, input, 32*9);
+    __lsx_vst(zero, input, 32*10);
+    __lsx_vst(zero, input, 32*11);
+    __lsx_vst(zero, input, 32*12);
+    __lsx_vst(zero, input, 32*13);
+    __lsx_vst(zero, input, 32*14);
+    __lsx_vst(zero, input, 32*15);
+
+    VP9_DOTP_CONST_PAIR(reg2, reg14, cospi_28_64, cospi_4_64, reg2, reg14);
+    VP9_DOTP_CONST_PAIR(reg10, reg6, cospi_12_64, cospi_20_64, reg10, reg6);
+    LSX_BUTTERFLY_4_H(reg2, reg14, reg6, reg10, loc0, loc1, reg14, reg2);
+    VP9_DOTP_CONST_PAIR(reg14, reg2, cospi_16_64, cospi_16_64, loc2, loc3);
+    VP9_DOTP_CONST_PAIR(reg0, reg8, cospi_16_64, cospi_16_64, reg0, reg8);
+    VP9_DOTP_CONST_PAIR(reg4, reg12, cospi_24_64, cospi_8_64, reg4, reg12);
+    LSX_BUTTERFLY_4_H(reg8, reg0, reg4, reg12, reg2, reg6, reg10, reg14);
+
+    reg0 = __lsx_vsub_h(reg2, loc1);
+    reg2 = __lsx_vadd_h(reg2, loc1);
+    reg12 = __lsx_vsub_h(reg14, loc0);
+    reg14 = __lsx_vadd_h(reg14, loc0);
+    reg4 = __lsx_vsub_h(reg6, loc3);
+    reg6 = __lsx_vadd_h(reg6, loc3);
+    reg8 = __lsx_vsub_h(reg10, loc2);
+    reg10 = __lsx_vadd_h(reg10, loc2);
+
+    /* stage2 */
+    VP9_DOTP_CONST_PAIR(reg1, reg15, cospi_30_64, cospi_2_64, reg1, reg15);
+    VP9_DOTP_CONST_PAIR(reg9, reg7, cospi_14_64, cospi_18_64, loc2, loc3);
+
+    reg9 = __lsx_vsub_h(reg1, loc2);
+    reg1 = __lsx_vadd_h(reg1, loc2);
+    reg7 = __lsx_vsub_h(reg15, loc3);
+    reg15 = __lsx_vadd_h(reg15, loc3);
+
+    VP9_DOTP_CONST_PAIR(reg5, reg11, cospi_22_64, cospi_10_64, reg5, reg11);
+    VP9_DOTP_CONST_PAIR(reg13, reg3, cospi_6_64, cospi_26_64, loc0, loc1);
+    LSX_BUTTERFLY_4_H(loc0, loc1, reg11, reg5, reg13, reg3, reg11, reg5);
+
+    loc1 = __lsx_vadd_h(reg15, reg3);
+    reg3 = __lsx_vsub_h(reg15, reg3);
+    loc2 = __lsx_vadd_h(reg2, loc1);
+    reg15 = __lsx_vsub_h(reg2, loc1);
+
+    loc1 = __lsx_vadd_h(reg1, reg13);
+    reg13 = __lsx_vsub_h(reg1, reg13);
+    loc0 = __lsx_vadd_h(reg0, loc1);
+    loc1 = __lsx_vsub_h(reg0, loc1);
+    tmp6 = loc0;
+    tmp7 = loc1;
+    reg0 = loc2;
+
+    VP9_DOTP_CONST_PAIR(reg7, reg9, cospi_24_64, cospi_8_64, reg7, reg9);
+    VP9_DOTP_CONST_PAIR(__lsx_vneg_h(reg5), __lsx_vneg_h(reg11), cospi_8_64,
+                        cospi_24_64, reg5, reg11);
+
+    loc0 = __lsx_vadd_h(reg9, reg5);
+    reg5 = __lsx_vsub_h(reg9, reg5);
+    reg2 = __lsx_vadd_h(reg6, loc0);
+    reg1 = __lsx_vsub_h(reg6, loc0);
+
+    loc0 = __lsx_vadd_h(reg7, reg11);
+    reg11 = __lsx_vsub_h(reg7, reg11);
+    loc1 = __lsx_vadd_h(reg4, loc0);
+    loc2 = __lsx_vsub_h(reg4, loc0);
+
+    tmp5 = loc1;
+
+    VP9_DOTP_CONST_PAIR(reg5, reg11, cospi_16_64, cospi_16_64, reg5, reg11);
+    LSX_BUTTERFLY_4_H(reg8, reg10, reg11, reg5, loc0, reg4, reg9, loc1);
+
+    reg10 = loc0;
+    reg11 = loc1;
+
+    VP9_DOTP_CONST_PAIR(reg3, reg13, cospi_16_64, cospi_16_64, reg3, reg13);
+    LSX_BUTTERFLY_4_H(reg12, reg14, reg13, reg3, reg8, reg6, reg7, reg5);
+    reg13 = loc2;
+
+    /* Transpose and store the output */
+    reg12 = tmp5;
+    reg14 = tmp6;
+    reg3 = tmp7;
+
+    /* transpose block */
+    LSX_TRANSPOSE8x8_H(reg0, reg2, reg4, reg6, reg8, reg10, reg12, reg14,
+                       reg0, reg2, reg4, reg6, reg8, reg10, reg12, reg14);
+
+    __lsx_vst(reg0, output, 32*0);
+    __lsx_vst(reg2, output, 32*1);
+    __lsx_vst(reg4, output, 32*2);
+    __lsx_vst(reg6, output, 32*3);
+    __lsx_vst(reg8, output, 32*4);
+    __lsx_vst(reg10, output, 32*5);
+    __lsx_vst(reg12, output, 32*6);
+    __lsx_vst(reg14, output, 32*7);
+
+    /* transpose block */
+    LSX_TRANSPOSE8x8_H(reg3, reg13, reg11, reg5, reg7, reg9, reg1, reg15,
+                       reg3, reg13, reg11, reg5, reg7, reg9, reg1, reg15);
+
+    offset = output + 8;
+    __lsx_vst(reg3, offset, 32*0);
+    __lsx_vst(reg13, offset, 32*1);
+    __lsx_vst(reg11, offset, 32*2);
+    __lsx_vst(reg5, offset, 32*3);
+
+    offset = output + 8 + 4 * 16;
+    __lsx_vst(reg7, offset, 32*0);
+    __lsx_vst(reg9, offset, 32*1);
+    __lsx_vst(reg1, offset, 32*2);
+    __lsx_vst(reg15, offset, 32*3);
+}
+
+static void vp9_idct16x16_1_add_lsx(int16_t *input, uint8_t *dst,
+                                    int32_t dst_stride)
+{
+    uint8_t i;
+    int16_t out;
+    __m128i vec, res0, res1, res2, res3, res4, res5, res6, res7;
+    __m128i dst0, dst1, dst2, dst3, tmp0, tmp1, tmp2, tmp3;
+    int32_t stride2 = dst_stride << 1;
+    int32_t stride3 = stride2 + dst_stride;
+    int32_t stride4 = stride2 << 1;
+
+    out = ROUND_POWER_OF_TWO((input[0] * cospi_16_64), VP9_DCT_CONST_BITS);
+    out = ROUND_POWER_OF_TWO((out * cospi_16_64), VP9_DCT_CONST_BITS);
+    out = ROUND_POWER_OF_TWO(out, 6);
+    input[0] = 0;
+    vec = __lsx_vreplgr2vr_h(out);
+
+    for (i = 4; i--;) {
+        dst0 = __lsx_vld(dst, 0);
+        DUP2_ARG2(__lsx_vldx, dst, dst_stride, dst, stride2, dst1, dst2);
+        dst3 = __lsx_vldx(dst, stride3);
+        VP9_UNPCK_UB_SH(dst0, res4, res0);
+        VP9_UNPCK_UB_SH(dst1, res5, res1);
+        VP9_UNPCK_UB_SH(dst2, res6, res2);
+        VP9_UNPCK_UB_SH(dst3, res7, res3);
+        DUP4_ARG2(__lsx_vadd_h, res0, vec, res1, vec, res2, vec, res3, vec,
+                  res0, res1, res2, res3);
+        DUP4_ARG2(__lsx_vadd_h, res4, vec, res5, vec, res6, vec, res7, vec,
+                  res4, res5, res6, res7);
+        DUP4_ARG1(__lsx_vclip255_h, res0, res1, res2, res3,
+                  res0, res1, res2, res3);
+        DUP4_ARG1(__lsx_vclip255_h, res4, res5, res6, res7,
+                  res4, res5, res6, res7);
+        DUP4_ARG2(__lsx_vpickev_b, res4, res0, res5, res1, res6,
+                  res2, res7, res3, tmp0, tmp1, tmp2, tmp3);
+        __lsx_vst(tmp0, dst, 0);
+        __lsx_vstx(tmp1, dst, dst_stride);
+        __lsx_vstx(tmp2, dst, stride2);
+        __lsx_vstx(tmp3, dst, stride3);
+        dst += stride4;
+    }
+}
+
+static void vp9_idct16x16_10_colcol_addblk_lsx(int16_t *input, uint8_t *dst,
+                                               int32_t dst_stride)
+{
+    int32_t i;
+    int16_t out_arr[16 * 16] ALLOC_ALIGNED(16);
+    int16_t *out = out_arr;
+    __m128i zero = __lsx_vldi(0);
+
+    /* transform rows */
+    vp9_idct16_1d_columns_lsx(input, out);
+
+    /* short case just considers top 4 rows as valid output */
+    out += 4 * 16;
+    for (i = 3; i--;) {
+        __lsx_vst(zero, out, 0);
+        __lsx_vst(zero, out, 16);
+        __lsx_vst(zero, out, 32);
+        __lsx_vst(zero, out, 48);
+        __lsx_vst(zero, out, 64);
+        __lsx_vst(zero, out, 80);
+        __lsx_vst(zero, out, 96);
+        __lsx_vst(zero, out, 112);
+        out += 64;
+    }
+
+    out = out_arr;
+
+    /* transform columns */
+    for (i = 0; i < 2; i++) {
+        /* process 8 * 16 block */
+        vp9_idct16_1d_columns_addblk_lsx((out + (i << 3)), (dst + (i << 3)),
+                                         dst_stride);
+    }
+}
+
+static void vp9_idct16x16_colcol_addblk_lsx(int16_t *input, uint8_t *dst,
+                                            int32_t dst_stride)
+{
+    int32_t i;
+    int16_t out_arr[16 * 16] ALLOC_ALIGNED(16);
+    int16_t *out = out_arr;
+
+    /* transform rows */
+    for (i = 0; i < 2; i++) {
+        /* process 8 * 16 block */
+        vp9_idct16_1d_columns_lsx((input + (i << 3)), (out + (i << 7)));
+    }
+
+    /* transform columns */
+    for (i = 0; i < 2; i++) {
+        /* process 8 * 16 block */
+        vp9_idct16_1d_columns_addblk_lsx((out + (i << 3)), (dst + (i << 3)),
+                                         dst_stride);
+    }
+}
+
+static void vp9_idct_butterfly_transpose_store(int16_t *tmp_buf,
+                                               int16_t *tmp_eve_buf,
+                                               int16_t *tmp_odd_buf,
+                                               int16_t *dst)
+{
+    __m128i vec0, vec1, vec2, vec3, loc0, loc1, loc2, loc3;
+    __m128i m0, m1, m2, m3, m4, m5, m6, m7, n0, n1, n2, n3, n4, n5, n6, n7;
+
+    /* FINAL BUTTERFLY : Dependency on Even & Odd */
+    vec0 = __lsx_vld(tmp_odd_buf, 0);
+    vec1 = __lsx_vld(tmp_odd_buf, 9 * 16);
+    vec2 = __lsx_vld(tmp_odd_buf, 14 * 16);
+    vec3 = __lsx_vld(tmp_odd_buf, 6 * 16);
+    loc0 = __lsx_vld(tmp_eve_buf, 0);
+    loc1 = __lsx_vld(tmp_eve_buf, 8 * 16);
+    loc2 = __lsx_vld(tmp_eve_buf, 4 * 16);
+    loc3 = __lsx_vld(tmp_eve_buf, 12 * 16);
+
+    DUP4_ARG2(__lsx_vadd_h,loc0, vec3, loc1, vec2, loc2, vec1, loc3, vec0,
+              m0, m4, m2, m6);
+
+    #define SUB(a, b) __lsx_vsub_h(a, b)
+
+    __lsx_vst(SUB(loc0, vec3), tmp_buf, 31 * 16);
+    __lsx_vst(SUB(loc1, vec2), tmp_buf, 23 * 16);
+    __lsx_vst(SUB(loc2, vec1), tmp_buf, 27 * 16);
+    __lsx_vst(SUB(loc3, vec0), tmp_buf, 19 * 16);
+
+    /* Load 8 & Store 8 */
+    vec0 = __lsx_vld(tmp_odd_buf, 4 * 16);
+    vec1 = __lsx_vld(tmp_odd_buf, 13 * 16);
+    vec2 = __lsx_vld(tmp_odd_buf, 10 * 16);
+    vec3 = __lsx_vld(tmp_odd_buf, 3 * 16);
+    loc0 = __lsx_vld(tmp_eve_buf, 2 * 16);
+    loc1 = __lsx_vld(tmp_eve_buf, 10 * 16);
+    loc2 = __lsx_vld(tmp_eve_buf, 6 * 16);
+    loc3 = __lsx_vld(tmp_eve_buf, 14 * 16);
+
+    DUP4_ARG2(__lsx_vadd_h, loc0, vec3, loc1, vec2, loc2, vec1, loc3, vec0,
+              m1, m5, m3, m7);
+
+    __lsx_vst(SUB(loc0, vec3), tmp_buf, 29 * 16);
+    __lsx_vst(SUB(loc1, vec2), tmp_buf, 21 * 16);
+    __lsx_vst(SUB(loc2, vec1), tmp_buf, 25 * 16);
+    __lsx_vst(SUB(loc3, vec0), tmp_buf, 17 * 16);
+
+    /* Load 8 & Store 8 */
+    vec0 = __lsx_vld(tmp_odd_buf, 2 * 16);
+    vec1 = __lsx_vld(tmp_odd_buf, 11 * 16);
+    vec2 = __lsx_vld(tmp_odd_buf, 12 * 16);
+    vec3 = __lsx_vld(tmp_odd_buf, 7 * 16);
+    loc0 = __lsx_vld(tmp_eve_buf, 1 * 16);
+    loc1 = __lsx_vld(tmp_eve_buf, 9 * 16);
+    loc2 = __lsx_vld(tmp_eve_buf, 5 * 16);
+    loc3 = __lsx_vld(tmp_eve_buf, 13 * 16);
+
+    DUP4_ARG2(__lsx_vadd_h, loc0, vec3, loc1, vec2, loc2, vec1, loc3, vec0,
+              n0, n4, n2, n6);
+
+    __lsx_vst(SUB(loc0, vec3), tmp_buf, 30 * 16);
+    __lsx_vst(SUB(loc1, vec2), tmp_buf, 22 * 16);
+    __lsx_vst(SUB(loc2, vec1), tmp_buf, 26 * 16);
+    __lsx_vst(SUB(loc3, vec0), tmp_buf, 18 * 16);
+
+    /* Load 8 & Store 8 */
+    vec0 = __lsx_vld(tmp_odd_buf, 5 * 16);
+    vec1 = __lsx_vld(tmp_odd_buf, 15 * 16);
+    vec2 = __lsx_vld(tmp_odd_buf, 8 * 16);
+    vec3 = __lsx_vld(tmp_odd_buf, 1 * 16);
+    loc0 = __lsx_vld(tmp_eve_buf, 3 * 16);
+    loc1 = __lsx_vld(tmp_eve_buf, 11 * 16);
+    loc2 = __lsx_vld(tmp_eve_buf, 7 * 16);
+    loc3 = __lsx_vld(tmp_eve_buf, 15 * 16);
+
+    DUP4_ARG2(__lsx_vadd_h, loc0, vec3, loc1, vec2, loc2, vec1, loc3, vec0,
+              n1, n5, n3, n7);
+
+    __lsx_vst(SUB(loc0, vec3), tmp_buf, 28 * 16);
+    __lsx_vst(SUB(loc1, vec2), tmp_buf, 20 * 16);
+    __lsx_vst(SUB(loc2, vec1), tmp_buf, 24 * 16);
+    __lsx_vst(SUB(loc3, vec0), tmp_buf, 16 * 16);
+
+    /* Transpose : 16 vectors */
+    /* 1st & 2nd 8x8 */
+    LSX_TRANSPOSE8x8_H(m0, n0, m1, n1, m2, n2, m3, n3,
+                       m0, n0, m1, n1, m2, n2, m3, n3);
+    __lsx_vst(m0, dst, 0);
+    __lsx_vst(n0, dst, 32 * 2);
+    __lsx_vst(m1, dst, 32 * 4);
+    __lsx_vst(n1, dst, 32 * 6);
+    __lsx_vst(m2, dst, 32 * 8);
+    __lsx_vst(n2, dst, 32 * 10);
+    __lsx_vst(m3, dst, 32 * 12);
+    __lsx_vst(n3, dst, 32 * 14);
+
+    LSX_TRANSPOSE8x8_H(m4, n4, m5, n5, m6, n6, m7, n7,
+                       m4, n4, m5, n5, m6, n6, m7, n7);
+
+    __lsx_vst(m4, dst, 16);
+    __lsx_vst(n4, dst, 16 + 32 * 2);
+    __lsx_vst(m5, dst, 16 + 32 * 4);
+    __lsx_vst(n5, dst, 16 + 32 * 6);
+    __lsx_vst(m6, dst, 16 + 32 * 8);
+    __lsx_vst(n6, dst, 16 + 32 * 10);
+    __lsx_vst(m7, dst, 16 + 32 * 12);
+    __lsx_vst(n7, dst, 16 + 32 * 14);
+
+    /* 3rd & 4th 8x8 */
+    DUP4_ARG2(__lsx_vld, tmp_buf, 16 * 16, tmp_buf, 16 * 17,
+              tmp_buf, 16 * 18, tmp_buf, 16 * 19, m0, n0, m1, n1);
+    DUP4_ARG2(__lsx_vld, tmp_buf, 16 * 20, tmp_buf, 16 * 21,
+              tmp_buf, 16 * 22, tmp_buf, 16 * 23, m2, n2, m3, n3);
+
+    DUP4_ARG2(__lsx_vld, tmp_buf, 16 * 24, tmp_buf, 16 * 25,
+              tmp_buf, 16 * 26, tmp_buf, 16 * 27, m4, n4, m5, n5);
+    DUP4_ARG2(__lsx_vld, tmp_buf, 16 * 28, tmp_buf, 16 * 29,
+              tmp_buf, 16 * 30, tmp_buf, 16 * 31, m6, n6, m7, n7);
+
+    LSX_TRANSPOSE8x8_H(m0, n0, m1, n1, m2, n2, m3, n3,
+                       m0, n0, m1, n1, m2, n2, m3, n3);
+
+    __lsx_vst(m0, dst, 32);
+    __lsx_vst(n0, dst, 32 + 32 * 2);
+    __lsx_vst(m1, dst, 32 + 32 * 4);
+    __lsx_vst(n1, dst, 32 + 32 * 6);
+    __lsx_vst(m2, dst, 32 + 32 * 8);
+    __lsx_vst(n2, dst, 32 + 32 * 10);
+    __lsx_vst(m3, dst, 32 + 32 * 12);
+    __lsx_vst(n3, dst, 32 + 32 * 14);
+
+    LSX_TRANSPOSE8x8_H(m4, n4, m5, n5, m6, n6, m7, n7,
+                       m4, n4, m5, n5, m6, n6, m7, n7);
+
+    __lsx_vst(m4, dst, 48);
+    __lsx_vst(n4, dst, 48 + 32 * 2);
+    __lsx_vst(m5, dst, 48 + 32 * 4);
+    __lsx_vst(n5, dst, 48 + 32 * 6);
+    __lsx_vst(m6, dst, 48 + 32 * 8);
+    __lsx_vst(n6, dst, 48 + 32 * 10);
+    __lsx_vst(m7, dst, 48 + 32 * 12);
+    __lsx_vst(n7, dst, 48 + 32 * 14);
+}
+
+static void vp9_idct8x32_column_even_process_store(int16_t *tmp_buf,
+                                                   int16_t *tmp_eve_buf)
+{
+    __m128i vec0, vec1, vec2, vec3, loc0, loc1, loc2, loc3;
+    __m128i reg0, reg1, reg2, reg3, reg4, reg5, reg6, reg7;
+    __m128i stp0, stp1, stp2, stp3, stp4, stp5, stp6, stp7;
+    __m128i zero = __lsx_vldi(0);
+
+    /* Even stage 1 */
+    DUP4_ARG2(__lsx_vld, tmp_buf, 0, tmp_buf, 32 * 8,
+              tmp_buf, 32 * 16, tmp_buf, 32 * 24, reg0, reg1, reg2, reg3);
+    DUP4_ARG2(__lsx_vld, tmp_buf, 32 * 32, tmp_buf, 32 * 40,
+              tmp_buf, 32 * 48, tmp_buf, 32 * 56, reg4, reg5, reg6, reg7);
+
+    __lsx_vst(zero, tmp_buf, 0);
+    __lsx_vst(zero, tmp_buf, 32 * 8);
+    __lsx_vst(zero, tmp_buf, 32 * 16);
+    __lsx_vst(zero, tmp_buf, 32 * 24);
+    __lsx_vst(zero, tmp_buf, 32 * 32);
+    __lsx_vst(zero, tmp_buf, 32 * 40);
+    __lsx_vst(zero, tmp_buf, 32 * 48);
+    __lsx_vst(zero, tmp_buf, 32 * 56);
+
+    tmp_buf += (2 * 32);
+
+    VP9_DOTP_CONST_PAIR(reg1, reg7, cospi_28_64, cospi_4_64, reg1, reg7);
+    VP9_DOTP_CONST_PAIR(reg5, reg3, cospi_12_64, cospi_20_64, reg5, reg3);
+    LSX_BUTTERFLY_4_H(reg1, reg7, reg3, reg5, vec1, vec3, vec2, vec0);
+    VP9_DOTP_CONST_PAIR(vec2, vec0, cospi_16_64, cospi_16_64, loc2, loc3);
+
+    loc1 = vec3;
+    loc0 = vec1;
+
+    VP9_DOTP_CONST_PAIR(reg0, reg4, cospi_16_64, cospi_16_64, reg0, reg4);
+    VP9_DOTP_CONST_PAIR(reg2, reg6, cospi_24_64, cospi_8_64, reg2, reg6);
+    LSX_BUTTERFLY_4_H(reg4, reg0, reg2, reg6, vec1, vec3, vec2, vec0);
+    LSX_BUTTERFLY_4_H(vec0, vec1, loc1, loc0, stp3, stp0, stp7, stp4);
+    LSX_BUTTERFLY_4_H(vec2, vec3, loc3, loc2, stp2, stp1, stp6, stp5);
+
+    /* Even stage 2 */
+    /* Load 8 */
+    DUP4_ARG2(__lsx_vld, tmp_buf, 0, tmp_buf, 32 * 8,
+              tmp_buf, 32 * 16, tmp_buf, 32 * 24, reg0, reg1, reg2, reg3);
+    DUP4_ARG2(__lsx_vld, tmp_buf, 32 * 32, tmp_buf, 32 * 40,
+              tmp_buf, 32 * 48, tmp_buf, 32 * 56, reg4, reg5, reg6, reg7);
+
+    __lsx_vst(zero, tmp_buf, 0);
+    __lsx_vst(zero, tmp_buf, 32 * 8);
+    __lsx_vst(zero, tmp_buf, 32 * 16);
+    __lsx_vst(zero, tmp_buf, 32 * 24);
+    __lsx_vst(zero, tmp_buf, 32 * 32);
+    __lsx_vst(zero, tmp_buf, 32 * 40);
+    __lsx_vst(zero, tmp_buf, 32 * 48);
+    __lsx_vst(zero, tmp_buf, 32 * 56);
+
+    VP9_DOTP_CONST_PAIR(reg0, reg7, cospi_30_64, cospi_2_64, reg0, reg7);
+    VP9_DOTP_CONST_PAIR(reg4, reg3, cospi_14_64, cospi_18_64, reg4, reg3);
+    VP9_DOTP_CONST_PAIR(reg2, reg5, cospi_22_64, cospi_10_64, reg2, reg5);
+    VP9_DOTP_CONST_PAIR(reg6, reg1, cospi_6_64, cospi_26_64, reg6, reg1);
+
+    vec0 = __lsx_vadd_h(reg0, reg4);
+    reg0 = __lsx_vsub_h(reg0, reg4);
+    reg4 = __lsx_vadd_h(reg6, reg2);
+    reg6 = __lsx_vsub_h(reg6, reg2);
+    reg2 = __lsx_vadd_h(reg1, reg5);
+    reg1 = __lsx_vsub_h(reg1, reg5);
+    reg5 = __lsx_vadd_h(reg7, reg3);
+    reg7 = __lsx_vsub_h(reg7, reg3);
+    reg3 = vec0;
+
+    vec1 = reg2;
+    reg2 = __lsx_vadd_h(reg3, reg4);
+    reg3 = __lsx_vsub_h(reg3, reg4);
+    reg4 = __lsx_vsub_h(reg5, vec1);
+    reg5 = __lsx_vadd_h(reg5, vec1);
+
+    VP9_DOTP_CONST_PAIR(reg7, reg0, cospi_24_64, cospi_8_64, reg0, reg7);
+    VP9_DOTP_CONST_PAIR(__lsx_vneg_h(reg6), reg1, cospi_24_64, cospi_8_64,
+                        reg6, reg1);
+
+    vec0 = __lsx_vsub_h(reg0, reg6);
+    reg0 = __lsx_vadd_h(reg0, reg6);
+    vec1 = __lsx_vsub_h(reg7, reg1);
+    reg7 = __lsx_vadd_h(reg7, reg1);
+
+    VP9_DOTP_CONST_PAIR(vec1, vec0, cospi_16_64, cospi_16_64, reg6, reg1);
+    VP9_DOTP_CONST_PAIR(reg4, reg3, cospi_16_64, cospi_16_64, reg3, reg4);
+
+    /* Even stage 3 : Dependency on Even stage 1 & Even stage 2 */
+    /* Store 8 */
+    LSX_BUTTERFLY_4_H(stp0, stp1, reg7, reg5, loc1, loc3, loc2, loc0);
+    __lsx_vst(loc1, tmp_eve_buf, 0);
+    __lsx_vst(loc3, tmp_eve_buf, 16);
+    __lsx_vst(loc2, tmp_eve_buf, 14 * 16);
+    __lsx_vst(loc0, tmp_eve_buf, 14 * 16 + 16);
+    LSX_BUTTERFLY_4_H(stp2, stp3, reg4, reg1, loc1, loc3, loc2, loc0);
+    __lsx_vst(loc1, tmp_eve_buf, 2 * 16);
+    __lsx_vst(loc3, tmp_eve_buf, 2 * 16 + 16);
+    __lsx_vst(loc2, tmp_eve_buf, 12 * 16);
+    __lsx_vst(loc0, tmp_eve_buf, 12 * 16 + 16);
+
+    /* Store 8 */
+    LSX_BUTTERFLY_4_H(stp4, stp5, reg6, reg3, loc1, loc3, loc2, loc0);
+    __lsx_vst(loc1, tmp_eve_buf, 4 * 16);
+    __lsx_vst(loc3, tmp_eve_buf, 4 * 16 + 16);
+    __lsx_vst(loc2, tmp_eve_buf, 10 * 16);
+    __lsx_vst(loc0, tmp_eve_buf, 10 * 16 + 16);
+
+    LSX_BUTTERFLY_4_H(stp6, stp7, reg2, reg0, loc1, loc3, loc2, loc0);
+    __lsx_vst(loc1, tmp_eve_buf, 6 * 16);
+    __lsx_vst(loc3, tmp_eve_buf, 6 * 16 + 16);
+    __lsx_vst(loc2, tmp_eve_buf, 8 * 16);
+    __lsx_vst(loc0, tmp_eve_buf, 8 * 16 + 16);
+}
+
+static void vp9_idct8x32_column_odd_process_store(int16_t *tmp_buf,
+                                                  int16_t *tmp_odd_buf)
+{
+    __m128i vec0, vec1, vec2, vec3, loc0, loc1, loc2, loc3;
+    __m128i reg0, reg1, reg2, reg3, reg4, reg5, reg6, reg7;
+    __m128i zero = __lsx_vldi(0);
+
+    /* Odd stage 1 */
+    reg0 = __lsx_vld(tmp_buf, 64);
+    reg1 = __lsx_vld(tmp_buf, 7 * 64);
+    reg2 = __lsx_vld(tmp_buf, 9 * 64);
+    reg3 = __lsx_vld(tmp_buf, 15 * 64);
+    reg4 = __lsx_vld(tmp_buf, 17 * 64);
+    reg5 = __lsx_vld(tmp_buf, 23 * 64);
+    reg6 = __lsx_vld(tmp_buf, 25 * 64);
+    reg7 = __lsx_vld(tmp_buf, 31 * 64);
+
+    __lsx_vst(zero, tmp_buf, 64);
+    __lsx_vst(zero, tmp_buf, 7 * 64);
+    __lsx_vst(zero, tmp_buf, 9 * 64);
+    __lsx_vst(zero, tmp_buf, 15 * 64);
+    __lsx_vst(zero, tmp_buf, 17 * 64);
+    __lsx_vst(zero, tmp_buf, 23 * 64);
+    __lsx_vst(zero, tmp_buf, 25 * 64);
+    __lsx_vst(zero, tmp_buf, 31 * 64);
+
+    VP9_DOTP_CONST_PAIR(reg0, reg7, cospi_31_64, cospi_1_64, reg0, reg7);
+    VP9_DOTP_CONST_PAIR(reg4, reg3, cospi_15_64, cospi_17_64, reg3, reg4);
+    VP9_DOTP_CONST_PAIR(reg2, reg5, cospi_23_64, cospi_9_64, reg2, reg5);
+    VP9_DOTP_CONST_PAIR(reg6, reg1, cospi_7_64, cospi_25_64, reg1, reg6);
+
+    vec0 = __lsx_vadd_h(reg0, reg3);
+    reg0 = __lsx_vsub_h(reg0, reg3);
+    reg3 = __lsx_vadd_h(reg7, reg4);
+    reg7 = __lsx_vsub_h(reg7, reg4);
+    reg4 = __lsx_vadd_h(reg1, reg2);
+    reg1 = __lsx_vsub_h(reg1, reg2);
+    reg2 = __lsx_vadd_h(reg6, reg5);
+    reg6 = __lsx_vsub_h(reg6, reg5);
+    reg5 = vec0;
+
+    /* 4 Stores */
+    DUP2_ARG2(__lsx_vadd_h, reg5, reg4, reg3, reg2, vec0, vec1);
+    __lsx_vst(vec0, tmp_odd_buf, 4 * 16);
+    __lsx_vst(vec1, tmp_odd_buf, 4 * 16 + 16);
+    DUP2_ARG2(__lsx_vsub_h, reg5, reg4, reg3, reg2, vec0, vec1);
+    VP9_DOTP_CONST_PAIR(vec1, vec0, cospi_24_64, cospi_8_64, vec0, vec1);
+    __lsx_vst(vec0, tmp_odd_buf, 0);
+    __lsx_vst(vec1, tmp_odd_buf, 16);
+
+    /* 4 Stores */
+    VP9_DOTP_CONST_PAIR(reg7, reg0, cospi_28_64, cospi_4_64, reg0, reg7);
+    VP9_DOTP_CONST_PAIR(reg6, reg1, -cospi_4_64, cospi_28_64, reg1, reg6);
+    LSX_BUTTERFLY_4_H(reg0, reg7, reg6, reg1, vec0, vec1, vec2, vec3);
+    __lsx_vst(vec0, tmp_odd_buf, 6 * 16);
+    __lsx_vst(vec1, tmp_odd_buf, 6 * 16 + 16);
+    VP9_DOTP_CONST_PAIR(vec2, vec3, cospi_24_64, cospi_8_64, vec2, vec3);
+    __lsx_vst(vec2, tmp_odd_buf, 2 * 16);
+    __lsx_vst(vec3, tmp_odd_buf, 2 * 16 + 16);
+
+    /* Odd stage 2 */
+    /* 8 loads */
+    reg0 = __lsx_vld(tmp_buf, 3 * 64);
+    reg1 = __lsx_vld(tmp_buf, 5 * 64);
+    reg2 = __lsx_vld(tmp_buf, 11 * 64);
+    reg3 = __lsx_vld(tmp_buf, 13 * 64);
+    reg4 = __lsx_vld(tmp_buf, 19 * 64);
+    reg5 = __lsx_vld(tmp_buf, 21 * 64);
+    reg6 = __lsx_vld(tmp_buf, 27 * 64);
+    reg7 = __lsx_vld(tmp_buf, 29 * 64);
+
+    __lsx_vst(zero, tmp_buf, 3 * 64);
+    __lsx_vst(zero, tmp_buf, 5 * 64);
+    __lsx_vst(zero, tmp_buf, 11 * 64);
+    __lsx_vst(zero, tmp_buf, 13 * 64);
+    __lsx_vst(zero, tmp_buf, 19 * 64);
+    __lsx_vst(zero, tmp_buf, 21 * 64);
+    __lsx_vst(zero, tmp_buf, 27 * 64);
+    __lsx_vst(zero, tmp_buf, 29 * 64);
+
+    VP9_DOTP_CONST_PAIR(reg1, reg6, cospi_27_64, cospi_5_64, reg1, reg6);
+    VP9_DOTP_CONST_PAIR(reg5, reg2, cospi_11_64, cospi_21_64, reg2, reg5);
+    VP9_DOTP_CONST_PAIR(reg3, reg4, cospi_19_64, cospi_13_64, reg3, reg4);
+    VP9_DOTP_CONST_PAIR(reg7, reg0, cospi_3_64, cospi_29_64, reg0, reg7);
+
+    /* 4 Stores */
+    DUP4_ARG2(__lsx_vsub_h,reg1, reg2, reg6, reg5, reg0, reg3, reg7, reg4,
+              vec0, vec1, vec2, vec3);
+    VP9_DOTP_CONST_PAIR(vec1, vec0, cospi_12_64, cospi_20_64, loc0, loc1);
+    VP9_DOTP_CONST_PAIR(vec3, vec2, -cospi_20_64, cospi_12_64, loc2, loc3);
+    LSX_BUTTERFLY_4_H(loc2, loc3, loc1, loc0, vec0, vec1, vec3, vec2);
+    __lsx_vst(vec0, tmp_odd_buf, 12 * 16);
+    __lsx_vst(vec1, tmp_odd_buf, 12 * 16 + 3 * 16);
+    VP9_DOTP_CONST_PAIR(vec3, vec2, -cospi_8_64, cospi_24_64, vec0, vec1);
+    __lsx_vst(vec0, tmp_odd_buf, 10 * 16);
+    __lsx_vst(vec1, tmp_odd_buf, 10 * 16 + 16);
+
+    /* 4 Stores */
+    DUP4_ARG2(__lsx_vadd_h, reg0, reg3, reg1, reg2, reg5, reg6, reg4, reg7,
+              vec0, vec1, vec2, vec3);
+    LSX_BUTTERFLY_4_H(vec0, vec3, vec2, vec1, reg0, reg1, reg3, reg2);
+    __lsx_vst(reg0, tmp_odd_buf, 13 * 16);
+    __lsx_vst(reg1, tmp_odd_buf, 13 * 16 + 16);
+    VP9_DOTP_CONST_PAIR(reg3, reg2, -cospi_8_64, cospi_24_64,
+                        reg0, reg1);
+    __lsx_vst(reg0, tmp_odd_buf, 8 * 16);
+    __lsx_vst(reg1, tmp_odd_buf, 8 * 16 + 16);
+
+    /* Odd stage 3 : Dependency on Odd stage 1 & Odd stage 2 */
+    /* Load 8 & Store 8 */
+    DUP4_ARG2(__lsx_vld, tmp_odd_buf, 0, tmp_odd_buf, 16,
+              tmp_odd_buf, 32, tmp_odd_buf, 48, reg0, reg1, reg2, reg3);
+    DUP4_ARG2(__lsx_vld, tmp_odd_buf, 8 * 16, tmp_odd_buf, 8 * 16 + 16,
+              tmp_odd_buf, 8 * 16 + 32, tmp_odd_buf, 8 * 16 + 48,
+              reg4, reg5, reg6, reg7);
+
+    DUP4_ARG2(__lsx_vadd_h, reg0, reg4, reg1, reg5, reg2, reg6, reg3, reg7,
+                  loc0, loc1, loc2, loc3);
+    __lsx_vst(loc0, tmp_odd_buf, 0);
+    __lsx_vst(loc1, tmp_odd_buf, 16);
+    __lsx_vst(loc2, tmp_odd_buf, 32);
+    __lsx_vst(loc3, tmp_odd_buf, 48);
+    DUP2_ARG2(__lsx_vsub_h, reg0, reg4, reg1, reg5, vec0, vec1);
+    VP9_DOTP_CONST_PAIR(vec1, vec0, cospi_16_64, cospi_16_64, loc0, loc1);
+
+    DUP2_ARG2(__lsx_vsub_h, reg2, reg6, reg3, reg7, vec0, vec1);
+    VP9_DOTP_CONST_PAIR(vec1, vec0, cospi_16_64, cospi_16_64, loc2, loc3);
+    __lsx_vst(loc0, tmp_odd_buf, 8 * 16);
+    __lsx_vst(loc1, tmp_odd_buf, 8 * 16 + 16);
+    __lsx_vst(loc2, tmp_odd_buf, 8 * 16 + 32);
+    __lsx_vst(loc3, tmp_odd_buf, 8 * 16 + 48);
+
+    /* Load 8 & Store 8 */
+    DUP4_ARG2(__lsx_vld, tmp_odd_buf, 4 * 16, tmp_odd_buf, 4 * 16 + 16,
+              tmp_odd_buf, 4 * 16 + 32, tmp_odd_buf, 4 * 16 + 48,
+              reg1, reg2, reg0, reg3);
+    DUP4_ARG2(__lsx_vld, tmp_odd_buf, 12 * 16, tmp_odd_buf, 12 * 16 + 16,
+              tmp_odd_buf, 12 * 16 + 32, tmp_odd_buf, 12 * 16 + 48,
+              reg4, reg5, reg6, reg7);
+
+    DUP4_ARG2(__lsx_vadd_h, reg0, reg4, reg1, reg5, reg2, reg6, reg3, reg7,
+              loc0, loc1, loc2, loc3);
+    __lsx_vst(loc0, tmp_odd_buf, 4 * 16);
+    __lsx_vst(loc1, tmp_odd_buf, 4 * 16 + 16);
+    __lsx_vst(loc2, tmp_odd_buf, 4 * 16 + 32);
+    __lsx_vst(loc3, tmp_odd_buf, 4 * 16 + 48);
+
+    DUP2_ARG2(__lsx_vsub_h, reg0, reg4, reg3, reg7, vec0, vec1);
+    VP9_DOTP_CONST_PAIR(vec1, vec0, cospi_16_64, cospi_16_64, loc0, loc1);
+
+    DUP2_ARG2(__lsx_vsub_h, reg1, reg5, reg2, reg6, vec0, vec1);
+    VP9_DOTP_CONST_PAIR(vec1, vec0, cospi_16_64, cospi_16_64, loc2, loc3);
+    __lsx_vst(loc0, tmp_odd_buf, 12 * 16);
+    __lsx_vst(loc1, tmp_odd_buf, 12 * 16 + 16);
+    __lsx_vst(loc2, tmp_odd_buf, 12 * 16 + 32);
+    __lsx_vst(loc3, tmp_odd_buf, 12 * 16 + 48);
+}
+
+static void vp9_idct8x32_column_butterfly_addblk(int16_t *tmp_eve_buf,
+                                                 int16_t *tmp_odd_buf,
+                                                 uint8_t *dst,
+                                                 int32_t dst_stride)
+{
+    __m128i vec0, vec1, vec2, vec3, loc0, loc1, loc2, loc3;
+    __m128i m0, m1, m2, m3, m4, m5, m6, m7, n0, n1, n2, n3, n4, n5, n6, n7;
+
+    /* FINAL BUTTERFLY : Dependency on Even & Odd */
+    vec0 = __lsx_vld(tmp_odd_buf, 0);
+    vec1 = __lsx_vld(tmp_odd_buf, 9 * 16);
+    vec2 = __lsx_vld(tmp_odd_buf, 14 * 16);
+    vec3 = __lsx_vld(tmp_odd_buf, 6 * 16);
+    loc0 = __lsx_vld(tmp_eve_buf, 0);
+    loc1 = __lsx_vld(tmp_eve_buf, 8 * 16);
+    loc2 = __lsx_vld(tmp_eve_buf, 4 * 16);
+    loc3 = __lsx_vld(tmp_eve_buf, 12 * 16);
+
+    DUP4_ARG2(__lsx_vadd_h, loc0, vec3, loc1, vec2, loc2, vec1, loc3, vec0,
+              m0, m4, m2, m6);
+    DUP4_ARG2(__lsx_vsrari_h, m0, 6, m2, 6, m4, 6, m6, 6, m0, m2, m4, m6);
+    VP9_ADDBLK_ST8x4_UB(dst, (4 * dst_stride), m0, m2, m4, m6);
+
+    DUP4_ARG2(__lsx_vsub_h, loc0, vec3, loc1, vec2, loc2, vec1, loc3, vec0,
+              m6, m2, m4, m0);
+    DUP4_ARG2(__lsx_vsrari_h, m0, 6, m2, 6, m4, 6, m6, 6, m0, m2, m4, m6);
+    VP9_ADDBLK_ST8x4_UB((dst + 19 * dst_stride), (4 * dst_stride),
+                        m0, m2, m4, m6);
+
+    /* Load 8 & Store 8 */
+    vec0 = __lsx_vld(tmp_odd_buf, 4 * 16);
+    vec1 = __lsx_vld(tmp_odd_buf, 13 * 16);
+    vec2 = __lsx_vld(tmp_odd_buf, 10 * 16);
+    vec3 = __lsx_vld(tmp_odd_buf, 3 * 16);
+    loc0 = __lsx_vld(tmp_eve_buf, 2 * 16);
+    loc1 = __lsx_vld(tmp_eve_buf, 10 * 16);
+    loc2 = __lsx_vld(tmp_eve_buf, 6 * 16);
+    loc3 = __lsx_vld(tmp_eve_buf, 14 * 16);
+
+    DUP4_ARG2(__lsx_vadd_h, loc0, vec3, loc1, vec2, loc2, vec1, loc3, vec0,
+               m1, m5, m3, m7);
+    DUP4_ARG2(__lsx_vsrari_h, m1, 6, m3, 6, m5, 6, m7, 6, m1, m3, m5, m7);
+    VP9_ADDBLK_ST8x4_UB((dst + 2 * dst_stride), (4 * dst_stride),
+                        m1, m3, m5, m7);
+
+    DUP4_ARG2(__lsx_vsub_h, loc0, vec3, loc1, vec2, loc2, vec1, loc3, vec0,
+              m7, m3, m5, m1);
+    DUP4_ARG2(__lsx_vsrari_h, m1, 6, m3, 6, m5, 6, m7, 6, m1, m3, m5, m7);
+    VP9_ADDBLK_ST8x4_UB((dst + 17 * dst_stride), (4 * dst_stride),
+                        m1, m3, m5, m7);
+
+    /* Load 8 & Store 8 */
+    vec0 = __lsx_vld(tmp_odd_buf, 2 * 16);
+    vec1 = __lsx_vld(tmp_odd_buf, 11 * 16);
+    vec2 = __lsx_vld(tmp_odd_buf, 12 * 16);
+    vec3 = __lsx_vld(tmp_odd_buf, 7 * 16);
+    loc0 = __lsx_vld(tmp_eve_buf, 1 * 16);
+    loc1 = __lsx_vld(tmp_eve_buf, 9 * 16);
+    loc2 = __lsx_vld(tmp_eve_buf, 5 * 16);
+    loc3 = __lsx_vld(tmp_eve_buf, 13 * 16);
+
+    DUP4_ARG2(__lsx_vadd_h, loc0, vec3, loc1, vec2, loc2, vec1, loc3, vec0,
+              n0, n4, n2, n6);
+    DUP4_ARG2(__lsx_vsrari_h, n0, 6, n2, 6, n4, 6, n6, 6, n0, n2, n4, n6);
+    VP9_ADDBLK_ST8x4_UB((dst + 1 * dst_stride), (4 * dst_stride),
+                        n0, n2, n4, n6);
+    DUP4_ARG2(__lsx_vsub_h, loc0, vec3, loc1, vec2, loc2, vec1, loc3, vec0,
+              n6, n2, n4, n0);
+    DUP4_ARG2(__lsx_vsrari_h, n0, 6, n2, 6, n4, 6, n6, 6, n0, n2, n4, n6);
+    VP9_ADDBLK_ST8x4_UB((dst + 18 * dst_stride), (4 * dst_stride),
+                        n0, n2, n4, n6);
+
+    /* Load 8 & Store 8 */
+    vec0 = __lsx_vld(tmp_odd_buf, 5 * 16);
+    vec1 = __lsx_vld(tmp_odd_buf, 15 * 16);
+    vec2 = __lsx_vld(tmp_odd_buf, 8 * 16);
+    vec3 = __lsx_vld(tmp_odd_buf, 1 * 16);
+    loc0 = __lsx_vld(tmp_eve_buf, 3 * 16);
+    loc1 = __lsx_vld(tmp_eve_buf, 11 * 16);
+    loc2 = __lsx_vld(tmp_eve_buf, 7 * 16);
+    loc3 = __lsx_vld(tmp_eve_buf, 15 * 16);
+
+    DUP4_ARG2(__lsx_vadd_h, loc0, vec3, loc1, vec2, loc2, vec1, loc3, vec0,
+              n1, n5, n3, n7);
+    DUP4_ARG2(__lsx_vsrari_h, n1, 6, n3, 6, n5, 6, n7, 6, n1, n3, n5, n7);
+    VP9_ADDBLK_ST8x4_UB((dst + 3 * dst_stride), (4 * dst_stride),
+                        n1, n3, n5, n7);
+    DUP4_ARG2(__lsx_vsub_h, loc0, vec3, loc1, vec2, loc2, vec1, loc3, vec0,
+              n7, n3, n5, n1);
+    DUP4_ARG2(__lsx_vsrari_h, n1, 6, n3, 6, n5, 6, n7, 6, n1, n3, n5, n7);
+    VP9_ADDBLK_ST8x4_UB((dst + 16 * dst_stride), (4 * dst_stride),
+                        n1, n3, n5, n7);
+}
+
+static void vp9_idct8x32_1d_columns_addblk_lsx(int16_t *input, uint8_t *dst,
+                                               int32_t dst_stride)
+{
+    int16_t tmp_odd_buf[16 * 8] ALLOC_ALIGNED(16);
+    int16_t tmp_eve_buf[16 * 8] ALLOC_ALIGNED(16);
+
+    vp9_idct8x32_column_even_process_store(input, &tmp_eve_buf[0]);
+    vp9_idct8x32_column_odd_process_store(input, &tmp_odd_buf[0]);
+    vp9_idct8x32_column_butterfly_addblk(&tmp_eve_buf[0], &tmp_odd_buf[0],
+                                         dst, dst_stride);
+}
+
+static void vp9_idct8x32_1d_columns_lsx(int16_t *input, int16_t *output,
+                                        int16_t *tmp_buf)
+{
+    int16_t tmp_odd_buf[16 * 8] ALLOC_ALIGNED(16);
+    int16_t tmp_eve_buf[16 * 8] ALLOC_ALIGNED(16);
+
+    vp9_idct8x32_column_even_process_store(input, &tmp_eve_buf[0]);
+    vp9_idct8x32_column_odd_process_store(input, &tmp_odd_buf[0]);
+    vp9_idct_butterfly_transpose_store(tmp_buf, &tmp_eve_buf[0],
+                                       &tmp_odd_buf[0], output);
+}
+
+static void vp9_idct32x32_1_add_lsx(int16_t *input, uint8_t *dst,
+                                    int32_t dst_stride)
+{
+    int32_t i;
+    int16_t out;
+    uint8_t *dst_tmp = dst + dst_stride;
+    __m128i zero = __lsx_vldi(0);
+    __m128i dst0, dst1, dst2, dst3, tmp0, tmp1, tmp2, tmp3;
+    __m128i res0, res1, res2, res3, res4, res5, res6, res7, vec;
+
+    out = ROUND_POWER_OF_TWO((input[0] * cospi_16_64), VP9_DCT_CONST_BITS);
+    out = ROUND_POWER_OF_TWO((out * cospi_16_64), VP9_DCT_CONST_BITS);
+    out = ROUND_POWER_OF_TWO(out, 6);
+    input[0] = 0;
+
+    vec = __lsx_vreplgr2vr_h(out);
+
+    for (i = 16; i--;) {
+        DUP2_ARG2(__lsx_vld, dst, 0, dst, 16, dst0, dst1);
+        DUP2_ARG2(__lsx_vld, dst_tmp, 0, dst_tmp, 16, dst2, dst3);
+
+        DUP4_ARG2(__lsx_vilvl_b, zero, dst0, zero, dst1, zero, dst2, zero, dst3,
+                  res0, res1, res2, res3);
+        DUP4_ARG2(__lsx_vilvh_b, zero, dst0, zero, dst1, zero, dst2, zero, dst3,
+                  res4, res5, res6, res7);
+        DUP4_ARG2(__lsx_vadd_h, res0, vec, res1, vec, res2, vec, res3, vec,
+                  res0, res1, res2, res3);
+        DUP4_ARG2(__lsx_vadd_h, res4, vec, res5, vec, res6, vec, res7, vec,
+                  res4, res5, res6, res7);
+        DUP4_ARG1(__lsx_vclip255_h, res0, res1, res2, res3, res0, res1, res2, res3);
+        DUP4_ARG1(__lsx_vclip255_h, res4, res5, res6, res7, res4, res5, res6, res7);
+        DUP4_ARG2(__lsx_vpickev_b, res4, res0, res5, res1, res6, res2, res7, res3,
+                  tmp0, tmp1, tmp2, tmp3);
+
+        __lsx_vst(tmp0, dst, 0);
+        __lsx_vst(tmp1, dst, 16);
+        __lsx_vst(tmp2, dst_tmp, 0);
+        __lsx_vst(tmp3, dst_tmp, 16);
+        dst = dst_tmp + dst_stride;
+        dst_tmp = dst + dst_stride;
+    }
+}
+
+static void vp9_idct32x32_34_colcol_addblk_lsx(int16_t *input, uint8_t *dst,
+                                               int32_t dst_stride)
+{
+    int32_t i;
+    int16_t out_arr[32 * 32] ALLOC_ALIGNED(16);
+    int16_t *out_ptr = out_arr;
+    int16_t tmp_buf[8 * 32] ALLOC_ALIGNED(16);
+    __m128i zero = __lsx_vldi(0);
+
+    for (i = 16; i--;) {
+        __lsx_vst(zero, out_ptr, 0);
+        __lsx_vst(zero, out_ptr, 16);
+        __lsx_vst(zero, out_ptr, 32);
+        __lsx_vst(zero, out_ptr, 48);
+        __lsx_vst(zero, out_ptr, 64);
+        __lsx_vst(zero, out_ptr, 80);
+        __lsx_vst(zero, out_ptr, 96);
+        __lsx_vst(zero, out_ptr, 112);
+        out_ptr += 64;
+    }
+
+    out_ptr = out_arr;
+
+    /* process 8*32 block */
+    vp9_idct8x32_1d_columns_lsx(input, out_ptr, &tmp_buf[0]);
+
+    /* transform columns */
+    for (i = 0; i < 4; i++) {
+        /* process 8*32 block */
+        vp9_idct8x32_1d_columns_addblk_lsx((out_ptr + (i << 3)),
+                                           (dst + (i << 3)), dst_stride);
+    }
+}
+
+static void vp9_idct32x32_colcol_addblk_lsx(int16_t *input, uint8_t *dst,
+                                            int32_t dst_stride)
+{
+    int32_t i;
+    int16_t out_arr[32 * 32] ALLOC_ALIGNED(16);
+    int16_t *out_ptr = out_arr;
+    int16_t tmp_buf[8 * 32] ALLOC_ALIGNED(16);
+
+    /* transform rows */
+    for (i = 0; i < 4; i++) {
+        /* process 8*32 block */
+        vp9_idct8x32_1d_columns_lsx((input + (i << 3)), (out_ptr + (i << 8)),
+                                    &tmp_buf[0]);
+    }
+
+    /* transform columns */
+    for (i = 0; i < 4; i++) {
+        /* process 8*32 block */
+        vp9_idct8x32_1d_columns_addblk_lsx((out_ptr + (i << 3)),
+                                           (dst + (i << 3)), dst_stride);
+    }
+}
+
+void ff_idct_idct_8x8_add_lsx(uint8_t *dst, ptrdiff_t stride,
+                              int16_t *block, int eob)
+{
+    if (eob == 1) {
+        vp9_idct8x8_1_add_lsx(block, dst, stride);
+    }
+    else if (eob <= 12) {
+        vp9_idct8x8_12_colcol_addblk_lsx(block, dst, stride);
+    }
+    else {
+        vp9_idct8x8_colcol_addblk_lsx(block, dst, stride);
+    }
+}
+
+void ff_idct_idct_16x16_add_lsx(uint8_t *dst, ptrdiff_t stride,
+                                int16_t *block, int eob)
+{
+    if (eob == 1) {
+        /* DC only DCT coefficient. */
+        vp9_idct16x16_1_add_lsx(block, dst, stride);
+    }
+    else if (eob <= 10) {
+        vp9_idct16x16_10_colcol_addblk_lsx(block, dst, stride);
+    }
+    else {
+        vp9_idct16x16_colcol_addblk_lsx(block, dst, stride);
+    }
+}
+
+void ff_idct_idct_32x32_add_lsx(uint8_t *dst, ptrdiff_t stride,
+                                int16_t *block, int eob)
+{
+    if (eob == 1) {
+        vp9_idct32x32_1_add_lsx(block, dst, stride);
+    }
+    else if (eob <= 34) {
+        vp9_idct32x32_34_colcol_addblk_lsx(block, dst, stride);
+    }
+    else {
+        vp9_idct32x32_colcol_addblk_lsx(block, dst, stride);
+    }
+}
diff --git a/libavcodec/loongarch/vp9_intra_lsx.c b/libavcodec/loongarch/vp9_intra_lsx.c
new file mode 100644
index 0000000000..d3f32646f3
--- /dev/null
+++ b/libavcodec/loongarch/vp9_intra_lsx.c
@@ -0,0 +1,653 @@
+/*
+ * Copyright (c) 2021 Loongson Technology Corporation Limited
+ * Contributed by Hao Chen <chenhao@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "libavcodec/vp9dsp.h"
+#include "libavutil/loongarch/loongson_intrinsics.h"
+#include "vp9dsp_loongarch.h"
+
+#define LSX_ST_8(_dst0, _dst1, _dst2, _dst3, _dst4,   \
+                 _dst5, _dst6, _dst7, _dst, _stride,  \
+                 _stride2, _stride3, _stride4)        \
+{                                                     \
+    __lsx_vst(_dst0, _dst, 0);                        \
+    __lsx_vstx(_dst1, _dst, _stride);                 \
+    __lsx_vstx(_dst2, _dst, _stride2);                \
+    __lsx_vstx(_dst3, _dst, _stride3);                \
+    _dst += _stride4;                                 \
+    __lsx_vst(_dst4, _dst, 0);                        \
+    __lsx_vstx(_dst5, _dst, _stride);                 \
+    __lsx_vstx(_dst6, _dst, _stride2);                \
+    __lsx_vstx(_dst7, _dst, _stride3);                \
+}
+
+#define LSX_ST_8X16(_dst0, _dst1, _dst2, _dst3, _dst4,   \
+                    _dst5, _dst6, _dst7, _dst, _stride)  \
+{                                                        \
+    __lsx_vst(_dst0, _dst, 0);                           \
+    __lsx_vst(_dst0, _dst, 16);                          \
+    _dst += _stride;                                     \
+    __lsx_vst(_dst1, _dst, 0);                           \
+    __lsx_vst(_dst1, _dst, 16);                          \
+    _dst += _stride;                                     \
+    __lsx_vst(_dst2, _dst, 0);                           \
+    __lsx_vst(_dst2, _dst, 16);                          \
+    _dst += _stride;                                     \
+    __lsx_vst(_dst3, _dst, 0);                           \
+    __lsx_vst(_dst3, _dst, 16);                          \
+    _dst += _stride;                                     \
+    __lsx_vst(_dst4, _dst, 0);                           \
+    __lsx_vst(_dst4, _dst, 16);                          \
+    _dst += _stride;                                     \
+    __lsx_vst(_dst5, _dst, 0);                           \
+    __lsx_vst(_dst5, _dst, 16);                          \
+    _dst += _stride;                                     \
+    __lsx_vst(_dst6, _dst, 0);                           \
+    __lsx_vst(_dst6, _dst, 16);                          \
+    _dst += _stride;                                     \
+    __lsx_vst(_dst7, _dst, 0);                           \
+    __lsx_vst(_dst7, _dst, 16);                          \
+    _dst += _stride;                                     \
+}
+
+void ff_vert_16x16_lsx(uint8_t *dst, ptrdiff_t dst_stride, const uint8_t *left,
+                       const uint8_t *src)
+{
+    __m128i src0;
+    ptrdiff_t stride2 = dst_stride << 1;
+    ptrdiff_t stride3 = stride2 + dst_stride;
+    ptrdiff_t stride4 = stride2 << 1;
+    src0 = __lsx_vld(src, 0);
+    LSX_ST_8(src0, src0, src0, src0, src0, src0, src0, src0, dst,
+             dst_stride, stride2, stride3, stride4);
+    dst += stride4;
+    LSX_ST_8(src0, src0, src0, src0, src0, src0, src0, src0, dst,
+             dst_stride, stride2, stride3, stride4);
+}
+
+void ff_vert_32x32_lsx(uint8_t *dst, ptrdiff_t dst_stride, const uint8_t *left,
+                       const uint8_t *src)
+{
+    uint32_t row;
+    __m128i src0, src1;
+
+    DUP2_ARG2(__lsx_vld, src, 0, src, 16, src0, src1);
+    for (row = 32; row--;) {
+        __lsx_vst(src0, dst, 0);
+        __lsx_vst(src1, dst, 16);
+        dst += dst_stride;
+    }
+}
+
+void ff_hor_16x16_lsx(uint8_t *dst, ptrdiff_t dst_stride, const uint8_t *src,
+                      const uint8_t *top)
+{
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m128i src8, src9, src10, src11, src12, src13, src14, src15;
+    ptrdiff_t stride2 = dst_stride << 1;
+    ptrdiff_t stride3 = stride2 + dst_stride;
+    ptrdiff_t stride4 = stride2 << 1;
+
+    src15 = __lsx_vldrepl_b(src, 0);
+    src14 = __lsx_vldrepl_b(src, 1);
+    src13 = __lsx_vldrepl_b(src, 2);
+    src12 = __lsx_vldrepl_b(src, 3);
+    src11 = __lsx_vldrepl_b(src, 4);
+    src10 = __lsx_vldrepl_b(src, 5);
+    src9  = __lsx_vldrepl_b(src, 6);
+    src8  = __lsx_vldrepl_b(src, 7);
+    src7  = __lsx_vldrepl_b(src, 8);
+    src6  = __lsx_vldrepl_b(src, 9);
+    src5  = __lsx_vldrepl_b(src, 10);
+    src4  = __lsx_vldrepl_b(src, 11);
+    src3  = __lsx_vldrepl_b(src, 12);
+    src2  = __lsx_vldrepl_b(src, 13);
+    src1  = __lsx_vldrepl_b(src, 14);
+    src0  = __lsx_vldrepl_b(src, 15);
+    LSX_ST_8(src0, src1, src2, src3, src4, src5, src6, src7, dst,
+             dst_stride, stride2, stride3, stride4);
+    dst += stride4;
+    LSX_ST_8(src8, src9, src10, src11, src12, src13, src14, src15, dst,
+             dst_stride, stride2, stride3, stride4);
+}
+
+void ff_hor_32x32_lsx(uint8_t *dst, ptrdiff_t dst_stride, const uint8_t *src,
+                      const uint8_t *top)
+{
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m128i src8, src9, src10, src11, src12, src13, src14, src15;
+    __m128i src16, src17, src18, src19, src20, src21, src22, src23;
+    __m128i src24, src25, src26, src27, src28, src29, src30, src31;
+
+    src31 = __lsx_vldrepl_b(src, 0);
+    src30 = __lsx_vldrepl_b(src, 1);
+    src29 = __lsx_vldrepl_b(src, 2);
+    src28 = __lsx_vldrepl_b(src, 3);
+    src27 = __lsx_vldrepl_b(src, 4);
+    src26 = __lsx_vldrepl_b(src, 5);
+    src25 = __lsx_vldrepl_b(src, 6);
+    src24 = __lsx_vldrepl_b(src, 7);
+    src23 = __lsx_vldrepl_b(src, 8);
+    src22 = __lsx_vldrepl_b(src, 9);
+    src21 = __lsx_vldrepl_b(src, 10);
+    src20 = __lsx_vldrepl_b(src, 11);
+    src19 = __lsx_vldrepl_b(src, 12);
+    src18 = __lsx_vldrepl_b(src, 13);
+    src17 = __lsx_vldrepl_b(src, 14);
+    src16 = __lsx_vldrepl_b(src, 15);
+    src15 = __lsx_vldrepl_b(src, 16);
+    src14 = __lsx_vldrepl_b(src, 17);
+    src13 = __lsx_vldrepl_b(src, 18);
+    src12 = __lsx_vldrepl_b(src, 19);
+    src11 = __lsx_vldrepl_b(src, 20);
+    src10 = __lsx_vldrepl_b(src, 21);
+    src9  = __lsx_vldrepl_b(src, 22);
+    src8  = __lsx_vldrepl_b(src, 23);
+    src7  = __lsx_vldrepl_b(src, 24);
+    src6  = __lsx_vldrepl_b(src, 25);
+    src5  = __lsx_vldrepl_b(src, 26);
+    src4  = __lsx_vldrepl_b(src, 27);
+    src3  = __lsx_vldrepl_b(src, 28);
+    src2  = __lsx_vldrepl_b(src, 29);
+    src1  = __lsx_vldrepl_b(src, 30);
+    src0  = __lsx_vldrepl_b(src, 31);
+    LSX_ST_8X16(src0, src1, src2, src3, src4, src5, src6, src7,
+                dst, dst_stride);
+    LSX_ST_8X16(src8, src9, src10, src11, src12, src13, src14, src15,
+                dst, dst_stride);
+    LSX_ST_8X16(src16, src17, src18, src19, src20, src21, src22, src23,
+                dst, dst_stride);
+    LSX_ST_8X16(src24, src25, src26, src27, src28, src29, src30, src31,
+                dst, dst_stride);
+}
+
+void ff_dc_4x4_lsx(uint8_t *dst, ptrdiff_t dst_stride, const uint8_t *src_left,
+                   const uint8_t *src_top)
+{
+    __m128i tmp0, tmp1, dst0;
+
+    tmp0 = __lsx_vldrepl_w(src_top, 0);
+    tmp1 = __lsx_vldrepl_w(src_left, 0);
+    dst0 = __lsx_vilvl_w(tmp1, tmp0);
+    dst0 = __lsx_vhaddw_hu_bu(dst0, dst0);
+    dst0 = __lsx_vhaddw_wu_hu(dst0, dst0);
+    dst0 = __lsx_vhaddw_du_wu(dst0, dst0);
+    dst0 = __lsx_vsrari_w(dst0, 3);
+    dst0 = __lsx_vshuf4i_b(dst0, 0);
+    __lsx_vstelm_w(dst0, dst, 0, 0);
+    dst += dst_stride;
+    __lsx_vstelm_w(dst0, dst, 0, 0);
+    dst += dst_stride;
+    __lsx_vstelm_w(dst0, dst, 0, 0);
+    dst += dst_stride;
+    __lsx_vstelm_w(dst0, dst, 0, 0);
+}
+
+#define INTRA_DC_TL_4X4(dir)                                            \
+void ff_dc_##dir##_4x4_lsx(uint8_t *dst, ptrdiff_t dst_stride,          \
+                          const uint8_t *left,                          \
+                          const uint8_t *top)                           \
+{                                                                       \
+    __m128i tmp0, dst0;                                                 \
+                                                                        \
+    tmp0 = __lsx_vldrepl_w(dir, 0);                                     \
+    dst0 = __lsx_vhaddw_hu_bu(tmp0, tmp0);                              \
+    dst0 = __lsx_vhaddw_wu_hu(dst0, dst0);                              \
+    dst0 = __lsx_vsrari_w(dst0, 2);                                     \
+    dst0 = __lsx_vshuf4i_b(dst0, 0);                                    \
+    __lsx_vstelm_w(dst0, dst, 0, 0);                                    \
+    dst += dst_stride;                                                  \
+    __lsx_vstelm_w(dst0, dst, 0, 0);                                    \
+    dst += dst_stride;                                                  \
+    __lsx_vstelm_w(dst0, dst, 0, 0);                                    \
+    dst += dst_stride;                                                  \
+    __lsx_vstelm_w(dst0, dst, 0, 0);                                    \
+}
+INTRA_DC_TL_4X4(top);
+INTRA_DC_TL_4X4(left);
+
+void ff_dc_8x8_lsx(uint8_t *dst, ptrdiff_t dst_stride, const uint8_t *src_left,
+                   const uint8_t *src_top)
+{
+    __m128i tmp0, tmp1, dst0;
+
+    tmp0 = __lsx_vldrepl_d(src_top, 0);
+    tmp1 = __lsx_vldrepl_d(src_left, 0);
+    dst0 = __lsx_vilvl_d(tmp1, tmp0);
+    dst0 = __lsx_vhaddw_hu_bu(dst0, dst0);
+    dst0 = __lsx_vhaddw_wu_hu(dst0, dst0);
+    dst0 = __lsx_vhaddw_du_wu(dst0, dst0);
+    dst0 = __lsx_vhaddw_qu_du(dst0, dst0);
+    dst0 = __lsx_vsrari_w(dst0, 4);
+    dst0 = __lsx_vreplvei_b(dst0, 0);
+    __lsx_vstelm_d(dst0, dst, 0, 0);
+    dst += dst_stride;
+    __lsx_vstelm_d(dst0, dst, 0, 0);
+    dst += dst_stride;
+    __lsx_vstelm_d(dst0, dst, 0, 0);
+    dst += dst_stride;
+    __lsx_vstelm_d(dst0, dst, 0, 0);
+    dst += dst_stride;
+    __lsx_vstelm_d(dst0, dst, 0, 0);
+    dst += dst_stride;
+    __lsx_vstelm_d(dst0, dst, 0, 0);
+    dst += dst_stride;
+    __lsx_vstelm_d(dst0, dst, 0, 0);
+    dst += dst_stride;
+    __lsx_vstelm_d(dst0, dst, 0, 0);
+}
+
+#define INTRA_DC_TL_8X8(dir)                                                  \
+void ff_dc_##dir##_8x8_lsx(uint8_t *dst, ptrdiff_t dst_stride,                \
+                           const uint8_t *left,                               \
+                           const uint8_t *top)                                \
+{                                                                             \
+    __m128i tmp0, dst0;                                                       \
+                                                                              \
+    tmp0 = __lsx_vldrepl_d(dir, 0);                                           \
+    dst0 = __lsx_vhaddw_hu_bu(tmp0, tmp0);                                    \
+    dst0 = __lsx_vhaddw_wu_hu(dst0, dst0);                                    \
+    dst0 = __lsx_vhaddw_du_wu(dst0, dst0);                                    \
+    dst0 = __lsx_vsrari_w(dst0, 3);                                           \
+    dst0 = __lsx_vreplvei_b(dst0, 0);                                         \
+    __lsx_vstelm_d(dst0, dst, 0, 0);                                          \
+    dst += dst_stride;                                                        \
+    __lsx_vstelm_d(dst0, dst, 0, 0);                                          \
+    dst += dst_stride;                                                        \
+    __lsx_vstelm_d(dst0, dst, 0, 0);                                          \
+    dst += dst_stride;                                                        \
+    __lsx_vstelm_d(dst0, dst, 0, 0);                                          \
+    dst += dst_stride;                                                        \
+    __lsx_vstelm_d(dst0, dst, 0, 0);                                          \
+    dst += dst_stride;                                                        \
+    __lsx_vstelm_d(dst0, dst, 0, 0);                                          \
+    dst += dst_stride;                                                        \
+    __lsx_vstelm_d(dst0, dst, 0, 0);                                          \
+    dst += dst_stride;                                                        \
+    __lsx_vstelm_d(dst0, dst, 0, 0);                                          \
+}
+
+INTRA_DC_TL_8X8(top);
+INTRA_DC_TL_8X8(left);
+
+void ff_dc_16x16_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                     const uint8_t *src_left, const uint8_t *src_top)
+{
+    __m128i tmp0, tmp1, dst0;
+    ptrdiff_t stride2 = dst_stride << 1;
+    ptrdiff_t stride3 = stride2 + dst_stride;
+    ptrdiff_t stride4 = stride2 << 1;
+
+    tmp0 = __lsx_vld(src_top, 0);
+    tmp1 = __lsx_vld(src_left, 0);
+    DUP2_ARG2(__lsx_vhaddw_hu_bu, tmp0, tmp0, tmp1, tmp1, tmp0, tmp1);
+    dst0 = __lsx_vadd_h(tmp0, tmp1);
+    dst0 = __lsx_vhaddw_wu_hu(dst0, dst0);
+    dst0 = __lsx_vhaddw_du_wu(dst0, dst0);
+    dst0 = __lsx_vhaddw_qu_du(dst0, dst0);
+    dst0 = __lsx_vsrari_w(dst0, 5);
+    dst0 = __lsx_vreplvei_b(dst0, 0);
+    LSX_ST_8(dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst,
+             dst_stride, stride2, stride3, stride4);
+    dst += stride4;
+    LSX_ST_8(dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst,
+             dst_stride, stride2, stride3, stride4);
+}
+
+#define INTRA_DC_TL_16X16(dir)                                                \
+void ff_dc_##dir##_16x16_lsx(uint8_t *dst, ptrdiff_t dst_stride,              \
+                             const uint8_t *left,                             \
+                             const uint8_t *top)                              \
+{                                                                             \
+    __m128i tmp0, dst0;                                                       \
+    ptrdiff_t stride2 = dst_stride << 1;                                      \
+    ptrdiff_t stride3 = stride2 + dst_stride;                                 \
+    ptrdiff_t stride4 = stride2 << 1;                                         \
+                                                                              \
+    tmp0 = __lsx_vld(dir, 0);                                                 \
+    dst0 = __lsx_vhaddw_hu_bu(tmp0, tmp0);                                    \
+    dst0 = __lsx_vhaddw_wu_hu(dst0, dst0);                                    \
+    dst0 = __lsx_vhaddw_du_wu(dst0, dst0);                                    \
+    dst0 = __lsx_vhaddw_qu_du(dst0, dst0);                                    \
+    dst0 = __lsx_vsrari_w(dst0, 4);                                           \
+    dst0 = __lsx_vreplvei_b(dst0, 0);                                         \
+    LSX_ST_8(dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst,             \
+             dst_stride, stride2, stride3, stride4);                          \
+    dst += stride4;                                                           \
+    LSX_ST_8(dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst,             \
+             dst_stride, stride2, stride3, stride4);                          \
+}
+
+INTRA_DC_TL_16X16(top);
+INTRA_DC_TL_16X16(left);
+
+void ff_dc_32x32_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                     const uint8_t *src_left, const uint8_t *src_top)
+{
+    __m128i tmp0, tmp1, tmp2, tmp3, dst0;
+
+    DUP2_ARG2(__lsx_vld, src_top, 0, src_top, 16, tmp0, tmp1);
+    DUP2_ARG2(__lsx_vld, src_left, 0, src_left, 16, tmp2, tmp3);
+    DUP4_ARG2(__lsx_vhaddw_hu_bu, tmp0, tmp0, tmp1, tmp1, tmp2, tmp2,
+              tmp3, tmp3, tmp0, tmp1, tmp2, tmp3);
+    DUP2_ARG2(__lsx_vadd_h, tmp0, tmp1, tmp2, tmp3, tmp0, tmp1);
+    dst0 = __lsx_vadd_h(tmp0, tmp1);
+    dst0 = __lsx_vhaddw_wu_hu(dst0, dst0);
+    dst0 = __lsx_vhaddw_du_wu(dst0, dst0);
+    dst0 = __lsx_vhaddw_qu_du(dst0, dst0);
+    dst0 = __lsx_vsrari_w(dst0, 6);
+    dst0 = __lsx_vreplvei_b(dst0, 0);
+    LSX_ST_8X16(dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst0,
+                dst, dst_stride);
+    LSX_ST_8X16(dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst0,
+                dst, dst_stride);
+    LSX_ST_8X16(dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst0,
+                dst, dst_stride);
+    LSX_ST_8X16(dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst0,
+                dst, dst_stride);
+}
+
+#define INTRA_DC_TL_32X32(dir)                                               \
+void ff_dc_##dir##_32x32_lsx(uint8_t *dst, ptrdiff_t dst_stride,             \
+                             const uint8_t *left,                            \
+                             const uint8_t *top)                             \
+{                                                                            \
+    __m128i tmp0, tmp1, dst0;                                                \
+                                                                             \
+    DUP2_ARG2(__lsx_vld, dir, 0, dir, 16, tmp0, tmp1);                       \
+    DUP2_ARG2(__lsx_vhaddw_hu_bu, tmp0, tmp0, tmp1, tmp1, tmp0, tmp1);       \
+    dst0 = __lsx_vadd_h(tmp0, tmp1);                                         \
+    dst0 = __lsx_vhaddw_wu_hu(dst0, dst0);                                   \
+    dst0 = __lsx_vhaddw_du_wu(dst0, dst0);                                   \
+    dst0 = __lsx_vhaddw_qu_du(dst0, dst0);                                   \
+    dst0 = __lsx_vsrari_w(dst0, 5);                                          \
+    dst0 = __lsx_vreplvei_b(dst0, 0);                                        \
+    LSX_ST_8X16(dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst0,              \
+                dst, dst_stride);                                            \
+    LSX_ST_8X16(dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst0,              \
+                dst, dst_stride);                                            \
+    LSX_ST_8X16(dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst0,              \
+                dst, dst_stride);                                            \
+    LSX_ST_8X16(dst0, dst0, dst0, dst0, dst0, dst0, dst0, dst0,              \
+                dst, dst_stride);                                            \
+}
+
+INTRA_DC_TL_32X32(top);
+INTRA_DC_TL_32X32(left);
+
+#define INTRA_PREDICT_VALDC_16X16_LSX(val)                             \
+void ff_dc_##val##_16x16_lsx(uint8_t *dst, ptrdiff_t dst_stride,       \
+                             const uint8_t *left, const uint8_t *top)  \
+{                                                                      \
+    __m128i out = __lsx_vldi(val);                                     \
+    ptrdiff_t stride2 = dst_stride << 1;                               \
+    ptrdiff_t stride3 = stride2 + dst_stride;                          \
+    ptrdiff_t stride4 = stride2 << 1;                                  \
+                                                                       \
+    LSX_ST_8(out, out, out, out, out, out, out, out, dst,              \
+             dst_stride, stride2, stride3, stride4);                   \
+    dst += stride4;                                                    \
+    LSX_ST_8(out, out, out, out, out, out, out, out, dst,              \
+             dst_stride, stride2, stride3, stride4);                   \
+}
+
+INTRA_PREDICT_VALDC_16X16_LSX(127);
+INTRA_PREDICT_VALDC_16X16_LSX(128);
+INTRA_PREDICT_VALDC_16X16_LSX(129);
+
+#define INTRA_PREDICT_VALDC_32X32_LSX(val)                               \
+void ff_dc_##val##_32x32_lsx(uint8_t *dst, ptrdiff_t dst_stride,         \
+                             const uint8_t *left, const uint8_t *top)    \
+{                                                                        \
+    __m128i out = __lsx_vldi(val);                                       \
+                                                                         \
+    LSX_ST_8X16(out, out, out, out, out, out, out, out, dst, dst_stride);\
+    LSX_ST_8X16(out, out, out, out, out, out, out, out, dst, dst_stride);\
+    LSX_ST_8X16(out, out, out, out, out, out, out, out, dst, dst_stride);\
+    LSX_ST_8X16(out, out, out, out, out, out, out, out, dst, dst_stride);\
+}
+
+INTRA_PREDICT_VALDC_32X32_LSX(127);
+INTRA_PREDICT_VALDC_32X32_LSX(128);
+INTRA_PREDICT_VALDC_32X32_LSX(129);
+
+void ff_tm_4x4_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                   const uint8_t *src_left, const uint8_t *src_top_ptr)
+{
+    uint8_t top_left = src_top_ptr[-1];
+    __m128i tmp0, tmp1, tmp2, tmp3, reg0, reg1;
+    __m128i src0, src1, src2, src3;
+    __m128i dst0, dst1, dst2, dst3;
+
+    reg0 = __lsx_vreplgr2vr_h(top_left);
+    reg1 = __lsx_vld(src_top_ptr, 0);
+    DUP4_ARG2(__lsx_vldrepl_b, src_left, 0, src_left, 1, src_left, 2, src_left,
+              3, tmp3, tmp2, tmp1, tmp0);
+    DUP4_ARG2(__lsx_vilvl_b, tmp0, reg1, tmp1, reg1, tmp2, reg1, tmp3, reg1,
+              src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vhaddw_hu_bu, src0, src0, src1, src1, src2, src2, src3,
+              src3, dst0, dst1, dst2, dst3);
+    DUP4_ARG2(__lsx_vssub_hu, dst0, reg0, dst1, reg0, dst2, reg0, dst3, reg0,
+              dst0, dst1, dst2, dst3);
+    DUP4_ARG2(__lsx_vsat_hu, dst0, 7, dst1, 7, dst2, 7, dst3, 7,
+              dst0, dst1, dst2, dst3);
+    DUP2_ARG2(__lsx_vpickev_b, dst1, dst0, dst3, dst2, dst0, dst1);
+    __lsx_vstelm_w(dst0, dst, 0, 0);
+    dst += dst_stride;
+    __lsx_vstelm_w(dst0, dst, 0, 2);
+    dst += dst_stride;
+    __lsx_vstelm_w(dst1, dst, 0, 0);
+    dst += dst_stride;
+    __lsx_vstelm_w(dst1, dst, 0, 2);
+}
+
+void ff_tm_8x8_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                   const uint8_t *src_left, const uint8_t *src_top_ptr)
+{
+    uint8_t top_left = src_top_ptr[-1];
+    __m128i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m128i reg0, reg1;
+
+    reg0 = __lsx_vreplgr2vr_h(top_left);
+    reg1 = __lsx_vld(src_top_ptr, 0);
+    DUP4_ARG2(__lsx_vldrepl_b, src_left, 0, src_left, 1, src_left, 2, src_left,
+              3, tmp7, tmp6, tmp5, tmp4);
+    DUP4_ARG2(__lsx_vldrepl_b, src_left, 4, src_left, 5, src_left, 6, src_left,
+              7, tmp3, tmp2, tmp1, tmp0);
+    DUP4_ARG2(__lsx_vilvl_b, tmp0, reg1, tmp1, reg1, tmp2, reg1, tmp3, reg1,
+              src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vilvl_b, tmp4, reg1, tmp5, reg1, tmp6, reg1, tmp7, reg1,
+              src4, src5, src6, src7);
+    DUP4_ARG2(__lsx_vhaddw_hu_bu, src0, src0, src1, src1, src2, src2, src3,
+              src3, src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vhaddw_hu_bu, src4, src4, src5, src5, src6, src6, src7,
+              src7, src4, src5, src6, src7);
+    DUP4_ARG2(__lsx_vssub_hu, src0, reg0, src1, reg0, src2, reg0, src3, reg0,
+              src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vssub_hu, src4, reg0, src5, reg0, src6, reg0, src7, reg0,
+              src4, src5, src6, src7);
+    DUP4_ARG2(__lsx_vsat_hu, src0, 7, src1, 7, src2, 7, src3, 7,
+              src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vsat_hu, src4, 7, src5, 7, src6, 7, src7, 7,
+              src4, src5, src6, src7);
+    DUP4_ARG2(__lsx_vpickev_b, src1, src0, src3, src2, src5, src4, src7, src6,
+              src0, src1, src2, src3);
+    __lsx_vstelm_d(src0, dst, 0, 0);
+    dst += dst_stride;
+    __lsx_vstelm_d(src0, dst, 0, 1);
+    dst += dst_stride;
+    __lsx_vstelm_d(src1, dst, 0, 0);
+    dst += dst_stride;
+    __lsx_vstelm_d(src1, dst, 0, 1);
+    dst += dst_stride;
+    __lsx_vstelm_d(src2, dst, 0, 0);
+    dst += dst_stride;
+    __lsx_vstelm_d(src2, dst, 0, 1);
+    dst += dst_stride;
+    __lsx_vstelm_d(src3, dst, 0, 0);
+    dst += dst_stride;
+    __lsx_vstelm_d(src3, dst, 0, 1);
+}
+
+void ff_tm_16x16_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                     const uint8_t *src_left, const uint8_t *src_top_ptr)
+{
+    uint8_t top_left = src_top_ptr[-1];
+    __m128i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
+    __m128i tmp8, tmp9, tmp10, tmp11, tmp12, tmp13, tmp14, tmp15;
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m128i reg0, reg1;
+    ptrdiff_t stride2 = dst_stride << 1;
+    ptrdiff_t stride3 = stride2 + dst_stride;
+    ptrdiff_t stride4 = stride2 << 1;
+
+    reg0 = __lsx_vreplgr2vr_h(top_left);
+    reg1 = __lsx_vld(src_top_ptr, 0);
+    DUP4_ARG2(__lsx_vldrepl_b, src_left, 0, src_left, 1, src_left, 2, src_left,
+              3, tmp15, tmp14, tmp13, tmp12);
+    DUP4_ARG2(__lsx_vldrepl_b, src_left, 4, src_left, 5, src_left, 6, src_left,
+              7, tmp11, tmp10, tmp9, tmp8);
+    DUP4_ARG2(__lsx_vldrepl_b, src_left, 8, src_left, 9, src_left, 10,
+              src_left, 11, tmp7, tmp6, tmp5, tmp4);
+    DUP4_ARG2(__lsx_vldrepl_b, src_left, 12, src_left, 13, src_left, 14,
+              src_left, 15, tmp3, tmp2, tmp1, tmp0);
+    DUP4_ARG2(__lsx_vaddwev_h_bu, tmp0, reg1, tmp1, reg1, tmp2, reg1, tmp3,
+              reg1, src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vaddwod_h_bu, tmp0, reg1, tmp1, reg1, tmp2, reg1, tmp3,
+              reg1, src4, src5, src6, src7);
+    DUP4_ARG2(__lsx_vssub_hu, src0, reg0, src1, reg0, src2, reg0, src3, reg0,
+              src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vssub_hu, src4, reg0, src5, reg0, src6, reg0, src7, reg0,
+              src4, src5, src6, src7);
+    DUP4_ARG2(__lsx_vsat_hu, src0, 7, src1, 7, src2, 7, src3, 7,
+              src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vsat_hu, src4, 7, src5, 7, src6, 7, src7, 7,
+              src4, src5, src6, src7);
+    DUP4_ARG2(__lsx_vpackev_b, src4, src0, src5, src1, src6, src2, src7, src3,
+              tmp0, tmp1, tmp2, tmp3);
+    DUP4_ARG2(__lsx_vaddwev_h_bu, tmp4, reg1, tmp5, reg1, tmp6, reg1, tmp7,
+              reg1, src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vaddwod_h_bu, tmp4, reg1, tmp5, reg1, tmp6, reg1, tmp7,
+              reg1, src4, src5, src6, src7);
+    DUP4_ARG2(__lsx_vssub_hu, src0, reg0, src1, reg0, src2, reg0, src3, reg0,
+              src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vssub_hu, src4, reg0, src5, reg0, src6, reg0, src7, reg0,
+              src4, src5, src6, src7);
+    DUP4_ARG2(__lsx_vsat_hu, src0, 7, src1, 7, src2, 7, src3, 7,
+              src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vsat_hu, src4, 7, src5, 7, src6, 7, src7, 7,
+              src4, src5, src6, src7);
+    DUP4_ARG2(__lsx_vpackev_b, src4, src0, src5, src1, src6, src2, src7, src3,
+              tmp4, tmp5, tmp6, tmp7);
+    DUP4_ARG2(__lsx_vaddwev_h_bu, tmp8, reg1, tmp9, reg1, tmp10, reg1, tmp11,
+              reg1, src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vaddwod_h_bu, tmp8, reg1, tmp9, reg1, tmp10, reg1, tmp11,
+              reg1, src4, src5, src6, src7);
+    DUP4_ARG2(__lsx_vssub_hu, src0, reg0, src1, reg0, src2, reg0, src3, reg0,
+              src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vssub_hu, src4, reg0, src5, reg0, src6, reg0, src7, reg0,
+              src4, src5, src6, src7);
+    DUP4_ARG2(__lsx_vsat_hu, src0, 7, src1, 7, src2, 7, src3, 7,
+              src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vsat_hu, src4, 7, src5, 7, src6, 7, src7, 7,
+              src4, src5, src6, src7);
+    DUP4_ARG2(__lsx_vpackev_b, src4, src0, src5, src1, src6, src2, src7, src3,
+              tmp8, tmp9, tmp10, tmp11);
+    DUP4_ARG2(__lsx_vaddwev_h_bu, tmp12, reg1, tmp13, reg1, tmp14, reg1,
+              tmp15, reg1, src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vaddwod_h_bu, tmp12, reg1, tmp13, reg1, tmp14, reg1,
+              tmp15, reg1, src4, src5, src6, src7);
+    DUP4_ARG2(__lsx_vssub_hu, src0, reg0, src1, reg0, src2, reg0, src3, reg0,
+              src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vssub_hu, src4, reg0, src5, reg0, src6, reg0, src7, reg0,
+              src4, src5, src6, src7);
+    DUP4_ARG2(__lsx_vsat_hu, src0, 7, src1, 7, src2, 7, src3, 7,
+              src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vsat_hu, src4, 7, src5, 7, src6, 7, src7, 7,
+              src4, src5, src6, src7);
+    DUP4_ARG2(__lsx_vpackev_b, src4, src0, src5, src1, src6, src2, src7, src3,
+              tmp12, tmp13, tmp14, tmp15);
+    LSX_ST_8(tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7, dst,
+             dst_stride, stride2, stride3, stride4);
+    dst += stride4;
+    LSX_ST_8(tmp8, tmp9, tmp10, tmp11, tmp12, tmp13, tmp14, tmp15, dst,
+             dst_stride, stride2, stride3, stride4);
+}
+
+void ff_tm_32x32_lsx(uint8_t *dst, ptrdiff_t dst_stride,
+                     const uint8_t *src_left, const uint8_t *src_top_ptr)
+{
+    uint8_t top_left = src_top_ptr[-1];
+    uint32_t loop_cnt;
+    __m128i tmp0, tmp1, tmp2, tmp3, reg0, reg1, reg2;
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m128i dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+
+    reg0 = __lsx_vreplgr2vr_h(top_left);
+    DUP2_ARG2(__lsx_vld, src_top_ptr, 0, src_top_ptr, 16, reg1, reg2);
+
+    src_left += 28;
+    for (loop_cnt = 8; loop_cnt--;) {
+        DUP4_ARG2(__lsx_vldrepl_b, src_left, 0, src_left, 1, src_left, 2,
+                  src_left, 3, tmp3, tmp2, tmp1, tmp0);
+        src_left -= 4;
+        DUP4_ARG2(__lsx_vaddwev_h_bu, tmp0, reg1, tmp1, reg1, tmp2, reg1,
+                  tmp3, reg1, src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vaddwod_h_bu, tmp0, reg1, tmp1, reg1, tmp2, reg1,
+                  tmp3, reg1, src4, src5, src6, src7);
+        DUP4_ARG2(__lsx_vssub_hu, src0, reg0, src1, reg0, src2, reg0, src3,
+                  reg0, src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vssub_hu, src4, reg0, src5, reg0, src6, reg0, src7,
+                  reg0, src4, src5, src6, src7);
+        DUP4_ARG2(__lsx_vaddwev_h_bu, tmp0, reg2, tmp1, reg2, tmp2, reg2,
+                  tmp3, reg2, dst0, dst1, dst2, dst3);
+        DUP4_ARG2(__lsx_vaddwod_h_bu, tmp0, reg2, tmp1, reg2, tmp2, reg2,
+                  tmp3, reg2, dst4, dst5, dst6, dst7);
+        DUP4_ARG2(__lsx_vssub_hu, dst0, reg0, dst1, reg0, dst2, reg0, dst3,
+                  reg0, dst0, dst1, dst2, dst3);
+        DUP4_ARG2(__lsx_vssub_hu, dst4, reg0, dst5, reg0, dst6, reg0, dst7,
+                  reg0, dst4, dst5, dst6, dst7);
+        DUP4_ARG2(__lsx_vsat_hu, src0, 7, src1, 7, src2, 7, src3, 7,
+                  src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vsat_hu, src4, 7, src5, 7, src6, 7, src7, 7,
+                  src4, src5, src6, src7);
+        DUP4_ARG2(__lsx_vsat_hu, dst0, 7, dst1, 7, dst2, 7, dst3, 7,
+                  dst0, dst1, dst2, dst3);
+        DUP4_ARG2(__lsx_vsat_hu, dst4, 7, dst5, 7, dst6, 7, dst7, 7,
+                  dst4, dst5, dst6, dst7);
+        DUP4_ARG2(__lsx_vpackev_b, src4, src0, src5, src1, src6, src2, src7,
+                  src3, src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vpackev_b, dst4, dst0, dst5, dst1, dst6, dst2, dst7,
+                  dst3, dst0, dst1, dst2, dst3);
+        __lsx_vst(src0, dst, 0);
+        __lsx_vst(dst0, dst, 16);
+        dst += dst_stride;
+        __lsx_vst(src1, dst, 0);
+        __lsx_vst(dst1, dst, 16);
+        dst += dst_stride;
+        __lsx_vst(src2, dst, 0);
+        __lsx_vst(dst2, dst, 16);
+        dst += dst_stride;
+        __lsx_vst(src3, dst, 0);
+        __lsx_vst(dst3, dst, 16);
+        dst += dst_stride;
+    }
+}
diff --git a/libavcodec/loongarch/vp9_lpf_lsx.c b/libavcodec/loongarch/vp9_lpf_lsx.c
new file mode 100644
index 0000000000..8e1915b888
--- /dev/null
+++ b/libavcodec/loongarch/vp9_lpf_lsx.c
@@ -0,0 +1,3141 @@
+/*
+ * Copyright (c) 2021 Loongson Technology Corporation Limited
+ * Contributed by Jin Bo <jinbo@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "libavcodec/vp9dsp.h"
+#include "libavutil/loongarch/loongson_intrinsics.h"
+#include "libavutil/common.h"
+#include "vp9dsp_loongarch.h"
+
+#define LSX_LD_8(_src, _stride, _stride2, _stride3, _stride4, _in0, _in1, _in2, \
+                 _in3, _in4, _in5, _in6, _in7)                                  \
+{                                                                               \
+    _in0 = __lsx_vld(_src, 0);                                                  \
+    _in1 = __lsx_vldx(_src, _stride);                                           \
+    _in2 = __lsx_vldx(_src, _stride2);                                          \
+    _in3 = __lsx_vldx(_src, _stride3);                                          \
+    _src += _stride4;                                                           \
+    _in4 = __lsx_vld(_src, 0);                                                  \
+    _in5 = __lsx_vldx(_src, _stride);                                           \
+    _in6 = __lsx_vldx(_src, _stride2);                                          \
+    _in7 = __lsx_vldx(_src, _stride3);                                          \
+}
+
+#define LSX_ST_8(_dst0, _dst1, _dst2, _dst3, _dst4, _dst5, _dst6, _dst7,        \
+                 _dst, _stride, _stride2, _stride3, _stride4)                   \
+{                                                                               \
+    __lsx_vst(_dst0, _dst, 0);                                                  \
+    __lsx_vstx(_dst1, _dst, _stride);                                           \
+    __lsx_vstx(_dst2, _dst, _stride2);                                          \
+    __lsx_vstx(_dst3, _dst, _stride3);                                          \
+    _dst += _stride4;                                                           \
+    __lsx_vst(_dst4, _dst, 0);                                                  \
+    __lsx_vstx(_dst5, _dst, _stride);                                           \
+    __lsx_vstx(_dst6, _dst, _stride2);                                          \
+    __lsx_vstx(_dst7, _dst, _stride3);                                          \
+}
+
+#define VP9_LPF_FILTER4_4W(p1_src, p0_src, q0_src, q1_src, mask_src, hev_src, \
+                           p1_dst, p0_dst, q0_dst, q1_dst)                    \
+{                                                                             \
+    __m128i p1_tmp, p0_tmp, q0_tmp, q1_tmp, q0_sub_p0, filt, filt1, filt2;    \
+    const __m128i cnst3b = __lsx_vldi(3);                                     \
+    const __m128i cnst4b = __lsx_vldi(4);                                     \
+                                                                              \
+    p1_tmp = __lsx_vxori_b(p1_src, 0x80);                                     \
+    p0_tmp = __lsx_vxori_b(p0_src, 0x80);                                     \
+    q0_tmp = __lsx_vxori_b(q0_src, 0x80);                                     \
+    q1_tmp = __lsx_vxori_b(q1_src, 0x80);                                     \
+                                                                              \
+    filt = __lsx_vssub_b(p1_tmp, q1_tmp);                                     \
+                                                                              \
+    filt = filt & hev_src;                                                    \
+                                                                              \
+    q0_sub_p0 = __lsx_vssub_b(q0_tmp, p0_tmp);                                \
+    filt = __lsx_vsadd_b(filt, q0_sub_p0);                                    \
+    filt = __lsx_vsadd_b(filt, q0_sub_p0);                                    \
+    filt = __lsx_vsadd_b(filt, q0_sub_p0);                                    \
+    filt = filt & mask_src;                                                   \
+                                                                              \
+    filt1 = __lsx_vsadd_b(filt, cnst4b);                                      \
+    filt1 = __lsx_vsrai_b(filt1, 3);                                          \
+                                                                              \
+    filt2 = __lsx_vsadd_b(filt, cnst3b);                                      \
+    filt2 = __lsx_vsrai_b(filt2, 3);                                          \
+                                                                              \
+    q0_tmp = __lsx_vssub_b(q0_tmp, filt1);                                    \
+    q0_dst = __lsx_vxori_b(q0_tmp, 0x80);                                     \
+    p0_tmp = __lsx_vsadd_b(p0_tmp, filt2);                                    \
+    p0_dst = __lsx_vxori_b(p0_tmp, 0x80);                                     \
+                                                                              \
+    filt = __lsx_vsrari_b(filt1, 1);                                          \
+    hev_src = __lsx_vxori_b(hev_src, 0xff);                                   \
+    filt = filt & hev_src;                                                    \
+                                                                              \
+    q1_tmp = __lsx_vssub_b(q1_tmp, filt);                                     \
+    q1_dst = __lsx_vxori_b(q1_tmp, 0x80);                                     \
+    p1_tmp = __lsx_vsadd_b(p1_tmp, filt);                                     \
+    p1_dst = __lsx_vxori_b(p1_tmp, 0x80);                                     \
+}
+
+#define VP9_FLAT4(p3_src, p2_src, p0_src, q0_src, q2_src, q3_src, flat_dst)  \
+{                                                                            \
+    __m128i f_tmp = __lsx_vldi(1);                                           \
+    __m128i p2_a_sub_p0, q2_a_sub_q0, p3_a_sub_p0, q3_a_sub_q0;              \
+                                                                             \
+    p2_a_sub_p0 = __lsx_vabsd_bu(p2_src, p0_src);                            \
+    q2_a_sub_q0 = __lsx_vabsd_bu(q2_src, q0_src);                            \
+    p3_a_sub_p0 = __lsx_vabsd_bu(p3_src, p0_src);                            \
+    q3_a_sub_q0 = __lsx_vabsd_bu(q3_src, q0_src);                            \
+                                                                             \
+    p2_a_sub_p0 = __lsx_vmax_bu(p2_a_sub_p0, q2_a_sub_q0);                   \
+    flat_dst = __lsx_vmax_bu(p2_a_sub_p0, flat_dst);                         \
+    p3_a_sub_p0 = __lsx_vmax_bu(p3_a_sub_p0, q3_a_sub_q0);                   \
+    flat_dst = __lsx_vmax_bu(p3_a_sub_p0, flat_dst);                         \
+                                                                             \
+    flat_dst = __lsx_vslt_bu(f_tmp, flat_dst);                               \
+    flat_dst = __lsx_vxori_b(flat_dst, 0xff);                                \
+    flat_dst = flat_dst & mask;                                              \
+}
+
+#define VP9_FLAT5(p7_src, p6_src, p5_src, p4_src, p0_src, q0_src, q4_src, \
+                  q5_src, q6_src, q7_src, flat_src, flat2_dst)            \
+{                                                                         \
+    __m128i f_tmp = __lsx_vldi(1);                                        \
+    __m128i p4_a_sub_p0, q4_a_sub_q0, p5_a_sub_p0, q5_a_sub_q0;           \
+    __m128i p6_a_sub_p0, q6_a_sub_q0, p7_a_sub_p0, q7_a_sub_q0;           \
+                                                                          \
+    p4_a_sub_p0 = __lsx_vabsd_bu(p4_src, p0_src);                         \
+    q4_a_sub_q0 = __lsx_vabsd_bu(q4_src, q0_src);                         \
+    p5_a_sub_p0 = __lsx_vabsd_bu(p5_src, p0_src);                         \
+    q5_a_sub_q0 = __lsx_vabsd_bu(q5_src, q0_src);                         \
+    p6_a_sub_p0 = __lsx_vabsd_bu(p6_src, p0_src);                         \
+    q6_a_sub_q0 = __lsx_vabsd_bu(q6_src, q0_src);                         \
+    p7_a_sub_p0 = __lsx_vabsd_bu(p7_src, p0_src);                         \
+    q7_a_sub_q0 = __lsx_vabsd_bu(q7_src, q0_src);                         \
+                                                                          \
+    p4_a_sub_p0 = __lsx_vmax_bu(p4_a_sub_p0, q4_a_sub_q0);                \
+    flat2_dst = __lsx_vmax_bu(p5_a_sub_p0, q5_a_sub_q0);                  \
+    flat2_dst = __lsx_vmax_bu(p4_a_sub_p0, flat2_dst);                    \
+    p6_a_sub_p0 = __lsx_vmax_bu(p6_a_sub_p0, q6_a_sub_q0);                \
+    flat2_dst = __lsx_vmax_bu(p6_a_sub_p0, flat2_dst);                    \
+    p7_a_sub_p0 = __lsx_vmax_bu(p7_a_sub_p0, q7_a_sub_q0);                \
+    flat2_dst = __lsx_vmax_bu(p7_a_sub_p0, flat2_dst);                    \
+                                                                          \
+    flat2_dst = __lsx_vslt_bu(f_tmp, flat2_dst);                          \
+    flat2_dst = __lsx_vxori_b(flat2_dst, 0xff);                           \
+    flat2_dst = flat2_dst & flat_src;                                     \
+}
+
+#define VP9_FILTER8(p3_src, p2_src, p1_src, p0_src,            \
+                    q0_src, q1_src, q2_src, q3_src,            \
+                    p2_filt8_dst, p1_filt8_dst, p0_filt8_dst,  \
+                    q0_filt8_dst, q1_filt8_dst, q2_filt8_dst)  \
+{                                                              \
+    __m128i tmp0, tmp1, tmp2;                                  \
+                                                               \
+    tmp2 = __lsx_vadd_h(p2_src, p1_src);                       \
+    tmp2 = __lsx_vadd_h(tmp2, p0_src);                         \
+    tmp0 = __lsx_vslli_h(p3_src, 1);                           \
+                                                               \
+    tmp0 = __lsx_vadd_h(tmp0, tmp2);                           \
+    tmp0 = __lsx_vadd_h(tmp0, q0_src);                         \
+    tmp1 = __lsx_vadd_h(tmp0, p3_src);                         \
+    tmp1 = __lsx_vadd_h(tmp1, p2_src);                         \
+    p2_filt8_dst = __lsx_vsrari_h(tmp1, 3);                    \
+                                                               \
+    tmp1 = __lsx_vadd_h(tmp0, p1_src);                         \
+    tmp1 = __lsx_vadd_h(tmp1, q1_src);                         \
+    p1_filt8_dst = __lsx_vsrari_h(tmp1, 3);                    \
+                                                               \
+    tmp1 = __lsx_vadd_h(q2_src, q1_src);                       \
+    tmp1 = __lsx_vadd_h(tmp1, q0_src);                         \
+    tmp2 = __lsx_vadd_h(tmp2, tmp1);                           \
+    tmp0 = __lsx_vadd_h(tmp2, p0_src);                         \
+    tmp0 = __lsx_vadd_h(tmp0, p3_src);                         \
+    p0_filt8_dst = __lsx_vsrari_h(tmp0, 3);                    \
+                                                               \
+    tmp0 = __lsx_vadd_h(q2_src, q3_src);                       \
+    tmp0 = __lsx_vadd_h(tmp0, p0_src);                         \
+    tmp0 = __lsx_vadd_h(tmp0, tmp1);                           \
+    tmp1 = __lsx_vadd_h(q3_src, q3_src);                       \
+    tmp1 = __lsx_vadd_h(tmp1, tmp0);                           \
+    q2_filt8_dst = __lsx_vsrari_h(tmp1, 3);                    \
+                                                               \
+    tmp0 = __lsx_vadd_h(tmp2, q3_src);                         \
+    tmp1 = __lsx_vadd_h(tmp0, q0_src);                         \
+    q0_filt8_dst = __lsx_vsrari_h(tmp1, 3);                    \
+                                                               \
+    tmp1 = __lsx_vsub_h(tmp0, p2_src);                         \
+    tmp0 = __lsx_vadd_h(q1_src, q3_src);                       \
+    tmp1 = __lsx_vadd_h(tmp0, tmp1);                           \
+    q1_filt8_dst = __lsx_vsrari_h(tmp1, 3);                    \
+}
+
+#define LPF_MASK_HEV(p3_src, p2_src, p1_src, p0_src, q0_src, q1_src,        \
+                     q2_src, q3_src, limit_src, b_limit_src, thresh_src,    \
+                     hev_dst, mask_dst, flat_dst)                           \
+{                                                                           \
+    __m128i p3_asub_p2_tmp, p2_asub_p1_tmp, p1_asub_p0_tmp, q1_asub_q0_tmp; \
+    __m128i p1_asub_q1_tmp, p0_asub_q0_tmp, q3_asub_q2_tmp, q2_asub_q1_tmp; \
+                                                                            \
+    /* absolute subtraction of pixel values */                              \
+    p3_asub_p2_tmp = __lsx_vabsd_bu(p3_src, p2_src);                        \
+    p2_asub_p1_tmp = __lsx_vabsd_bu(p2_src, p1_src);                        \
+    p1_asub_p0_tmp = __lsx_vabsd_bu(p1_src, p0_src);                        \
+    q1_asub_q0_tmp = __lsx_vabsd_bu(q1_src, q0_src);                        \
+    q2_asub_q1_tmp = __lsx_vabsd_bu(q2_src, q1_src);                        \
+    q3_asub_q2_tmp = __lsx_vabsd_bu(q3_src, q2_src);                        \
+    p0_asub_q0_tmp = __lsx_vabsd_bu(p0_src, q0_src);                        \
+    p1_asub_q1_tmp = __lsx_vabsd_bu(p1_src, q1_src);                        \
+                                                                            \
+    /* calculation of hev */                                                \
+    flat_dst = __lsx_vmax_bu(p1_asub_p0_tmp, q1_asub_q0_tmp);               \
+    hev_dst = __lsx_vslt_bu(thresh_src, flat_dst);                          \
+                                                                            \
+    /* calculation of mask */                                               \
+    p0_asub_q0_tmp = __lsx_vsadd_bu(p0_asub_q0_tmp, p0_asub_q0_tmp);        \
+    p1_asub_q1_tmp = __lsx_vsrli_b(p1_asub_q1_tmp, 1);                      \
+    p0_asub_q0_tmp = __lsx_vsadd_bu(p0_asub_q0_tmp, p1_asub_q1_tmp);        \
+                                                                            \
+    mask_dst = __lsx_vslt_bu(b_limit_src, p0_asub_q0_tmp);                  \
+    mask_dst = __lsx_vmax_bu(flat_dst, mask_dst);                           \
+    p3_asub_p2_tmp = __lsx_vmax_bu(p3_asub_p2_tmp, p2_asub_p1_tmp);         \
+    mask_dst = __lsx_vmax_bu(p3_asub_p2_tmp, mask_dst);                     \
+    q2_asub_q1_tmp = __lsx_vmax_bu(q2_asub_q1_tmp, q3_asub_q2_tmp);         \
+    mask_dst = __lsx_vmax_bu(q2_asub_q1_tmp, mask_dst);                     \
+                                                                            \
+    mask_dst = __lsx_vslt_bu(limit_src, mask_dst);                          \
+    mask_dst = __lsx_vxori_b(mask_dst, 0xff);                               \
+}
+
+void ff_loop_filter_v_4_8_lsx(uint8_t *dst, ptrdiff_t stride,
+                              int32_t b_limit_ptr,
+                              int32_t limit_ptr,
+                              int32_t thresh_ptr)
+{
+    ptrdiff_t stride2 = stride << 1;
+    ptrdiff_t stride3 = stride2 + stride;
+    ptrdiff_t stride4 = stride2 << 1;
+    __m128i mask, hev, flat, thresh, b_limit, limit;
+    __m128i p3, p2, p1, p0, q3, q2, q1, q0, p1_out, p0_out, q0_out, q1_out;
+
+    DUP4_ARG2(__lsx_vldx, dst, -stride4, dst, -stride3, dst, -stride2,
+              dst, -stride, p3, p2, p1, p0);
+    q0 = __lsx_vld(dst, 0);
+    DUP2_ARG2(__lsx_vldx, dst, stride, dst, stride2, q1, q2);
+    q3 = __lsx_vldx(dst, stride3);
+
+    thresh  = __lsx_vreplgr2vr_b(thresh_ptr);
+    b_limit = __lsx_vreplgr2vr_b(b_limit_ptr);
+    limit   = __lsx_vreplgr2vr_b(limit_ptr);
+
+    LPF_MASK_HEV(p3, p2, p1, p0, q0, q1, q2, q3, limit, b_limit, thresh,
+                 hev, mask, flat);
+
+    VP9_LPF_FILTER4_4W(p1, p0, q0, q1, mask, hev, p1_out, p0_out, q0_out,
+                       q1_out);
+
+    __lsx_vstelm_d(p1_out, dst - stride2, 0, 0);
+    __lsx_vstelm_d(p0_out, dst -  stride, 0, 0);
+    __lsx_vstelm_d(q0_out, dst          , 0, 0);
+    __lsx_vstelm_d(q1_out, dst +  stride, 0, 0);
+}
+
+void ff_loop_filter_v_44_16_lsx(uint8_t *dst, ptrdiff_t stride,
+                                int32_t b_limit_ptr,
+                                int32_t limit_ptr,
+                                int32_t thresh_ptr)
+{
+    ptrdiff_t stride2 = stride << 1;
+    ptrdiff_t stride3 = stride2 + stride;
+    ptrdiff_t stride4 = stride2 << 1;
+    __m128i mask, hev, flat, thresh0, b_limit0;
+    __m128i limit0, thresh1, b_limit1, limit1;
+    __m128i p3, p2, p1, p0, q3, q2, q1, q0;
+
+    DUP4_ARG2(__lsx_vldx, dst, -stride4, dst, -stride3, dst, -stride2,
+              dst, -stride, p3, p2, p1, p0);
+    q0 = __lsx_vld(dst, 0);
+    DUP2_ARG2(__lsx_vldx, dst, stride, dst, stride2, q1, q2);
+    q3 = __lsx_vldx(dst, stride3);
+
+    thresh0 = __lsx_vreplgr2vr_b(thresh_ptr);
+    thresh1 = __lsx_vreplgr2vr_b(thresh_ptr >> 8);
+    thresh0 = __lsx_vilvl_d(thresh1, thresh0);
+
+    b_limit0 = __lsx_vreplgr2vr_b(b_limit_ptr);
+    b_limit1 = __lsx_vreplgr2vr_b(b_limit_ptr >> 8);
+    b_limit0 = __lsx_vilvl_d(b_limit1, b_limit0);
+
+    limit0 = __lsx_vreplgr2vr_b(limit_ptr);
+    limit1 = __lsx_vreplgr2vr_b(limit_ptr >> 8);
+    limit0 = __lsx_vilvl_d(limit1, limit0);
+
+    LPF_MASK_HEV(p3, p2, p1, p0, q0, q1, q2, q3, limit0, b_limit0, thresh0,
+                 hev, mask, flat);
+    VP9_LPF_FILTER4_4W(p1, p0, q0, q1, mask, hev, p1, p0, q0, q1);
+
+    __lsx_vst(p1, dst - stride2, 0);
+    __lsx_vst(p0, dst -  stride, 0);
+    __lsx_vst(q0, dst          , 0);
+    __lsx_vst(q1, dst +  stride, 0);
+}
+
+void ff_loop_filter_v_8_8_lsx(uint8_t *dst, ptrdiff_t stride,
+                              int32_t b_limit_ptr,
+                              int32_t limit_ptr,
+                              int32_t thresh_ptr)
+{
+    ptrdiff_t stride2 = stride << 1;
+    ptrdiff_t stride3 = stride2 + stride;
+    ptrdiff_t stride4 = stride2 << 1;
+    __m128i mask, hev, flat, thresh, b_limit, limit;
+    __m128i p3, p2, p1, p0, q3, q2, q1, q0;
+    __m128i p2_out, p1_out, p0_out, q0_out, q1_out, q2_out;
+    __m128i p2_filter8, p1_filter8, p0_filter8;
+    __m128i q0_filter8, q1_filter8, q2_filter8;
+    __m128i p3_l, p2_l, p1_l, p0_l, q3_l, q2_l, q1_l, q0_l;
+    __m128i zero = __lsx_vldi(0);
+
+    DUP4_ARG2(__lsx_vldx, dst, -stride4, dst, -stride3, dst, -stride2,
+              dst, -stride, p3, p2, p1, p0);
+    q0 = __lsx_vld(dst, 0);
+    DUP2_ARG2(__lsx_vldx, dst, stride, dst, stride2, q1, q2);
+    q3 = __lsx_vldx(dst, stride3);
+
+    thresh  = __lsx_vreplgr2vr_b(thresh_ptr);
+    b_limit = __lsx_vreplgr2vr_b(b_limit_ptr);
+    limit   = __lsx_vreplgr2vr_b(limit_ptr);
+
+    LPF_MASK_HEV(p3, p2, p1, p0, q0, q1, q2, q3, limit, b_limit, thresh,
+                 hev, mask, flat);
+    VP9_FLAT4(p3, p2, p0, q0, q2, q3, flat);
+    VP9_LPF_FILTER4_4W(p1, p0, q0, q1, mask, hev, p1_out, p0_out, q0_out,
+                       q1_out);
+
+    flat = __lsx_vilvl_d(zero, flat);
+
+    /* if flat is zero for all pixels, then no need to calculate other filter */
+    if (__lsx_bz_v(flat)) {
+        __lsx_vstelm_d(p1_out, dst - stride2, 0, 0);
+        __lsx_vstelm_d(p0_out, dst -  stride, 0, 0);
+        __lsx_vstelm_d(q0_out, dst          , 0, 0);
+        __lsx_vstelm_d(q1_out, dst +  stride, 0, 0);
+    } else {
+        DUP4_ARG2(__lsx_vilvl_b, zero, p3, zero, p2, zero, p1, zero, p0,
+                  p3_l, p2_l, p1_l, p0_l);
+        DUP4_ARG2(__lsx_vilvl_b, zero, q0, zero, q1, zero, q2, zero, q3,
+                  q0_l, q1_l, q2_l, q3_l);
+        VP9_FILTER8(p3_l, p2_l, p1_l, p0_l, q0_l, q1_l, q2_l, q3_l, p2_filter8,
+                    p1_filter8, p0_filter8, q0_filter8, q1_filter8, q2_filter8);
+
+        /* convert 16 bit output data into 8 bit */
+        DUP4_ARG2(__lsx_vpickev_b, zero, p2_filter8, zero, p1_filter8,
+                  zero, p0_filter8, zero, q0_filter8, p2_filter8,
+                  p1_filter8, p0_filter8, q0_filter8);
+        DUP2_ARG2(__lsx_vpickev_b, zero, q1_filter8, zero, q2_filter8,
+                  q1_filter8, q2_filter8);
+
+        /* store pixel values */
+        p2_out = __lsx_vbitsel_v(p2, p2_filter8, flat);
+        p1_out = __lsx_vbitsel_v(p1_out, p1_filter8, flat);
+        p0_out = __lsx_vbitsel_v(p0_out, p0_filter8, flat);
+        q0_out = __lsx_vbitsel_v(q0_out, q0_filter8, flat);
+        q1_out = __lsx_vbitsel_v(q1_out, q1_filter8, flat);
+        q2_out = __lsx_vbitsel_v(q2, q2_filter8, flat);
+
+        __lsx_vstelm_d(p2_out, dst - stride3, 0, 0);
+        __lsx_vstelm_d(p1_out, dst - stride2, 0, 0);
+        __lsx_vstelm_d(p0_out, dst - stride, 0, 0);
+        __lsx_vstelm_d(q0_out, dst, 0, 0);
+        __lsx_vstelm_d(q1_out, dst + stride, 0, 0);
+        __lsx_vstelm_d(q2_out, dst + stride2, 0, 0);
+    }
+}
+
+void ff_loop_filter_v_88_16_lsx(uint8_t *dst, ptrdiff_t stride,
+                                int32_t b_limit_ptr,
+                                int32_t limit_ptr,
+                                int32_t thresh_ptr)
+{
+    ptrdiff_t stride2 = stride << 1;
+    ptrdiff_t stride3 = stride2 + stride;
+    ptrdiff_t stride4 = stride2 << 1;
+    __m128i p3, p2, p1, p0, q3, q2, q1, q0;
+    __m128i p2_out, p1_out, p0_out, q0_out, q1_out, q2_out;
+    __m128i flat, mask, hev, tmp, thresh, b_limit, limit;
+    __m128i p3_l, p2_l, p1_l, p0_l, q0_l, q1_l, q2_l, q3_l;
+    __m128i p3_h, p2_h, p1_h, p0_h, q0_h, q1_h, q2_h, q3_h;
+    __m128i p2_filt8_l, p1_filt8_l, p0_filt8_l;
+    __m128i q0_filt8_l, q1_filt8_l, q2_filt8_l;
+    __m128i p2_filt8_h, p1_filt8_h, p0_filt8_h;
+    __m128i q0_filt8_h, q1_filt8_h, q2_filt8_h;
+    __m128i zero = __lsx_vldi(0);
+
+    /* load vector elements */
+    DUP4_ARG2(__lsx_vldx, dst, -stride4, dst, -stride3, dst, -stride2,
+              dst, -stride, p3, p2, p1, p0);
+    q0 = __lsx_vld(dst, 0);
+    DUP2_ARG2(__lsx_vldx, dst, stride, dst, stride2, q1, q2);
+    q3 = __lsx_vldx(dst, stride3);
+
+    thresh = __lsx_vreplgr2vr_b(thresh_ptr);
+    tmp    = __lsx_vreplgr2vr_b(thresh_ptr >> 8);
+    thresh = __lsx_vilvl_d(tmp, thresh);
+
+    b_limit = __lsx_vreplgr2vr_b(b_limit_ptr);
+    tmp     = __lsx_vreplgr2vr_b(b_limit_ptr >> 8);
+    b_limit = __lsx_vilvl_d(tmp, b_limit);
+
+    limit = __lsx_vreplgr2vr_b(limit_ptr);
+    tmp   = __lsx_vreplgr2vr_b(limit_ptr >> 8);
+    limit = __lsx_vilvl_d(tmp, limit);
+
+    /* mask and hev */
+    LPF_MASK_HEV(p3, p2, p1, p0, q0, q1, q2, q3, limit, b_limit, thresh,
+                 hev, mask, flat);
+    VP9_FLAT4(p3, p2, p0, q0, q2, q3, flat);
+    VP9_LPF_FILTER4_4W(p1, p0, q0, q1, mask, hev, p1_out, p0_out, q0_out,
+                       q1_out);
+
+    /* if flat is zero for all pixels, then no need to calculate other filter */
+    if (__lsx_bz_v(flat)) {
+        __lsx_vst(p1_out, dst - stride2, 0);
+        __lsx_vst(p0_out, dst - stride, 0);
+        __lsx_vst(q0_out, dst, 0);
+        __lsx_vst(q1_out, dst + stride, 0);
+    } else {
+        DUP4_ARG2(__lsx_vilvl_b, zero, p3, zero, p2, zero, p1, zero, p0,
+                  p3_l, p2_l, p1_l, p0_l);
+        DUP4_ARG2(__lsx_vilvl_b, zero, q0, zero, q1, zero, q2, zero, q3,
+                  q0_l, q1_l, q2_l, q3_l);
+        VP9_FILTER8(p3_l, p2_l, p1_l, p0_l, q0_l, q1_l, q2_l, q3_l, p2_filt8_l,
+                    p1_filt8_l, p0_filt8_l, q0_filt8_l, q1_filt8_l, q2_filt8_l);
+
+        DUP4_ARG2(__lsx_vilvh_b, zero, p3, zero, p2, zero, p1, zero, p0,
+                  p3_h, p2_h, p1_h, p0_h);
+        DUP4_ARG2(__lsx_vilvh_b, zero, q0, zero, q1, zero, q2, zero, q3,
+                  q0_h, q1_h, q2_h, q3_h);
+        VP9_FILTER8(p3_h, p2_h, p1_h, p0_h, q0_h, q1_h, q2_h, q3_h, p2_filt8_h,
+                    p1_filt8_h, p0_filt8_h, q0_filt8_h, q1_filt8_h, q2_filt8_h);
+
+        /* convert 16 bit output data into 8 bit */
+        DUP4_ARG2(__lsx_vpickev_b, p2_filt8_h, p2_filt8_l, p1_filt8_h,
+                  p1_filt8_l, p0_filt8_h, p0_filt8_l, q0_filt8_h, q0_filt8_l,
+                  p2_filt8_l, p1_filt8_l, p0_filt8_l, q0_filt8_l);
+        DUP2_ARG2(__lsx_vpickev_b, q1_filt8_h, q1_filt8_l, q2_filt8_h,
+                  q2_filt8_l, q1_filt8_l, q2_filt8_l);
+
+        /* store pixel values */
+        p2_out = __lsx_vbitsel_v(p2, p2_filt8_l, flat);
+        p1_out = __lsx_vbitsel_v(p1_out, p1_filt8_l, flat);
+        p0_out = __lsx_vbitsel_v(p0_out, p0_filt8_l, flat);
+        q0_out = __lsx_vbitsel_v(q0_out, q0_filt8_l, flat);
+        q1_out = __lsx_vbitsel_v(q1_out, q1_filt8_l, flat);
+        q2_out = __lsx_vbitsel_v(q2, q2_filt8_l, flat);
+
+
+        __lsx_vstx(p2_out, dst, -stride3);
+        __lsx_vstx(p1_out, dst, -stride2);
+        __lsx_vstx(p0_out, dst, -stride);
+        __lsx_vst(q0_out, dst, 0);
+        __lsx_vstx(q1_out, dst, stride);
+        __lsx_vstx(q2_out, dst, stride2);
+    }
+}
+
+void ff_loop_filter_v_84_16_lsx(uint8_t *dst, ptrdiff_t stride,
+                                int32_t b_limit_ptr,
+                                int32_t limit_ptr,
+                                int32_t thresh_ptr)
+{
+    ptrdiff_t stride2 = stride << 1;
+    ptrdiff_t stride3 = stride2 + stride;
+    ptrdiff_t stride4 = stride2 << 1;
+    __m128i p3, p2, p1, p0, q3, q2, q1, q0;
+    __m128i p2_out, p1_out, p0_out, q0_out, q1_out, q2_out;
+    __m128i flat, mask, hev, tmp, thresh, b_limit, limit;
+    __m128i p3_l, p2_l, p1_l, p0_l, q0_l, q1_l, q2_l, q3_l;
+    __m128i p2_filt8_l, p1_filt8_l, p0_filt8_l;
+    __m128i q0_filt8_l, q1_filt8_l, q2_filt8_l;
+    __m128i zero = __lsx_vldi(0);
+
+    /* load vector elements */
+    DUP4_ARG2(__lsx_vldx, dst, -stride4, dst, -stride3, dst, -stride2,
+              dst, -stride, p3, p2, p1, p0);
+    q0 = __lsx_vld(dst, 0);
+    DUP2_ARG2(__lsx_vldx, dst, stride, dst, stride2, q1, q2);
+    q3 = __lsx_vldx(dst, stride3);
+
+    thresh = __lsx_vreplgr2vr_b(thresh_ptr);
+    tmp    = __lsx_vreplgr2vr_b(thresh_ptr >> 8);
+    thresh = __lsx_vilvl_d(tmp, thresh);
+
+    b_limit = __lsx_vreplgr2vr_b(b_limit_ptr);
+    tmp     = __lsx_vreplgr2vr_b(b_limit_ptr >> 8);
+    b_limit = __lsx_vilvl_d(tmp, b_limit);
+
+    limit = __lsx_vreplgr2vr_b(limit_ptr);
+    tmp   = __lsx_vreplgr2vr_b(limit_ptr >> 8);
+    limit = __lsx_vilvl_d(tmp, limit);
+
+    /* mask and hev */
+    LPF_MASK_HEV(p3, p2, p1, p0, q0, q1, q2, q3, limit, b_limit, thresh,
+                 hev, mask, flat);
+    VP9_FLAT4(p3, p2, p0, q0, q2, q3, flat);
+    VP9_LPF_FILTER4_4W(p1, p0, q0, q1, mask, hev, p1_out, p0_out, q0_out,
+                       q1_out);
+
+    flat = __lsx_vilvl_d(zero, flat);
+
+    /* if flat is zero for all pixels, then no need to calculate other filter */
+    if (__lsx_bz_v(flat)) {
+        __lsx_vstx(p1_out, dst, -stride2);
+        __lsx_vstx(p0_out, dst, -stride);
+        __lsx_vst(q0_out, dst, 0);
+        __lsx_vstx(q1_out, dst, stride);
+    } else {
+        DUP4_ARG2(__lsx_vilvl_b, zero, p3, zero, p2, zero, p1, zero, p0,
+                  p3_l, p2_l, p1_l, p0_l);
+        DUP4_ARG2(__lsx_vilvl_b, zero, q0, zero, q1, zero, q2, zero, q3,
+                  q0_l, q1_l, q2_l, q3_l);
+        VP9_FILTER8(p3_l, p2_l, p1_l, p0_l, q0_l, q1_l, q2_l, q3_l, p2_filt8_l,
+                    p1_filt8_l, p0_filt8_l, q0_filt8_l, q1_filt8_l, q2_filt8_l);
+
+        /* convert 16 bit output data into 8 bit */
+        DUP4_ARG2(__lsx_vpickev_b, p2_filt8_l, p2_filt8_l, p1_filt8_l,
+                  p1_filt8_l, p0_filt8_l, p0_filt8_l, q0_filt8_l, q0_filt8_l,
+                  p2_filt8_l, p1_filt8_l, p0_filt8_l, q0_filt8_l);
+        DUP2_ARG2(__lsx_vpickev_b, q1_filt8_l, q1_filt8_l, q2_filt8_l,
+                  q2_filt8_l, q1_filt8_l, q2_filt8_l);
+
+        /* store pixel values */
+        p2_out = __lsx_vbitsel_v(p2, p2_filt8_l, flat);
+        p1_out = __lsx_vbitsel_v(p1_out, p1_filt8_l, flat);
+        p0_out = __lsx_vbitsel_v(p0_out, p0_filt8_l, flat);
+        q0_out = __lsx_vbitsel_v(q0_out, q0_filt8_l, flat);
+        q1_out = __lsx_vbitsel_v(q1_out, q1_filt8_l, flat);
+        q2_out = __lsx_vbitsel_v(q2, q2_filt8_l, flat);
+
+        __lsx_vstx(p2_out, dst, -stride3);
+        __lsx_vstx(p1_out, dst, -stride2);
+        __lsx_vstx(p0_out, dst, -stride);
+        __lsx_vst(q0_out, dst, 0);
+        __lsx_vstx(q1_out, dst, stride);
+        __lsx_vstx(q2_out, dst, stride2);
+    }
+}
+
+void ff_loop_filter_v_48_16_lsx(uint8_t *dst, ptrdiff_t stride,
+                                int32_t b_limit_ptr,
+                                int32_t limit_ptr,
+                                int32_t thresh_ptr)
+{
+    ptrdiff_t stride2 = stride << 1;
+    ptrdiff_t stride3 = stride2 + stride;
+    ptrdiff_t stride4 = stride2 << 1;
+    __m128i p3, p2, p1, p0, q3, q2, q1, q0;
+    __m128i p2_out, p1_out, p0_out, q0_out, q1_out, q2_out;
+    __m128i flat, mask, hev, tmp, thresh, b_limit, limit;
+    __m128i p3_h, p2_h, p1_h, p0_h, q0_h, q1_h, q2_h, q3_h;
+    __m128i p2_filt8_h, p1_filt8_h, p0_filt8_h;
+    __m128i q0_filt8_h, q1_filt8_h, q2_filt8_h;
+    __m128i zero = { 0 };
+
+    /* load vector elements */
+    DUP4_ARG2(__lsx_vldx, dst, -stride4, dst, -stride3, dst, -stride2,
+              dst, -stride, p3, p2, p1, p0);
+    q0 = __lsx_vld(dst, 0);
+    DUP2_ARG2(__lsx_vldx, dst, stride, dst, stride2, q1, q2);
+    q3 = __lsx_vldx(dst, stride3);
+
+    thresh = __lsx_vreplgr2vr_b(thresh_ptr);
+    tmp    = __lsx_vreplgr2vr_b(thresh_ptr >> 8);
+    thresh = __lsx_vilvl_d(tmp, thresh);
+
+    b_limit = __lsx_vreplgr2vr_b(b_limit_ptr);
+    tmp     = __lsx_vreplgr2vr_b(b_limit_ptr >> 8);
+    b_limit = __lsx_vilvl_d(tmp, b_limit);
+
+    limit = __lsx_vreplgr2vr_b(limit_ptr);
+    tmp   = __lsx_vreplgr2vr_b(limit_ptr >> 8);
+    limit = __lsx_vilvl_d(tmp, limit);
+
+    /* mask and hev */
+    LPF_MASK_HEV(p3, p2, p1, p0, q0, q1, q2, q3, limit, b_limit, thresh,
+                 hev, mask, flat);
+    VP9_FLAT4(p3, p2, p0, q0, q2, q3, flat);
+    VP9_LPF_FILTER4_4W(p1, p0, q0, q1, mask, hev, p1_out, p0_out, q0_out,
+                       q1_out);
+
+    flat = __lsx_vilvh_d(flat, zero);
+
+    /* if flat is zero for all pixels, then no need to calculate other filter */
+    if (__lsx_bz_v(flat)) {
+        __lsx_vstx(p1_out, dst, -stride2);
+        __lsx_vstx(p0_out, dst, -stride);
+        __lsx_vst(q0_out, dst, 0);
+        __lsx_vstx(q1_out, dst, stride);
+    } else {
+        DUP4_ARG2(__lsx_vilvh_b, zero, p3, zero, p2, zero, p1, zero, p0,
+                  p3_h, p2_h, p1_h, p0_h);
+        DUP4_ARG2(__lsx_vilvh_b, zero, q0, zero, q1, zero, q2, zero, q3,
+                  q0_h, q1_h, q2_h, q3_h);
+        VP9_FILTER8(p3_h, p2_h, p1_h, p0_h, q0_h, q1_h, q2_h, q3_h, p2_filt8_h,
+                    p1_filt8_h, p0_filt8_h, q0_filt8_h, q1_filt8_h, q2_filt8_h);
+
+        /* convert 16 bit output data into 8 bit */
+        DUP4_ARG2(__lsx_vpickev_b, p2_filt8_h, p2_filt8_h, p1_filt8_h,
+                  p1_filt8_h, p0_filt8_h, p0_filt8_h, q0_filt8_h, q0_filt8_h,
+                  p2_filt8_h, p1_filt8_h, p0_filt8_h, q0_filt8_h);
+        DUP2_ARG2(__lsx_vpickev_b, q1_filt8_h, q1_filt8_h, q2_filt8_h,
+                  q2_filt8_h, q1_filt8_h, q2_filt8_h);
+
+        /* store pixel values */
+        p2_out = __lsx_vbitsel_v(p2, p2_filt8_h, flat);
+        p1_out = __lsx_vbitsel_v(p1_out, p1_filt8_h, flat);
+        p0_out = __lsx_vbitsel_v(p0_out, p0_filt8_h, flat);
+        q0_out = __lsx_vbitsel_v(q0_out, q0_filt8_h, flat);
+        q1_out = __lsx_vbitsel_v(q1_out, q1_filt8_h, flat);
+        q2_out = __lsx_vbitsel_v(q2, q2_filt8_h, flat);
+
+        __lsx_vstx(p2_out, dst, -stride3);
+        __lsx_vstx(p1_out, dst, -stride2);
+        __lsx_vstx(p0_out, dst, -stride);
+        __lsx_vst(q0_out, dst, 0);
+        __lsx_vstx(q1_out, dst, stride);
+        __lsx_vstx(q2_out, dst, stride2);
+    }
+}
+
+static int32_t vp9_hz_lpf_t4_and_t8_16w(uint8_t *dst, ptrdiff_t stride,
+                                        uint8_t *filter48,
+                                        int32_t b_limit_ptr,
+                                        int32_t limit_ptr,
+                                        int32_t thresh_ptr)
+{
+    ptrdiff_t stride2 = stride << 1;
+    ptrdiff_t stride3 = stride2 + stride;
+    ptrdiff_t stride4 = stride2 << 1;
+    __m128i p3, p2, p1, p0, q3, q2, q1, q0;
+    __m128i p2_out, p1_out, p0_out, q0_out, q1_out, q2_out;
+    __m128i flat, mask, hev, thresh, b_limit, limit;
+    __m128i p3_l, p2_l, p1_l, p0_l, q0_l, q1_l, q2_l, q3_l;
+    __m128i p3_h, p2_h, p1_h, p0_h, q0_h, q1_h, q2_h, q3_h;
+    __m128i p2_filt8_l, p1_filt8_l, p0_filt8_l;
+    __m128i q0_filt8_l, q1_filt8_l, q2_filt8_l;
+    __m128i p2_filt8_h, p1_filt8_h, p0_filt8_h;
+    __m128i q0_filt8_h, q1_filt8_h, q2_filt8_h;
+    __m128i zero = __lsx_vldi(0);
+
+    /* load vector elements */
+    DUP4_ARG2(__lsx_vldx, dst, -stride4, dst, -stride3, dst, -stride2,
+              dst, -stride, p3, p2, p1, p0);
+    q0 = __lsx_vld(dst, 0);
+    DUP2_ARG2(__lsx_vldx, dst, stride, dst, stride2, q1, q2);
+    q3 = __lsx_vldx(dst, stride3);
+
+    thresh  = __lsx_vreplgr2vr_b(thresh_ptr);
+    b_limit = __lsx_vreplgr2vr_b(b_limit_ptr);
+    limit   = __lsx_vreplgr2vr_b(limit_ptr);
+
+    /* mask and hev */
+    LPF_MASK_HEV(p3, p2, p1, p0, q0, q1, q2, q3, limit, b_limit, thresh,
+                 hev, mask, flat);
+    VP9_FLAT4(p3, p2, p0, q0, q2, q3, flat);
+    VP9_LPF_FILTER4_4W(p1, p0, q0, q1, mask, hev, p1_out, p0_out, q0_out,
+                       q1_out);
+
+    /* if flat is zero for all pixels, then no need to calculate other filter */
+    if (__lsx_bz_v(flat)) {
+        __lsx_vstx(p1_out, dst, -stride2);
+        __lsx_vstx(p0_out, dst, -stride);
+        __lsx_vst(q0_out, dst, 0);
+        __lsx_vstx(q1_out, dst, stride);
+        return 1;
+    } else {
+        DUP4_ARG2(__lsx_vilvl_b, zero, p3, zero, p2, zero, p1, zero, p0,
+                  p3_l, p2_l, p1_l, p0_l);
+        DUP4_ARG2(__lsx_vilvl_b, zero, q0, zero, q1, zero, q2, zero, q3,
+                  q0_l, q1_l, q2_l, q3_l);
+        VP9_FILTER8(p3_l, p2_l, p1_l, p0_l, q0_l, q1_l, q2_l, q3_l, p2_filt8_l,
+                    p1_filt8_l, p0_filt8_l, q0_filt8_l, q1_filt8_l, q2_filt8_l);
+
+        DUP4_ARG2(__lsx_vilvh_b, zero, p3, zero, p2, zero, p1, zero, p0,
+                  p3_h, p2_h, p1_h, p0_h);
+        DUP4_ARG2(__lsx_vilvh_b, zero, q0, zero, q1, zero, q2, zero, q3,
+                  q0_h, q1_h, q2_h, q3_h);
+        VP9_FILTER8(p3_h, p2_h, p1_h, p0_h, q0_h, q1_h, q2_h, q3_h, p2_filt8_h,
+                    p1_filt8_h, p0_filt8_h, q0_filt8_h, q1_filt8_h, q2_filt8_h);
+
+        /* convert 16 bit output data into 8 bit */
+        DUP4_ARG2(__lsx_vpickev_b, p2_filt8_h, p2_filt8_l, p1_filt8_h,
+                  p1_filt8_l, p0_filt8_h, p0_filt8_l, q0_filt8_h, q0_filt8_l,
+                  p2_filt8_l, p1_filt8_l, p0_filt8_l, q0_filt8_l);
+        DUP2_ARG2(__lsx_vpickev_b, q1_filt8_h, q1_filt8_l, q2_filt8_h,
+                  q2_filt8_l, q1_filt8_l, q2_filt8_l);
+
+        /* store pixel values */
+        p2_out = __lsx_vbitsel_v(p2, p2_filt8_l, flat);
+        p1_out = __lsx_vbitsel_v(p1_out, p1_filt8_l, flat);
+        p0_out = __lsx_vbitsel_v(p0_out, p0_filt8_l, flat);
+        q0_out = __lsx_vbitsel_v(q0_out, q0_filt8_l, flat);
+        q1_out = __lsx_vbitsel_v(q1_out, q1_filt8_l, flat);
+        q2_out = __lsx_vbitsel_v(q2, q2_filt8_l, flat);
+
+        __lsx_vst(p2_out, filter48, 0);
+        __lsx_vst(p1_out, filter48, 16);
+        __lsx_vst(p0_out, filter48, 32);
+        __lsx_vst(q0_out, filter48, 48);
+        __lsx_vst(q1_out, filter48, 64);
+        __lsx_vst(q2_out, filter48, 80);
+        __lsx_vst(flat, filter48, 96);
+
+        return 0;
+    }
+}
+
+static void vp9_hz_lpf_t16_16w(uint8_t *dst, ptrdiff_t stride,
+                               uint8_t *filter48)
+{
+    ptrdiff_t stride2 = stride << 1;
+    ptrdiff_t stride3 = stride2 + stride;
+    ptrdiff_t stride4 = stride2 << 1;
+    uint8_t *dst_tmp = dst - stride4;
+    uint8_t *dst_tmp1 = dst + stride4;
+    __m128i p7, p6, p5, p4, p3, p2, p1, p0, q0, q1, q2, q3, q4, q5, q6, q7;
+    __m128i flat, flat2, filter8;
+    __m128i zero = __lsx_vldi(0);
+    __m128i out_h, out_l;
+    v8u16 p7_l_in, p6_l_in, p5_l_in, p4_l_in;
+    v8u16 p3_l_in, p2_l_in, p1_l_in, p0_l_in;
+    v8u16 q7_l_in, q6_l_in, q5_l_in, q4_l_in;
+    v8u16 q3_l_in, q2_l_in, q1_l_in, q0_l_in;
+    v8u16 p7_h_in, p6_h_in, p5_h_in, p4_h_in;
+    v8u16 p3_h_in, p2_h_in, p1_h_in, p0_h_in;
+    v8u16 q7_h_in, q6_h_in, q5_h_in, q4_h_in;
+    v8u16 q3_h_in, q2_h_in, q1_h_in, q0_h_in;
+    v8u16 tmp0_l, tmp1_l, tmp0_h, tmp1_h;
+
+    flat = __lsx_vld(filter48, 96);
+
+    DUP4_ARG2(__lsx_vldx, dst_tmp, -stride4, dst_tmp, -stride3, dst_tmp,
+              -stride2, dst_tmp, -stride, p7, p6, p5, p4);
+    p3 = __lsx_vld(dst_tmp, 0);
+    DUP2_ARG2(__lsx_vldx, dst_tmp, stride, dst_tmp, stride2, p2, p1);
+    p0 = __lsx_vldx(dst_tmp, stride3);
+
+    q0 = __lsx_vld(dst, 0);
+    DUP2_ARG2(__lsx_vldx, dst, stride, dst, stride2, q1, q2);
+    q3 = __lsx_vldx(dst, stride3);
+
+    q4 = __lsx_vld(dst_tmp1, 0);
+    DUP2_ARG2(__lsx_vldx, dst_tmp1, stride, dst_tmp1, stride2, q5, q6);
+    q7 = __lsx_vldx(dst_tmp1, stride3);
+    VP9_FLAT5(p7, p6, p5, p4, p0, q0, q4, q5, q6, q7, flat, flat2);
+
+    /* if flat2 is zero for all pixels, then no need to calculate other filter */
+    if (__lsx_bz_v(flat2)) {
+        DUP4_ARG2(__lsx_vld, filter48, 0, filter48, 16, filter48, 32, filter48,
+                  48, p2, p1, p0, q0);
+        DUP2_ARG2(__lsx_vld, filter48, 64, filter48, 80, q1, q2);
+
+        __lsx_vstx(p2, dst, -stride3);
+        __lsx_vstx(p1, dst, -stride2);
+        __lsx_vstx(p0, dst, -stride);
+        __lsx_vst(q0, dst, 0);
+        __lsx_vstx(q1, dst, stride);
+        __lsx_vstx(q2, dst, stride2);
+    } else {
+        dst = dst_tmp - stride3;
+
+        p7_l_in = (v8u16)__lsx_vilvl_b(zero, p7);
+        p6_l_in = (v8u16)__lsx_vilvl_b(zero, p6);
+        p5_l_in = (v8u16)__lsx_vilvl_b(zero, p5);
+        p4_l_in = (v8u16)__lsx_vilvl_b(zero, p4);
+        p3_l_in = (v8u16)__lsx_vilvl_b(zero, p3);
+        p2_l_in = (v8u16)__lsx_vilvl_b(zero, p2);
+        p1_l_in = (v8u16)__lsx_vilvl_b(zero, p1);
+        p0_l_in = (v8u16)__lsx_vilvl_b(zero, p0);
+
+        q0_l_in = (v8u16)__lsx_vilvl_b(zero, q0);
+
+        tmp0_l = p7_l_in << 3;
+        tmp0_l -= p7_l_in;
+        tmp0_l += p6_l_in;
+        tmp0_l += q0_l_in;
+        tmp1_l = p6_l_in + p5_l_in;
+        tmp1_l += p4_l_in;
+        tmp1_l += p3_l_in;
+        tmp1_l += p2_l_in;
+        tmp1_l += p1_l_in;
+        tmp1_l += p0_l_in;
+        tmp1_l += tmp0_l;
+
+        out_l = __lsx_vsrari_h((__m128i)tmp1_l, 4);
+
+        p7_h_in = (v8u16)__lsx_vilvh_b(zero, p7);
+        p6_h_in = (v8u16)__lsx_vilvh_b(zero, p6);
+        p5_h_in = (v8u16)__lsx_vilvh_b(zero, p5);
+        p4_h_in = (v8u16)__lsx_vilvh_b(zero, p4);
+
+        p3_h_in = (v8u16)__lsx_vilvh_b(zero, p3);
+        p2_h_in = (v8u16)__lsx_vilvh_b(zero, p2);
+        p1_h_in = (v8u16)__lsx_vilvh_b(zero, p1);
+        p0_h_in = (v8u16)__lsx_vilvh_b(zero, p0);
+        q0_h_in = (v8u16)__lsx_vilvh_b(zero, q0);
+
+        tmp0_h = p7_h_in << 3;
+        tmp0_h -= p7_h_in;
+        tmp0_h += p6_h_in;
+        tmp0_h += q0_h_in;
+        tmp1_h = p6_h_in + p5_h_in;
+        tmp1_h += p4_h_in;
+        tmp1_h += p3_h_in;
+        tmp1_h += p2_h_in;
+        tmp1_h += p1_h_in;
+        tmp1_h += p0_h_in;
+        tmp1_h += tmp0_h;
+
+        out_h = __lsx_vsrari_h((__m128i)tmp1_h, 4);
+
+        out_l = __lsx_vpickev_b(out_h, out_l);
+        p6 = __lsx_vbitsel_v(p6, out_l, flat2);
+        __lsx_vst(p6, dst, 0);
+        dst += stride;
+
+        /* p5 */
+        q1_l_in = (v8u16)__lsx_vilvl_b(zero, q1);
+        tmp0_l = p5_l_in - p6_l_in;
+        tmp0_l += q1_l_in;
+        tmp0_l -= p7_l_in;
+        tmp1_l += tmp0_l;
+        out_l = __lsx_vsrari_h((__m128i)tmp1_l, 4);
+
+        q1_h_in = (v8u16)__lsx_vilvh_b(zero, q1);
+        tmp0_h = p5_h_in - p6_h_in;
+        tmp0_h += q1_h_in;
+        tmp0_h -= p7_h_in;
+        tmp1_h += tmp0_h;
+        out_h = __lsx_vsrari_h((__m128i)tmp1_h, 4);
+
+        out_l = __lsx_vpickev_b(out_h, out_l);
+        p5 = __lsx_vbitsel_v(p5, out_l, flat2);
+        __lsx_vst(p5, dst, 0);
+        dst += stride;
+
+        /* p4 */
+        q2_l_in = (v8u16)__lsx_vilvl_b(zero, q2);
+        tmp0_l = p4_l_in - p5_l_in;
+        tmp0_l += q2_l_in;
+        tmp0_l -= p7_l_in;
+        tmp1_l += tmp0_l;
+        out_l = __lsx_vsrari_h((__m128i)tmp1_l, 4);
+
+        q2_h_in = (v8u16)__lsx_vilvh_b(zero, q2);
+        tmp0_h = p4_h_in - p5_h_in;
+        tmp0_h += q2_h_in;
+        tmp0_h -= p7_h_in;
+        tmp1_h += tmp0_h;
+        out_h = __lsx_vsrari_h((__m128i)tmp1_h, 4);
+
+        out_l = __lsx_vpickev_b(out_h, out_l);
+        p4 = __lsx_vbitsel_v(p4, out_l, flat2);
+        __lsx_vst(p4, dst, 0);
+        dst += stride;
+
+        /* p3 */
+        q3_l_in = (v8u16)__lsx_vilvl_b(zero, q3);
+        tmp0_l = p3_l_in - p4_l_in;
+        tmp0_l += q3_l_in;
+        tmp0_l -= p7_l_in;
+        tmp1_l += tmp0_l;
+        out_l = __lsx_vsrari_h((__m128i)tmp1_l, 4);
+
+        q3_h_in = (v8u16)__lsx_vilvh_b(zero, q3);
+        tmp0_h = p3_h_in - p4_h_in;
+        tmp0_h += q3_h_in;
+        tmp0_h -= p7_h_in;
+        tmp1_h += tmp0_h;
+        out_h = __lsx_vsrari_h((__m128i)tmp1_h, 4);
+
+        out_l = __lsx_vpickev_b(out_h, out_l);
+        p3 = __lsx_vbitsel_v(p3, out_l, flat2);
+        __lsx_vst(p3, dst, 0);
+        dst += stride;
+
+        /* p2 */
+        q4_l_in = (v8u16)__lsx_vilvl_b(zero, q4);
+        filter8 = __lsx_vld(filter48, 0);
+        tmp0_l = p2_l_in - p3_l_in;
+        tmp0_l += q4_l_in;
+        tmp0_l -= p7_l_in;
+        tmp1_l += tmp0_l;
+        out_l = __lsx_vsrari_h((__m128i)tmp1_l, 4);
+
+        q4_h_in = (v8u16)__lsx_vilvh_b(zero, q4);
+        tmp0_h = p2_h_in - p3_h_in;
+        tmp0_h += q4_h_in;
+        tmp0_h -= p7_h_in;
+        tmp1_h += tmp0_h;
+        out_h = __lsx_vsrari_h((__m128i)tmp1_h, 4);
+
+        out_l = __lsx_vpickev_b(out_h, out_l);
+        filter8 = __lsx_vbitsel_v(filter8, out_l, flat2);
+        __lsx_vst(filter8, dst, 0);
+        dst += stride;
+
+        /* p1 */
+        q5_l_in = (v8u16)__lsx_vilvl_b(zero, q5);
+        filter8 = __lsx_vld(filter48, 16);
+        tmp0_l = p1_l_in - p2_l_in;
+        tmp0_l += q5_l_in;
+        tmp0_l -= p7_l_in;
+        tmp1_l += tmp0_l;
+        out_l = __lsx_vsrari_h((__m128i)tmp1_l, 4);
+
+        q5_h_in = (v8u16)__lsx_vilvh_b(zero, q5);
+        tmp0_h = p1_h_in - p2_h_in;
+        tmp0_h += q5_h_in;
+        tmp0_h -= p7_h_in;
+        tmp1_h += tmp0_h;
+        out_h = __lsx_vsrari_h((__m128i)tmp1_h, 4);
+
+        out_l = __lsx_vpickev_b(out_h, out_l);
+        filter8 = __lsx_vbitsel_v(filter8, out_l, flat2);
+        __lsx_vst(filter8, dst, 0);
+        dst += stride;
+
+        /* p0 */
+        q6_l_in = (v8u16)__lsx_vilvl_b(zero, q6);
+        filter8 = __lsx_vld(filter48, 32);
+        tmp0_l = p0_l_in - p1_l_in;
+        tmp0_l += q6_l_in;
+        tmp0_l -= p7_l_in;
+        tmp1_l += tmp0_l;
+        out_l = __lsx_vsrari_h((__m128i)tmp1_l, 4);
+
+        q6_h_in = (v8u16)__lsx_vilvh_b(zero, q6);
+        tmp0_h = p0_h_in - p1_h_in;
+        tmp0_h += q6_h_in;
+        tmp0_h -= p7_h_in;
+        tmp1_h += tmp0_h;
+        out_h = __lsx_vsrari_h((__m128i)tmp1_h, 4);
+
+        out_l = __lsx_vpickev_b(out_h, out_l);
+        filter8 = __lsx_vbitsel_v(filter8, out_l, flat2);
+        __lsx_vst(filter8, dst, 0);
+        dst += stride;
+
+        /* q0 */
+        q7_l_in = (v8u16)__lsx_vilvl_b(zero, q7);
+        filter8 = __lsx_vld(filter48, 48);
+        tmp0_l = q7_l_in - p0_l_in;
+        tmp0_l += q0_l_in;
+        tmp0_l -= p7_l_in;
+        tmp1_l += tmp0_l;
+        out_l = __lsx_vsrari_h((__m128i)tmp1_l, 4);
+
+        q7_h_in = (v8u16)__lsx_vilvh_b(zero, q7);
+        tmp0_h = q7_h_in - p0_h_in;
+        tmp0_h += q0_h_in;
+        tmp0_h -= p7_h_in;
+        tmp1_h += tmp0_h;
+        out_h = __lsx_vsrari_h((__m128i)tmp1_h, 4);
+
+        out_l = __lsx_vpickev_b(out_h, out_l);
+        filter8 = __lsx_vbitsel_v(filter8, out_l, flat2);
+        __lsx_vst(filter8, dst, 0);
+        dst += stride;
+
+        /* q1 */
+        filter8 = __lsx_vld(filter48, 64);
+        tmp0_l = q7_l_in - q0_l_in;
+        tmp0_l += q1_l_in;
+        tmp0_l -= p6_l_in;
+        tmp1_l += tmp0_l;
+        out_l = __lsx_vsrari_h((__m128i)tmp1_l, 4);
+
+        tmp0_h = q7_h_in - q0_h_in;
+        tmp0_h += q1_h_in;
+        tmp0_h -= p6_h_in;
+        tmp1_h += tmp0_h;
+        out_h = __lsx_vsrari_h((__m128i)tmp1_h, 4);
+
+        out_l = __lsx_vpickev_b(out_h, out_l);
+        filter8 = __lsx_vbitsel_v(filter8, out_l, flat2);
+        __lsx_vst(filter8, dst, 0);
+        dst += stride;
+
+        /* q2 */
+        filter8 = __lsx_vld(filter48, 80);
+        tmp0_l = q7_l_in - q1_l_in;
+        tmp0_l += q2_l_in;
+        tmp0_l -= p5_l_in;
+        tmp1_l += tmp0_l;
+        out_l = __lsx_vsrari_h((__m128i)tmp1_l, 4);
+
+        tmp0_h = q7_h_in - q1_h_in;
+        tmp0_h += q2_h_in;
+        tmp0_h -= p5_h_in;
+        tmp1_h += tmp0_h;
+        out_h = __lsx_vsrari_h((__m128i)tmp1_h, 4);
+
+        out_l = __lsx_vpickev_b(out_h, out_l);
+        filter8 = __lsx_vbitsel_v(filter8, out_l, flat2);
+        __lsx_vst(filter8, dst, 0);
+        dst += stride;
+
+        /* q3 */
+        tmp0_l = q7_l_in - q2_l_in;
+        tmp0_l += q3_l_in;
+        tmp0_l -= p4_l_in;
+        tmp1_l += tmp0_l;
+        out_l = __lsx_vsrari_h((__m128i)tmp1_l, 4);
+
+        tmp0_h = q7_h_in - q2_h_in;
+        tmp0_h += q3_h_in;
+        tmp0_h -= p4_h_in;
+        tmp1_h += tmp0_h;
+        out_h = __lsx_vsrari_h((__m128i)tmp1_h, 4);
+
+        out_l = __lsx_vpickev_b(out_h, out_l);
+        q3 = __lsx_vbitsel_v(q3, out_l, flat2);
+        __lsx_vst(q3, dst, 0);
+        dst += stride;
+
+        /* q4 */
+        tmp0_l = q7_l_in - q3_l_in;
+        tmp0_l += q4_l_in;
+        tmp0_l -= p3_l_in;
+        tmp1_l += tmp0_l;
+        out_l = __lsx_vsrari_h((__m128i)tmp1_l, 4);
+
+        tmp0_h = q7_h_in - q3_h_in;
+        tmp0_h += q4_h_in;
+        tmp0_h -= p3_h_in;
+        tmp1_h += tmp0_h;
+        out_h = __lsx_vsrari_h((__m128i)tmp1_h, 4);
+
+        out_l = __lsx_vpickev_b(out_h, out_l);
+        q4 = __lsx_vbitsel_v(q4, out_l, flat2);
+        __lsx_vst(q4, dst, 0);
+        dst += stride;
+
+        /* q5 */
+        tmp0_l = q7_l_in - q4_l_in;
+        tmp0_l += q5_l_in;
+        tmp0_l -= p2_l_in;
+        tmp1_l += tmp0_l;
+        out_l = __lsx_vsrari_h((__m128i)tmp1_l, 4);
+
+        tmp0_h = q7_h_in - q4_h_in;
+        tmp0_h += q5_h_in;
+        tmp0_h -= p2_h_in;
+        tmp1_h += tmp0_h;
+        out_h = __lsx_vsrari_h((__m128i)tmp1_h, 4);
+
+        out_l = __lsx_vpickev_b(out_h, out_l);
+        q5 = __lsx_vbitsel_v(q5, out_l, flat2);
+        __lsx_vst(q5, dst, 0);
+        dst += stride;
+
+        /* q6 */
+        tmp0_l = q7_l_in - q5_l_in;
+        tmp0_l += q6_l_in;
+        tmp0_l -= p1_l_in;
+        tmp1_l += tmp0_l;
+        out_l = __lsx_vsrari_h((__m128i)tmp1_l, 4);
+
+        tmp0_h = q7_h_in - q5_h_in;
+        tmp0_h += q6_h_in;
+        tmp0_h -= p1_h_in;
+        tmp1_h += tmp0_h;
+        out_h = __lsx_vsrari_h((__m128i)tmp1_h, 4);
+
+        out_l = __lsx_vpickev_b(out_h, out_l);
+        q6 = __lsx_vbitsel_v(q6, out_l, flat2);
+        __lsx_vst(q6, dst, 0);
+    }
+}
+
+void ff_loop_filter_v_16_16_lsx(uint8_t *dst, ptrdiff_t stride,
+                                int32_t b_limit_ptr,
+                                int32_t limit_ptr,
+                                int32_t thresh_ptr)
+{
+    uint8_t filter48[16 * 8] __attribute__ ((aligned(16)));
+    uint8_t early_exit = 0;
+
+    early_exit = vp9_hz_lpf_t4_and_t8_16w(dst, stride, &filter48[0],
+                                          b_limit_ptr, limit_ptr, thresh_ptr);
+
+    if (0 == early_exit) {
+        vp9_hz_lpf_t16_16w(dst, stride, filter48);
+    }
+}
+
+void ff_loop_filter_v_16_8_lsx(uint8_t *dst, ptrdiff_t stride,
+                               int32_t b_limit_ptr,
+                               int32_t limit_ptr,
+                               int32_t thresh_ptr)
+{
+    ptrdiff_t stride2 = stride << 1;
+    ptrdiff_t stride3 = stride2 + stride;
+    ptrdiff_t stride4 = stride2 << 1;
+    uint8_t *dst_tmp = dst - stride4;
+    uint8_t *dst_tmp1 = dst + stride4;
+    __m128i zero = __lsx_vldi(0);
+    __m128i flat2, mask, hev, flat, thresh, b_limit, limit;
+    __m128i p3, p2, p1, p0, q3, q2, q1, q0, p7, p6, p5, p4, q4, q5, q6, q7;
+    __m128i p2_out, p1_out, p0_out, q0_out, q1_out, q2_out;
+    __m128i p0_filter16, p1_filter16;
+    __m128i p2_filter8, p1_filter8, p0_filter8;
+    __m128i q0_filter8, q1_filter8, q2_filter8;
+    __m128i p7_l, p6_l, p5_l, p4_l, q7_l, q6_l, q5_l, q4_l;
+    __m128i p3_l, p2_l, p1_l, p0_l, q3_l, q2_l, q1_l, q0_l;
+    __m128i tmp0, tmp1, tmp2;
+
+    /* load vector elements */
+    DUP4_ARG2(__lsx_vldx, dst, -stride4, dst, -stride3, dst, -stride2,
+              dst, -stride, p3, p2, p1, p0);
+    q0 = __lsx_vld(dst, 0);
+    DUP2_ARG2(__lsx_vldx, dst, stride, dst, stride2, q1, q2);
+    q3 = __lsx_vldx(dst, stride3);
+
+    thresh  = __lsx_vreplgr2vr_b(thresh_ptr);
+    b_limit = __lsx_vreplgr2vr_b(b_limit_ptr);
+    limit   = __lsx_vreplgr2vr_b(limit_ptr);
+
+    LPF_MASK_HEV(p3, p2, p1, p0, q0, q1, q2, q3, limit, b_limit, thresh,
+                 hev, mask, flat);
+    VP9_FLAT4(p3, p2, p0, q0, q2, q3, flat);
+    VP9_LPF_FILTER4_4W(p1, p0, q0, q1, mask, hev, p1_out, p0_out, q0_out,
+                       q1_out);
+
+    flat = __lsx_vilvl_d(zero, flat);
+
+    /* if flat is zero for all pixels, then no need to calculate other filter */
+    if (__lsx_bz_v(flat)) {
+        __lsx_vstelm_d(p1_out, dst - stride2, 0, 0);
+        __lsx_vstelm_d(p0_out, dst -   stride, 0, 0);
+        __lsx_vstelm_d(q0_out, dst           , 0, 0);
+        __lsx_vstelm_d(q1_out, dst +   stride, 0, 0);
+    } else {
+        /* convert 8 bit input data into 16 bit */
+        DUP4_ARG2(__lsx_vilvl_b, zero, p3, zero, p2, zero, p1, zero, p0,
+                  p3_l, p2_l, p1_l, p0_l);
+        DUP4_ARG2(__lsx_vilvl_b, zero, q0, zero, q1, zero, q2, zero, q3,
+                  q0_l, q1_l, q2_l, q3_l);
+        VP9_FILTER8(p3_l, p2_l, p1_l, p0_l, q0_l, q1_l, q2_l, q3_l,
+                    p2_filter8, p1_filter8, p0_filter8, q0_filter8,
+                    q1_filter8, q2_filter8);
+
+        /* convert 16 bit output data into 8 bit */
+        DUP4_ARG2(__lsx_vpickev_b, zero, p2_filter8, zero, p1_filter8,
+                  zero, p0_filter8, zero, q0_filter8, p2_filter8,
+                  p1_filter8, p0_filter8, q0_filter8);
+        DUP2_ARG2(__lsx_vpickev_b, zero, q1_filter8, zero, q2_filter8,
+                  q1_filter8, q2_filter8);
+
+        /* store pixel values */
+        p2_out = __lsx_vbitsel_v(p2, p2_filter8, flat);
+        p1_out = __lsx_vbitsel_v(p1_out, p1_filter8, flat);
+        p0_out = __lsx_vbitsel_v(p0_out, p0_filter8, flat);
+        q0_out = __lsx_vbitsel_v(q0_out, q0_filter8, flat);
+        q1_out = __lsx_vbitsel_v(q1_out, q1_filter8, flat);
+        q2_out = __lsx_vbitsel_v(q2, q2_filter8, flat);
+
+        /* load 16 vector elements */
+        DUP4_ARG2(__lsx_vld, dst_tmp - stride4, 0, dst_tmp - stride3, 0,
+                  dst_tmp - stride2, 0, dst_tmp - stride, 0, p7, p6, p5, p4);
+        DUP4_ARG2(__lsx_vld, dst_tmp1, 0, dst_tmp1 + stride, 0,
+                dst_tmp1 + stride2, 0, dst_tmp1 + stride3, 0, q4, q5, q6, q7);
+
+        VP9_FLAT5(p7, p6, p5, p4, p0, q0, q4, q5, q6, q7, flat, flat2);
+
+        /* if flat2 is zero for all pixels, then no need to calculate other filter */
+        if (__lsx_bz_v(flat2)) {
+            dst -= stride3;
+            __lsx_vstelm_d(p2_out, dst, 0, 0);
+            dst += stride;
+            __lsx_vstelm_d(p1_out, dst, 0, 0);
+            dst += stride;
+            __lsx_vstelm_d(p0_out, dst, 0, 0);
+            dst += stride;
+            __lsx_vstelm_d(q0_out, dst, 0, 0);
+            dst += stride;
+            __lsx_vstelm_d(q1_out, dst, 0, 0);
+            dst += stride;
+            __lsx_vstelm_d(q2_out, dst, 0, 0);
+        } else {
+            /* LSB(right) 8 pixel operation */
+            DUP4_ARG2(__lsx_vilvl_b, zero, p7, zero, p6, zero, p5, zero, p4,
+                      p7_l, p6_l, p5_l, p4_l);
+            DUP4_ARG2(__lsx_vilvl_b, zero, q4, zero, q5, zero, q6, zero, q7,
+                      q4_l, q5_l, q6_l, q7_l);
+
+            tmp0 = __lsx_vslli_h(p7_l, 3);
+            tmp0 = __lsx_vsub_h(tmp0, p7_l);
+            tmp0 = __lsx_vadd_h(tmp0, p6_l);
+            tmp0 = __lsx_vadd_h(tmp0, q0_l);
+
+            dst = dst_tmp - stride3;
+
+            /* calculation of p6 and p5 */
+            tmp1 = __lsx_vadd_h(p6_l, p5_l);
+            tmp1 = __lsx_vadd_h(tmp1, p4_l);
+            tmp1 = __lsx_vadd_h(tmp1, p3_l);
+            tmp1 = __lsx_vadd_h(tmp1, p2_l);
+            tmp1 = __lsx_vadd_h(tmp1, p1_l);
+            tmp1 = __lsx_vadd_h(tmp1, p0_l);
+            tmp1 = __lsx_vadd_h(tmp1, tmp0);
+
+            p0_filter16 = __lsx_vsrari_h(tmp1, 4);
+            tmp0 = __lsx_vsub_h(p5_l, p6_l);
+            tmp0 = __lsx_vadd_h(tmp0, q1_l);
+            tmp0 = __lsx_vsub_h(tmp0, p7_l);
+            tmp1 = __lsx_vadd_h(tmp1, tmp0);
+
+            p1_filter16 = __lsx_vsrari_h(tmp1, 4);
+            DUP2_ARG2(__lsx_vpickev_b, zero, p0_filter16, zero,
+                      p1_filter16, p0_filter16, p1_filter16);
+            p0_filter16 = __lsx_vbitsel_v(p6, p0_filter16, flat2);
+            p1_filter16 = __lsx_vbitsel_v(p5, p1_filter16, flat2);
+            __lsx_vstelm_d(p0_filter16, dst, 0, 0);
+            dst += stride;
+            __lsx_vstelm_d(p1_filter16, dst, 0, 0);
+            dst += stride;
+
+            /* calculation of p4 and p3 */
+            tmp0 = __lsx_vsub_h(p4_l, p5_l);
+            tmp0 = __lsx_vadd_h(tmp0, q2_l);
+            tmp0 = __lsx_vsub_h(tmp0, p7_l);
+            tmp2 = __lsx_vsub_h(p3_l, p4_l);
+            tmp2 = __lsx_vadd_h(tmp2, q3_l);
+            tmp2 = __lsx_vsub_h(tmp2, p7_l);
+            tmp1 = __lsx_vadd_h(tmp1, tmp0);
+            p0_filter16 = __lsx_vsrari_h(tmp1, 4);
+            tmp1 = __lsx_vadd_h(tmp1, tmp2);
+            p1_filter16 = __lsx_vsrari_h(tmp1, 4);
+            DUP2_ARG2(__lsx_vpickev_b, zero, p0_filter16, zero,
+                      p1_filter16, p0_filter16, p1_filter16);
+            p0_filter16 = __lsx_vbitsel_v(p4, p0_filter16, flat2);
+            p1_filter16 = __lsx_vbitsel_v(p3, p1_filter16, flat2);
+            __lsx_vstelm_d(p0_filter16, dst, 0, 0);
+            dst += stride;
+            __lsx_vstelm_d(p1_filter16, dst, 0, 0);
+            dst += stride;
+
+            /* calculation of p2 and p1 */
+            tmp0 = __lsx_vsub_h(p2_l, p3_l);
+            tmp0 = __lsx_vadd_h(tmp0, q4_l);
+            tmp0 = __lsx_vsub_h(tmp0, p7_l);
+            tmp2 = __lsx_vsub_h(p1_l, p2_l);
+            tmp2 = __lsx_vadd_h(tmp2, q5_l);
+            tmp2 = __lsx_vsub_h(tmp2, p7_l);
+            tmp1 = __lsx_vadd_h(tmp1, tmp0);
+            p0_filter16 = __lsx_vsrari_h(tmp1, 4);
+            tmp1 = __lsx_vadd_h(tmp1, tmp2);
+            p1_filter16 = __lsx_vsrari_h(tmp1, 4);
+            DUP2_ARG2(__lsx_vpickev_b, zero, p0_filter16, zero,
+                      p1_filter16, p0_filter16, p1_filter16);
+            p0_filter16 = __lsx_vbitsel_v(p2_out, p0_filter16, flat2);
+            p1_filter16 = __lsx_vbitsel_v(p1_out, p1_filter16, flat2);
+            __lsx_vstelm_d(p0_filter16, dst, 0, 0);
+            dst += stride;
+            __lsx_vstelm_d(p1_filter16, dst, 0, 0);
+            dst += stride;
+
+            /* calculation of p0 and q0 */
+            tmp0 = __lsx_vsub_h(p0_l, p1_l);
+            tmp0 = __lsx_vadd_h(tmp0, q6_l);
+            tmp0 = __lsx_vsub_h(tmp0, p7_l);
+            tmp2 = __lsx_vsub_h(q7_l, p0_l);
+            tmp2 = __lsx_vadd_h(tmp2, q0_l);
+            tmp2 = __lsx_vsub_h(tmp2, p7_l);
+            tmp1 = __lsx_vadd_h(tmp1, tmp0);
+            p0_filter16 = __lsx_vsrari_h((__m128i)tmp1, 4);
+            tmp1 = __lsx_vadd_h(tmp1, tmp2);
+            p1_filter16 = __lsx_vsrari_h((__m128i)tmp1, 4);
+            DUP2_ARG2(__lsx_vpickev_b, zero, p0_filter16, zero,
+                      p1_filter16, p0_filter16, p1_filter16);
+            p0_filter16 = __lsx_vbitsel_v(p0_out, p0_filter16, flat2);
+            p1_filter16 = __lsx_vbitsel_v(q0_out, p1_filter16, flat2);
+            __lsx_vstelm_d(p0_filter16, dst, 0, 0);
+            dst += stride;
+            __lsx_vstelm_d(p1_filter16, dst, 0, 0);
+            dst += stride;
+
+            /* calculation of q1 and q2 */
+            tmp0 = __lsx_vsub_h(q7_l, q0_l);
+            tmp0 = __lsx_vadd_h(tmp0, q1_l);
+            tmp0 = __lsx_vsub_h(tmp0, p6_l);
+            tmp2 = __lsx_vsub_h(q7_l, q1_l);
+            tmp2 = __lsx_vadd_h(tmp2, q2_l);
+            tmp2 = __lsx_vsub_h(tmp2, p5_l);
+            tmp1 = __lsx_vadd_h(tmp1, tmp0);
+            p0_filter16 = __lsx_vsrari_h(tmp1, 4);
+            tmp1 = __lsx_vadd_h(tmp1, tmp2);
+            p1_filter16 = __lsx_vsrari_h(tmp1, 4);
+            DUP2_ARG2(__lsx_vpickev_b, zero, p0_filter16, zero,
+                      p1_filter16, p0_filter16, p1_filter16);
+            p0_filter16 = __lsx_vbitsel_v(q1_out, p0_filter16, flat2);
+            p1_filter16 = __lsx_vbitsel_v(q2_out, p1_filter16, flat2);
+            __lsx_vstelm_d(p0_filter16, dst, 0, 0);
+            dst += stride;
+            __lsx_vstelm_d(p1_filter16, dst, 0, 0);
+            dst += stride;
+
+            /* calculation of q3 and q4 */
+            tmp0 = __lsx_vsub_h(q7_l, q2_l);
+            tmp0 = __lsx_vadd_h(tmp0, q3_l);
+            tmp0 = __lsx_vsub_h(tmp0, p4_l);
+            tmp2 = __lsx_vsub_h(q7_l, q3_l);
+            tmp2 = __lsx_vadd_h(tmp2, q4_l);
+            tmp2 = __lsx_vsub_h(tmp2, p3_l);
+            tmp1 = __lsx_vadd_h(tmp1, tmp0);
+            p0_filter16 = __lsx_vsrari_h(tmp1, 4);
+            tmp1 = __lsx_vadd_h(tmp1, tmp2);
+            p1_filter16 = __lsx_vsrari_h(tmp1, 4);
+            DUP2_ARG2(__lsx_vpickev_b, zero, p0_filter16, zero,
+                      p1_filter16, p0_filter16, p1_filter16);
+            p0_filter16 = __lsx_vbitsel_v(q3, p0_filter16, flat2);
+            p1_filter16 = __lsx_vbitsel_v(q4, p1_filter16, flat2);
+            __lsx_vstelm_d(p0_filter16, dst, 0, 0);
+            dst += stride;
+            __lsx_vstelm_d(p1_filter16, dst, 0, 0);
+            dst += stride;
+
+            /* calculation of q5 and q6 */
+            tmp0 = __lsx_vsub_h(q7_l, q4_l);
+            tmp0 = __lsx_vadd_h(tmp0, q5_l);
+            tmp0 = __lsx_vsub_h(tmp0, p2_l);
+            tmp2 = __lsx_vsub_h(q7_l, q5_l);
+            tmp2 = __lsx_vadd_h(tmp2, q6_l);
+            tmp2 = __lsx_vsub_h(tmp2, p1_l);
+            tmp1 = __lsx_vadd_h(tmp1, tmp0);
+            p0_filter16 = __lsx_vsrari_h(tmp1, 4);
+            tmp1 = __lsx_vadd_h(tmp1, tmp2);
+            p1_filter16 = __lsx_vsrari_h(tmp1, 4);
+            DUP2_ARG2(__lsx_vpickev_b, zero, p0_filter16, zero,
+                      p1_filter16, p0_filter16, p1_filter16);
+            p0_filter16 = __lsx_vbitsel_v(q5, p0_filter16, flat2);
+            p1_filter16 = __lsx_vbitsel_v(q6, p1_filter16, flat2);
+            __lsx_vstelm_d(p0_filter16, dst, 0, 0);
+            dst += stride;
+            __lsx_vstelm_d(p1_filter16, dst, 0, 0);
+        }
+    }
+}
+
+void ff_loop_filter_h_4_8_lsx(uint8_t *dst, ptrdiff_t stride,
+                              int32_t b_limit_ptr,
+                              int32_t limit_ptr,
+                              int32_t thresh_ptr)
+{
+    ptrdiff_t stride2 = stride << 1;
+    ptrdiff_t stride3 = stride2 + stride;
+    ptrdiff_t stride4 = stride2 << 1;
+    uint8_t *dst_tmp1 = dst - 4;
+    uint8_t *dst_tmp2 = dst_tmp1 + stride4;
+    __m128i mask, hev, flat, limit, thresh, b_limit;
+    __m128i p3, p2, p1, p0, q3, q2, q1, q0;
+    __m128i vec0, vec1, vec2, vec3;
+
+    p3 = __lsx_vld(dst_tmp1, 0);
+    DUP2_ARG2(__lsx_vldx, dst_tmp1, stride, dst_tmp1, stride2, p2, p1);
+    p0 = __lsx_vldx(dst_tmp1, stride3);
+    q0 = __lsx_vld(dst_tmp2, 0);
+    DUP2_ARG2(__lsx_vldx, dst_tmp2, stride, dst_tmp2, stride2, q1, q2);
+    q3 = __lsx_vldx(dst_tmp2, stride3);
+
+    thresh  = __lsx_vreplgr2vr_b(thresh_ptr);
+    b_limit = __lsx_vreplgr2vr_b(b_limit_ptr);
+    limit   = __lsx_vreplgr2vr_b(limit_ptr);
+
+    LSX_TRANSPOSE8x8_B(p3, p2, p1, p0, q0, q1, q2, q3,
+                       p3, p2, p1, p0, q0, q1, q2, q3);
+    LPF_MASK_HEV(p3, p2, p1, p0, q0, q1, q2, q3, limit, b_limit, thresh,
+                 hev, mask, flat);
+    VP9_LPF_FILTER4_4W(p1, p0, q0, q1, mask, hev, p1, p0, q0, q1);
+    DUP2_ARG2(__lsx_vilvl_b, p0, p1, q1, q0, vec0, vec1);
+    vec2 = __lsx_vilvl_h(vec1, vec0);
+    vec3 = __lsx_vilvh_h(vec1, vec0);
+
+    dst -= 2;
+    __lsx_vstelm_w(vec2, dst, 0, 0);
+    __lsx_vstelm_w(vec2, dst + stride, 0, 1);
+    __lsx_vstelm_w(vec2, dst + stride2, 0, 2);
+    __lsx_vstelm_w(vec2, dst + stride3, 0, 3);
+    dst += stride4;
+    __lsx_vstelm_w(vec3, dst, 0, 0);
+    __lsx_vstelm_w(vec3, dst + stride, 0, 1);
+    __lsx_vstelm_w(vec3, dst + stride2, 0, 2);
+    __lsx_vstelm_w(vec3, dst + stride3, 0, 3);
+}
+
+void ff_loop_filter_h_44_16_lsx(uint8_t *dst, ptrdiff_t stride,
+                                int32_t b_limit_ptr,
+                                int32_t limit_ptr,
+                                int32_t thresh_ptr)
+{
+    ptrdiff_t stride2 = stride << 1;
+    ptrdiff_t stride3 = stride2 + stride;
+    ptrdiff_t stride4 = stride2 << 1;
+    uint8_t *dst_tmp = dst - 4;
+    __m128i mask, hev, flat;
+    __m128i thresh0, b_limit0, limit0, thresh1, b_limit1, limit1;
+    __m128i p3, p2, p1, p0, q3, q2, q1, q0;
+    __m128i row0, row1, row2, row3, row4, row5, row6, row7;
+    __m128i row8, row9, row10, row11, row12, row13, row14, row15;
+    __m128i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5;
+
+    row0 = __lsx_vld(dst_tmp, 0);
+    DUP2_ARG2(__lsx_vldx, dst_tmp, stride, dst_tmp, stride2, row1, row2);
+    row3 = __lsx_vldx(dst_tmp, stride3);
+    dst_tmp += stride4;
+    row4 = __lsx_vld(dst_tmp, 0);
+    DUP2_ARG2(__lsx_vldx, dst_tmp, stride, dst_tmp, stride2, row5, row6);
+    row7 = __lsx_vldx(dst_tmp, stride3);
+    dst_tmp += stride4;
+    row8 = __lsx_vld(dst_tmp, 0);
+    DUP2_ARG2(__lsx_vldx, dst_tmp, stride, dst_tmp, stride2, row9, row10);
+    row11 = __lsx_vldx(dst_tmp, stride3);
+    dst_tmp += stride4;
+    row12 = __lsx_vld(dst_tmp, 0);
+    DUP2_ARG2(__lsx_vldx, dst_tmp, stride, dst_tmp, stride2, row13, row14);
+    row15 = __lsx_vldx(dst_tmp, stride3);
+
+    LSX_TRANSPOSE16x8_B(row0, row1, row2, row3, row4, row5, row6, row7,
+                        row8, row9, row10, row11, row12, row13, row14, row15,
+                        p3, p2, p1, p0, q0, q1, q2, q3);
+
+    thresh0 = __lsx_vreplgr2vr_b(thresh_ptr);
+    thresh1 = __lsx_vreplgr2vr_b(thresh_ptr >> 8);
+    thresh0 = __lsx_vilvl_d(thresh1, thresh0);
+
+    b_limit0 = __lsx_vreplgr2vr_b(b_limit_ptr);
+    b_limit1 = __lsx_vreplgr2vr_b(b_limit_ptr >> 8);
+    b_limit0 = __lsx_vilvl_d(b_limit1, b_limit0);
+
+    limit0 = __lsx_vreplgr2vr_b(limit_ptr);
+    limit1 = __lsx_vreplgr2vr_b(limit_ptr >> 8);
+    limit0 = __lsx_vilvl_d(limit1, limit0);
+
+    LPF_MASK_HEV(p3, p2, p1, p0, q0, q1, q2, q3, limit0, b_limit0, thresh0,
+                 hev, mask, flat);
+    VP9_LPF_FILTER4_4W(p1, p0, q0, q1, mask, hev, p1, p0, q0, q1);
+    DUP2_ARG2(__lsx_vilvl_b, p0, p1, q1, q0, tmp0, tmp1);
+    tmp2 = __lsx_vilvl_h(tmp1, tmp0);
+    tmp3 = __lsx_vilvh_h(tmp1, tmp0);
+    DUP2_ARG2(__lsx_vilvh_b, p0, p1, q1, q0, tmp0, tmp1);
+    tmp4 = __lsx_vilvl_h(tmp1, tmp0);
+    tmp5 = __lsx_vilvh_h(tmp1, tmp0);
+
+    dst -= 2;
+    __lsx_vstelm_w(tmp2, dst, 0, 0);
+    __lsx_vstelm_w(tmp2, dst + stride, 0, 1);
+    __lsx_vstelm_w(tmp2, dst + stride2, 0, 2);
+    __lsx_vstelm_w(tmp2, dst + stride3, 0, 3);
+    dst += stride4;
+    __lsx_vstelm_w(tmp3, dst, 0, 0);
+    __lsx_vstelm_w(tmp3, dst + stride, 0, 1);
+    __lsx_vstelm_w(tmp3, dst + stride2, 0, 2);
+    __lsx_vstelm_w(tmp3, dst + stride3, 0, 3);
+    dst += stride4;
+    __lsx_vstelm_w(tmp4, dst, 0, 0);
+    __lsx_vstelm_w(tmp4, dst + stride, 0, 1);
+    __lsx_vstelm_w(tmp4, dst + stride2, 0, 2);
+    __lsx_vstelm_w(tmp4, dst + stride3, 0, 3);
+    dst += stride4;
+    __lsx_vstelm_w(tmp5, dst, 0, 0);
+    __lsx_vstelm_w(tmp5, dst + stride, 0, 1);
+    __lsx_vstelm_w(tmp5, dst + stride2, 0, 2);
+    __lsx_vstelm_w(tmp5, dst + stride3, 0, 3);
+}
+
+void ff_loop_filter_h_8_8_lsx(uint8_t *dst, ptrdiff_t stride,
+                              int32_t b_limit_ptr,
+                              int32_t limit_ptr,
+                              int32_t thresh_ptr)
+{
+    ptrdiff_t stride2 = stride << 1;
+    ptrdiff_t stride3 = stride2 + stride;
+    ptrdiff_t stride4 = stride2 << 1;
+    uint8_t *dst_tmp = dst - 4;
+    __m128i p3, p2, p1, p0, q3, q2, q1, q0;
+    __m128i p1_out, p0_out, q0_out, q1_out;
+    __m128i flat, mask, hev, thresh, b_limit, limit;
+    __m128i p3_l, p2_l, p1_l, p0_l, q0_l, q1_l, q2_l, q3_l;
+    __m128i p2_filt8_l, p1_filt8_l, p0_filt8_l;
+    __m128i q0_filt8_l, q1_filt8_l, q2_filt8_l;
+    __m128i vec0, vec1, vec2, vec3, vec4;
+    __m128i zero = __lsx_vldi(0);
+
+    /* load vector elements */
+    p3 = __lsx_vld(dst_tmp, 0);
+    DUP2_ARG2(__lsx_vldx, dst_tmp, stride, dst_tmp, stride2, p2, p1);
+    p0 = __lsx_vldx(dst_tmp, stride3);
+    dst_tmp += stride4;
+    q0 = __lsx_vld(dst_tmp, 0);
+    DUP2_ARG2(__lsx_vldx, dst_tmp, stride, dst_tmp, stride2, q1, q2);
+    q3 = __lsx_vldx(dst_tmp, stride3);
+
+    LSX_TRANSPOSE8x8_B(p3, p2, p1, p0, q0, q1, q2, q3,
+                       p3, p2, p1, p0, q0, q1, q2, q3);
+
+    thresh  = __lsx_vreplgr2vr_b(thresh_ptr);
+    b_limit = __lsx_vreplgr2vr_b(b_limit_ptr);
+    limit   = __lsx_vreplgr2vr_b(limit_ptr);
+
+    /* mask and hev */
+    LPF_MASK_HEV(p3, p2, p1, p0, q0, q1, q2, q3, limit, b_limit, thresh,
+                 hev, mask, flat);
+    /* flat4 */
+    VP9_FLAT4(p3, p2, p0, q0, q2, q3, flat);
+    /* filter4 */
+    VP9_LPF_FILTER4_4W(p1, p0, q0, q1, mask, hev, p1_out, p0_out, q0_out,
+                       q1_out);
+
+    flat = __lsx_vilvl_d(zero, flat);
+
+    /* if flat is zero for all pixels, then no need to calculate other filter */
+    if (__lsx_bz_v(flat)) {
+        /* Store 4 pixels p1-_q1 */
+        DUP2_ARG2(__lsx_vilvl_b, p0_out, p1_out, q1_out, q0_out, vec0, vec1);
+        vec2 = __lsx_vilvl_h(vec1, vec0);
+        vec3 = __lsx_vilvh_h(vec1, vec0);
+
+        dst -= 2;
+        __lsx_vstelm_w(vec2, dst, 0, 0);
+        __lsx_vstelm_w(vec2, dst + stride, 0, 1);
+        __lsx_vstelm_w(vec2, dst + stride2, 0, 2);
+        __lsx_vstelm_w(vec2, dst + stride3, 0, 3);
+        dst += stride4;
+        __lsx_vstelm_w(vec3, dst, 0, 0);
+        __lsx_vstelm_w(vec3, dst + stride, 0, 1);
+        __lsx_vstelm_w(vec3, dst + stride2, 0, 2);
+        __lsx_vstelm_w(vec3, dst + stride3, 0, 3);
+    } else {
+        DUP4_ARG2(__lsx_vilvl_b, zero, p3, zero, p2, zero, p1, zero, p0,
+                  p3_l, p2_l, p1_l, p0_l);
+        DUP4_ARG2(__lsx_vilvl_b, zero, q0, zero, q1, zero, q2, zero, q3,
+                  q0_l, q1_l, q2_l, q3_l);
+        VP9_FILTER8(p3_l, p2_l, p1_l, p0_l, q0_l, q1_l, q2_l, q3_l, p2_filt8_l,
+                    p1_filt8_l, p0_filt8_l, q0_filt8_l, q1_filt8_l, q2_filt8_l);
+        /* convert 16 bit output data into 8 bit */
+        DUP4_ARG2(__lsx_vpickev_b, p2_filt8_l, p2_filt8_l, p1_filt8_l,
+                  p1_filt8_l, p0_filt8_l, p0_filt8_l, q0_filt8_l,
+                  q0_filt8_l, p2_filt8_l, p1_filt8_l, p0_filt8_l,
+                  q0_filt8_l);
+        DUP2_ARG2(__lsx_vpickev_b, q1_filt8_l, q1_filt8_l, q2_filt8_l,
+                  q2_filt8_l, q1_filt8_l, q2_filt8_l);
+
+        /* store pixel values */
+        p2 = __lsx_vbitsel_v(p2, p2_filt8_l, flat);
+        p1 = __lsx_vbitsel_v(p1_out, p1_filt8_l, flat);
+        p0 = __lsx_vbitsel_v(p0_out, p0_filt8_l, flat);
+        q0 = __lsx_vbitsel_v(q0_out, q0_filt8_l, flat);
+        q1 = __lsx_vbitsel_v(q1_out, q1_filt8_l, flat);
+        q2 = __lsx_vbitsel_v(q2, q2_filt8_l, flat);
+
+        /* Store 6 pixels p2-_q2 */
+        DUP2_ARG2(__lsx_vilvl_b, p1, p2, q0, p0, vec0, vec1);
+        vec2 = __lsx_vilvl_h(vec1, vec0);
+        vec3 = __lsx_vilvh_h(vec1, vec0);
+        vec4 = __lsx_vilvl_b(q2, q1);
+
+        dst -= 3;
+        __lsx_vstelm_w(vec2, dst, 0, 0);
+        __lsx_vstelm_h(vec4, dst, 4, 0);
+        dst += stride;
+        __lsx_vstelm_w(vec2, dst, 0, 1);
+        __lsx_vstelm_h(vec4, dst, 4, 1);
+        dst += stride;
+        __lsx_vstelm_w(vec2, dst, 0, 2);
+        __lsx_vstelm_h(vec4, dst, 4, 2);
+        dst += stride;
+        __lsx_vstelm_w(vec2, dst, 0, 3);
+        __lsx_vstelm_h(vec4, dst, 4, 3);
+        dst += stride;
+        __lsx_vstelm_w(vec3, dst, 0, 0);
+        __lsx_vstelm_h(vec4, dst, 4, 4);
+        dst += stride;
+        __lsx_vstelm_w(vec3, dst, 0, 1);
+        __lsx_vstelm_h(vec4, dst, 4, 5);
+        dst += stride;
+        __lsx_vstelm_w(vec3, dst, 0, 2);
+        __lsx_vstelm_h(vec4, dst, 4, 6);
+        dst += stride;
+        __lsx_vstelm_w(vec3, dst, 0, 3);
+        __lsx_vstelm_h(vec4, dst, 4, 7);
+    }
+}
+
+void ff_loop_filter_h_88_16_lsx(uint8_t *dst, ptrdiff_t stride,
+                                int32_t b_limit_ptr,
+                                int32_t limit_ptr,
+                                int32_t thresh_ptr)
+{
+    ptrdiff_t stride2 = stride << 1;
+    ptrdiff_t stride3 = stride2 + stride;
+    ptrdiff_t stride4 = stride2 << 1;
+    uint8_t *dst_tmp = dst - 4;
+    __m128i p3, p2, p1, p0, q3, q2, q1, q0;
+    __m128i p1_out, p0_out, q0_out, q1_out;
+    __m128i flat, mask, hev, thresh, b_limit, limit;
+    __m128i row4, row5, row6, row7, row12, row13, row14, row15;
+    __m128i p3_l, p2_l, p1_l, p0_l, q0_l, q1_l, q2_l, q3_l;
+    __m128i p3_h, p2_h, p1_h, p0_h, q0_h, q1_h, q2_h, q3_h;
+    __m128i p2_filt8_l, p1_filt8_l, p0_filt8_l;
+    __m128i q0_filt8_l, q1_filt8_l, q2_filt8_l;
+    __m128i p2_filt8_h, p1_filt8_h, p0_filt8_h;
+    __m128i q0_filt8_h, q1_filt8_h, q2_filt8_h;
+    __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
+    __m128i zero = __lsx_vldi(0);
+
+    p0 = __lsx_vld(dst_tmp, 0);
+    DUP2_ARG2(__lsx_vldx, dst_tmp, stride, dst_tmp, stride2, p1, p2);
+    p3 = __lsx_vldx(dst_tmp, stride3);
+    dst_tmp += stride4;
+    row4 = __lsx_vld(dst_tmp, 0);
+    DUP2_ARG2(__lsx_vldx, dst_tmp, stride, dst_tmp, stride2, row5, row6);
+    row7 = __lsx_vldx(dst_tmp, stride3);
+    dst_tmp += stride4;
+    q3 = __lsx_vld(dst_tmp, 0);
+    DUP2_ARG2(__lsx_vldx, dst_tmp, stride, dst_tmp, stride2, q2, q1);
+    q0 = __lsx_vldx(dst_tmp, stride3);
+    dst_tmp += stride4;
+    row12 = __lsx_vld(dst_tmp, 0);
+    DUP2_ARG2(__lsx_vldx, dst_tmp, stride, dst_tmp, stride2, row13, row14);
+    row15 = __lsx_vldx(dst_tmp, stride3);
+
+    /* transpose 16x8 matrix into 8x16 */
+    LSX_TRANSPOSE16x8_B(p0, p1, p2, p3, row4, row5, row6, row7,
+                        q3, q2, q1, q0, row12, row13, row14, row15,
+                        p3, p2, p1, p0, q0, q1, q2, q3);
+
+    thresh = __lsx_vreplgr2vr_b(thresh_ptr);
+    vec0   = __lsx_vreplgr2vr_b(thresh_ptr >> 8);
+    thresh = __lsx_vilvl_d(vec0, thresh);
+
+    b_limit = __lsx_vreplgr2vr_b(b_limit_ptr);
+    vec0    = __lsx_vreplgr2vr_b(b_limit_ptr >> 8);
+    b_limit = __lsx_vilvl_d(vec0, b_limit);
+
+    limit = __lsx_vreplgr2vr_b(limit_ptr);
+    vec0  = __lsx_vreplgr2vr_b(limit_ptr >> 8);
+    limit = __lsx_vilvl_d(vec0, limit);
+
+    /* mask and hev */
+    LPF_MASK_HEV(p3, p2, p1, p0, q0, q1, q2, q3, limit, b_limit, thresh,
+                 hev, mask, flat);
+    /* flat4 */
+    VP9_FLAT4(p3, p2, p0, q0, q2, q3, flat);
+    /* filter4 */
+    VP9_LPF_FILTER4_4W(p1, p0, q0, q1, mask, hev, p1_out, p0_out, q0_out,
+                       q1_out);
+
+    /* if flat is zero for all pixels, then no need to calculate other filter */
+    if (__lsx_bz_v(flat)) {
+        DUP2_ARG2(__lsx_vilvl_b, p0_out, p1_out, q1_out, q0_out, vec0, vec1);
+        vec2 = __lsx_vilvl_h(vec1, vec0);
+        vec3 = __lsx_vilvh_h(vec1, vec0);
+        DUP2_ARG2(__lsx_vilvh_b, p0_out, p1_out, q1_out, q0_out, vec0, vec1);
+        vec4 = __lsx_vilvl_h(vec1, vec0);
+        vec5 = __lsx_vilvh_h(vec1, vec0);
+
+        dst -= 2;
+        __lsx_vstelm_w(vec2, dst, 0, 0);
+        __lsx_vstelm_w(vec2, dst + stride, 0, 1);
+        __lsx_vstelm_w(vec2, dst + stride2, 0, 2);
+        __lsx_vstelm_w(vec2, dst + stride3, 0, 3);
+        dst += stride4;
+        __lsx_vstelm_w(vec3, dst, 0, 0);
+        __lsx_vstelm_w(vec3, dst + stride, 0, 1);
+        __lsx_vstelm_w(vec3, dst + stride2, 0, 2);
+        __lsx_vstelm_w(vec3, dst + stride3, 0, 3);
+        dst += stride4;
+        __lsx_vstelm_w(vec4, dst, 0, 0);
+        __lsx_vstelm_w(vec4, dst + stride, 0, 1);
+        __lsx_vstelm_w(vec4, dst + stride2, 0, 2);
+        __lsx_vstelm_w(vec4, dst + stride3, 0, 3);
+        dst += stride4;
+        __lsx_vstelm_w(vec5, dst, 0, 0);
+        __lsx_vstelm_w(vec5, dst + stride, 0, 1);
+        __lsx_vstelm_w(vec5, dst + stride2, 0, 2);
+        __lsx_vstelm_w(vec5, dst + stride3, 0, 3);
+    } else {
+        DUP4_ARG2(__lsx_vilvl_b, zero, p3, zero, p2, zero, p1, zero, p0,
+                  p3_l, p2_l, p1_l, p0_l);
+        DUP4_ARG2(__lsx_vilvl_b, zero, q0, zero, q1, zero, q2, zero, q3,
+                  q0_l, q1_l, q2_l, q3_l);
+        VP9_FILTER8(p3_l, p2_l, p1_l, p0_l, q0_l, q1_l, q2_l, q3_l, p2_filt8_l,
+                    p1_filt8_l, p0_filt8_l, q0_filt8_l, q1_filt8_l, q2_filt8_l);
+
+        DUP4_ARG2(__lsx_vilvh_b, zero, p3, zero, p2, zero, p1, zero, p0,
+                  p3_h, p2_h, p1_h, p0_h);
+        DUP4_ARG2(__lsx_vilvh_b, zero, q0, zero, q1, zero, q2, zero, q3,
+                  q0_h, q1_h, q2_h, q3_h);
+
+        /* filter8 */
+        VP9_FILTER8(p3_h, p2_h, p1_h, p0_h, q0_h, q1_h, q2_h, q3_h, p2_filt8_h,
+                    p1_filt8_h, p0_filt8_h, q0_filt8_h, q1_filt8_h, q2_filt8_h);
+
+        /* convert 16 bit output data into 8 bit */
+        DUP4_ARG2(__lsx_vpickev_b, p2_filt8_h, p2_filt8_l, p1_filt8_h,
+                  p1_filt8_l, p0_filt8_h, p0_filt8_l, q0_filt8_h, q0_filt8_l,
+                  p2_filt8_l, p1_filt8_l, p0_filt8_l, q0_filt8_l);
+        DUP2_ARG2(__lsx_vpickev_b, q1_filt8_h, q1_filt8_l, q2_filt8_h,
+                  q2_filt8_l, q1_filt8_l, q2_filt8_l);
+
+        /* store pixel values */
+        p2 = __lsx_vbitsel_v(p2, p2_filt8_l, flat);
+        p1 = __lsx_vbitsel_v(p1_out, p1_filt8_l, flat);
+        p0 = __lsx_vbitsel_v(p0_out, p0_filt8_l, flat);
+        q0 = __lsx_vbitsel_v(q0_out, q0_filt8_l, flat);
+        q1 = __lsx_vbitsel_v(q1_out, q1_filt8_l, flat);
+        q2 = __lsx_vbitsel_v(q2, q2_filt8_l, flat);
+
+        DUP2_ARG2(__lsx_vilvl_b, p1, p2, q0, p0, vec0, vec1);
+        vec3 = __lsx_vilvl_h(vec1, vec0);
+        vec4 = __lsx_vilvh_h(vec1, vec0);
+        DUP2_ARG2(__lsx_vilvh_b, p1, p2, q0, p0, vec0, vec1);
+        vec6 = __lsx_vilvl_h(vec1, vec0);
+        vec7 = __lsx_vilvh_h(vec1, vec0);
+        vec2 = __lsx_vilvl_b(q2, q1);
+        vec5 = __lsx_vilvh_b(q2, q1);
+
+        dst -= 3;
+        __lsx_vstelm_w(vec3, dst, 0, 0);
+        __lsx_vstelm_h(vec2, dst, 4, 0);
+        dst += stride;
+        __lsx_vstelm_w(vec3, dst, 0, 1);
+        __lsx_vstelm_h(vec2, dst, 4, 1);
+        dst += stride;
+        __lsx_vstelm_w(vec3, dst, 0, 2);
+        __lsx_vstelm_h(vec2, dst, 4, 2);
+        dst += stride;
+        __lsx_vstelm_w(vec3, dst, 0, 3);
+        __lsx_vstelm_h(vec2, dst, 4, 3);
+        dst += stride;
+        __lsx_vstelm_w(vec4, dst, 0, 0);
+        __lsx_vstelm_h(vec2, dst, 4, 4);
+        dst += stride;
+        __lsx_vstelm_w(vec4, dst, 0, 1);
+        __lsx_vstelm_h(vec2, dst, 4, 5);
+        dst += stride;
+        __lsx_vstelm_w(vec4, dst, 0, 2);
+        __lsx_vstelm_h(vec2, dst, 4, 6);
+        dst += stride;
+        __lsx_vstelm_w(vec4, dst, 0, 3);
+        __lsx_vstelm_h(vec2, dst, 4, 7);
+        dst += stride;
+        __lsx_vstelm_w(vec6, dst, 0, 0);
+        __lsx_vstelm_h(vec5, dst, 4, 0);
+        dst += stride;
+        __lsx_vstelm_w(vec6, dst, 0, 1);
+        __lsx_vstelm_h(vec5, dst, 4, 1);
+        dst += stride;
+        __lsx_vstelm_w(vec6, dst, 0, 2);
+        __lsx_vstelm_h(vec5, dst, 4, 2);
+        dst += stride;
+        __lsx_vstelm_w(vec6, dst, 0, 3);
+        __lsx_vstelm_h(vec5, dst, 4, 3);
+        dst += stride;
+        __lsx_vstelm_w(vec7, dst, 0, 0);
+        __lsx_vstelm_h(vec5, dst, 4, 4);
+        dst += stride;
+        __lsx_vstelm_w(vec7, dst, 0, 1);
+        __lsx_vstelm_h(vec5, dst, 4, 5);
+        dst += stride;
+        __lsx_vstelm_w(vec7, dst, 0, 2);
+        __lsx_vstelm_h(vec5, dst, 4, 6);
+        dst += stride;
+        __lsx_vstelm_w(vec7, dst, 0, 3);
+        __lsx_vstelm_h(vec5, dst, 4, 7);
+    }
+}
+
+void ff_loop_filter_h_84_16_lsx(uint8_t *dst, ptrdiff_t stride,
+                                int32_t b_limit_ptr,
+                                int32_t limit_ptr,
+                                int32_t thresh_ptr)
+{
+    ptrdiff_t stride2 = stride << 1;
+    ptrdiff_t stride3 = stride2 + stride;
+    ptrdiff_t stride4 = stride2 << 1;
+    uint8_t *dst_tmp = dst - 4;
+    __m128i p3, p2, p1, p0, q3, q2, q1, q0;
+    __m128i p1_out, p0_out, q0_out, q1_out;
+    __m128i flat, mask, hev, thresh, b_limit, limit;
+    __m128i row4, row5, row6, row7, row12, row13, row14, row15;
+    __m128i p3_l, p2_l, p1_l, p0_l, q0_l, q1_l, q2_l, q3_l;
+    __m128i p2_filt8_l, p1_filt8_l, p0_filt8_l;
+    __m128i q0_filt8_l, q1_filt8_l, q2_filt8_l;
+    __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
+    __m128i zero = __lsx_vldi(0);
+
+    p0 = __lsx_vld(dst_tmp, 0);
+    DUP2_ARG2(__lsx_vldx, dst_tmp, stride, dst_tmp, stride2, p1, p2);
+    p3 = __lsx_vldx(dst_tmp, stride3);
+    dst_tmp += stride4;
+    row4 = __lsx_vld(dst_tmp, 0);
+    DUP2_ARG2(__lsx_vldx, dst_tmp, stride, dst_tmp, stride2, row5, row6);
+    row7 = __lsx_vldx(dst_tmp, stride3);
+    dst_tmp += stride4;
+    q3 = __lsx_vld(dst_tmp, 0);
+    DUP2_ARG2(__lsx_vldx, dst_tmp, stride, dst_tmp, stride2, q2, q1);
+    q0 = __lsx_vldx(dst_tmp, stride3);
+    dst_tmp += stride4;
+    row12 = __lsx_vld(dst_tmp, 0);
+    DUP2_ARG2(__lsx_vldx, dst_tmp, stride, dst_tmp, stride2, row13, row14);
+    row15 = __lsx_vldx(dst_tmp, stride3);
+
+    /* transpose 16x8 matrix into 8x16 */
+    LSX_TRANSPOSE16x8_B(p0, p1, p2, p3, row4, row5, row6, row7,
+                        q3, q2, q1, q0, row12, row13, row14, row15,
+                        p3, p2, p1, p0, q0, q1, q2, q3);
+
+    thresh = __lsx_vreplgr2vr_b(thresh_ptr);
+    vec0   = __lsx_vreplgr2vr_b(thresh_ptr >> 8);
+    thresh = __lsx_vilvl_d(vec0, thresh);
+
+    b_limit = __lsx_vreplgr2vr_b(b_limit_ptr);
+    vec0    = __lsx_vreplgr2vr_b(b_limit_ptr >> 8);
+    b_limit = __lsx_vilvl_d(vec0, b_limit);
+
+    limit = __lsx_vreplgr2vr_b(limit_ptr);
+    vec0  = __lsx_vreplgr2vr_b(limit_ptr >> 8);
+    limit = __lsx_vilvl_d(vec0, limit);
+
+    /* mask and hev */
+    LPF_MASK_HEV(p3, p2, p1, p0, q0, q1, q2, q3, limit, b_limit, thresh,
+                 hev, mask, flat);
+    /* flat4 */
+    VP9_FLAT4(p3, p2, p0, q0, q2, q3, flat);
+    /* filter4 */
+    VP9_LPF_FILTER4_4W(p1, p0, q0, q1, mask, hev, p1_out, p0_out, q0_out,
+                       q1_out);
+
+    flat = __lsx_vilvl_d(zero, flat);
+
+    /* if flat is zero for all pixels, then no need to calculate other filter */
+    if (__lsx_bz_v(flat)) {
+        DUP2_ARG2(__lsx_vilvl_b, p0_out, p1_out, q1_out, q0_out, vec0, vec1);
+        vec2 = __lsx_vilvl_h(vec1, vec0);
+        vec3 = __lsx_vilvh_h(vec1, vec0);
+        DUP2_ARG2(__lsx_vilvh_b, p0_out, p1_out, q1_out, q0_out, vec0, vec1);
+        vec4 = __lsx_vilvl_h(vec1, vec0);
+        vec5 = __lsx_vilvh_h(vec1, vec0);
+
+        dst -= 2;
+        __lsx_vstelm_w(vec2, dst, 0, 0);
+        __lsx_vstelm_w(vec2, dst + stride, 0, 1);
+        __lsx_vstelm_w(vec2, dst + stride2, 0, 2);
+        __lsx_vstelm_w(vec2, dst + stride3, 0, 3);
+        dst += stride4;
+        __lsx_vstelm_w(vec3, dst, 0, 0);
+        __lsx_vstelm_w(vec3, dst + stride, 0, 1);
+        __lsx_vstelm_w(vec3, dst + stride2, 0, 2);
+        __lsx_vstelm_w(vec3, dst + stride3, 0, 3);
+        dst += stride4;
+        __lsx_vstelm_w(vec4, dst, 0, 0);
+        __lsx_vstelm_w(vec4, dst + stride, 0, 1);
+        __lsx_vstelm_w(vec4, dst + stride2, 0, 2);
+        __lsx_vstelm_w(vec4, dst + stride3, 0, 3);
+        dst += stride4;
+        __lsx_vstelm_w(vec5, dst, 0, 0);
+        __lsx_vstelm_w(vec5, dst + stride, 0, 1);
+        __lsx_vstelm_w(vec5, dst + stride2, 0, 2);
+        __lsx_vstelm_w(vec5, dst + stride3, 0, 3);
+    } else {
+        DUP4_ARG2(__lsx_vilvl_b, zero, p3, zero, p2, zero, p1, zero, p0,
+                  p3_l, p2_l, p1_l, p0_l);
+        DUP4_ARG2(__lsx_vilvl_b, zero, q0, zero, q1, zero, q2, zero, q3,
+                  q0_l, q1_l, q2_l, q3_l);
+        VP9_FILTER8(p3_l, p2_l, p1_l, p0_l, q0_l, q1_l, q2_l, q3_l, p2_filt8_l,
+                    p1_filt8_l, p0_filt8_l, q0_filt8_l, q1_filt8_l, q2_filt8_l);
+
+        /* convert 16 bit output data into 8 bit */
+        DUP4_ARG2(__lsx_vpickev_b, p2_filt8_l, p2_filt8_l, p1_filt8_l, p1_filt8_l,
+                  p0_filt8_l, p0_filt8_l, q0_filt8_l, q0_filt8_l, p2_filt8_l,
+                  p1_filt8_l, p0_filt8_l, q0_filt8_l);
+        DUP2_ARG2(__lsx_vpickev_b, q1_filt8_l, q1_filt8_l, q2_filt8_l, q2_filt8_l,
+                  q1_filt8_l, q2_filt8_l);
+
+        /* store pixel values */
+        p2 = __lsx_vbitsel_v(p2, p2_filt8_l, flat);
+        p1 = __lsx_vbitsel_v(p1_out, p1_filt8_l, flat);
+        p0 = __lsx_vbitsel_v(p0_out, p0_filt8_l, flat);
+        q0 = __lsx_vbitsel_v(q0_out, q0_filt8_l, flat);
+        q1 = __lsx_vbitsel_v(q1_out, q1_filt8_l, flat);
+        q2 = __lsx_vbitsel_v(q2, q2_filt8_l, flat);
+
+        DUP2_ARG2(__lsx_vilvl_b, p1, p2, q0, p0, vec0, vec1);
+        vec3 = __lsx_vilvl_h(vec1, vec0);
+        vec4 = __lsx_vilvh_h(vec1, vec0);
+        DUP2_ARG2(__lsx_vilvh_b, p1, p2, q0, p0, vec0, vec1);
+        vec6 = __lsx_vilvl_h(vec1, vec0);
+        vec7 = __lsx_vilvh_h(vec1, vec0);
+        vec2 = __lsx_vilvl_b(q2, q1);
+        vec5 = __lsx_vilvh_b(q2, q1);
+
+        dst -= 3;
+        __lsx_vstelm_w(vec3, dst, 0, 0);
+        __lsx_vstelm_h(vec2, dst, 4, 0);
+        dst += stride;
+        __lsx_vstelm_w(vec3, dst, 0, 1);
+        __lsx_vstelm_h(vec2, dst, 4, 1);
+        dst += stride;
+        __lsx_vstelm_w(vec3, dst, 0, 2);
+        __lsx_vstelm_h(vec2, dst, 4, 2);
+        dst += stride;
+        __lsx_vstelm_w(vec3, dst, 0, 3);
+        __lsx_vstelm_h(vec2, dst, 4, 3);
+        dst += stride;
+        __lsx_vstelm_w(vec4, dst, 0, 0);
+        __lsx_vstelm_h(vec2, dst, 4, 4);
+        dst += stride;
+        __lsx_vstelm_w(vec4, dst, 0, 1);
+        __lsx_vstelm_h(vec2, dst, 4, 5);
+        dst += stride;
+        __lsx_vstelm_w(vec4, dst, 0, 2);
+        __lsx_vstelm_h(vec2, dst, 4, 6);
+        dst += stride;
+        __lsx_vstelm_w(vec4, dst, 0, 3);
+        __lsx_vstelm_h(vec2, dst, 4, 7);
+        dst += stride;
+        __lsx_vstelm_w(vec6, dst, 0, 0);
+        __lsx_vstelm_h(vec5, dst, 4, 0);
+        dst += stride;
+        __lsx_vstelm_w(vec6, dst, 0, 1);
+        __lsx_vstelm_h(vec5, dst, 4, 1);
+        dst += stride;
+        __lsx_vstelm_w(vec6, dst, 0, 2);
+        __lsx_vstelm_h(vec5, dst, 4, 2);
+        dst += stride;
+        __lsx_vstelm_w(vec6, dst, 0, 3);
+        __lsx_vstelm_h(vec5, dst, 4, 3);
+        dst += stride;
+        __lsx_vstelm_w(vec7, dst, 0, 0);
+        __lsx_vstelm_h(vec5, dst, 4, 4);
+        dst += stride;
+        __lsx_vstelm_w(vec7, dst, 0, 1);
+        __lsx_vstelm_h(vec5, dst, 4, 5);
+        dst += stride;
+        __lsx_vstelm_w(vec7, dst, 0, 2);
+        __lsx_vstelm_h(vec5, dst, 4, 6);
+        dst += stride;
+        __lsx_vstelm_w(vec7, dst, 0, 3);
+        __lsx_vstelm_h(vec5, dst, 4, 7);
+    }
+}
+
+void ff_loop_filter_h_48_16_lsx(uint8_t *dst, ptrdiff_t stride,
+                                int32_t b_limit_ptr,
+                                int32_t limit_ptr,
+                                int32_t thresh_ptr)
+{
+    ptrdiff_t stride2 = stride << 1;
+    ptrdiff_t stride3 = stride2 + stride;
+    ptrdiff_t stride4 = stride2 << 1;
+    uint8_t *dst_tmp = dst - 4;
+    __m128i p3, p2, p1, p0, q3, q2, q1, q0;
+    __m128i p1_out, p0_out, q0_out, q1_out;
+    __m128i flat, mask, hev, thresh, b_limit, limit;
+    __m128i row4, row5, row6, row7, row12, row13, row14, row15;
+    __m128i p3_h, p2_h, p1_h, p0_h, q0_h, q1_h, q2_h, q3_h;
+    __m128i p2_filt8_h, p1_filt8_h, p0_filt8_h;
+    __m128i q0_filt8_h, q1_filt8_h, q2_filt8_h;
+    __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
+    __m128i zero = __lsx_vldi(0);
+
+    p0 = __lsx_vld(dst_tmp, 0);
+    DUP2_ARG2(__lsx_vldx, dst_tmp, stride, dst_tmp, stride2, p1, p2);
+    p3 = __lsx_vldx(dst_tmp, stride3);
+    dst_tmp += stride4;
+    row4 = __lsx_vld(dst_tmp, 0);
+    DUP2_ARG2(__lsx_vldx, dst_tmp, stride, dst_tmp, stride2, row5, row6);
+    row7 = __lsx_vldx(dst_tmp, stride3);
+    dst_tmp += stride4;
+    q3 = __lsx_vld(dst_tmp, 0);
+    DUP2_ARG2(__lsx_vldx, dst_tmp, stride, dst_tmp, stride2, q2, q1);
+    q0 = __lsx_vldx(dst_tmp, stride3);
+    dst_tmp += stride4;
+    row12 = __lsx_vld(dst_tmp, 0);
+    DUP2_ARG2(__lsx_vldx, dst_tmp, stride, dst_tmp, stride2, row13, row14);
+    row15 = __lsx_vldx(dst_tmp, stride3);
+
+    /* transpose 16x8 matrix into 8x16 */
+    LSX_TRANSPOSE16x8_B(p0, p1, p2, p3, row4, row5, row6, row7,
+                        q3, q2, q1, q0, row12, row13, row14, row15,
+                        p3, p2, p1, p0, q0, q1, q2, q3);
+
+    thresh = __lsx_vreplgr2vr_b(thresh_ptr);
+    vec0   = __lsx_vreplgr2vr_b(thresh_ptr >> 8);
+    thresh = __lsx_vilvl_d(vec0, thresh);
+
+    b_limit = __lsx_vreplgr2vr_b(b_limit_ptr);
+    vec0    = __lsx_vreplgr2vr_b(b_limit_ptr >> 8);
+    b_limit = __lsx_vilvl_d(vec0, b_limit);
+
+    limit = __lsx_vreplgr2vr_b(limit_ptr);
+    vec0  = __lsx_vreplgr2vr_b(limit_ptr >> 8);
+    limit = __lsx_vilvl_d(vec0, limit);
+
+    /* mask and hev */
+    LPF_MASK_HEV(p3, p2, p1, p0, q0, q1, q2, q3, limit, b_limit, thresh,
+                 hev, mask, flat);
+    /* flat4 */
+    VP9_FLAT4(p3, p2, p0, q0, q2, q3, flat);
+    /* filter4 */
+    VP9_LPF_FILTER4_4W(p1, p0, q0, q1, mask, hev, p1_out, p0_out, q0_out,
+                       q1_out);
+
+    flat = __lsx_vilvh_d(flat, zero);
+
+    /* if flat is zero for all pixels, then no need to calculate other filter */
+    if (__lsx_bz_v(flat)) {
+        DUP2_ARG2(__lsx_vilvl_b, p0_out, p1_out, q1_out, q0_out, vec0, vec1);
+        vec2 = __lsx_vilvl_h(vec1, vec0);
+        vec3 = __lsx_vilvh_h(vec1, vec0);
+        DUP2_ARG2(__lsx_vilvh_b, p0_out, p1_out, q1_out, q0_out, vec0, vec1);
+        vec4 = __lsx_vilvl_h(vec1, vec0);
+        vec5 = __lsx_vilvh_h(vec1, vec0);
+
+        dst -= 2;
+        __lsx_vstelm_w(vec2, dst, 0, 0);
+        __lsx_vstelm_w(vec2, dst + stride, 0, 1);
+        __lsx_vstelm_w(vec2, dst + stride2, 0, 2);
+        __lsx_vstelm_w(vec2, dst + stride3, 0, 3);
+        dst += stride4;
+        __lsx_vstelm_w(vec3, dst, 0, 0);
+        __lsx_vstelm_w(vec3, dst + stride, 0, 1);
+        __lsx_vstelm_w(vec3, dst + stride2, 0, 2);
+        __lsx_vstelm_w(vec3, dst + stride3, 0, 3);
+        dst += stride4;
+        __lsx_vstelm_w(vec4, dst, 0, 0);
+        __lsx_vstelm_w(vec4, dst + stride, 0, 1);
+        __lsx_vstelm_w(vec4, dst + stride2, 0, 2);
+        __lsx_vstelm_w(vec4, dst + stride3, 0, 3);
+        dst += stride4;
+        __lsx_vstelm_w(vec5, dst, 0, 0);
+        __lsx_vstelm_w(vec5, dst + stride, 0, 1);
+        __lsx_vstelm_w(vec5, dst + stride2, 0, 2);
+        __lsx_vstelm_w(vec5, dst + stride3, 0, 3);
+    } else {
+        DUP4_ARG2(__lsx_vilvh_b, zero, p3, zero, p2, zero, p1, zero, p0,
+                  p3_h, p2_h, p1_h, p0_h);
+        DUP4_ARG2(__lsx_vilvh_b, zero, q0, zero, q1, zero, q2, zero, q3,
+                  q0_h, q1_h, q2_h, q3_h);
+
+        VP9_FILTER8(p3_h, p2_h, p1_h, p0_h, q0_h, q1_h, q2_h, q3_h, p2_filt8_h,
+                    p1_filt8_h, p0_filt8_h, q0_filt8_h, q1_filt8_h, q2_filt8_h);
+
+        /* convert 16 bit output data into 8 bit */
+        DUP4_ARG2(__lsx_vpickev_b, p2_filt8_h, p2_filt8_h, p1_filt8_h,
+                  p1_filt8_h, p0_filt8_h, p0_filt8_h, q0_filt8_h, q0_filt8_h,
+                  p2_filt8_h, p1_filt8_h, p0_filt8_h, q0_filt8_h);
+        DUP2_ARG2(__lsx_vpickev_b, q1_filt8_h, q1_filt8_h, q2_filt8_h,
+                  q2_filt8_h, q1_filt8_h, q2_filt8_h);
+
+        /* store pixel values */
+        p2 = __lsx_vbitsel_v(p2, p2_filt8_h, flat);
+        p1 = __lsx_vbitsel_v(p1_out, p1_filt8_h, flat);
+        p0 = __lsx_vbitsel_v(p0_out, p0_filt8_h, flat);
+        q0 = __lsx_vbitsel_v(q0_out, q0_filt8_h, flat);
+        q1 = __lsx_vbitsel_v(q1_out, q1_filt8_h, flat);
+        q2 = __lsx_vbitsel_v(q2, q2_filt8_h, flat);
+
+        DUP2_ARG2(__lsx_vilvl_b, p1, p2, q0, p0, vec0, vec1);
+        vec3 = __lsx_vilvl_h(vec1, vec0);
+        vec4 = __lsx_vilvh_h(vec1, vec0);
+        DUP2_ARG2(__lsx_vilvh_b, p1, p2, q0, p0, vec0, vec1);
+        vec6 = __lsx_vilvl_h(vec1, vec0);
+        vec7 = __lsx_vilvh_h(vec1, vec0);
+        vec2 = __lsx_vilvl_b(q2, q1);
+        vec5 = __lsx_vilvh_b(q2, q1);
+
+        dst -= 3;
+        __lsx_vstelm_w(vec3, dst, 0, 0);
+        __lsx_vstelm_h(vec2, dst, 4, 0);
+        dst += stride;
+        __lsx_vstelm_w(vec3, dst, 0, 1);
+        __lsx_vstelm_h(vec2, dst, 4, 1);
+        dst += stride;
+        __lsx_vstelm_w(vec3, dst, 0, 2);
+        __lsx_vstelm_h(vec2, dst, 4, 2);
+        dst += stride;
+        __lsx_vstelm_w(vec3, dst, 0, 3);
+        __lsx_vstelm_h(vec2, dst, 4, 3);
+        dst += stride;
+        __lsx_vstelm_w(vec4, dst, 0, 0);
+        __lsx_vstelm_h(vec2, dst, 4, 4);
+        dst += stride;
+        __lsx_vstelm_w(vec4, dst, 0, 1);
+        __lsx_vstelm_h(vec2, dst, 4, 5);
+        dst += stride;
+        __lsx_vstelm_w(vec4, dst, 0, 2);
+        __lsx_vstelm_h(vec2, dst, 4, 6);
+        dst += stride;
+        __lsx_vstelm_w(vec4, dst, 0, 3);
+        __lsx_vstelm_h(vec2, dst, 4, 7);
+        dst += stride;
+        __lsx_vstelm_w(vec6, dst, 0, 0);
+        __lsx_vstelm_h(vec5, dst, 4, 0);
+        dst += stride;
+        __lsx_vstelm_w(vec6, dst, 0, 1);
+        __lsx_vstelm_h(vec5, dst, 4, 1);
+        dst += stride;
+        __lsx_vstelm_w(vec6, dst, 0, 2);
+        __lsx_vstelm_h(vec5, dst, 4, 2);
+        dst += stride;
+        __lsx_vstelm_w(vec6, dst, 0, 3);
+        __lsx_vstelm_h(vec5, dst, 4, 3);
+        dst += stride;
+        __lsx_vstelm_w(vec7, dst, 0, 0);
+        __lsx_vstelm_h(vec5, dst, 4, 4);
+        dst += stride;
+        __lsx_vstelm_w(vec7, dst, 0, 1);
+        __lsx_vstelm_h(vec5, dst, 4, 5);
+        dst += stride;
+        __lsx_vstelm_w(vec7, dst, 0, 2);
+        __lsx_vstelm_h(vec5, dst, 4, 6);
+        dst += stride;
+        __lsx_vstelm_w(vec7, dst, 0, 3);
+        __lsx_vstelm_h(vec5, dst, 4, 7);
+    }
+}
+
+static void vp9_transpose_16x8_to_8x16(uint8_t *input, ptrdiff_t in_pitch,
+                                       uint8_t *output)
+{
+    __m128i p7_org, p6_org, p5_org, p4_org, p3_org, p2_org, p1_org, p0_org;
+    __m128i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
+    __m128i p7, p6, p5, p4, p3, p2, p1, p0, q0, q1, q2, q3, q4, q5, q6, q7;
+    ptrdiff_t in_pitch2 = in_pitch << 1;
+    ptrdiff_t in_pitch3 = in_pitch2 + in_pitch;
+    ptrdiff_t in_pitch4 = in_pitch2 << 1;
+
+    LSX_LD_8(input, in_pitch, in_pitch2, in_pitch3, in_pitch4,
+             p7_org, p6_org, p5_org, p4_org, p3_org, p2_org, p1_org, p0_org);
+    /* 8x8 transpose */
+    LSX_TRANSPOSE8x8_B(p7_org, p6_org, p5_org, p4_org, p3_org, p2_org, p1_org,
+                       p0_org, p7, p6, p5, p4, p3, p2, p1, p0);
+    /* 8x8 transpose */
+    DUP4_ARG2(__lsx_vilvh_b, p5_org, p7_org, p4_org, p6_org, p1_org,
+              p3_org, p0_org, p2_org, tmp0, tmp1, tmp2, tmp3);
+    DUP2_ARG2(__lsx_vilvl_b, tmp1, tmp0, tmp3, tmp2, tmp4, tmp6);
+    DUP2_ARG2(__lsx_vilvh_b, tmp1, tmp0, tmp3, tmp2, tmp5, tmp7);
+    DUP2_ARG2(__lsx_vilvl_w, tmp6, tmp4, tmp7, tmp5, q0, q4);
+    DUP2_ARG2(__lsx_vilvh_w, tmp6, tmp4, tmp7, tmp5, q2, q6);
+    DUP4_ARG2(__lsx_vbsrl_v, q0, 8, q2, 8, q4, 8, q6, 8, q1, q3, q5, q7);
+
+    __lsx_vst(p7, output, 0);
+    __lsx_vst(p6, output, 16);
+    __lsx_vst(p5, output, 32);
+    __lsx_vst(p4, output, 48);
+    __lsx_vst(p3, output, 64);
+    __lsx_vst(p2, output, 80);
+    __lsx_vst(p1, output, 96);
+    __lsx_vst(p0, output, 112);
+    __lsx_vst(q0, output, 128);
+    __lsx_vst(q1, output, 144);
+    __lsx_vst(q2, output, 160);
+    __lsx_vst(q3, output, 176);
+    __lsx_vst(q4, output, 192);
+    __lsx_vst(q5, output, 208);
+    __lsx_vst(q6, output, 224);
+    __lsx_vst(q7, output, 240);
+}
+
+static void vp9_transpose_8x16_to_16x8(uint8_t *input, uint8_t *output,
+                                       ptrdiff_t out_pitch)
+{
+    __m128i p7_o, p6_o, p5_o, p4_o, p3_o, p2_o, p1_o, p0_o;
+    __m128i p7, p6, p5, p4, p3, p2, p1, p0, q0, q1, q2, q3, q4, q5, q6, q7;
+    ptrdiff_t out_pitch2 = out_pitch << 1;
+    ptrdiff_t out_pitch3 = out_pitch2 + out_pitch;
+    ptrdiff_t out_pitch4 = out_pitch2 << 1;
+
+    DUP4_ARG2(__lsx_vld, input, 0, input, 16, input, 32, input, 48,
+              p7, p6, p5, p4);
+    DUP4_ARG2(__lsx_vld, input, 64, input, 80, input, 96, input, 112,
+              p3, p2, p1, p0);
+    DUP4_ARG2(__lsx_vld, input, 128, input, 144, input, 160, input, 176,
+              q0, q1, q2, q3);
+    DUP4_ARG2(__lsx_vld, input, 192, input, 208, input, 224, input, 240,
+              q4, q5, q6, q7);
+    LSX_TRANSPOSE16x8_B(p7, p6, p5, p4, p3, p2, p1, p0, q0, q1, q2, q3, q4, q5,
+                        q6, q7, p7_o, p6_o, p5_o, p4_o, p3_o, p2_o, p1_o, p0_o);
+    LSX_ST_8(p7_o, p6_o, p5_o, p4_o, p3_o, p2_o, p1_o, p0_o,
+             output, out_pitch, out_pitch2, out_pitch3, out_pitch4);
+}
+
+static void vp9_transpose_16x16(uint8_t *input, int32_t in_stride,
+                                uint8_t *output, int32_t out_stride)
+{
+    __m128i row0, row1, row2, row3, row4, row5, row6, row7;
+    __m128i row8, row9, row10, row11, row12, row13, row14, row15;
+    __m128i tmp0, tmp1, tmp4, tmp5, tmp6, tmp7;
+    __m128i tmp2, tmp3;
+    __m128i p7, p6, p5, p4, p3, p2, p1, p0, q0, q1, q2, q3, q4, q5, q6, q7;
+    int32_t in_stride2 = in_stride << 1;
+    int32_t in_stride3 = in_stride2 + in_stride;
+    int32_t in_stride4 = in_stride2 << 1;
+    int32_t out_stride2 = out_stride << 1;
+    int32_t out_stride3 = out_stride2 + out_stride;
+    int32_t out_stride4 = out_stride2 << 1;
+
+    LSX_LD_8(input, in_stride, in_stride2, in_stride3, in_stride4,
+             row0, row1, row2, row3, row4, row5, row6, row7);
+    input += in_stride4;
+    LSX_LD_8(input, in_stride, in_stride2, in_stride3, in_stride4,
+             row8, row9, row10, row11, row12, row13, row14, row15);
+
+    LSX_TRANSPOSE16x8_B(row0, row1, row2, row3, row4, row5, row6, row7,
+                        row8, row9, row10, row11, row12, row13, row14, row15,
+                        p7, p6, p5, p4, p3, p2, p1, p0);
+
+    /* transpose 16x8 matrix into 8x16 */
+    /* total 8 intermediate register and 32 instructions */
+    q7 = __lsx_vpackod_d(row8, row0);
+    q6 = __lsx_vpackod_d(row9, row1);
+    q5 = __lsx_vpackod_d(row10, row2);
+    q4 = __lsx_vpackod_d(row11, row3);
+    q3 = __lsx_vpackod_d(row12, row4);
+    q2 = __lsx_vpackod_d(row13, row5);
+    q1 = __lsx_vpackod_d(row14, row6);
+    q0 = __lsx_vpackod_d(row15, row7);
+
+    DUP2_ARG2(__lsx_vpackev_b, q6, q7, q4, q5, tmp0, tmp1);
+    DUP2_ARG2(__lsx_vpackod_b, q6, q7, q4, q5, tmp4, tmp5);
+
+    DUP2_ARG2(__lsx_vpackev_b, q2, q3, q0, q1, q5, q7);
+    DUP2_ARG2(__lsx_vpackod_b, q2, q3, q0, q1, tmp6, tmp7);
+
+    DUP2_ARG2(__lsx_vpackev_h, tmp1, tmp0, q7, q5, tmp2, tmp3);
+    q0 = __lsx_vpackev_w(tmp3, tmp2);
+    q4 = __lsx_vpackod_w(tmp3, tmp2);
+
+    tmp2 = __lsx_vpackod_h(tmp1, tmp0);
+    tmp3 = __lsx_vpackod_h(q7, q5);
+    q2 = __lsx_vpackev_w(tmp3, tmp2);
+    q6 = __lsx_vpackod_w(tmp3, tmp2);
+
+    DUP2_ARG2(__lsx_vpackev_h, tmp5, tmp4, tmp7, tmp6, tmp2, tmp3);
+    q1 = __lsx_vpackev_w(tmp3, tmp2);
+    q5 = __lsx_vpackod_w(tmp3, tmp2);
+
+    tmp2 = __lsx_vpackod_h(tmp5, tmp4);
+    tmp3 = __lsx_vpackod_h(tmp7, tmp6);
+    q3 = __lsx_vpackev_w(tmp3, tmp2);
+    q7 = __lsx_vpackod_w(tmp3, tmp2);
+
+    LSX_ST_8(p7, p6, p5, p4, p3, p2, p1, p0, output, out_stride,
+             out_stride2, out_stride3, out_stride4);
+    output += out_stride4;
+    LSX_ST_8(q0, q1, q2, q3, q4, q5, q6, q7, output, out_stride,
+             out_stride2, out_stride3, out_stride4);
+}
+
+static int32_t vp9_vt_lpf_t4_and_t8_8w(uint8_t *src, uint8_t *filter48,
+                                       uint8_t *src_org, int32_t pitch_org,
+                                       int32_t b_limit_ptr,
+                                       int32_t limit_ptr,
+                                       int32_t thresh_ptr)
+{
+    __m128i p3, p2, p1, p0, q3, q2, q1, q0;
+    __m128i p2_out, p1_out, p0_out, q0_out, q1_out, q2_out;
+    __m128i flat, mask, hev, thresh, b_limit, limit;
+    __m128i p3_l, p2_l, p1_l, p0_l, q0_l, q1_l, q2_l, q3_l;
+    __m128i p2_filt8_l, p1_filt8_l, p0_filt8_l;
+    __m128i q0_filt8_l, q1_filt8_l, q2_filt8_l;
+    __m128i vec0, vec1, vec2, vec3;
+    __m128i zero = __lsx_vldi(0);
+
+    /* load vector elements */
+    DUP4_ARG2(__lsx_vld, src, -64, src, -48, src, -32, src, -16,
+              p3, p2, p1, p0);
+    DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48, q0, q1, q2, q3);
+
+    thresh  = __lsx_vreplgr2vr_b(thresh_ptr);
+    b_limit = __lsx_vreplgr2vr_b(b_limit_ptr);
+    limit   = __lsx_vreplgr2vr_b(limit_ptr);
+
+    /* mask and hev */
+    LPF_MASK_HEV(p3, p2, p1, p0, q0, q1, q2, q3, limit, b_limit, thresh,
+                 hev, mask, flat);
+    /* flat4 */
+    VP9_FLAT4(p3, p2, p0, q0, q2, q3, flat);
+    /* filter4 */
+    VP9_LPF_FILTER4_4W(p1, p0, q0, q1, mask, hev, p1_out, p0_out, q0_out,
+                       q1_out);
+
+    flat = __lsx_vilvl_d(zero, flat);
+
+    /* if flat is zero for all pixels, then no need to calculate other filter */
+    if (__lsx_bz_v(flat)) {
+        DUP2_ARG2(__lsx_vilvl_b, p0_out, p1_out, q1_out, q0_out, vec0, vec1);
+        vec2 = __lsx_vilvl_h(vec1, vec0);
+        vec3 = __lsx_vilvh_h(vec1, vec0);
+
+        src_org -= 2;
+        __lsx_vstelm_w(vec2, src_org, 0, 0);
+        src_org += pitch_org;
+        __lsx_vstelm_w(vec2, src_org, 0, 1);
+        src_org += pitch_org;
+        __lsx_vstelm_w(vec2, src_org, 0, 2);
+        src_org += pitch_org;
+        __lsx_vstelm_w(vec2, src_org, 0, 3);
+        src_org += pitch_org;
+        __lsx_vstelm_w(vec3, src_org, 0, 0);
+        src_org += pitch_org;
+        __lsx_vstelm_w(vec3, src_org, 0, 1);
+        src_org += pitch_org;
+        __lsx_vstelm_w(vec3, src_org, 0, 2);
+        src_org += pitch_org;
+        __lsx_vstelm_w(vec3, src_org, 0, 3);
+        return 1;
+    } else {
+        DUP4_ARG2(__lsx_vilvl_b, zero, p3, zero, p2, zero, p1, zero, p0,
+                  p3_l, p2_l, p1_l, p0_l);
+        DUP4_ARG2(__lsx_vilvl_b, zero, q0, zero, q1, zero, q2, zero, q3,
+                  q0_l, q1_l, q2_l, q3_l);
+        VP9_FILTER8(p3_l, p2_l, p1_l, p0_l, q0_l, q1_l, q2_l, q3_l, p2_filt8_l,
+                    p1_filt8_l, p0_filt8_l, q0_filt8_l, q1_filt8_l, q2_filt8_l);
+
+        /* convert 16 bit output data into 8 bit */
+        p2_l = __lsx_vpickev_b(p2_filt8_l, p2_filt8_l);
+        p1_l = __lsx_vpickev_b(p1_filt8_l, p1_filt8_l);
+        p0_l = __lsx_vpickev_b(p0_filt8_l, p0_filt8_l);
+        q0_l = __lsx_vpickev_b(q0_filt8_l, q0_filt8_l);
+        q1_l = __lsx_vpickev_b(q1_filt8_l, q1_filt8_l);
+        q2_l = __lsx_vpickev_b(q2_filt8_l, q2_filt8_l);
+
+        /* store pixel values */
+        p2_out = __lsx_vbitsel_v(p2, p2_l, flat);
+        p1_out = __lsx_vbitsel_v(p1_out, p1_l, flat);
+        p0_out = __lsx_vbitsel_v(p0_out, p0_l, flat);
+        q0_out = __lsx_vbitsel_v(q0_out, q0_l, flat);
+        q1_out = __lsx_vbitsel_v(q1_out, q1_l, flat);
+        q2_out = __lsx_vbitsel_v(q2, q2_l, flat);
+
+        __lsx_vst(p2_out, filter48, 0);
+        __lsx_vst(p1_out, filter48, 16);
+        __lsx_vst(p0_out, filter48, 32);
+        __lsx_vst(q0_out, filter48, 48);
+        __lsx_vst(q1_out, filter48, 64);
+        __lsx_vst(q2_out, filter48, 80);
+        __lsx_vst(flat, filter48, 96);
+
+        return 0;
+    }
+}
+
+static int32_t vp9_vt_lpf_t16_8w(uint8_t *dst, uint8_t *dst_org,
+                                 ptrdiff_t stride,
+                                 uint8_t *filter48)
+{
+    __m128i zero = __lsx_vldi(0);
+    __m128i filter8, flat, flat2;
+    __m128i p7, p6, p5, p4, p3, p2, p1, p0, q0, q1, q2, q3, q4, q5, q6, q7;
+    v8u16 p7_l_in, p6_l_in, p5_l_in, p4_l_in;
+    v8u16 p3_l_in, p2_l_in, p1_l_in, p0_l_in;
+    v8u16 q7_l_in, q6_l_in, q5_l_in, q4_l_in;
+    v8u16 q3_l_in, q2_l_in, q1_l_in, q0_l_in;
+    v8u16 tmp0_l, tmp1_l;
+    __m128i out_l;
+    uint8_t *dst_tmp = dst - 128;
+
+    /* load vector elements */
+    DUP4_ARG2(__lsx_vld, dst_tmp, 0, dst_tmp, 16, dst_tmp, 32,
+              dst_tmp, 48, p7, p6, p5, p4);
+    DUP4_ARG2(__lsx_vld, dst_tmp, 64, dst_tmp, 80, dst_tmp, 96,
+              dst_tmp, 112, p3, p2, p1, p0);
+    DUP4_ARG2(__lsx_vld, dst, 0, dst, 16, dst, 32, dst, 48, q0, q1, q2, q3);
+    DUP4_ARG2(__lsx_vld, dst, 64, dst, 80, dst, 96, dst, 112, q4, q5, q6, q7);
+
+    flat = __lsx_vld(filter48, 96);
+
+
+    VP9_FLAT5(p7, p6, p5, p4, p0, q0, q4, q5, q6, q7, flat, flat2);
+
+    /* if flat2 is zero for all pixels, then no need to calculate other filter */
+    if (__lsx_bz_v(flat2)) {
+        __m128i vec0, vec1, vec2, vec3, vec4;
+
+        DUP4_ARG2(__lsx_vld, filter48, 0, filter48, 16, filter48, 32,
+                  filter48, 48, p2, p1, p0, q0);
+        DUP2_ARG2(__lsx_vld, filter48, 64, filter48, 80, q1, q2);
+
+        DUP2_ARG2(__lsx_vilvl_b, p1, p2, q0, p0, vec0, vec1);
+        vec3 = __lsx_vilvl_h(vec1, vec0);
+        vec4 = __lsx_vilvh_h(vec1, vec0);
+        vec2 = __lsx_vilvl_b(q2, q1);
+
+        dst_org -= 3;
+        __lsx_vstelm_w(vec3, dst_org, 0, 0);
+        __lsx_vstelm_h(vec2, dst_org, 4, 0);
+        dst_org += stride;
+        __lsx_vstelm_w(vec3, dst_org, 0, 1);
+        __lsx_vstelm_h(vec2, dst_org, 4, 1);
+        dst_org += stride;
+        __lsx_vstelm_w(vec3, dst_org, 0, 2);
+        __lsx_vstelm_h(vec2, dst_org, 4, 2);
+        dst_org += stride;
+        __lsx_vstelm_w(vec3, dst_org, 0, 3);
+        __lsx_vstelm_h(vec2, dst_org, 4, 3);
+        dst_org += stride;
+        __lsx_vstelm_w(vec4, dst_org, 0, 0);
+        __lsx_vstelm_h(vec2, dst_org, 4, 4);
+        dst_org += stride;
+        __lsx_vstelm_w(vec4, dst_org, 0, 1);
+        __lsx_vstelm_h(vec2, dst_org, 4, 5);
+        dst_org += stride;
+        __lsx_vstelm_w(vec4, dst_org, 0, 2);
+        __lsx_vstelm_h(vec2, dst_org, 4, 6);
+        dst_org += stride;
+        __lsx_vstelm_w(vec4, dst_org, 0, 3);
+        __lsx_vstelm_h(vec2, dst_org, 4, 7);
+        return 1;
+    } else {
+        dst -= 7 * 16;
+
+        p7_l_in = (v8u16)__lsx_vilvl_b(zero, p7);
+        p6_l_in = (v8u16)__lsx_vilvl_b(zero, p6);
+        p5_l_in = (v8u16)__lsx_vilvl_b(zero, p5);
+        p4_l_in = (v8u16)__lsx_vilvl_b(zero, p4);
+        p3_l_in = (v8u16)__lsx_vilvl_b(zero, p3);
+        p2_l_in = (v8u16)__lsx_vilvl_b(zero, p2);
+        p1_l_in = (v8u16)__lsx_vilvl_b(zero, p1);
+        p0_l_in = (v8u16)__lsx_vilvl_b(zero, p0);
+        q0_l_in = (v8u16)__lsx_vilvl_b(zero, q0);
+
+        tmp0_l = p7_l_in << 3;
+        tmp0_l -= p7_l_in;
+        tmp0_l += p6_l_in;
+        tmp0_l += q0_l_in;
+        tmp1_l = p6_l_in + p5_l_in;
+        tmp1_l += p4_l_in;
+        tmp1_l += p3_l_in;
+        tmp1_l += p2_l_in;
+        tmp1_l += p1_l_in;
+        tmp1_l += p0_l_in;
+        tmp1_l += tmp0_l;
+
+        out_l =__lsx_vsrari_h((__m128i)tmp1_l, 4);
+        out_l =__lsx_vpickev_b(out_l, out_l);
+        p6 = __lsx_vbitsel_v(p6, out_l, flat2);
+        __lsx_vstelm_d(p6, dst, 0, 0);
+        dst += 16;
+
+        /* p5 */
+        q1_l_in = (v8u16)__lsx_vilvl_b(zero, q1);
+        tmp0_l = p5_l_in - p6_l_in;
+        tmp0_l += q1_l_in;
+        tmp0_l -= p7_l_in;
+        tmp1_l += tmp0_l;
+        out_l = __lsx_vsrari_h((__m128i)tmp1_l, 4);
+        out_l = __lsx_vpickev_b(out_l, out_l);
+        p5 = __lsx_vbitsel_v(p5, out_l, flat2);
+        __lsx_vstelm_d(p5, dst, 0, 0);
+        dst += 16;
+
+        /* p4 */
+        q2_l_in = (v8u16)__lsx_vilvl_b(zero, q2);
+        tmp0_l = p4_l_in - p5_l_in;
+        tmp0_l += q2_l_in;
+        tmp0_l -= p7_l_in;
+        tmp1_l += tmp0_l;
+        out_l = __lsx_vsrari_h((__m128i)tmp1_l, 4);
+        out_l = __lsx_vpickev_b(out_l, out_l);
+        p4 = __lsx_vbitsel_v(p4, out_l, flat2);
+        __lsx_vstelm_d(p4, dst, 0, 0);
+        dst += 16;
+
+        /* p3 */
+        q3_l_in = (v8u16)__lsx_vilvl_b(zero, q3);
+        tmp0_l = p3_l_in - p4_l_in;
+        tmp0_l += q3_l_in;
+        tmp0_l -= p7_l_in;
+        tmp1_l += tmp0_l;
+        out_l = __lsx_vsrari_h((__m128i)tmp1_l, 4);
+        out_l = __lsx_vpickev_b(out_l, out_l);
+        p3 = __lsx_vbitsel_v(p3, out_l, flat2);
+        __lsx_vstelm_d(p3, dst, 0, 0);
+        dst += 16;
+
+        /* p2 */
+        q4_l_in = (v8u16)__lsx_vilvl_b(zero, q4);
+        filter8 = __lsx_vld(filter48, 0);
+        tmp0_l = p2_l_in - p3_l_in;
+        tmp0_l += q4_l_in;
+        tmp0_l -= p7_l_in;
+        tmp1_l += tmp0_l;
+        out_l = __lsx_vsrari_h((__m128i)tmp1_l, 4);
+        out_l = __lsx_vpickev_b(out_l, out_l);
+        filter8 = __lsx_vbitsel_v(filter8, out_l, flat2);
+        __lsx_vstelm_d(filter8, dst, 0, 0);
+        dst += 16;
+
+        /* p1 */
+        q5_l_in = (v8u16)__lsx_vilvl_b(zero, q5);
+        filter8 = __lsx_vld(filter48, 16);
+        tmp0_l = p1_l_in - p2_l_in;
+        tmp0_l += q5_l_in;
+        tmp0_l -= p7_l_in;
+        tmp1_l += tmp0_l;
+        out_l = __lsx_vsrari_h((__m128i)tmp1_l, 4);
+        out_l = __lsx_vpickev_b(out_l, out_l);
+        filter8 = __lsx_vbitsel_v(filter8, out_l, flat2);
+        __lsx_vstelm_d(filter8, dst, 0, 0);
+        dst += 16;
+
+        /* p0 */
+        q6_l_in = (v8u16)__lsx_vilvl_b(zero, q6);
+        filter8 = __lsx_vld(filter48, 32);
+        tmp0_l = p0_l_in - p1_l_in;
+        tmp0_l += q6_l_in;
+        tmp0_l -= p7_l_in;
+        tmp1_l += tmp0_l;
+        out_l = __lsx_vsrari_h((__m128i)tmp1_l, 4);
+        out_l = __lsx_vpickev_b(out_l, out_l);
+        filter8 = __lsx_vbitsel_v(filter8, out_l, flat2);
+        __lsx_vstelm_d(filter8, dst, 0, 0);
+        dst += 16;
+
+        /* q0 */
+        q7_l_in = (v8u16)__lsx_vilvl_b(zero, q7);
+        filter8 = __lsx_vld(filter48, 48);
+        tmp0_l = q7_l_in - p0_l_in;
+        tmp0_l += q0_l_in;
+        tmp0_l -= p7_l_in;
+        tmp1_l += tmp0_l;
+        out_l = __lsx_vsrari_h((v8i16) tmp1_l, 4);
+        out_l = __lsx_vpickev_b(out_l, out_l);
+        filter8 = __lsx_vbitsel_v(filter8, out_l, flat2);
+        __lsx_vstelm_d(filter8, dst, 0, 0);
+        dst += 16;
+
+        /* q1 */
+        filter8 = __lsx_vld(filter48, 64);
+        tmp0_l = q7_l_in - q0_l_in;
+        tmp0_l += q1_l_in;
+        tmp0_l -= p6_l_in;
+        tmp1_l += tmp0_l;
+        out_l = __lsx_vsrari_h((__m128i)tmp1_l, 4);
+        out_l = __lsx_vpickev_b(out_l, out_l);
+        filter8 = __lsx_vbitsel_v(filter8, out_l, flat2);
+        __lsx_vstelm_d(filter8, dst, 0, 0);
+        dst += 16;
+
+        /* q2 */
+        filter8 = __lsx_vld(filter48, 80);
+        tmp0_l = q7_l_in - q1_l_in;
+        tmp0_l += q2_l_in;
+        tmp0_l -= p5_l_in;
+        tmp1_l += tmp0_l;
+        out_l = __lsx_vsrari_h((__m128i)tmp1_l, 4);
+        out_l = __lsx_vpickev_b(out_l, out_l);
+        filter8 = __lsx_vbitsel_v(filter8, out_l, flat2);
+        __lsx_vstelm_d(filter8, dst, 0, 0);
+        dst += 16;
+
+        /* q3 */
+        tmp0_l = q7_l_in - q2_l_in;
+        tmp0_l += q3_l_in;
+        tmp0_l -= p4_l_in;
+        tmp1_l += tmp0_l;
+        out_l = __lsx_vsrari_h((__m128i)tmp1_l, 4);
+        out_l = __lsx_vpickev_b(out_l, out_l);
+        q3 = __lsx_vbitsel_v(q3, out_l, flat2);
+        __lsx_vstelm_d(q3, dst, 0, 0);
+        dst += 16;
+
+        /* q4 */
+        tmp0_l = q7_l_in - q3_l_in;
+        tmp0_l += q4_l_in;
+        tmp0_l -= p3_l_in;
+        tmp1_l += tmp0_l;
+        out_l = __lsx_vsrari_h((__m128i)tmp1_l, 4);
+        out_l = __lsx_vpickev_b(out_l, out_l);
+        q4 = __lsx_vbitsel_v(q4, out_l, flat2);
+        __lsx_vstelm_d(q4, dst, 0, 0);
+        dst += 16;
+
+        /* q5 */
+        tmp0_l = q7_l_in - q4_l_in;
+        tmp0_l += q5_l_in;
+        tmp0_l -= p2_l_in;
+        tmp1_l += tmp0_l;
+        out_l = __lsx_vsrari_h((__m128i)tmp1_l, 4);
+        out_l = __lsx_vpickev_b(out_l, out_l);
+        q5 = __lsx_vbitsel_v(q5, out_l, flat2);
+        __lsx_vstelm_d(q5, dst, 0, 0);
+        dst += 16;
+
+        /* q6 */
+        tmp0_l = q7_l_in - q5_l_in;
+        tmp0_l += q6_l_in;
+        tmp0_l -= p1_l_in;
+        tmp1_l += tmp0_l;
+        out_l = __lsx_vsrari_h((__m128i)tmp1_l, 4);
+        out_l = __lsx_vpickev_b(out_l, out_l);
+        q6 = __lsx_vbitsel_v(q6, out_l, flat2);
+        __lsx_vstelm_d(q6, dst, 0, 0);
+
+        return 0;
+    }
+}
+
+void ff_loop_filter_h_16_8_lsx(uint8_t *dst, ptrdiff_t stride,
+                               int32_t b_limit_ptr,
+                               int32_t limit_ptr,
+                               int32_t thresh_ptr)
+{
+    uint8_t early_exit = 0;
+    uint8_t transposed_input[16 * 24] __attribute__ ((aligned(16)));
+    uint8_t *filter48 = &transposed_input[16 * 16];
+
+    vp9_transpose_16x8_to_8x16(dst - 8, stride, transposed_input);
+
+    early_exit = vp9_vt_lpf_t4_and_t8_8w((transposed_input + 16 * 8),
+                                         &filter48[0], dst, stride,
+                                         b_limit_ptr, limit_ptr, thresh_ptr);
+
+    if (0 == early_exit) {
+        early_exit = vp9_vt_lpf_t16_8w((transposed_input + 16 * 8), dst, stride,
+                                       &filter48[0]);
+
+        if (0 == early_exit) {
+            vp9_transpose_8x16_to_16x8(transposed_input, dst - 8, stride);
+        }
+    }
+}
+
+static int32_t vp9_vt_lpf_t4_and_t8_16w(uint8_t *dst, uint8_t *filter48,
+                                        uint8_t *dst_org, ptrdiff_t stride,
+                                        int32_t b_limit_ptr,
+                                        int32_t limit_ptr,
+                                        int32_t thresh_ptr)
+{
+    ptrdiff_t stride2 = stride << 1;
+    ptrdiff_t stride3 = stride2 + stride;
+    ptrdiff_t stride4 = stride2 << 1;
+    __m128i p3, p2, p1, p0, q3, q2, q1, q0;
+    __m128i p2_out, p1_out, p0_out, q0_out, q1_out, q2_out;
+    __m128i flat, mask, hev, thresh, b_limit, limit;
+    __m128i p3_l, p2_l, p1_l, p0_l, q0_l, q1_l, q2_l, q3_l;
+    __m128i p3_h, p2_h, p1_h, p0_h, q0_h, q1_h, q2_h, q3_h;
+    __m128i p2_filt8_l, p1_filt8_l, p0_filt8_l;
+    __m128i q0_filt8_l, q1_filt8_l, q2_filt8_l;
+    __m128i p2_filt8_h, p1_filt8_h, p0_filt8_h;
+    __m128i q0_filt8_h, q1_filt8_h, q2_filt8_h;
+    __m128i vec0, vec1, vec2, vec3, vec4, vec5;
+    __m128i zero = __lsx_vldi(0);
+
+    /* load vector elements */
+    DUP4_ARG2(__lsx_vld, dst, -64, dst, -48, dst, -32, dst, -16,
+              p3, p2, p1, p0);
+    DUP4_ARG2(__lsx_vld, dst, 0, dst, 16, dst, 32, dst, 48, q0, q1, q2, q3);
+
+    thresh  = __lsx_vreplgr2vr_b(thresh_ptr);
+    b_limit = __lsx_vreplgr2vr_b(b_limit_ptr);
+    limit   = __lsx_vreplgr2vr_b(limit_ptr);
+
+    /* mask and hev */
+    LPF_MASK_HEV(p3, p2, p1, p0, q0, q1, q2, q3, limit, b_limit, thresh,
+                 hev, mask, flat);
+    /* flat4 */
+    VP9_FLAT4(p3, p2, p0, q0, q2, q3, flat);
+    /* filter4 */
+    VP9_LPF_FILTER4_4W(p1, p0, q0, q1, mask, hev, p1_out, p0_out, q0_out,
+                       q1_out);
+
+    /* if flat is zero for all pixels, then no need to calculate other filter */
+    if (__lsx_bz_v(flat)) {
+        DUP2_ARG2(__lsx_vilvl_b, p0_out, p1_out, q1_out, q0_out, vec0, vec1);
+        vec2 = __lsx_vilvl_h(vec1, vec0);
+        vec3 = __lsx_vilvh_h(vec1, vec0);
+        DUP2_ARG2(__lsx_vilvh_b, p0_out, p1_out, q1_out, q0_out, vec0, vec1);
+        vec4 = __lsx_vilvl_h(vec1, vec0);
+        vec5 = __lsx_vilvh_h(vec1, vec0);
+
+        dst_org -= 2;
+        __lsx_vstelm_w(vec2, dst_org, 0, 0);
+        __lsx_vstelm_w(vec2, dst_org + stride, 0, 1);
+        __lsx_vstelm_w(vec2, dst_org + stride2, 0, 2);
+        __lsx_vstelm_w(vec2, dst_org + stride3, 0, 3);
+        dst_org += stride4;
+        __lsx_vstelm_w(vec3, dst_org, 0, 0);
+        __lsx_vstelm_w(vec3, dst_org + stride, 0, 1);
+        __lsx_vstelm_w(vec3, dst_org + stride2, 0, 2);
+        __lsx_vstelm_w(vec3, dst_org + stride3, 0, 3);
+        dst_org += stride4;
+        __lsx_vstelm_w(vec4, dst_org, 0, 0);
+        __lsx_vstelm_w(vec4, dst_org + stride, 0, 1);
+        __lsx_vstelm_w(vec4, dst_org + stride2, 0, 2);
+        __lsx_vstelm_w(vec4, dst_org + stride3, 0, 3);
+        dst_org += stride4;
+        __lsx_vstelm_w(vec5, dst_org, 0, 0);
+        __lsx_vstelm_w(vec5, dst_org + stride, 0, 1);
+        __lsx_vstelm_w(vec5, dst_org + stride2, 0, 2);
+        __lsx_vstelm_w(vec5, dst_org + stride3, 0, 3);
+
+        return 1;
+    } else {
+        DUP4_ARG2(__lsx_vilvl_b, zero, p3, zero, p2, zero, p1, zero, p0,
+                  p3_l, p2_l, p1_l, p0_l);
+        DUP4_ARG2(__lsx_vilvl_b, zero, q0, zero, q1, zero, q2, zero, q3,
+                  q0_l, q1_l, q2_l, q3_l);
+        VP9_FILTER8(p3_l, p2_l, p1_l, p0_l, q0_l, q1_l, q2_l, q3_l, p2_filt8_l,
+                    p1_filt8_l, p0_filt8_l, q0_filt8_l, q1_filt8_l, q2_filt8_l);
+        DUP4_ARG2(__lsx_vilvh_b, zero, p3, zero, p2, zero, p1, zero, p0,
+                  p3_h, p2_h, p1_h, p0_h);
+        DUP4_ARG2(__lsx_vilvh_b, zero, q0, zero, q1, zero, q2, zero, q3,
+                  q0_h, q1_h, q2_h, q3_h);
+        VP9_FILTER8(p3_h, p2_h, p1_h, p0_h, q0_h, q1_h, q2_h, q3_h, p2_filt8_h,
+                    p1_filt8_h, p0_filt8_h, q0_filt8_h, q1_filt8_h, q2_filt8_h);
+
+        /* convert 16 bit output data into 8 bit */
+        DUP4_ARG2(__lsx_vpickev_b, p2_filt8_h, p2_filt8_l, p1_filt8_h,
+                      p1_filt8_l, p0_filt8_h, p0_filt8_l, q0_filt8_h,
+                      q0_filt8_l, p2_filt8_l, p1_filt8_l, p0_filt8_l,
+                      q0_filt8_l);
+        DUP2_ARG2(__lsx_vpickev_b, q1_filt8_h, q1_filt8_l, q2_filt8_h,
+                  q2_filt8_l, q1_filt8_l, q2_filt8_l);
+
+        /* store pixel values */
+        p2_out = __lsx_vbitsel_v(p2, p2_filt8_l, flat);
+        p1_out = __lsx_vbitsel_v(p1_out, p1_filt8_l, flat);
+        p0_out = __lsx_vbitsel_v(p0_out, p0_filt8_l, flat);
+        q0_out = __lsx_vbitsel_v(q0_out, q0_filt8_l, flat);
+        q1_out = __lsx_vbitsel_v(q1_out, q1_filt8_l, flat);
+        q2_out = __lsx_vbitsel_v(q2, q2_filt8_l, flat);
+
+        __lsx_vst(p2_out, filter48, 0);
+        __lsx_vst(p1_out, filter48, 16);
+        __lsx_vst(p0_out, filter48, 32);
+        __lsx_vst(q0_out, filter48, 48);
+        __lsx_vst(q1_out, filter48, 64);
+        __lsx_vst(q2_out, filter48, 80);
+        __lsx_vst(flat, filter48, 96);
+
+        return 0;
+    }
+}
+
+static int32_t vp9_vt_lpf_t16_16w(uint8_t *dst, uint8_t *dst_org,
+                                  ptrdiff_t stride,
+                                  uint8_t *filter48)
+{
+    __m128i zero = __lsx_vldi(0);
+    __m128i flat, flat2, filter8;
+    __m128i p7, p6, p5, p4, p3, p2, p1, p0, q0, q1, q2, q3, q4, q5, q6, q7;
+    v8u16 p7_l_in, p6_l_in, p5_l_in, p4_l_in;
+    v8u16 p3_l_in, p2_l_in, p1_l_in, p0_l_in;
+    v8u16 q7_l_in, q6_l_in, q5_l_in, q4_l_in;
+    v8u16 q3_l_in, q2_l_in, q1_l_in, q0_l_in;
+    v8u16 p7_h_in, p6_h_in, p5_h_in, p4_h_in;
+    v8u16 p3_h_in, p2_h_in, p1_h_in, p0_h_in;
+    v8u16 q7_h_in, q6_h_in, q5_h_in, q4_h_in;
+    v8u16 q3_h_in, q2_h_in, q1_h_in, q0_h_in;
+    v8u16 tmp0_l, tmp1_l, tmp0_h, tmp1_h;
+    __m128i out_l, out_h;
+    uint8_t *dst_tmp = dst - 128;
+
+    flat = __lsx_vld(filter48, 96);
+
+    DUP4_ARG2(__lsx_vld, dst_tmp, 0, dst_tmp, 16, dst_tmp, 32,
+              dst_tmp, 48, p7, p6, p5, p4);
+    DUP4_ARG2(__lsx_vld, dst_tmp, 64, dst_tmp, 80, dst_tmp, 96,
+              dst_tmp, 112, p3, p2, p1, p0);
+    DUP4_ARG2(__lsx_vld, dst, 0, dst, 16, dst, 32, dst, 48, q0, q1, q2, q3);
+    DUP4_ARG2(__lsx_vld, dst, 64, dst, 80, dst, 96, dst, 112, q4, q5, q6, q7);
+
+    VP9_FLAT5(p7, p6, p5, p4, p0, q0, q4, q5, q6, q7, flat, flat2);
+
+    /* if flat2 is zero for all pixels, then no need to calculate other filter */
+    if (__lsx_bz_v(flat2)) {
+        __m128i vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
+
+        DUP4_ARG2(__lsx_vld, filter48, 0, filter48, 16, filter48, 32,
+                  filter48, 48, p2, p1, p0, q0);
+        DUP2_ARG2(__lsx_vld, filter48, 64, filter48, 80, q1, q2);
+
+        DUP2_ARG2(__lsx_vilvl_b, p1, p2, q0, p0, vec0, vec1);
+        vec3 = __lsx_vilvl_h(vec1, vec0);
+        vec4 = __lsx_vilvh_h(vec1, vec0);
+        DUP2_ARG2(__lsx_vilvh_b, p1, p2, q0, p0, vec0, vec1);
+        vec6 = __lsx_vilvl_h(vec1, vec0);
+        vec7 = __lsx_vilvh_h(vec1, vec0);
+        vec2 = __lsx_vilvl_b(q2, q1);
+        vec5 = __lsx_vilvh_b(q2, q1);
+
+        dst_org -= 3;
+        __lsx_vstelm_w(vec3, dst_org, 0, 0);
+        __lsx_vstelm_h(vec2, dst_org, 4, 0);
+        dst_org += stride;
+        __lsx_vstelm_w(vec3, dst_org, 0, 1);
+        __lsx_vstelm_h(vec2, dst_org, 4, 1);
+        dst_org += stride;
+        __lsx_vstelm_w(vec3, dst_org, 0, 2);
+        __lsx_vstelm_h(vec2, dst_org, 4, 2);
+        dst_org += stride;
+        __lsx_vstelm_w(vec3, dst_org, 0, 3);
+        __lsx_vstelm_h(vec2, dst_org, 4, 3);
+        dst_org += stride;
+        __lsx_vstelm_w(vec4, dst_org, 0, 0);
+        __lsx_vstelm_h(vec2, dst_org, 4, 4);
+        dst_org += stride;
+        __lsx_vstelm_w(vec4, dst_org, 0, 1);
+        __lsx_vstelm_h(vec2, dst_org, 4, 5);
+        dst_org += stride;
+        __lsx_vstelm_w(vec4, dst_org, 0, 2);
+        __lsx_vstelm_h(vec2, dst_org, 4, 6);
+        dst_org += stride;
+        __lsx_vstelm_w(vec4, dst_org, 0, 3);
+        __lsx_vstelm_h(vec2, dst_org, 4, 7);
+        dst_org += stride;
+        __lsx_vstelm_w(vec6, dst_org, 0, 0);
+        __lsx_vstelm_h(vec5, dst_org, 4, 0);
+        dst_org += stride;
+        __lsx_vstelm_w(vec6, dst_org, 0, 1);
+        __lsx_vstelm_h(vec5, dst_org, 4, 1);
+        dst_org += stride;
+        __lsx_vstelm_w(vec6, dst_org, 0, 2);
+        __lsx_vstelm_h(vec5, dst_org, 4, 2);
+        dst_org += stride;
+        __lsx_vstelm_w(vec6, dst_org, 0, 3);
+        __lsx_vstelm_h(vec5, dst_org, 4, 3);
+        dst_org += stride;
+        __lsx_vstelm_w(vec7, dst_org, 0, 0);
+        __lsx_vstelm_h(vec5, dst_org, 4, 4);
+        dst_org += stride;
+        __lsx_vstelm_w(vec7, dst_org, 0, 1);
+        __lsx_vstelm_h(vec5, dst_org, 4, 5);
+        dst_org += stride;
+        __lsx_vstelm_w(vec7, dst_org, 0, 2);
+        __lsx_vstelm_h(vec5, dst_org, 4, 6);
+        dst_org += stride;
+        __lsx_vstelm_w(vec7, dst_org, 0, 3);
+        __lsx_vstelm_h(vec5, dst_org, 4, 7);
+
+        return 1;
+    } else {
+        dst -= 7 * 16;
+
+        p7_l_in = (v8u16)__lsx_vilvl_b(zero, p7);
+        p6_l_in = (v8u16)__lsx_vilvl_b(zero, p6);
+        p5_l_in = (v8u16)__lsx_vilvl_b(zero, p5);
+        p4_l_in = (v8u16)__lsx_vilvl_b(zero, p4);
+        p3_l_in = (v8u16)__lsx_vilvl_b(zero, p3);
+        p2_l_in = (v8u16)__lsx_vilvl_b(zero, p2);
+        p1_l_in = (v8u16)__lsx_vilvl_b(zero, p1);
+        p0_l_in = (v8u16)__lsx_vilvl_b(zero, p0);
+        q0_l_in = (v8u16)__lsx_vilvl_b(zero, q0);
+
+        tmp0_l = p7_l_in << 3;
+        tmp0_l -= p7_l_in;
+        tmp0_l += p6_l_in;
+        tmp0_l += q0_l_in;
+        tmp1_l = p6_l_in + p5_l_in;
+        tmp1_l += p4_l_in;
+        tmp1_l += p3_l_in;
+        tmp1_l += p2_l_in;
+        tmp1_l += p1_l_in;
+        tmp1_l += p0_l_in;
+        tmp1_l += tmp0_l;
+        out_l = __lsx_vsrari_h((__m128i)tmp1_l, 4);
+
+        p7_h_in = (v8u16)__lsx_vilvh_b(zero, p7);
+        p6_h_in = (v8u16)__lsx_vilvh_b(zero, p6);
+        p5_h_in = (v8u16)__lsx_vilvh_b(zero, p5);
+        p4_h_in = (v8u16)__lsx_vilvh_b(zero, p4);
+        p3_h_in = (v8u16)__lsx_vilvh_b(zero, p3);
+        p2_h_in = (v8u16)__lsx_vilvh_b(zero, p2);
+        p1_h_in = (v8u16)__lsx_vilvh_b(zero, p1);
+        p0_h_in = (v8u16)__lsx_vilvh_b(zero, p0);
+        q0_h_in = (v8u16)__lsx_vilvh_b(zero, q0);
+
+        tmp0_h = p7_h_in << 3;
+        tmp0_h -= p7_h_in;
+        tmp0_h += p6_h_in;
+        tmp0_h += q0_h_in;
+        tmp1_h = p6_h_in + p5_h_in;
+        tmp1_h += p4_h_in;
+        tmp1_h += p3_h_in;
+        tmp1_h += p2_h_in;
+        tmp1_h += p1_h_in;
+        tmp1_h += p0_h_in;
+        tmp1_h += tmp0_h;
+        out_h = __lsx_vsrari_h((__m128i)tmp1_h, 4);
+
+        out_l = __lsx_vpickev_b(out_h, out_l);
+        p6 = __lsx_vbitsel_v(p6, out_l, flat2);
+        __lsx_vst(p6, dst, 0);
+
+        /* p5 */
+        q1_l_in = (v8u16)__lsx_vilvl_b(zero, q1);
+        tmp0_l = p5_l_in - p6_l_in;
+        tmp0_l += q1_l_in;
+        tmp0_l -= p7_l_in;
+        tmp1_l += tmp0_l;
+        out_l = __lsx_vsrari_h((__m128i)tmp1_l, 4);
+        q1_h_in = (v8u16)__lsx_vilvh_b(zero, q1);
+        tmp0_h = p5_h_in - p6_h_in;
+        tmp0_h += q1_h_in;
+        tmp0_h -= p7_h_in;
+        tmp1_h += tmp0_h;
+        out_h = __lsx_vsrari_h((__m128i)tmp1_h, 4);
+        out_l = __lsx_vpickev_b(out_h, out_l);
+        p5 = __lsx_vbitsel_v(p5, out_l, flat2);
+        __lsx_vst(p5, dst, 16);
+
+        /* p4 */
+        q2_l_in = (v8u16)__lsx_vilvl_b(zero, q2);
+        tmp0_l = p4_l_in - p5_l_in;
+        tmp0_l += q2_l_in;
+        tmp0_l -= p7_l_in;
+        tmp1_l += tmp0_l;
+        out_l = __lsx_vsrari_h((__m128i)tmp1_l, 4);
+        q2_h_in = (v8u16)__lsx_vilvh_b(zero, q2);
+        tmp0_h = p4_h_in - p5_h_in;
+        tmp0_h += q2_h_in;
+        tmp0_h -= p7_h_in;
+        tmp1_h += tmp0_h;
+        out_h = __lsx_vsrari_h((__m128i)tmp1_h, 4);
+        out_l = __lsx_vpickev_b(out_h, out_l);
+        p4 = __lsx_vbitsel_v(p4, out_l, flat2);
+        __lsx_vst(p4, dst, 16*2);
+
+        /* p3 */
+        q3_l_in = (v8u16)__lsx_vilvl_b(zero, q3);
+        tmp0_l = p3_l_in - p4_l_in;
+        tmp0_l += q3_l_in;
+        tmp0_l -= p7_l_in;
+        tmp1_l += tmp0_l;
+        out_l = __lsx_vsrari_h((__m128i)tmp1_l, 4);
+        q3_h_in = (v8u16)__lsx_vilvh_b(zero, q3);
+        tmp0_h = p3_h_in - p4_h_in;
+        tmp0_h += q3_h_in;
+        tmp0_h -= p7_h_in;
+        tmp1_h += tmp0_h;
+        out_h = __lsx_vsrari_h((__m128i)tmp1_h, 4);
+        out_l = __lsx_vpickev_b(out_h, out_l);
+        p3 = __lsx_vbitsel_v(p3, out_l, flat2);
+        __lsx_vst(p3, dst, 16*3);
+
+        /* p2 */
+        q4_l_in = (v8u16)__lsx_vilvl_b(zero, q4);
+        filter8 = __lsx_vld(filter48, 0);
+        tmp0_l = p2_l_in - p3_l_in;
+        tmp0_l += q4_l_in;
+        tmp0_l -= p7_l_in;
+        tmp1_l += tmp0_l;
+        out_l = __lsx_vsrari_h((__m128i)tmp1_l, 4);
+        q4_h_in = (v8u16)__lsx_vilvh_b(zero, q4);
+        tmp0_h = p2_h_in - p3_h_in;
+        tmp0_h += q4_h_in;
+        tmp0_h -= p7_h_in;
+        tmp1_h += tmp0_h;
+        out_h = __lsx_vsrari_h((__m128i)tmp1_h, 4);
+        out_l = __lsx_vpickev_b(out_h, out_l);
+        filter8 = __lsx_vbitsel_v(filter8, out_l, flat2);
+        __lsx_vst(filter8, dst, 16*4);
+
+        /* p1 */
+        q5_l_in = (v8u16)__lsx_vilvl_b(zero, q5);
+        filter8 = __lsx_vld(filter48, 16);
+        tmp0_l = p1_l_in - p2_l_in;
+        tmp0_l += q5_l_in;
+        tmp0_l -= p7_l_in;
+        tmp1_l += tmp0_l;
+        out_l = __lsx_vsrari_h((__m128i)tmp1_l, 4);
+        q5_h_in = (v8u16)__lsx_vilvh_b(zero, q5);
+        tmp0_h = p1_h_in - p2_h_in;
+        tmp0_h += q5_h_in;
+        tmp0_h -= p7_h_in;
+        tmp1_h += tmp0_h;
+        out_h = __lsx_vsrari_h((__m128i)(tmp1_h), 4);
+        out_l = __lsx_vpickev_b(out_h, out_l);
+        filter8 = __lsx_vbitsel_v(filter8, out_l, flat2);
+        __lsx_vst(filter8, dst, 16*5);
+
+        /* p0 */
+        q6_l_in = (v8u16)__lsx_vilvl_b(zero, q6);
+        filter8 = __lsx_vld(filter48, 32);
+        tmp0_l = p0_l_in - p1_l_in;
+        tmp0_l += q6_l_in;
+        tmp0_l -= p7_l_in;
+        tmp1_l += tmp0_l;
+        out_l = __lsx_vsrari_h((__m128i)tmp1_l, 4);
+        q6_h_in = (v8u16)__lsx_vilvh_b(zero, q6);
+        tmp0_h = p0_h_in - p1_h_in;
+        tmp0_h += q6_h_in;
+        tmp0_h -= p7_h_in;
+        tmp1_h += tmp0_h;
+        out_h = __lsx_vsrari_h((__m128i)tmp1_h, 4);
+        out_l = __lsx_vpickev_b(out_h, out_l);
+        filter8 = __lsx_vbitsel_v(filter8, out_l, flat2);
+        __lsx_vst(filter8, dst, 16*6);
+
+        /* q0 */
+        q7_l_in = (v8u16)__lsx_vilvl_b(zero, q7);
+        filter8 = __lsx_vld(filter48, 48);
+        tmp0_l = q7_l_in - p0_l_in;
+        tmp0_l += q0_l_in;
+        tmp0_l -= p7_l_in;
+        tmp1_l += tmp0_l;
+        out_l = __lsx_vsrari_h((__m128i)tmp1_l, 4);
+        q7_h_in = (v8u16)__lsx_vilvh_b(zero, q7);
+        tmp0_h = q7_h_in - p0_h_in;
+        tmp0_h += q0_h_in;
+        tmp0_h -= p7_h_in;
+        tmp1_h += tmp0_h;
+        out_h = __lsx_vsrari_h((__m128i)tmp1_h, 4);
+        out_l = __lsx_vpickev_b(out_h, out_l);
+        filter8 = __lsx_vbitsel_v(filter8, out_l, flat2);
+        __lsx_vst(filter8, dst, 16*7);
+
+        /* q1 */
+        filter8 = __lsx_vld(filter48, 64);
+        tmp0_l = q7_l_in - q0_l_in;
+        tmp0_l += q1_l_in;
+        tmp0_l -= p6_l_in;
+        tmp1_l += tmp0_l;
+        out_l = __lsx_vsrari_h((__m128i)tmp1_l, 4);
+        tmp0_h = q7_h_in - q0_h_in;
+        tmp0_h += q1_h_in;
+        tmp0_h -= p6_h_in;
+        tmp1_h += tmp0_h;
+        out_h = __lsx_vsrari_h((__m128i)tmp1_h, 4);
+        out_l = __lsx_vpickev_b(out_h, out_l);
+        filter8 = __lsx_vbitsel_v(filter8, out_l, flat2);
+        __lsx_vst(filter8, dst, 16*8);
+
+        /* q2 */
+        filter8 = __lsx_vld(filter48, 80);
+        tmp0_l = q7_l_in - q1_l_in;
+        tmp0_l += q2_l_in;
+        tmp0_l -= p5_l_in;
+        tmp1_l += tmp0_l;
+        out_l = __lsx_vsrari_h((__m128i)tmp1_l, 4);
+        tmp0_h = q7_h_in - q1_h_in;
+        tmp0_h += q2_h_in;
+        tmp0_h -= p5_h_in;
+        tmp1_h += tmp0_h;
+        out_h = __lsx_vsrari_h((__m128i)tmp1_h, 4);
+        out_l = __lsx_vpickev_b(out_h, out_l);
+        filter8 = __lsx_vbitsel_v(filter8, out_l, flat2);
+        __lsx_vst(filter8, dst, 16*9);
+
+        /* q3 */
+        tmp0_l = q7_l_in - q2_l_in;
+        tmp0_l += q3_l_in;
+        tmp0_l -= p4_l_in;
+        tmp1_l += tmp0_l;
+        out_l = __lsx_vsrari_h((__m128i)tmp1_l, 4);
+        tmp0_h = q7_h_in - q2_h_in;
+        tmp0_h += q3_h_in;
+        tmp0_h -= p4_h_in;
+        tmp1_h += tmp0_h;
+        out_h = __lsx_vsrari_h((__m128i)tmp1_h, 4);
+        out_l = __lsx_vpickev_b(out_h, out_l);
+        q3 = __lsx_vbitsel_v(q3, out_l, flat2);
+        __lsx_vst(q3, dst, 16*10);
+
+        /* q4 */
+        tmp0_l = q7_l_in - q3_l_in;
+        tmp0_l += q4_l_in;
+        tmp0_l -= p3_l_in;
+        tmp1_l += tmp0_l;
+        out_l = __lsx_vsrari_h((__m128i)tmp1_l, 4);
+        tmp0_h = q7_h_in - q3_h_in;
+        tmp0_h += q4_h_in;
+        tmp0_h -= p3_h_in;
+        tmp1_h += tmp0_h;
+        out_h = __lsx_vsrari_h((__m128i)tmp1_h, 4);
+        out_l = __lsx_vpickev_b(out_h, out_l);
+        q4 = __lsx_vbitsel_v(q4, out_l, flat2);
+        __lsx_vst(q4, dst, 16*11);
+
+        /* q5 */
+        tmp0_l = q7_l_in - q4_l_in;
+        tmp0_l += q5_l_in;
+        tmp0_l -= p2_l_in;
+        tmp1_l += tmp0_l;
+        out_l = __lsx_vsrari_h((__m128i)tmp1_l, 4);
+        tmp0_h = q7_h_in - q4_h_in;
+        tmp0_h += q5_h_in;
+        tmp0_h -= p2_h_in;
+        tmp1_h += tmp0_h;
+        out_h = __lsx_vsrari_h((__m128i)tmp1_h, 4);
+        out_l = __lsx_vpickev_b(out_h, out_l);
+        q5 = __lsx_vbitsel_v(q5, out_l, flat2);
+        __lsx_vst(q5, dst, 16*12);
+
+        /* q6 */
+        tmp0_l = q7_l_in - q5_l_in;
+        tmp0_l += q6_l_in;
+        tmp0_l -= p1_l_in;
+        tmp1_l += tmp0_l;
+        out_l = __lsx_vsrari_h((__m128i)tmp1_l, 4);
+        tmp0_h = q7_h_in - q5_h_in;
+        tmp0_h += q6_h_in;
+        tmp0_h -= p1_h_in;
+        tmp1_h += tmp0_h;
+        out_h = __lsx_vsrari_h((__m128i)tmp1_h, 4);
+        out_l = __lsx_vpickev_b(out_h, out_l);
+        q6 = __lsx_vbitsel_v(q6, out_l, flat2);
+        __lsx_vst(q6, dst, 16*13);
+
+        return 0;
+    }
+}
+
+void ff_loop_filter_h_16_16_lsx(uint8_t *dst, ptrdiff_t stride,
+                                int32_t b_limit_ptr,
+                                int32_t limit_ptr,
+                                int32_t thresh_ptr)
+{
+    uint8_t early_exit = 0;
+    uint8_t transposed_input[16 * 24] __attribute__ ((aligned(16)));
+    uint8_t *filter48 = &transposed_input[16 * 16];
+
+    vp9_transpose_16x16((dst - 8), stride, &transposed_input[0], 16);
+
+    early_exit = vp9_vt_lpf_t4_and_t8_16w((transposed_input + 16 * 8),
+                                          &filter48[0], dst, stride,
+                                          b_limit_ptr, limit_ptr, thresh_ptr);
+
+    if (0 == early_exit) {
+        early_exit = vp9_vt_lpf_t16_16w((transposed_input + 16 * 8), dst,
+                                         stride, &filter48[0]);
+
+        if (0 == early_exit) {
+            vp9_transpose_16x16(transposed_input, 16, (dst - 8), stride);
+        }
+    }
+}
diff --git a/libavcodec/loongarch/vp9_mc_lsx.c b/libavcodec/loongarch/vp9_mc_lsx.c
new file mode 100644
index 0000000000..c6746fd87f
--- /dev/null
+++ b/libavcodec/loongarch/vp9_mc_lsx.c
@@ -0,0 +1,2480 @@
+/*
+ * Copyright (c) 2021 Loongson Technology Corporation Limited
+ * Contributed by Hao Chen <chenhao@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "libavcodec/vp9dsp.h"
+#include "libavutil/loongarch/loongson_intrinsics.h"
+#include "vp9dsp_loongarch.h"
+
+static const uint8_t mc_filt_mask_arr[16 * 3] = {
+    /* 8 width cases */
+    0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8,
+    /* 4 width cases */
+    0, 1, 1, 2, 2, 3, 3, 4, 16, 17, 17, 18, 18, 19, 19, 20,
+    /* 4 width cases */
+    8, 9, 9, 10, 10, 11, 11, 12, 24, 25, 25, 26, 26, 27, 27, 28
+};
+
+
+#define HORIZ_8TAP_4WID_4VECS_FILT(_src0, _src1, _src2, _src3,                 \
+                                   _mask0, _mask1, _mask2, _mask3,             \
+                                   _filter0, _filter1, _filter2, _filter3,     \
+                                   _out0, _out1)                               \
+{                                                                              \
+    __m128i _tmp0, _tmp1, _tmp2, _tmp3, _tmp4, _tmp5, _tmp6, _tmp7;            \
+    __m128i _reg0, _reg1, _reg2, _reg3;                                        \
+                                                                               \
+    DUP2_ARG3(__lsx_vshuf_b, _src1, _src0, _mask0, _src3, _src2, _mask0,       \
+              _tmp0, _tmp1);                                                   \
+    DUP2_ARG2(__lsx_vdp2_h_b, _tmp0, _filter0, _tmp1, _filter0, _reg0, _reg1); \
+    DUP2_ARG3(__lsx_vshuf_b, _src1, _src0, _mask1, _src3, _src2, _mask1,       \
+               _tmp2, _tmp3);                                                  \
+    DUP2_ARG3(__lsx_vdp2add_h_b, _reg0, _tmp2, _filter1, _reg1, _tmp3,         \
+              _filter1, _reg0, _reg1);                                         \
+    DUP2_ARG3(__lsx_vshuf_b, _src1, _src0, _mask2, _src3, _src2, _mask2,       \
+               _tmp4, _tmp5);                                                  \
+    DUP2_ARG2(__lsx_vdp2_h_b, _tmp4, _filter2, _tmp5, _filter2, _reg2, _reg3); \
+    DUP2_ARG3(__lsx_vshuf_b, _src1, _src0, _mask3, _src3, _src2, _mask3,       \
+               _tmp6, _tmp7);                                                  \
+    DUP2_ARG3(__lsx_vdp2add_h_b, _reg2, _tmp6, _filter3, _reg3, _tmp7,         \
+              _filter3, _reg2, _reg3);                                         \
+    DUP2_ARG2(__lsx_vsadd_h, _reg0, _reg2, _reg1, _reg3, _out0, _out1);        \
+}
+
+#define HORIZ_8TAP_8WID_4VECS_FILT(_src0, _src1, _src2, _src3,                 \
+                                   _mask0, _mask1, _mask2, _mask3,             \
+                                   _filter0, _filter1, _filter2, _filter3,     \
+                                   _out0, _out1, _out2, _out3)                 \
+{                                                                              \
+    __m128i _tmp0, _tmp1, _tmp2, _tmp3, _tmp4, _tmp5, _tmp6, _tmp7;            \
+    __m128i _reg0, _reg1, _reg2, _reg3, _reg4, _reg5, _reg6, _reg7;            \
+                                                                               \
+    DUP4_ARG3(__lsx_vshuf_b, _src0, _src0, _mask0, _src1, _src1, _mask0, _src2,\
+              _src2, _mask0, _src3, _src3, _mask0, _tmp0, _tmp1, _tmp2, _tmp3);\
+    DUP4_ARG2(__lsx_vdp2_h_b, _tmp0, _filter0, _tmp1, _filter0, _tmp2,         \
+              _filter0, _tmp3, _filter0, _reg0, _reg1, _reg2, _reg3);          \
+    DUP4_ARG3(__lsx_vshuf_b, _src0, _src0, _mask2, _src1, _src1, _mask2, _src2,\
+              _src2, _mask2, _src3, _src3, _mask2, _tmp0, _tmp1, _tmp2, _tmp3);\
+    DUP4_ARG2(__lsx_vdp2_h_b, _tmp0, _filter2, _tmp1, _filter2, _tmp2,         \
+              _filter2, _tmp3, _filter2, _reg4, _reg5, _reg6, _reg7);          \
+    DUP4_ARG3(__lsx_vshuf_b, _src0, _src0, _mask1, _src1, _src1, _mask1, _src2,\
+              _src2, _mask1, _src3, _src3, _mask1, _tmp4, _tmp5, _tmp6, _tmp7);\
+    DUP4_ARG3(__lsx_vdp2add_h_b, _reg0, _tmp4, _filter1, _reg1, _tmp5,         \
+              _filter1, _reg2, _tmp6, _filter1, _reg3, _tmp7, _filter1, _reg0, \
+              _reg1, _reg2, _reg3);                                            \
+    DUP4_ARG3(__lsx_vshuf_b, _src0, _src0, _mask3, _src1, _src1, _mask3, _src2,\
+              _src2, _mask3, _src3, _src3, _mask3, _tmp4, _tmp5, _tmp6, _tmp7);\
+    DUP4_ARG3(__lsx_vdp2add_h_b, _reg4, _tmp4, _filter3, _reg5, _tmp5,         \
+              _filter3, _reg6, _tmp6, _filter3, _reg7, _tmp7, _filter3, _reg4, \
+              _reg5, _reg6, _reg7);                                            \
+    DUP4_ARG2(__lsx_vsadd_h, _reg0, _reg4, _reg1, _reg5, _reg2, _reg6, _reg3,  \
+              _reg7, _out0, _out1, _out2, _out3);                              \
+}
+
+#define FILT_8TAP_DPADD_S_H(_reg0, _reg1, _reg2, _reg3,                        \
+                             _filter0, _filter1, _filter2, _filter3)           \
+( {                                                                            \
+    __m128i _vec0, _vec1;                                                      \
+                                                                               \
+    _vec0 = __lsx_vdp2_h_b(_reg0, _filter0);                                   \
+    _vec0 = __lsx_vdp2add_h_b(_vec0, _reg1, _filter1);                         \
+    _vec1 = __lsx_vdp2_h_b(_reg2, _filter2);                                   \
+    _vec1 = __lsx_vdp2add_h_b(_vec1, _reg3, _filter3);                         \
+    _vec0 = __lsx_vsadd_h(_vec0, _vec1);                                       \
+                                                                               \
+    _vec0;                                                                     \
+} )
+
+#define HORIZ_8TAP_FILT(_src0, _src1, _mask0, _mask1, _mask2, _mask3,          \
+                        _filt_h0, _filt_h1, _filt_h2, _filt_h3)                \
+( {                                                                            \
+    __m128i _tmp0, _tmp1, _tmp2, _tmp3;                                        \
+    __m128i _out;                                                              \
+                                                                               \
+    DUP4_ARG3(__lsx_vshuf_b, _src1, _src0, _mask0, _src1, _src0, _mask1, _src1,\
+              _src0, _mask2, _src1, _src0, _mask3, _tmp0, _tmp1, _tmp2, _tmp3);\
+    _out = FILT_8TAP_DPADD_S_H(_tmp0, _tmp1, _tmp2, _tmp3, _filt_h0, _filt_h1, \
+                               _filt_h2, _filt_h3);                            \
+    _out = __lsx_vsrari_h(_out, 7);                                            \
+    _out = __lsx_vsat_h(_out, 7);                                              \
+                                                                               \
+    _out;                                                                      \
+} )
+
+#define LSX_LD_4(_src, _stride, _src0, _src1, _src2, _src3)               \
+{                                                                         \
+    _src0 = __lsx_vld(_src, 0);                                           \
+    _src += _stride;                                                      \
+    _src1 = __lsx_vld(_src, 0);                                           \
+    _src += _stride;                                                      \
+    _src2 = __lsx_vld(_src, 0);                                           \
+    _src += _stride;                                                      \
+    _src3 = __lsx_vld(_src, 0);                                           \
+}
+
+static void common_hz_8t_4x4_lsx(const uint8_t *src, int32_t src_stride,
+                                 uint8_t *dst, int32_t dst_stride,
+                                 const int8_t *filter)
+{
+    __m128i src0, src1, src2, src3;
+    __m128i filter0, filter1, filter2, filter3;
+    __m128i mask0, mask1, mask2, mask3;
+    __m128i out, out0, out1;
+
+    mask0 = __lsx_vld(mc_filt_mask_arr, 16);
+    src -= 3;
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filter0, filter1, filter2, filter3);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    mask3 = __lsx_vaddi_bu(mask0, 6);
+
+    LSX_LD_4(src, src_stride, src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+              src0, src1, src2, src3);
+    HORIZ_8TAP_4WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2,
+                     mask3, filter0, filter1, filter2, filter3, out0, out1);
+    out = __lsx_vssrarni_b_h(out1, out0, 7);
+    out = __lsx_vxori_b(out, 128);
+    __lsx_vstelm_w(out, dst, 0, 0);
+    dst += dst_stride;
+    __lsx_vstelm_w(out, dst, 0, 1);
+    dst += dst_stride;
+    __lsx_vstelm_w(out, dst, 0, 2);
+    dst += dst_stride;
+    __lsx_vstelm_w(out, dst, 0, 3);
+}
+
+static void common_hz_8t_4x8_lsx(const uint8_t *src, int32_t src_stride,
+                                 uint8_t *dst, int32_t dst_stride,
+                                 const int8_t *filter)
+{
+    int32_t src_stride2 = src_stride << 1;
+    int32_t src_stride3 = src_stride + src_stride2;
+    int32_t src_stride4 = src_stride2 << 1;
+    __m128i src0, src1, src2, src3;
+    __m128i filter0, filter1, filter2, filter3;
+    __m128i mask0, mask1, mask2, mask3;
+    __m128i out0, out1, out2, out3;
+    uint8_t *_src = (uint8_t*)src - 3;
+
+    mask0 = __lsx_vld(mc_filt_mask_arr, 16);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    mask3 = __lsx_vaddi_bu(mask0, 6);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filter0, filter1, filter2, filter3);
+
+    src0 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src1, src2);
+    src3 = __lsx_vldx(_src, src_stride3);
+    _src += src_stride4;
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+              src0, src1, src2, src3);
+    HORIZ_8TAP_4WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2,
+                     mask3, filter0, filter1, filter2, filter3, out0, out1);
+    src0 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src1, src2);
+    src3 = __lsx_vldx(_src, src_stride3);
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+              src0, src1, src2, src3);
+    HORIZ_8TAP_4WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2,
+                     mask3, filter0, filter1, filter2, filter3, out2, out3);
+    DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
+    DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
+    __lsx_vstelm_w(out0, dst, 0, 0);
+    dst += dst_stride;
+    __lsx_vstelm_w(out0, dst, 0, 1);
+    dst += dst_stride;
+    __lsx_vstelm_w(out0, dst, 0, 2);
+    dst += dst_stride;
+    __lsx_vstelm_w(out0, dst, 0, 3);
+    dst += dst_stride;
+    __lsx_vstelm_w(out1, dst, 0, 0);
+    dst += dst_stride;
+    __lsx_vstelm_w(out1, dst, 0, 1);
+    dst += dst_stride;
+    __lsx_vstelm_w(out1, dst, 0, 2);
+    dst += dst_stride;
+    __lsx_vstelm_w(out1, dst, 0, 3);
+}
+
+static void common_hz_8t_4w_lsx(const uint8_t *src, int32_t src_stride,
+                                uint8_t *dst, int32_t dst_stride,
+                                const int8_t *filter, int32_t height)
+{
+    if (height == 4) {
+        common_hz_8t_4x4_lsx(src, src_stride, dst, dst_stride, filter);
+    } else if (height == 8) {
+        common_hz_8t_4x8_lsx(src, src_stride, dst, dst_stride, filter);
+    }
+}
+
+static void common_hz_8t_8x4_lsx(const uint8_t *src, int32_t src_stride,
+                                 uint8_t *dst, int32_t dst_stride,
+                                 const int8_t *filter)
+{
+    __m128i src0, src1, src2, src3;
+    __m128i filter0, filter1, filter2, filter3;
+    __m128i mask0, mask1, mask2, mask3;
+    __m128i out0, out1, out2, out3;
+
+    mask0 = __lsx_vld(mc_filt_mask_arr, 0);
+    src -= 3;
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    mask3 = __lsx_vaddi_bu(mask0, 6);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filter0, filter1, filter2, filter3);
+
+    LSX_LD_4(src, src_stride, src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+              src0, src1, src2, src3);
+    HORIZ_8TAP_8WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2,
+         mask3, filter0, filter1, filter2, filter3, out0, out1, out2, out3);
+    DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
+    DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
+    __lsx_vstelm_d(out0, dst, 0, 0);
+    dst += dst_stride;
+    __lsx_vstelm_d(out0, dst, 0, 1);
+    dst += dst_stride;
+    __lsx_vstelm_d(out1, dst, 0, 0);
+    dst += dst_stride;
+    __lsx_vstelm_d(out1, dst, 0, 1);
+}
+
+static void common_hz_8t_8x8mult_lsx(const uint8_t *src, int32_t src_stride,
+                                     uint8_t *dst, int32_t dst_stride,
+                                     const int8_t *filter, int32_t height)
+{
+    uint32_t loop_cnt = height >> 2;
+    int32_t src_stride2 = src_stride << 1;
+    int32_t src_stride3 = src_stride + src_stride2;
+    int32_t src_stride4 = src_stride2 << 1;
+    __m128i src0, src1, src2, src3;
+    __m128i filter0, filter1, filter2, filter3;
+    __m128i mask0, mask1, mask2, mask3;
+    __m128i out0, out1, out2, out3;
+    uint8_t* _src = (uint8_t*)src - 3;
+
+    mask0 = __lsx_vld(mc_filt_mask_arr, 0);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    mask3 = __lsx_vaddi_bu(mask0, 6);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filter0, filter1, filter2, filter3);
+
+    for (; loop_cnt--;) {
+        src0 = __lsx_vld(_src, 0);
+        DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src1, src2);
+        src3 = __lsx_vldx(_src, src_stride3);
+        _src += src_stride4;
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+                  src0, src1, src2, src3);
+        HORIZ_8TAP_8WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2,
+             mask3, filter0, filter1, filter2, filter3, out0, out1, out2, out3);
+        DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
+        DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
+        __lsx_vstelm_d(out0, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(out0, dst, 0, 1);
+        dst += dst_stride;
+        __lsx_vstelm_d(out1, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(out1, dst, 0, 1);
+        dst += dst_stride;
+    }
+}
+
+static void common_hz_8t_8w_lsx(const uint8_t *src, int32_t src_stride,
+                                uint8_t *dst, int32_t dst_stride,
+                                const int8_t *filter, int32_t height)
+{
+    if (height == 4) {
+        common_hz_8t_8x4_lsx(src, src_stride, dst, dst_stride, filter);
+    } else {
+        common_hz_8t_8x8mult_lsx(src, src_stride, dst, dst_stride,
+                                 filter, height);
+    }
+}
+
+static void common_hz_8t_16w_lsx(const uint8_t *src, int32_t src_stride,
+                                 uint8_t *dst, int32_t dst_stride,
+                                 const int8_t *filter, int32_t height)
+{
+    uint32_t loop_cnt = height >> 1;
+    int32_t stride = src_stride << 1;
+    __m128i src0, src1, src2, src3;
+    __m128i filter0, filter1, filter2, filter3;
+    __m128i mask0, mask1, mask2, mask3;
+    __m128i out0, out1, out2, out3;
+
+    mask0 = __lsx_vld(mc_filt_mask_arr, 0);
+    src -= 3;
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    mask3 = __lsx_vaddi_bu(mask0, 6);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filter0, filter1, filter2, filter3);
+
+    for (; loop_cnt--;) {
+        const uint8_t* _src = src + src_stride;
+        DUP2_ARG2(__lsx_vld, src, 0, _src, 0, src0, src2);
+        DUP2_ARG2(__lsx_vld, src, 8, _src, 8, src1, src3);
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+                  src0, src1, src2, src3);
+        HORIZ_8TAP_8WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2,
+             mask3, filter0, filter1, filter2, filter3, out0, out1, out2, out3);
+        DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
+        DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
+        __lsx_vst(out0, dst, 0);
+        dst += dst_stride;
+        __lsx_vst(out1, dst, 0);
+        dst += dst_stride;
+        src += stride;
+    }
+}
+
+static void common_hz_8t_32w_lsx(const uint8_t *src, int32_t src_stride,
+                                 uint8_t *dst, int32_t dst_stride,
+                                 const int8_t *filter, int32_t height)
+{
+    uint32_t loop_cnt = height >> 1;
+    __m128i src0, src1, src2, src3;
+    __m128i filter0, filter1, filter2, filter3;
+    __m128i mask0, mask1, mask2, mask3;
+    __m128i out0, out1, out2, out3;
+    __m128i shuff = {0x0F0E0D0C0B0A0908, 0x1716151413121110};
+
+    mask0 = __lsx_vld(mc_filt_mask_arr, 0);
+    src -= 3;
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    mask3 = __lsx_vaddi_bu(mask0, 6);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filter0, filter1, filter2, filter3);
+
+    for (; loop_cnt--;) {
+        DUP2_ARG2(__lsx_vld, src, 0, src, 16, src0, src2);
+        src3 = __lsx_vld(src, 24);
+        src1 = __lsx_vshuf_b(src2, src0, shuff);
+        src += src_stride;
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+                  src0, src1, src2, src3);
+        HORIZ_8TAP_8WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2,
+             mask3, filter0, filter1, filter2, filter3, out0, out1, out2, out3);
+        DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
+        DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
+        __lsx_vst(out0, dst, 0);
+        __lsx_vst(out1, dst, 16);
+
+        DUP2_ARG2(__lsx_vld, src, 0, src, 16, src0, src2);
+        src3 = __lsx_vld(src, 24);
+        src1 = __lsx_vshuf_b(src2, src0, shuff);
+        src += src_stride;
+
+        dst += dst_stride;
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+                  src0, src1, src2, src3);
+        HORIZ_8TAP_8WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2,
+             mask3, filter0, filter1, filter2, filter3, out0, out1, out2, out3);
+        DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
+        DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
+        __lsx_vst(out0, dst, 0);
+        __lsx_vst(out1, dst, 16);
+        dst += dst_stride;
+    }
+}
+
+static void common_hz_8t_64w_lsx(const uint8_t *src, int32_t src_stride,
+                                 uint8_t *dst, int32_t dst_stride,
+                                 const int8_t *filter, int32_t height)
+{
+    int32_t loop_cnt = height;
+    __m128i src0, src1, src2, src3;
+    __m128i filter0, filter1, filter2, filter3;
+    __m128i mask0, mask1, mask2, mask3;
+    __m128i out0, out1, out2, out3;
+    __m128i shuff = {0x0F0E0D0C0B0A0908, 0x1716151413121110};
+
+    mask0 = __lsx_vld(mc_filt_mask_arr, 0);
+    src -= 3;
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    mask3 = __lsx_vaddi_bu(mask0, 6);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filter0, filter1, filter2, filter3);
+
+    for (; loop_cnt--;) {
+        DUP2_ARG2(__lsx_vld, src, 0, src, 16, src0, src2);
+        src3 = __lsx_vld(src, 24);
+        src1 = __lsx_vshuf_b(src2, src0, shuff);
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+                  src0, src1, src2, src3);
+        HORIZ_8TAP_8WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2,
+             mask3, filter0, filter1, filter2, filter3, out0, out1, out2, out3);
+        DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
+        DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
+        __lsx_vst(out0, dst, 0);
+        __lsx_vst(out1, dst, 16);
+
+        DUP2_ARG2(__lsx_vld, src, 32, src, 48, src0, src2);
+        src3 = __lsx_vld(src, 56);
+        src1 = __lsx_vshuf_b(src2, src0, shuff);
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+                  src0, src1, src2, src3);
+        HORIZ_8TAP_8WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2,
+             mask3, filter0, filter1, filter2, filter3, out0, out1, out2, out3);
+        DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
+        DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
+        __lsx_vst(out0, dst, 32);
+        __lsx_vst(out1, dst, 48);
+        src += src_stride;
+        dst += dst_stride;
+    }
+}
+
+static void common_vt_8t_4w_lsx(const uint8_t *src, int32_t src_stride,
+                                uint8_t *dst, int32_t dst_stride,
+                                const int8_t *filter, int32_t height)
+{
+    uint32_t loop_cnt = height >> 2;
+    int32_t src_stride2 = src_stride << 1;
+    int32_t src_stride3 = src_stride + src_stride2;
+    int32_t src_stride4 = src_stride2 << 1;
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8, src9, src10;
+    __m128i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5;
+    __m128i reg0, reg1, reg2, reg3, reg4;
+    __m128i filter0, filter1, filter2, filter3;
+    __m128i out0, out1;
+    uint8_t* _src = (uint8_t*)src - src_stride3;
+
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filter0, filter1, filter2, filter3);
+    src0 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src1, src2);
+    src3 = __lsx_vldx(_src, src_stride3);
+    _src += src_stride4;
+    src4 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src5, src6);
+    _src += src_stride3;
+    DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1, tmp0,
+              tmp1, tmp2, tmp3);
+    DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, tmp4, tmp5);
+    DUP2_ARG2(__lsx_vilvl_d, tmp3, tmp0, tmp4, tmp1, reg0, reg1);
+    reg2 = __lsx_vilvl_d(tmp5, tmp2);
+    DUP2_ARG2(__lsx_vxori_b, reg0, 128, reg1, 128, reg0, reg1);
+    reg2 = __lsx_vxori_b(reg2, 128);
+
+    for (;loop_cnt--;) {
+        src7 = __lsx_vld(_src, 0);
+        DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src8, src9);
+        src10 = __lsx_vldx(_src, src_stride3);
+        _src += src_stride4;
+        DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10,
+                  src9, tmp0, tmp1, tmp2, tmp3);
+        DUP2_ARG2(__lsx_vilvl_d, tmp1, tmp0, tmp3, tmp2, reg3, reg4);
+        DUP2_ARG2(__lsx_vxori_b, reg3, 128, reg4, 128, reg3, reg4);
+        out0 = FILT_8TAP_DPADD_S_H(reg0, reg1, reg2, reg3, filter0, filter1,
+                                   filter2, filter3);
+        out1 = FILT_8TAP_DPADD_S_H(reg1, reg2, reg3, reg4, filter0, filter1,
+                                   filter2, filter3);
+        out0 = __lsx_vssrarni_b_h(out1, out0, 7);
+        out0 = __lsx_vxori_b(out0, 128);
+        __lsx_vstelm_w(out0, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_w(out0, dst, 0, 1);
+        dst += dst_stride;
+        __lsx_vstelm_w(out0, dst, 0, 2);
+        dst += dst_stride;
+        __lsx_vstelm_w(out0, dst, 0, 3);
+        dst += dst_stride;
+
+        reg0 = reg2;
+        reg1 = reg3;
+        reg2 = reg4;
+        src6 = src10;
+    }
+}
+
+static void common_vt_8t_8w_lsx(const uint8_t *src, int32_t src_stride,
+                                uint8_t *dst, int32_t dst_stride,
+                                const int8_t *filter, int32_t height)
+{
+    uint32_t loop_cnt = height >> 2;
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8, src9, src10;
+    __m128i tmp0, tmp1, tmp2, tmp3;
+    __m128i reg0, reg1, reg2, reg3, reg4, reg5;
+    __m128i filter0, filter1, filter2, filter3;
+    __m128i out0, out1, out2, out3;
+    int32_t src_stride2 = src_stride << 1;
+    int32_t src_stride3 = src_stride + src_stride2;
+    int32_t src_stride4 = src_stride2 << 1;
+    uint8_t* _src = (uint8_t*)src - src_stride3;
+
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filter0, filter1, filter2, filter3);
+
+    src0 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src1, src2);
+    src3 = __lsx_vldx(_src, src_stride3);
+    _src += src_stride4;
+    src4 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src5, src6);
+    _src += src_stride3;
+
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+              src0, src1, src2, src3);
+    DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
+    src6 = __lsx_vxori_b(src6, 128);
+    DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1,
+              reg0, reg1, reg2, reg3);
+    DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, reg4, reg5);
+
+    for (;loop_cnt--;) {
+        src7 = __lsx_vld(_src, 0);
+        DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src8, src9);
+        src10 = __lsx_vldx(_src, src_stride3);
+        _src += src_stride4;
+        DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
+                  src7, src8, src9, src10);
+        DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10,
+                  src9, tmp0, tmp1, tmp2, tmp3);
+        out0 = FILT_8TAP_DPADD_S_H(reg0, reg1, reg2, tmp0, filter0, filter1,
+                                   filter2, filter3);
+        out1 = FILT_8TAP_DPADD_S_H(reg3, reg4, reg5, tmp1, filter0, filter1,
+                                   filter2, filter3);
+        out2 = FILT_8TAP_DPADD_S_H(reg1, reg2, tmp0, tmp2, filter0, filter1,
+                                   filter2, filter3);
+        out3 = FILT_8TAP_DPADD_S_H(reg4, reg5, tmp1, tmp3, filter0, filter1,
+                                   filter2, filter3);
+        DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
+        DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
+        __lsx_vstelm_d(out0, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(out0, dst, 0, 1);
+        dst += dst_stride;
+        __lsx_vstelm_d(out1, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(out1, dst, 0, 1);
+        dst += dst_stride;
+
+        reg0 = reg2;
+        reg1 = tmp0;
+        reg2 = tmp2;
+        reg3 = reg5;
+        reg4 = tmp1;
+        reg5 = tmp3;
+        src6 = src10;
+    }
+}
+
+static void common_vt_8t_16w_lsx(const uint8_t *src, int32_t src_stride,
+                                 uint8_t *dst, int32_t dst_stride,
+                                 const int8_t *filter, int32_t height)
+{
+    uint32_t loop_cnt = height >> 2;
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8, src9, src10;
+    __m128i filter0, filter1, filter2, filter3;
+    __m128i reg0, reg1, reg2, reg3, reg4, reg5;
+    __m128i reg6, reg7, reg8, reg9, reg10, reg11;
+    __m128i tmp0, tmp1, tmp2, tmp3;
+    int32_t src_stride2 = src_stride << 1;
+    int32_t src_stride3 = src_stride + src_stride2;
+    int32_t src_stride4 = src_stride2 << 1;
+    uint8_t* _src = (uint8_t*)src - src_stride3;
+
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filter0, filter1, filter2, filter3);
+    src0 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src1, src2);
+    src3 = __lsx_vldx(_src, src_stride3);
+    _src += src_stride4;
+    src4 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src5, src6);
+    _src += src_stride3;
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+              src0, src1, src2, src3);
+    DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
+    src6 = __lsx_vxori_b(src6, 128);
+    DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1,
+              reg0, reg1, reg2, reg3);
+    DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, reg4, reg5);
+    DUP4_ARG2(__lsx_vilvh_b, src1, src0, src3, src2, src5, src4, src2, src1,
+              reg6, reg7, reg8, reg9);
+    DUP2_ARG2(__lsx_vilvh_b, src4, src3, src6, src5, reg10, reg11);
+
+    for (;loop_cnt--;) {
+        src7 = __lsx_vld(_src, 0);
+        DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src8, src9);
+        src10 = __lsx_vldx(_src, src_stride3);
+        _src += src_stride4;
+        DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
+                  src7, src8, src9, src10);
+        DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10, src9,
+                  src0, src1, src2, src3);
+        DUP4_ARG2(__lsx_vilvh_b, src7, src6, src8, src7, src9, src8, src10, src9,
+                  src4, src5, src7, src8);
+        tmp0 = FILT_8TAP_DPADD_S_H(reg0, reg1, reg2, src0, filter0, filter1,
+                                   filter2, filter3);
+        tmp1 = FILT_8TAP_DPADD_S_H(reg3, reg4, reg5, src1, filter0, filter1,
+                                   filter2, filter3);
+        tmp2 = FILT_8TAP_DPADD_S_H(reg6, reg7, reg8, src4, filter0, filter1,
+                                   filter2, filter3);
+        tmp3 = FILT_8TAP_DPADD_S_H(reg9, reg10, reg11, src5, filter0, filter1,
+                                   filter2, filter3);
+        DUP2_ARG3(__lsx_vssrarni_b_h, tmp2, tmp0, 7, tmp3, tmp1, 7, tmp0, tmp1);
+        DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
+        __lsx_vst(tmp0, dst, 0);
+        dst += dst_stride;
+        __lsx_vst(tmp1, dst, 0);
+        dst += dst_stride;
+        tmp0 = FILT_8TAP_DPADD_S_H(reg1, reg2, src0, src2, filter0, filter1,
+                                   filter2, filter3);
+        tmp1 = FILT_8TAP_DPADD_S_H(reg4, reg5, src1, src3, filter0, filter1,
+                                   filter2, filter3);
+        tmp2 = FILT_8TAP_DPADD_S_H(reg7, reg8, src4, src7, filter0, filter1,
+                                   filter2, filter3);
+        tmp3 = FILT_8TAP_DPADD_S_H(reg10, reg11, src5, src8, filter0, filter1,
+                                   filter2, filter3);
+        DUP2_ARG3(__lsx_vssrarni_b_h, tmp2, tmp0, 7, tmp3, tmp1, 7, tmp0, tmp1);
+        DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
+        __lsx_vst(tmp0, dst, 0);
+        dst += dst_stride;
+        __lsx_vst(tmp1, dst, 0);
+        dst += dst_stride;
+
+        reg0 = reg2;
+        reg1 = src0;
+        reg2 = src2;
+        reg3 = reg5;
+        reg4 = src1;
+        reg5 = src3;
+        reg6 = reg8;
+        reg7 = src4;
+        reg8 = src7;
+        reg9 = reg11;
+        reg10 = src5;
+        reg11 = src8;
+        src6 = src10;
+    }
+}
+
+static void common_vt_8t_16w_mult_lsx(const uint8_t *src, int32_t src_stride,
+                                      uint8_t *dst, int32_t dst_stride,
+                                      const int8_t *filter, int32_t height,
+                                      int32_t width)
+{
+    uint8_t *src_tmp;
+    uint8_t *dst_tmp;
+    uint32_t cnt = width >> 4;
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8, src9, src10;
+    __m128i filter0, filter1, filter2, filter3;
+    __m128i reg0, reg1, reg2, reg3, reg4, reg5;
+    __m128i reg6, reg7, reg8, reg9, reg10, reg11;
+    __m128i tmp0, tmp1, tmp2, tmp3;
+    int32_t src_stride2 = src_stride << 1;
+    int32_t src_stride3 = src_stride + src_stride2;
+    int32_t src_stride4 = src_stride2 << 1;
+    int32_t dst_stride2 = dst_stride << 1;
+    int32_t dst_stride3 = dst_stride2 + dst_stride;
+    int32_t dst_stride4 = dst_stride2 << 1;
+    uint8_t* _src = (uint8_t*)src - src_stride3;
+
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filter0, filter1, filter2, filter3);
+    for (;cnt--;) {
+        uint32_t loop_cnt = height >> 2;
+
+        src_tmp = _src;
+        dst_tmp = dst;
+
+        src0 = __lsx_vld(src_tmp, 0);
+        DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride2,
+                  src1, src2);
+        src3 = __lsx_vldx(src_tmp, src_stride3);
+        src_tmp += src_stride4;
+        src4 = __lsx_vld(src_tmp, 0);
+        DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride2,
+                  src5, src6);
+        src_tmp += src_stride3;
+
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+                  src0, src1, src2, src3);
+        DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
+        src6 = __lsx_vxori_b(src6, 128);
+        DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1,
+                  reg0, reg1, reg2, reg3);
+        DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, reg4, reg5);
+        DUP4_ARG2(__lsx_vilvh_b, src1, src0, src3, src2, src5, src4, src2, src1,
+                  reg6, reg7, reg8, reg9);
+        DUP2_ARG2(__lsx_vilvh_b, src4, src3, src6, src5, reg10, reg11);
+
+        for (;loop_cnt--;) {
+            src7 = __lsx_vld(src_tmp, 0);
+            DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride2,
+                      src8, src9);
+            src10 = __lsx_vldx(src_tmp, src_stride3);
+            src_tmp += src_stride4;
+            DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10,
+                      128, src7, src8, src9, src10);
+            DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8,
+                      src10, src9, src0, src1, src2, src3);
+            DUP4_ARG2(__lsx_vilvh_b, src7, src6, src8, src7, src9, src8,
+                      src10, src9, src4, src5, src7, src8);
+            tmp0 = FILT_8TAP_DPADD_S_H(reg0, reg1, reg2, src0, filter0,
+                                       filter1, filter2, filter3);
+            tmp1 = FILT_8TAP_DPADD_S_H(reg3, reg4, reg5, src1, filter0,
+                                       filter1, filter2, filter3);
+            tmp2 = FILT_8TAP_DPADD_S_H(reg6, reg7, reg8, src4, filter0,
+                                       filter1, filter2, filter3);
+            tmp3 = FILT_8TAP_DPADD_S_H(reg9, reg10, reg11, src5, filter0,
+                                       filter1, filter2, filter3);
+            DUP2_ARG3(__lsx_vssrarni_b_h, tmp2, tmp0, 7, tmp3, tmp1, 7,
+                      tmp0, tmp1);
+            DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
+            __lsx_vst(tmp0, dst_tmp, 0);
+            __lsx_vstx(tmp1, dst_tmp, dst_stride);
+            tmp0 = FILT_8TAP_DPADD_S_H(reg1, reg2, src0, src2, filter0,
+                                       filter1, filter2, filter3);
+            tmp1 = FILT_8TAP_DPADD_S_H(reg4, reg5, src1, src3, filter0,
+                                       filter1, filter2, filter3);
+            tmp2 = FILT_8TAP_DPADD_S_H(reg7, reg8, src4, src7, filter0,
+                                       filter1, filter2, filter3);
+            tmp3 = FILT_8TAP_DPADD_S_H(reg10, reg11, src5, src8, filter0,
+                                       filter1, filter2, filter3);
+            DUP2_ARG3(__lsx_vssrarni_b_h, tmp2, tmp0, 7, tmp3, tmp1, 7,
+                      tmp0, tmp1);
+            DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
+            __lsx_vstx(tmp0, dst_tmp, dst_stride2);
+            __lsx_vstx(tmp1, dst_tmp, dst_stride3);
+            dst_tmp += dst_stride4;
+
+            reg0 = reg2;
+            reg1 = src0;
+            reg2 = src2;
+            reg3 = reg5;
+            reg4 = src1;
+            reg5 = src3;
+            reg6 = reg8;
+            reg7 = src4;
+            reg8 = src7;
+            reg9 = reg11;
+            reg10 = src5;
+            reg11 = src8;
+            src6 = src10;
+        }
+        _src += 16;
+        dst  += 16;
+    }
+}
+
+static void common_vt_8t_32w_lsx(const uint8_t *src, int32_t src_stride,
+                                 uint8_t *dst, int32_t dst_stride,
+                                 const int8_t *filter, int32_t height)
+{
+    common_vt_8t_16w_mult_lsx(src, src_stride, dst, dst_stride, filter, height, 32);
+}
+
+static void common_vt_8t_64w_lsx(const uint8_t *src, int32_t src_stride,
+                                 uint8_t *dst, int32_t dst_stride,
+                                 const int8_t *filter, int32_t height)
+{
+    common_vt_8t_16w_mult_lsx(src, src_stride, dst, dst_stride,
+                              filter, height, 64);
+}
+
+static void common_hv_8ht_8vt_4w_lsx(const uint8_t *src, int32_t src_stride,
+                                     uint8_t *dst, int32_t dst_stride,
+                                     const int8_t *filter_horiz,
+                                     const int8_t *filter_vert,
+                                     int32_t height)
+{
+    uint32_t loop_cnt = height >> 2;
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8, src9, src10;
+    __m128i filt_hz0, filt_hz1, filt_hz2, filt_hz3;
+    __m128i filt_vt0, filt_vt1, filt_vt2, filt_vt3;
+    __m128i mask0, mask1, mask2, mask3;
+    __m128i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5;
+    __m128i out0, out1;
+    __m128i shuff = {0x0F0E0D0C0B0A0908, 0x1716151413121110};
+    int32_t src_stride2 = src_stride << 1;
+    int32_t src_stride3 = src_stride + src_stride2;
+    int32_t src_stride4 = src_stride2 << 1;
+    uint8_t* _src = (uint8_t*)src - src_stride3 - 3;
+
+    mask0 = __lsx_vld(mc_filt_mask_arr, 16);
+    DUP4_ARG2(__lsx_vldrepl_h, filter_horiz, 0, filter_horiz, 2, filter_horiz, 4,
+              filter_horiz, 6, filt_hz0, filt_hz1, filt_hz2, filt_hz3);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    mask3 = __lsx_vaddi_bu(mask0, 6);
+
+    src0 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src1, src2);
+    src3 = __lsx_vldx(_src, src_stride3);
+    _src += src_stride4;
+    src4 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src5, src6);
+    _src += src_stride3;
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+              src0, src1, src2, src3);
+    DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
+    src6 = __lsx_vxori_b(src6, 128);
+
+    tmp0 = HORIZ_8TAP_FILT(src0, src1, mask0, mask1, mask2, mask3, filt_hz0,
+                           filt_hz1, filt_hz2, filt_hz3);
+    tmp2 = HORIZ_8TAP_FILT(src2, src3, mask0, mask1, mask2, mask3, filt_hz0,
+                           filt_hz1, filt_hz2, filt_hz3);
+    tmp4 = HORIZ_8TAP_FILT(src4, src5, mask0, mask1, mask2, mask3, filt_hz0,
+                           filt_hz1, filt_hz2, filt_hz3);
+    tmp5 = HORIZ_8TAP_FILT(src5, src6, mask0, mask1, mask2, mask3, filt_hz0,
+                           filt_hz1, filt_hz2, filt_hz3);
+    DUP2_ARG3(__lsx_vshuf_b, tmp2, tmp0, shuff, tmp4, tmp2, shuff, tmp1, tmp3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter_vert, 0, filter_vert, 2, filter_vert, 4,
+              filter_vert, 6, filt_vt0, filt_vt1, filt_vt2, filt_vt3);
+    DUP2_ARG2(__lsx_vpackev_b, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
+    tmp2 = __lsx_vpackev_b(tmp5, tmp4);
+
+    for (;loop_cnt--;) {
+        src7 = __lsx_vld(_src, 0);
+        DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src8, src9);
+        src10 = __lsx_vldx(_src, src_stride3);
+        _src += src_stride4;
+        DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
+                  src7, src8, src9, src10);
+        tmp3 = HORIZ_8TAP_FILT(src7, src8, mask0, mask1, mask2, mask3, filt_hz0,
+                               filt_hz1, filt_hz2, filt_hz3);
+        tmp4 = __lsx_vshuf_b(tmp3, tmp5, shuff);
+        tmp4 = __lsx_vpackev_b(tmp3, tmp4);
+        out0 = FILT_8TAP_DPADD_S_H(tmp0, tmp1, tmp2, tmp4, filt_vt0, filt_vt1,
+                                   filt_vt2, filt_vt3);
+        src1 = HORIZ_8TAP_FILT(src9, src10, mask0, mask1, mask2, mask3,
+                               filt_hz0, filt_hz1, filt_hz2, filt_hz3);
+        src0 = __lsx_vshuf_b(src1, tmp3, shuff);
+        src0 = __lsx_vpackev_b(src1, src0);
+        out1 = FILT_8TAP_DPADD_S_H(tmp1, tmp2, tmp4, src0, filt_vt0, filt_vt1,
+                                   filt_vt2, filt_vt3);
+        out0 = __lsx_vssrarni_b_h(out1, out0, 7);
+        out0 = __lsx_vxori_b(out0, 128);
+        __lsx_vstelm_w(out0, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_w(out0, dst, 0, 1);
+        dst += dst_stride;
+        __lsx_vstelm_w(out0, dst, 0, 2);
+        dst += dst_stride;
+        __lsx_vstelm_w(out0, dst, 0, 3);
+        dst += dst_stride;
+
+        tmp5 = src1;
+        tmp0 = tmp2;
+        tmp1 = tmp4;
+        tmp2 = src0;
+    }
+}
+
+static void common_hv_8ht_8vt_8w_lsx(const uint8_t *src, int32_t src_stride,
+                                     uint8_t *dst, int32_t dst_stride,
+                                     const int8_t *filter_horiz,
+                                     const int8_t *filter_vert,
+                                     int32_t height)
+{
+    uint32_t loop_cnt = height >> 2;
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8, src9, src10;
+    __m128i filt_hz0, filt_hz1, filt_hz2, filt_hz3;
+    __m128i filt_vt0, filt_vt1, filt_vt2, filt_vt3;
+    __m128i mask0, mask1, mask2, mask3;
+    __m128i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6;
+    __m128i out0, out1;
+    int32_t src_stride2 = src_stride << 1;
+    int32_t src_stride3 = src_stride + src_stride2;
+    int32_t src_stride4 = src_stride2 << 1;
+    uint8_t* _src = (uint8_t*)src - src_stride3 - 3;
+
+    mask0 = __lsx_vld(mc_filt_mask_arr, 0);
+    DUP4_ARG2(__lsx_vldrepl_h, filter_horiz, 0, filter_horiz, 2, filter_horiz,
+              4, filter_horiz, 6, filt_hz0, filt_hz1, filt_hz2, filt_hz3);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    mask3 = __lsx_vaddi_bu(mask0, 6);
+
+    src0 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src1, src2);
+    src3 = __lsx_vldx(_src, src_stride3);
+    _src += src_stride4;
+    src4 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src5, src6);
+    _src += src_stride3;
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+              src0, src1, src2, src3);
+    DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
+    src6 = __lsx_vxori_b(src6, 128);
+
+    src0 = HORIZ_8TAP_FILT(src0, src0, mask0, mask1, mask2, mask3, filt_hz0,
+                           filt_hz1, filt_hz2, filt_hz3);
+    src1 = HORIZ_8TAP_FILT(src1, src1, mask0, mask1, mask2, mask3, filt_hz0,
+                           filt_hz1, filt_hz2, filt_hz3);
+    src2 = HORIZ_8TAP_FILT(src2, src2, mask0, mask1, mask2, mask3, filt_hz0,
+                           filt_hz1, filt_hz2, filt_hz3);
+    src3 = HORIZ_8TAP_FILT(src3, src3, mask0, mask1, mask2, mask3, filt_hz0,
+                           filt_hz1, filt_hz2, filt_hz3);
+    src4 = HORIZ_8TAP_FILT(src4, src4, mask0, mask1, mask2, mask3, filt_hz0,
+                           filt_hz1, filt_hz2, filt_hz3);
+    src5 = HORIZ_8TAP_FILT(src5, src5, mask0, mask1, mask2, mask3, filt_hz0,
+                           filt_hz1, filt_hz2, filt_hz3);
+    src6 = HORIZ_8TAP_FILT(src6, src6, mask0, mask1, mask2, mask3, filt_hz0,
+                           filt_hz1, filt_hz2, filt_hz3);
+
+    DUP4_ARG2(__lsx_vldrepl_h, filter_vert, 0, filter_vert, 2, filter_vert, 4,
+              filter_vert, 6, filt_vt0, filt_vt1, filt_vt2, filt_vt3);
+    DUP4_ARG2(__lsx_vpackev_b, src1, src0, src3, src2, src5, src4,
+              src2, src1, tmp0, tmp1, tmp2, tmp4);
+    DUP2_ARG2(__lsx_vpackev_b, src4, src3, src6, src5, tmp5, tmp6);
+
+    for (;loop_cnt--;) {
+        src7 = __lsx_vld(_src, 0);
+        DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src8, src9);
+        src10 = __lsx_vldx(_src, src_stride3);
+        _src += src_stride4;
+
+        DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
+                  src7, src8, src9, src10);
+        src7 = HORIZ_8TAP_FILT(src7, src7, mask0, mask1, mask2, mask3, filt_hz0,
+                               filt_hz1, filt_hz2, filt_hz3);
+        tmp3 = __lsx_vpackev_b(src7, src6);
+        out0 = FILT_8TAP_DPADD_S_H(tmp0, tmp1, tmp2, tmp3, filt_vt0, filt_vt1,
+                                   filt_vt2, filt_vt3);
+        src8 = HORIZ_8TAP_FILT(src8, src8, mask0, mask1, mask2, mask3, filt_hz0,
+                               filt_hz1, filt_hz2, filt_hz3);
+        src0 = __lsx_vpackev_b(src8, src7);
+        out1 = FILT_8TAP_DPADD_S_H(tmp4, tmp5, tmp6, src0, filt_vt0, filt_vt1,
+                                   filt_vt2, filt_vt3);
+        src9 = HORIZ_8TAP_FILT(src9, src9, mask0, mask1, mask2, mask3, filt_hz0,
+                               filt_hz1, filt_hz2, filt_hz3);
+        src1 = __lsx_vpackev_b(src9, src8);
+        src3 = FILT_8TAP_DPADD_S_H(tmp1, tmp2, tmp3, src1, filt_vt0, filt_vt1,
+                                   filt_vt2, filt_vt3);
+        src10 = HORIZ_8TAP_FILT(src10, src10, mask0, mask1, mask2, mask3,
+                                filt_hz0, filt_hz1, filt_hz2, filt_hz3);
+        src2 = __lsx_vpackev_b(src10, src9);
+        src4 = FILT_8TAP_DPADD_S_H(tmp5, tmp6, src0, src2, filt_vt0, filt_vt1,
+                                   filt_vt2, filt_vt3);
+        DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, src4, src3, 7, out0, out1);
+        DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
+        __lsx_vstelm_d(out0, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(out0, dst, 0, 1);
+        dst += dst_stride;
+        __lsx_vstelm_d(out1, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(out1, dst, 0, 1);
+        dst += dst_stride;
+
+        src6 = src10;
+        tmp0 = tmp2;
+        tmp1 = tmp3;
+        tmp2 = src1;
+        tmp4 = tmp6;
+        tmp5 = src0;
+        tmp6 = src2;
+    }
+}
+
+static void common_hv_8ht_8vt_16w_lsx(const uint8_t *src, int32_t src_stride,
+                                      uint8_t *dst, int32_t dst_stride,
+                                      const int8_t *filter_horiz,
+                                      const int8_t *filter_vert,
+                                      int32_t height)
+{
+    int32_t multiple8_cnt;
+
+    for (multiple8_cnt = 2; multiple8_cnt--;) {
+        common_hv_8ht_8vt_8w_lsx(src, src_stride, dst, dst_stride, filter_horiz,
+                                 filter_vert, height);
+        src += 8;
+        dst += 8;
+    }
+}
+
+static void common_hv_8ht_8vt_32w_lsx(const uint8_t *src, int32_t src_stride,
+                                      uint8_t *dst, int32_t dst_stride,
+                                      const int8_t *filter_horiz,
+                                      const int8_t *filter_vert,
+                                      int32_t height)
+{
+    int32_t multiple8_cnt;
+
+    for (multiple8_cnt = 4; multiple8_cnt--;) {
+        common_hv_8ht_8vt_8w_lsx(src, src_stride, dst, dst_stride, filter_horiz,
+                                 filter_vert, height);
+        src += 8;
+        dst += 8;
+    }
+}
+
+static void common_hv_8ht_8vt_64w_lsx(const uint8_t *src, int32_t src_stride,
+                                      uint8_t *dst, int32_t dst_stride,
+                                      const int8_t *filter_horiz,
+                                      const int8_t *filter_vert,
+                                      int32_t height)
+{
+    int32_t multiple8_cnt;
+
+    for (multiple8_cnt = 8; multiple8_cnt--;) {
+        common_hv_8ht_8vt_8w_lsx(src, src_stride, dst, dst_stride, filter_horiz,
+                                 filter_vert, height);
+        src += 8;
+        dst += 8;
+    }
+}
+
+static void copy_width8_lsx(const uint8_t *src, int32_t src_stride,
+                            uint8_t *dst, int32_t dst_stride,
+                            int32_t height)
+{
+    int32_t cnt = height >> 2;
+    __m128i src0, src1, src2, src3;
+
+    for (;cnt--;) {
+        src0 = __lsx_vldrepl_d(src, 0);
+        src += src_stride;
+        src1 = __lsx_vldrepl_d(src, 0);
+        src += src_stride;
+        src2 = __lsx_vldrepl_d(src, 0);
+        src += src_stride;
+        src3 = __lsx_vldrepl_d(src, 0);
+        src += src_stride;
+        __lsx_vstelm_d(src0, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(src1, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(src2, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(src3, dst, 0, 0);
+        dst += dst_stride;
+    }
+}
+
+static void copy_width16_lsx(const uint8_t *src, int32_t src_stride,
+                             uint8_t *dst, int32_t dst_stride,
+                             int32_t height)
+{
+    int32_t cnt = height >> 2;
+    __m128i src0, src1, src2, src3;
+    int32_t src_stride2 = src_stride << 1;
+    int32_t src_stride3 = src_stride + src_stride2;
+    int32_t src_stride4 = src_stride2 << 1;
+    int32_t dst_stride2 = dst_stride << 1;
+    int32_t dst_stride3 = dst_stride2 + dst_stride;
+    int32_t dst_stride4 = dst_stride2 << 1;
+    uint8_t *_src = (uint8_t*)src;
+
+    for (;cnt--;) {
+        src0 = __lsx_vld(_src, 0);
+        DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src1, src2);
+        src3 = __lsx_vldx(_src, src_stride3);
+        _src += src_stride4;
+        __lsx_vst(src0, dst, 0);
+        __lsx_vstx(src1, dst, dst_stride);
+        __lsx_vstx(src2, dst, dst_stride2);
+        __lsx_vstx(src3, dst, dst_stride3);
+        dst += dst_stride4;
+    }
+}
+
+static void copy_width32_lsx(const uint8_t *src, int32_t src_stride,
+                             uint8_t *dst, int32_t dst_stride,
+                             int32_t height)
+{
+    int32_t cnt = height >> 2;
+    uint8_t *src_tmp1 = (uint8_t*)src;
+    uint8_t *dst_tmp1 = dst;
+    uint8_t *src_tmp2 = src_tmp1 + 16;
+    uint8_t *dst_tmp2 = dst_tmp1 + 16;
+    int32_t src_stride2 = src_stride << 1;
+    int32_t src_stride3 = src_stride + src_stride2;
+    int32_t src_stride4 = src_stride2 << 1;
+    int32_t dst_stride2 = dst_stride << 1;
+    int32_t dst_stride3 = dst_stride2 + dst_stride;
+    int32_t dst_stride4 = dst_stride2 << 1;
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7;
+
+    for (;cnt--;) {
+        src0 = __lsx_vld(src_tmp1, 0);
+        DUP2_ARG2(__lsx_vldx, src_tmp1, src_stride, src_tmp1, src_stride2,
+                  src1, src2);
+        src3 = __lsx_vldx(src_tmp1, src_stride3);
+        src_tmp1 += src_stride4;
+
+        src4 = __lsx_vld(src_tmp2, 0);
+        DUP2_ARG2(__lsx_vldx, src_tmp2, src_stride, src_tmp2, src_stride2,
+                  src5, src6);
+        src7 = __lsx_vldx(src_tmp2, src_stride3);
+        src_tmp2 += src_stride4;
+
+        __lsx_vst(src0, dst_tmp1, 0);
+        __lsx_vstx(src1, dst_tmp1, dst_stride);
+        __lsx_vstx(src2, dst_tmp1, dst_stride2);
+        __lsx_vstx(src3, dst_tmp1, dst_stride3);
+        dst_tmp1 += dst_stride4;
+        __lsx_vst(src4, dst_tmp2, 0);
+        __lsx_vstx(src5, dst_tmp2, dst_stride);
+        __lsx_vstx(src6, dst_tmp2, dst_stride2);
+        __lsx_vstx(src7, dst_tmp2, dst_stride3);
+        dst_tmp2 += dst_stride4;
+    }
+}
+
+static void copy_width64_lsx(const uint8_t *src, int32_t src_stride,
+                             uint8_t *dst, int32_t dst_stride,
+                             int32_t height)
+{
+    int32_t cnt = height >> 2;
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m128i src8, src9, src10, src11, src12, src13, src14, src15;
+
+    for (;cnt--;) {
+        DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48,
+                  src0, src1, src2, src3);
+        src += src_stride;
+        DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48,
+                  src4, src5, src6, src7);
+        src += src_stride;
+        DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48,
+                  src8, src9, src10, src11);
+        src += src_stride;
+        DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48,
+                  src12, src13, src14, src15);
+        src += src_stride;
+        __lsx_vst(src0, dst, 0);
+        __lsx_vst(src1, dst, 16);
+        __lsx_vst(src2, dst, 32);
+        __lsx_vst(src3, dst, 48);
+        dst += dst_stride;
+        __lsx_vst(src4, dst, 0);
+        __lsx_vst(src5, dst, 16);
+        __lsx_vst(src6, dst, 32);
+        __lsx_vst(src7, dst, 48);
+        dst += dst_stride;
+        __lsx_vst(src8, dst, 0);
+        __lsx_vst(src9, dst, 16);
+        __lsx_vst(src10, dst, 32);
+        __lsx_vst(src11, dst, 48);
+        dst += dst_stride;
+        __lsx_vst(src12, dst, 0);
+        __lsx_vst(src13, dst, 16);
+        __lsx_vst(src14, dst, 32);
+        __lsx_vst(src15, dst, 48);
+        dst += dst_stride;
+    }
+}
+
+static void common_hz_8t_and_aver_dst_4x4_lsx(const uint8_t *src,
+                                              int32_t src_stride,
+                                              uint8_t *dst, int32_t dst_stride,
+                                              const int8_t *filter)
+{
+    uint8_t *dst_tmp = dst;
+    __m128i src0, src1, src2, src3;
+    __m128i filter0, filter1, filter2, filter3;
+    __m128i mask0, mask1, mask2, mask3;
+    __m128i tmp0, tmp1;
+    __m128i dst0, dst1, dst2, dst3;
+
+    mask0 = __lsx_vld(mc_filt_mask_arr, 16);
+    src -= 3;
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    mask3 = __lsx_vaddi_bu(mask0, 6);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filter0, filter1, filter2, filter3);
+    LSX_LD_4(src, src_stride, src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+              src0, src1, src2, src3);
+    HORIZ_8TAP_4WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2, mask3,
+                               filter0, filter1, filter2, filter3, tmp0, tmp1);
+    dst0 = __lsx_vldrepl_w(dst_tmp, 0);
+    dst_tmp += dst_stride;
+    dst1 = __lsx_vldrepl_w(dst_tmp, 0);
+    dst_tmp += dst_stride;
+    dst2 = __lsx_vldrepl_w(dst_tmp, 0);
+    dst_tmp += dst_stride;
+    dst3 = __lsx_vldrepl_w(dst_tmp, 0);
+    dst0 = __lsx_vilvl_w(dst1, dst0);
+    dst1 = __lsx_vilvl_w(dst3, dst2);
+    dst0 = __lsx_vilvl_d(dst1, dst0);
+    tmp0 = __lsx_vssrarni_b_h(tmp1, tmp0, 7);
+    tmp0 = __lsx_vxori_b(tmp0, 128);
+    dst0 = __lsx_vavgr_bu(tmp0, dst0);
+    __lsx_vstelm_w(dst0, dst, 0, 0);
+    dst += dst_stride;
+    __lsx_vstelm_w(dst0, dst, 0, 1);
+    dst += dst_stride;
+    __lsx_vstelm_w(dst0, dst, 0, 2);
+    dst += dst_stride;
+    __lsx_vstelm_w(dst0, dst, 0, 3);
+}
+
+static void common_hz_8t_and_aver_dst_4x8_lsx(const uint8_t *src,
+                                              int32_t src_stride,
+                                              uint8_t *dst, int32_t dst_stride,
+                                              const int8_t *filter)
+{
+    uint8_t *dst_tmp = dst;
+    __m128i src0, src1, src2, src3, filter0, filter1, filter2, filter3;
+    __m128i mask0, mask1, mask2, mask3, tmp0, tmp1, tmp2, tmp3;
+    __m128i dst0, dst1;
+
+    mask0 = __lsx_vld(mc_filt_mask_arr, 16);
+    src -= 3;
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    mask3 = __lsx_vaddi_bu(mask0, 6);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filter0, filter1, filter2, filter3);
+
+    LSX_LD_4(src, src_stride, src0, src1, src2, src3);
+    src += src_stride;
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+              src0, src1, src2, src3);
+    tmp0 = __lsx_vldrepl_w(dst_tmp, 0);
+    dst_tmp += dst_stride;
+    tmp1 = __lsx_vldrepl_w(dst_tmp, 0);
+    dst_tmp += dst_stride;
+    tmp2 = __lsx_vldrepl_w(dst_tmp, 0);
+    dst_tmp += dst_stride;
+    tmp3 = __lsx_vldrepl_w(dst_tmp, 0);
+    dst_tmp += dst_stride;
+    tmp0 = __lsx_vilvl_w(tmp1, tmp0);
+    tmp1 = __lsx_vilvl_w(tmp3, tmp2);
+    dst0 = __lsx_vilvl_d(tmp1, tmp0);
+
+    tmp0 = __lsx_vldrepl_w(dst_tmp, 0);
+    dst_tmp += dst_stride;
+    tmp1 = __lsx_vldrepl_w(dst_tmp, 0);
+    dst_tmp += dst_stride;
+    tmp2 = __lsx_vldrepl_w(dst_tmp, 0);
+    dst_tmp += dst_stride;
+    tmp3 = __lsx_vldrepl_w(dst_tmp, 0);
+    tmp0 = __lsx_vilvl_w(tmp1, tmp0);
+    tmp1 = __lsx_vilvl_w(tmp3, tmp2);
+    dst1 = __lsx_vilvl_d(tmp1, tmp0);
+    HORIZ_8TAP_4WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2, mask3,
+                               filter0, filter1, filter2, filter3, tmp0, tmp1);
+    LSX_LD_4(src, src_stride, src0, src1, src2, src3);
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+              src0, src1, src2, src3);
+    HORIZ_8TAP_4WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2, mask3,
+                               filter0, filter1, filter2, filter3, tmp2, tmp3);
+    DUP4_ARG3(__lsx_vssrarni_b_h, tmp0, tmp0, 7, tmp1, tmp1, 7, tmp2, tmp2, 7,
+              tmp3, tmp3, 7, tmp0, tmp1, tmp2, tmp3);
+    DUP2_ARG2(__lsx_vilvl_d, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
+    DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
+    DUP2_ARG2(__lsx_vavgr_bu, tmp0, dst0, tmp1, dst1, dst0, dst1);
+    __lsx_vstelm_w(dst0, dst, 0, 0);
+    dst += dst_stride;
+    __lsx_vstelm_w(dst0, dst, 0, 1);
+    dst += dst_stride;
+    __lsx_vstelm_w(dst0, dst, 0, 2);
+    dst += dst_stride;
+    __lsx_vstelm_w(dst0, dst, 0, 3);
+    dst += dst_stride;
+    __lsx_vstelm_w(dst1, dst, 0, 0);
+    dst += dst_stride;
+    __lsx_vstelm_w(dst1, dst, 0, 1);
+    dst += dst_stride;
+    __lsx_vstelm_w(dst1, dst, 0, 2);
+    dst += dst_stride;
+    __lsx_vstelm_w(dst1, dst, 0, 3);
+}
+
+static void common_hz_8t_and_aver_dst_4w_lsx(const uint8_t *src,
+                                             int32_t src_stride,
+                                             uint8_t *dst, int32_t dst_stride,
+                                             const int8_t *filter,
+                                             int32_t height)
+{
+    if (height == 4) {
+        common_hz_8t_and_aver_dst_4x4_lsx(src, src_stride, dst, dst_stride, filter);
+    } else if (height == 8) {
+        common_hz_8t_and_aver_dst_4x8_lsx(src, src_stride, dst, dst_stride, filter);
+    }
+}
+
+static void common_hz_8t_and_aver_dst_8w_lsx(const uint8_t *src,
+                                             int32_t src_stride,
+                                             uint8_t *dst, int32_t dst_stride,
+                                             const int8_t *filter,
+                                             int32_t height)
+{
+    int32_t loop_cnt = height >> 2;
+    uint8_t *dst_tmp = dst;
+    __m128i src0, src1, src2, src3, filter0, filter1, filter2, filter3;
+    __m128i mask0, mask1, mask2, mask3;
+    __m128i tmp0, tmp1, tmp2, tmp3;
+    __m128i dst0, dst1, dst2, dst3;
+    int32_t src_stride2 = src_stride << 1;
+    int32_t src_stride3 = src_stride2 + src_stride;
+    int32_t src_stride4 = src_stride2 << 1;
+    uint8_t *_src = (uint8_t*)src - 3;
+
+    mask0 = __lsx_vld(mc_filt_mask_arr, 0);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    mask3 = __lsx_vaddi_bu(mask0, 6);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filter0, filter1, filter2, filter3);
+
+    for (;loop_cnt--;) {
+        src0 = __lsx_vld(_src, 0);
+        DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src1, src2);
+        src3 = __lsx_vldx(_src, src_stride3);
+        _src += src_stride4;
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+                  src0, src1, src2, src3);
+        HORIZ_8TAP_8WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2,
+              mask3,filter0, filter1, filter2, filter3, tmp0, tmp1, tmp2, tmp3);
+        dst0 = __lsx_vldrepl_d(dst_tmp, 0);
+        dst_tmp += dst_stride;
+        dst1 = __lsx_vldrepl_d(dst_tmp, 0);
+        dst_tmp += dst_stride;
+        dst2 = __lsx_vldrepl_d(dst_tmp, 0);
+        dst_tmp += dst_stride;
+        dst3 = __lsx_vldrepl_d(dst_tmp, 0);
+        dst_tmp += dst_stride;
+        DUP2_ARG2(__lsx_vilvl_d, dst1, dst0, dst3, dst2, dst0, dst1);
+        DUP2_ARG3(__lsx_vssrarni_b_h, tmp1, tmp0, 7, tmp3, tmp2, 7, tmp0, tmp1);
+        DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
+        DUP2_ARG2(__lsx_vavgr_bu, tmp0, dst0, tmp1, dst1, dst0, dst1);
+        __lsx_vstelm_d(dst0, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(dst0, dst, 0, 1);
+        dst += dst_stride;
+        __lsx_vstelm_d(dst1, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(dst1, dst, 0, 1);
+        dst += dst_stride;
+    }
+}
+
+static void common_hz_8t_and_aver_dst_16w_lsx(const uint8_t *src,
+                                              int32_t src_stride,
+                                              uint8_t *dst, int32_t dst_stride,
+                                              const int8_t *filter,
+                                              int32_t height)
+{
+    int32_t loop_cnt = height >> 1;
+    int32_t dst_stride2 = dst_stride << 1;
+    uint8_t *dst_tmp = dst;
+    __m128i src0, src1, src2, src3, filter0, filter1, filter2, filter3;
+    __m128i mask0, mask1, mask2, mask3, dst0, dst1, dst2, dst3;
+    __m128i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
+    __m128i tmp8, tmp9, tmp10, tmp11, tmp12, tmp13, tmp14, tmp15;
+
+    mask0 = __lsx_vld(mc_filt_mask_arr, 0);
+    src -= 3;
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    mask3 = __lsx_vaddi_bu(mask0, 6);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filter0, filter1, filter2, filter3);
+
+    for (;loop_cnt--;) {
+        DUP2_ARG2(__lsx_vld, src, 0, src, 8, src0, src1);
+        src += src_stride;
+        DUP2_ARG2(__lsx_vld, src, 0, src, 8, src2, src3);
+        src += src_stride;
+        dst0 = __lsx_vld(dst_tmp, 0);
+        dst1 = __lsx_vldx(dst_tmp, dst_stride);
+        dst_tmp += dst_stride2;
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+                  src0, src1, src2, src3);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src1, mask0, src2, src2,
+                  mask0, src3, src3, mask0, tmp0, tmp1, tmp2, tmp3);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src1, mask1, src2, src2,
+                  mask1, src3, src3, mask1, tmp4, tmp5, tmp6, tmp7);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src1, mask2, src2, src2,
+                  mask2, src3, src3, mask2, tmp8, tmp9, tmp10, tmp11);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src1, mask3, src2, src2,
+                  mask3, src3, src3, mask3, tmp12, tmp13, tmp14, tmp15);
+        DUP4_ARG2(__lsx_vdp2_h_b, tmp0, filter0, tmp1, filter0, tmp2, filter0, tmp3,
+                  filter0, tmp0, tmp1, tmp2, tmp3);
+        DUP4_ARG2(__lsx_vdp2_h_b, tmp8, filter2, tmp9, filter2, tmp10, filter2, tmp11,
+                  filter2, tmp8, tmp9, tmp10, tmp11);
+        DUP4_ARG3(__lsx_vdp2add_h_b, tmp0, tmp4, filter1, tmp1, tmp5, filter1, tmp2,
+                  tmp6, filter1, tmp3, tmp7, filter1, tmp0, tmp1, tmp2, tmp3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, tmp8, tmp12, filter3, tmp9, tmp13, filter3, tmp10,
+                  tmp14, filter3, tmp11, tmp15, filter3, tmp4, tmp5, tmp6, tmp7);
+        DUP4_ARG2(__lsx_vsadd_h, tmp0, tmp4, tmp1, tmp5, tmp2, tmp6, tmp3, tmp7,
+                  tmp0, tmp1, tmp2, tmp3);
+        DUP2_ARG3(__lsx_vssrarni_b_h, tmp1, tmp0, 7, tmp3, tmp2, 7, dst2, dst3);
+        DUP2_ARG2(__lsx_vxori_b, dst2, 128, dst3, 128, dst2, dst3);
+        DUP2_ARG2(__lsx_vavgr_bu, dst0, dst2, dst1, dst3, dst0, dst1);
+        __lsx_vst(dst0, dst, 0);
+        __lsx_vstx(dst1, dst, dst_stride);
+        dst += dst_stride2;
+    }
+}
+
+static void common_hz_8t_and_aver_dst_32w_lsx(const uint8_t *src,
+                                              int32_t src_stride,
+                                              uint8_t *dst, int32_t dst_stride,
+                                              const int8_t *filter,
+                                              int32_t height)
+{
+    uint32_t loop_cnt = height;
+    uint8_t *dst_tmp = dst;
+    __m128i src0, src1, src2, src3, filter0, filter1, filter2, filter3;
+    __m128i mask0, mask1, mask2, mask3, dst0, dst1;
+    __m128i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
+    __m128i tmp8, tmp9, tmp10, tmp11, tmp12, tmp13, tmp14, tmp15;
+    __m128i shuff = {0x0F0E0D0C0B0A0908, 0x1716151413121110};
+
+    mask0 = __lsx_vld(mc_filt_mask_arr, 0);
+    src -= 3;
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    mask3 = __lsx_vaddi_bu(mask0, 6);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+                  filter0, filter1, filter2, filter3);
+
+    for (;loop_cnt--;) {
+        DUP2_ARG2(__lsx_vld, src, 0, src, 16, src0, src2);
+        src3 = __lsx_vld(src, 24);
+        src1 = __lsx_vshuf_b(src2, src0, shuff);
+        src += src_stride;
+        DUP2_ARG2(__lsx_vld, dst_tmp, 0, dst, 16, dst0, dst1);
+        dst_tmp += dst_stride;
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+                  src0, src1, src2, src3);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask0, src1, src1, mask0, src2,
+                  src2, mask0, src3, src3, mask0, tmp0, tmp1, tmp2, tmp3);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask1, src1, src1, mask1, src2,
+                  src2, mask1, src3, src3, mask1, tmp4, tmp5, tmp6, tmp7);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask2, src1, src1, mask2, src2,
+                  src2, mask2, src3, src3, mask2, tmp8, tmp9, tmp10, tmp11);
+        DUP4_ARG3(__lsx_vshuf_b, src0, src0, mask3, src1, src1, mask3, src2,
+                  src2, mask3, src3, src3, mask3, tmp12, tmp13, tmp14, tmp15);
+        DUP4_ARG2(__lsx_vdp2_h_b, tmp0, filter0, tmp1, filter0, tmp2, filter0,
+                  tmp3, filter0, tmp0, tmp1, tmp2, tmp3);
+        DUP4_ARG2(__lsx_vdp2_h_b, tmp8, filter2, tmp9, filter2, tmp10, filter2,
+                  tmp11, filter2, tmp8, tmp9, tmp10, tmp11);
+        DUP4_ARG3(__lsx_vdp2add_h_b, tmp0, tmp4, filter1, tmp1, tmp5, filter1,
+             tmp2, tmp6, filter1, tmp3, tmp7, filter1, tmp0, tmp1, tmp2, tmp3);
+        DUP4_ARG3(__lsx_vdp2add_h_b, tmp8, tmp12, filter3, tmp9, tmp13, filter3,
+        tmp10, tmp14, filter3, tmp11, tmp15, filter3, tmp4, tmp5, tmp6, tmp7);
+        DUP4_ARG2(__lsx_vsadd_h, tmp0, tmp4, tmp1, tmp5, tmp2, tmp6, tmp3, tmp7,
+                  tmp0, tmp1, tmp2, tmp3);
+        DUP2_ARG3(__lsx_vssrarni_b_h, tmp1, tmp0, 7, tmp3, tmp2, 7, tmp0, tmp1);
+        DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
+        DUP2_ARG2(__lsx_vavgr_bu, dst0, tmp0, dst1, tmp1, dst0, dst1);
+        __lsx_vst(dst0, dst, 0);
+        __lsx_vst(dst1, dst, 16);
+        dst += dst_stride;
+    }
+}
+
+static void common_hz_8t_and_aver_dst_64w_lsx(const uint8_t *src,
+                                              int32_t src_stride,
+                                              uint8_t *dst, int32_t dst_stride,
+                                              const int8_t *filter,
+                                              int32_t height)
+{
+    int32_t loop_cnt = height;
+    __m128i src0, src1, src2, src3;
+    __m128i filter0, filter1, filter2, filter3;
+    __m128i mask0, mask1, mask2, mask3;
+    __m128i out0, out1, out2, out3, dst0, dst1;
+    __m128i shuff = {0x0F0E0D0C0B0A0908, 0x1716151413121110};
+
+    mask0 = __lsx_vld(mc_filt_mask_arr, 0);
+    src -= 3;
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    mask3 = __lsx_vaddi_bu(mask0, 6);
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+                  filter0, filter1, filter2, filter3);
+
+    for (;loop_cnt--;) {
+        DUP2_ARG2(__lsx_vld, src, 0, src, 16, src0, src2);
+        src3 = __lsx_vld(src, 24);
+        src1 = __lsx_vshuf_b(src2, src0, shuff);
+        DUP2_ARG2(__lsx_vld, dst, 0, dst, 16, dst0, dst1);
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+                  src0, src1, src2, src3);
+        HORIZ_8TAP_8WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2,
+             mask3, filter0, filter1, filter2, filter3, out0, out1, out2, out3);
+        DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
+        DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
+        DUP2_ARG2(__lsx_vavgr_bu, out0, dst0, out1, dst1, out0, out1);
+        __lsx_vst(out0, dst, 0);
+        __lsx_vst(out1, dst, 16);
+
+        DUP2_ARG2(__lsx_vld, src, 32, src, 48, src0, src2);
+        src3 = __lsx_vld(src, 56);
+        src1 = __lsx_vshuf_b(src2, src0, shuff);
+        DUP2_ARG2(__lsx_vld, dst, 32, dst, 48, dst0, dst1);
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+                  src0, src1, src2, src3);
+        HORIZ_8TAP_8WID_4VECS_FILT(src0, src1, src2, src3, mask0, mask1, mask2,
+             mask3, filter0, filter1, filter2, filter3, out0, out1, out2, out3);
+        DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
+        DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
+        DUP2_ARG2(__lsx_vavgr_bu, out0, dst0, out1, dst1, out0, out1);
+        __lsx_vst(out0, dst, 32);
+        __lsx_vst(out1, dst, 48);
+        src += src_stride;
+        dst += dst_stride;
+    }
+}
+
+static void common_vt_8t_and_aver_dst_4w_lsx(const uint8_t *src,
+                                             int32_t src_stride,
+                                             uint8_t *dst, int32_t dst_stride,
+                                             const int8_t *filter,
+                                             int32_t height)
+{
+    uint32_t loop_cnt = height >> 2;
+    uint8_t *dst_tmp = dst;
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8, src9, src10;
+    __m128i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5;
+    __m128i reg0, reg1, reg2, reg3, reg4;
+    __m128i filter0, filter1, filter2, filter3;
+    __m128i out0, out1;
+    int32_t src_stride2 = src_stride << 1;
+    int32_t src_stride3 = src_stride + src_stride2;
+    int32_t src_stride4 = src_stride2 << 1;
+    uint8_t* _src = (uint8_t*)src - src_stride3;
+
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filter0, filter1, filter2, filter3);
+    src0 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src1, src2);
+    src3 = __lsx_vldx(_src, src_stride3);
+    _src += src_stride4;
+    src4 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src5, src6);
+    _src += src_stride3;
+    DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1,
+              tmp0, tmp1, tmp2, tmp3);
+    DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, tmp4, tmp5);
+    DUP2_ARG2(__lsx_vilvl_d, tmp3, tmp0, tmp4, tmp1, reg0, reg1);
+    reg2 = __lsx_vilvl_d(tmp5, tmp2);
+    DUP2_ARG2(__lsx_vxori_b, reg0, 128, reg1, 128, reg0, reg1);
+    reg2 = __lsx_vxori_b(reg2, 128);
+
+    for (;loop_cnt--;) {
+        src7 = __lsx_vld(_src, 0);
+        DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src8, src9);
+        src10 = __lsx_vldx(_src, src_stride3);
+        _src += src_stride4;
+        src0 = __lsx_vldrepl_w(dst_tmp, 0);
+        dst_tmp += dst_stride;
+        src1 = __lsx_vldrepl_w(dst_tmp, 0);
+        dst_tmp += dst_stride;
+        src2 = __lsx_vldrepl_w(dst_tmp, 0);
+        dst_tmp += dst_stride;
+        src3 = __lsx_vldrepl_w(dst_tmp, 0);
+        dst_tmp += dst_stride;
+        DUP2_ARG2(__lsx_vilvl_w, src1, src0, src3, src2, src0, src1);
+        src0 = __lsx_vilvl_d(src1, src0);
+        DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10,
+                  src9, tmp0, tmp1, tmp2, tmp3);
+        DUP2_ARG2(__lsx_vilvl_d, tmp1, tmp0, tmp3, tmp2, reg3, reg4);
+        DUP2_ARG2(__lsx_vxori_b, reg3, 128, reg4, 128, reg3, reg4);
+        out0 = FILT_8TAP_DPADD_S_H(reg0, reg1, reg2, reg3, filter0,
+                                   filter1, filter2, filter3);
+        out1 = FILT_8TAP_DPADD_S_H(reg1, reg2, reg3, reg4, filter0,
+                                   filter1, filter2, filter3);
+        out0 = __lsx_vssrarni_b_h(out1, out0, 7);
+        out0 = __lsx_vxori_b(out0, 128);
+        out0 = __lsx_vavgr_bu(out0, src0);
+        __lsx_vstelm_w(out0, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_w(out0, dst, 0, 1);
+        dst += dst_stride;
+        __lsx_vstelm_w(out0, dst, 0, 2);
+        dst += dst_stride;
+        __lsx_vstelm_w(out0, dst, 0, 3);
+        dst += dst_stride;
+        reg0 = reg2;
+        reg1 = reg3;
+        reg2 = reg4;
+        src6 = src10;
+    }
+}
+
+static void common_vt_8t_and_aver_dst_8w_lsx(const uint8_t *src,
+                                             int32_t src_stride,
+                                             uint8_t *dst, int32_t dst_stride,
+                                             const int8_t *filter,
+                                             int32_t height)
+{
+    uint32_t loop_cnt = height >> 2;
+    uint8_t *dst_tmp = dst;
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8, src9, src10;
+    __m128i tmp0, tmp1, tmp2, tmp3;
+    __m128i reg0, reg1, reg2, reg3, reg4, reg5;
+    __m128i filter0, filter1, filter2, filter3;
+    __m128i out0, out1, out2, out3;
+    int32_t src_stride2 = src_stride << 1;
+    int32_t src_stride3 = src_stride + src_stride2;
+    int32_t src_stride4 = src_stride2 << 1;
+    uint8_t* _src = (uint8_t*)src - src_stride3;
+
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filter0, filter1, filter2, filter3);
+
+    src0 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src1, src2);
+    src3 = __lsx_vldx(_src, src_stride3);
+    _src += src_stride4;
+    src4 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src5, src6);
+    _src += src_stride3;
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+              src0, src1, src2, src3);
+    DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
+    src6 = __lsx_vxori_b(src6, 128);
+    DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2,
+              src1, reg0, reg1, reg2, reg3);
+    DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, reg4, reg5);
+
+    for (;loop_cnt--;) {
+        src7 = __lsx_vld(_src, 0);
+        DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src8, src9);
+        src10 = __lsx_vldx(_src, src_stride3);
+        _src += src_stride4;
+        src0 = __lsx_vldrepl_d(dst_tmp, 0);
+        dst_tmp += dst_stride;
+        src1 = __lsx_vldrepl_d(dst_tmp, 0);
+        dst_tmp += dst_stride;
+        src2 = __lsx_vldrepl_d(dst_tmp, 0);
+        dst_tmp += dst_stride;
+        src3 = __lsx_vldrepl_d(dst_tmp, 0);
+        dst_tmp += dst_stride;
+        DUP2_ARG2(__lsx_vilvl_d, src1, src0, src3, src2, src0, src1);
+        DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
+                  src7, src8, src9, src10);
+        DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8, src10,
+                  src9, tmp0, tmp1, tmp2, tmp3);
+        out0 = FILT_8TAP_DPADD_S_H(reg0, reg1, reg2, tmp0, filter0,
+                                   filter1, filter2, filter3);
+        out1 = FILT_8TAP_DPADD_S_H(reg3, reg4, reg5, tmp1, filter0,
+                                   filter1, filter2, filter3);
+        out2 = FILT_8TAP_DPADD_S_H(reg1, reg2, tmp0, tmp2, filter0,
+                                   filter1, filter2, filter3);
+        out3 = FILT_8TAP_DPADD_S_H(reg4, reg5, tmp1, tmp3, filter0,
+                                   filter1, filter2, filter3);
+        DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, out3, out2, 7, out0, out1);
+        DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
+        DUP2_ARG2(__lsx_vavgr_bu, out0, src0, out1, src1, out0, out1);
+        __lsx_vstelm_d(out0, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(out0, dst, 0, 1);
+        dst += dst_stride;
+        __lsx_vstelm_d(out1, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(out1, dst, 0, 1);
+        dst += dst_stride;
+
+        reg0 = reg2;
+        reg1 = tmp0;
+        reg2 = tmp2;
+        reg3 = reg5;
+        reg4 = tmp1;
+        reg5 = tmp3;
+        src6 = src10;
+    }
+}
+
+static void common_vt_8t_and_aver_dst_16w_mult_lsx(const uint8_t *src,
+                                                   int32_t src_stride,
+                                                   uint8_t *dst,
+                                                   int32_t dst_stride,
+                                                   const int8_t *filter,
+                                                   int32_t height,
+                                                   int32_t width)
+{
+    uint8_t *src_tmp;
+    uint32_t cnt = width >> 4;
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8, src9, src10;
+    __m128i filter0, filter1, filter2, filter3;
+    __m128i reg0, reg1, reg2, reg3, reg4, reg5;
+    __m128i reg6, reg7, reg8, reg9, reg10, reg11;
+    __m128i tmp0, tmp1, tmp2, tmp3;
+    int32_t src_stride2 = src_stride << 1;
+    int32_t src_stride3 = src_stride + src_stride2;
+    int32_t src_stride4 = src_stride2 << 1;
+    int32_t dst_stride2 = dst_stride << 1;
+    int32_t dst_stride3 = dst_stride2 + dst_stride;
+    int32_t dst_stride4 = dst_stride2 << 1;
+    uint8_t *_src = (uint8_t*)src - src_stride3;
+
+    DUP4_ARG2(__lsx_vldrepl_h, filter, 0, filter, 2, filter, 4, filter, 6,
+              filter0, filter1, filter2, filter3);
+    for (;cnt--;) {
+        uint32_t loop_cnt = height >> 2;
+        uint8_t *dst_reg = dst;
+
+        src_tmp = _src;
+        src0 = __lsx_vld(src_tmp, 0);
+        DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride2,
+                  src1, src2);
+        src3 = __lsx_vldx(src_tmp, src_stride3);
+        src_tmp += src_stride4;
+        src4 = __lsx_vld(src_tmp, 0);
+        DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride2,
+                  src5, src6);
+        src_tmp += src_stride3;
+        DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+                  src0, src1, src2, src3);
+        DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
+        src6 = __lsx_vxori_b(src6, 128);
+        DUP4_ARG2(__lsx_vilvl_b, src1, src0, src3, src2, src5, src4, src2, src1,
+                  reg0, reg1, reg2, reg3);
+        DUP2_ARG2(__lsx_vilvl_b, src4, src3, src6, src5, reg4, reg5);
+        DUP4_ARG2(__lsx_vilvh_b, src1, src0, src3, src2, src5, src4, src2, src1,
+                  reg6, reg7, reg8, reg9);
+        DUP2_ARG2(__lsx_vilvh_b, src4, src3, src6, src5, reg10, reg11);
+
+        for (;loop_cnt--;) {
+            src7 = __lsx_vld(src_tmp, 0);
+            DUP2_ARG2(__lsx_vldx, src_tmp, src_stride, src_tmp, src_stride2,
+                      src8, src9);
+            src10 = __lsx_vldx(src_tmp, src_stride3);
+            src_tmp += src_stride4;
+            DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10,
+                      128, src7, src8, src9, src10);
+            DUP4_ARG2(__lsx_vilvl_b, src7, src6, src8, src7, src9, src8,
+                      src10, src9, src0, src1, src2, src3);
+            DUP4_ARG2(__lsx_vilvh_b, src7, src6, src8, src7, src9, src8,
+                      src10, src9, src4, src5, src7, src8);
+            tmp0 = FILT_8TAP_DPADD_S_H(reg0, reg1, reg2, src0, filter0,
+                                       filter1, filter2, filter3);
+            tmp1 = FILT_8TAP_DPADD_S_H(reg3, reg4, reg5, src1, filter0,
+                                       filter1, filter2, filter3);
+            tmp2 = FILT_8TAP_DPADD_S_H(reg6, reg7, reg8, src4, filter0,
+                                       filter1, filter2, filter3);
+            tmp3 = FILT_8TAP_DPADD_S_H(reg9, reg10, reg11, src5, filter0,
+                                       filter1, filter2, filter3);
+            DUP2_ARG3(__lsx_vssrarni_b_h, tmp2, tmp0, 7, tmp3, tmp1, 7,
+                      tmp0, tmp1);
+            DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
+            tmp2 = __lsx_vld(dst_reg, 0);
+            tmp3 = __lsx_vldx(dst_reg, dst_stride);
+            DUP2_ARG2(__lsx_vavgr_bu, tmp0, tmp2, tmp1, tmp3, tmp0, tmp1);
+            __lsx_vst(tmp0, dst_reg, 0);
+            __lsx_vstx(tmp1, dst_reg, dst_stride);
+            tmp0 = FILT_8TAP_DPADD_S_H(reg1, reg2, src0, src2, filter0,
+                                       filter1, filter2, filter3);
+            tmp1 = FILT_8TAP_DPADD_S_H(reg4, reg5, src1, src3, filter0,
+                                       filter1, filter2, filter3);
+            tmp2 = FILT_8TAP_DPADD_S_H(reg7, reg8, src4, src7, filter0,
+                                       filter1, filter2, filter3);
+            tmp3 = FILT_8TAP_DPADD_S_H(reg10, reg11, src5, src8, filter0,
+                                       filter1, filter2, filter3);
+            DUP2_ARG3(__lsx_vssrarni_b_h, tmp2, tmp0, 7, tmp3, tmp1, 7,
+                      tmp0, tmp1);
+            DUP2_ARG2(__lsx_vxori_b, tmp0, 128, tmp1, 128, tmp0, tmp1);
+            tmp2 = __lsx_vldx(dst_reg, dst_stride2);
+            tmp3 = __lsx_vldx(dst_reg, dst_stride3);
+            DUP2_ARG2(__lsx_vavgr_bu, tmp0, tmp2, tmp1, tmp3, tmp0, tmp1);
+            __lsx_vstx(tmp0, dst_reg, dst_stride2);
+            __lsx_vstx(tmp1, dst_reg, dst_stride3);
+            dst_reg += dst_stride4;
+
+            reg0 = reg2;
+            reg1 = src0;
+            reg2 = src2;
+            reg3 = reg5;
+            reg4 = src1;
+            reg5 = src3;
+            reg6 = reg8;
+            reg7 = src4;
+            reg8 = src7;
+            reg9 = reg11;
+            reg10 = src5;
+            reg11 = src8;
+            src6 = src10;
+        }
+        _src += 16;
+        dst  += 16;
+    }
+}
+
+static void common_vt_8t_and_aver_dst_16w_lsx(const uint8_t *src,
+                                              int32_t src_stride,
+                                              uint8_t *dst, int32_t dst_stride,
+                                              const int8_t *filter,
+                                              int32_t height)
+{
+    common_vt_8t_and_aver_dst_16w_mult_lsx(src, src_stride, dst, dst_stride,
+                                           filter, height, 16);
+}
+
+static void common_vt_8t_and_aver_dst_32w_lsx(const uint8_t *src,
+                                              int32_t src_stride,
+                                              uint8_t *dst, int32_t dst_stride,
+                                              const int8_t *filter,
+                                              int32_t height)
+{
+    common_vt_8t_and_aver_dst_16w_mult_lsx(src, src_stride, dst, dst_stride,
+                                           filter, height, 32);
+}
+
+static void common_vt_8t_and_aver_dst_64w_lsx(const uint8_t *src,
+                                              int32_t src_stride,
+                                              uint8_t *dst, int32_t dst_stride,
+                                              const int8_t *filter,
+                                              int32_t height)
+{
+    common_vt_8t_and_aver_dst_16w_mult_lsx(src, src_stride, dst, dst_stride,
+                                           filter, height, 64);
+}
+
+static void common_hv_8ht_8vt_and_aver_dst_4w_lsx(const uint8_t *src,
+                                                  int32_t src_stride,
+                                                  uint8_t *dst,
+                                                  int32_t dst_stride,
+                                                  const int8_t *filter_horiz,
+                                                  const int8_t *filter_vert,
+                                                  int32_t height)
+{
+    uint32_t loop_cnt = height >> 2;
+    uint8_t *dst_tmp = dst;
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8, src9, src10;
+    __m128i filt_hz0, filt_hz1, filt_hz2, filt_hz3;
+    __m128i filt_vt0, filt_vt1, filt_vt2, filt_vt3;
+    __m128i mask0, mask1, mask2, mask3;
+    __m128i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5;
+    __m128i out0, out1;
+    __m128i shuff = {0x0F0E0D0C0B0A0908, 0x1716151413121110};
+    int32_t src_stride2 = src_stride << 1;
+    int32_t src_stride3 = src_stride + src_stride2;
+    int32_t src_stride4 = src_stride2 << 1;
+    uint8_t* _src = (uint8_t*)src - 3 - src_stride3;
+
+    mask0 = __lsx_vld(mc_filt_mask_arr, 16);
+    DUP4_ARG2(__lsx_vldrepl_h, filter_horiz, 0, filter_horiz, 2, filter_horiz,
+              4, filter_horiz, 6, filt_hz0, filt_hz1, filt_hz2, filt_hz3);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    mask3 = __lsx_vaddi_bu(mask0, 6);
+
+    src0 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src1, src2);
+    src3 = __lsx_vldx(_src, src_stride3);
+    _src += src_stride4;
+    src4 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src5, src6);
+    _src += src_stride3;
+
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+              src0, src1, src2, src3);
+    DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
+    src6 = __lsx_vxori_b(src6, 128);
+
+    tmp0 = HORIZ_8TAP_FILT(src0, src1, mask0, mask1, mask2, mask3, filt_hz0,
+                           filt_hz1, filt_hz2, filt_hz3);
+    tmp2 = HORIZ_8TAP_FILT(src2, src3, mask0, mask1, mask2, mask3, filt_hz0,
+                           filt_hz1, filt_hz2, filt_hz3);
+    tmp4 = HORIZ_8TAP_FILT(src4, src5, mask0, mask1, mask2, mask3, filt_hz0,
+                           filt_hz1, filt_hz2, filt_hz3);
+    tmp5 = HORIZ_8TAP_FILT(src5, src6, mask0, mask1, mask2, mask3, filt_hz0,
+                           filt_hz1, filt_hz2, filt_hz3);
+    DUP2_ARG3(__lsx_vshuf_b, tmp2, tmp0, shuff, tmp4, tmp2, shuff, tmp1, tmp3);
+    DUP4_ARG2(__lsx_vldrepl_h, filter_vert, 0, filter_vert, 2, filter_vert, 4,
+              filter_vert, 6, filt_vt0, filt_vt1, filt_vt2, filt_vt3);
+    DUP2_ARG2(__lsx_vpackev_b, tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
+    tmp2 = __lsx_vpackev_b(tmp5, tmp4);
+
+    for (;loop_cnt--;) {
+        src7 = __lsx_vld(_src, 0);
+        DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src8, src9);
+        src10 = __lsx_vldx(_src, src_stride3);
+        _src += src_stride4;
+        src2 = __lsx_vldrepl_w(dst_tmp, 0);
+        dst_tmp += dst_stride;
+        src3 = __lsx_vldrepl_w(dst_tmp, 0);
+        dst_tmp += dst_stride;
+        src4 = __lsx_vldrepl_w(dst_tmp, 0);
+        dst_tmp += dst_stride;
+        src5 = __lsx_vldrepl_w(dst_tmp, 0);
+        dst_tmp += dst_stride;
+        DUP2_ARG2(__lsx_vilvl_w, src3, src2, src5, src4, src2, src3);
+        src2 = __lsx_vilvl_d(src3, src2);
+        DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
+                  src7, src8, src9, src10);
+        tmp3 = HORIZ_8TAP_FILT(src7, src8, mask0, mask1, mask2, mask3, filt_hz0,
+                               filt_hz1, filt_hz2, filt_hz3);
+        tmp4 = __lsx_vshuf_b(tmp3, tmp5, shuff);
+        tmp4 = __lsx_vpackev_b(tmp3, tmp4);
+        out0 = FILT_8TAP_DPADD_S_H(tmp0, tmp1, tmp2, tmp4, filt_vt0, filt_vt1,
+                                   filt_vt2, filt_vt3);
+        src1 = HORIZ_8TAP_FILT(src9, src10, mask0, mask1, mask2, mask3,
+                               filt_hz0, filt_hz1, filt_hz2, filt_hz3);
+        src0 = __lsx_vshuf_b(src1, tmp3, shuff);
+        src0 = __lsx_vpackev_b(src1, src0);
+        out1 = FILT_8TAP_DPADD_S_H(tmp1, tmp2, tmp4, src0, filt_vt0, filt_vt1,
+                                   filt_vt2, filt_vt3);
+        out0 = __lsx_vssrarni_b_h(out1, out0, 7);
+        out0 = __lsx_vxori_b(out0, 128);
+        out0 = __lsx_vavgr_bu(out0, src2);
+        __lsx_vstelm_w(out0, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_w(out0, dst, 0, 1);
+        dst += dst_stride;
+        __lsx_vstelm_w(out0, dst, 0, 2);
+        dst += dst_stride;
+        __lsx_vstelm_w(out0, dst, 0, 3);
+        dst += dst_stride;
+
+        tmp5 = src1;
+        tmp0 = tmp2;
+        tmp1 = tmp4;
+        tmp2 = src0;
+    }
+}
+
+static void common_hv_8ht_8vt_and_aver_dst_8w_lsx(const uint8_t *src,
+                                                  int32_t src_stride,
+                                                  uint8_t *dst,
+                                                  int32_t dst_stride,
+                                                  const int8_t *filter_horiz,
+                                                  const int8_t *filter_vert,
+                                                  int32_t height)
+{
+    uint32_t loop_cnt = height >> 2;
+    uint8_t *dst_tmp = dst;
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7, src8, src9, src10;
+    __m128i filt_hz0, filt_hz1, filt_hz2, filt_hz3;
+    __m128i filt_vt0, filt_vt1, filt_vt2, filt_vt3;
+    __m128i mask0, mask1, mask2, mask3;
+    __m128i tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6;
+    __m128i out0, out1;
+    int32_t src_stride2 = src_stride << 1;
+    int32_t src_stride3 = src_stride + src_stride2;
+    int32_t src_stride4 = src_stride2 << 1;
+    uint8_t* _src = (uint8_t*)src - 3 - src_stride3;
+
+    mask0 = __lsx_vld(mc_filt_mask_arr, 0);
+    DUP4_ARG2(__lsx_vldrepl_h, filter_horiz, 0, filter_horiz, 2, filter_horiz,
+              4, filter_horiz, 6, filt_hz0, filt_hz1, filt_hz2, filt_hz3);
+    DUP2_ARG2(__lsx_vaddi_bu, mask0, 2, mask0, 4, mask1, mask2);
+    mask3 = __lsx_vaddi_bu(mask0, 6);
+
+    src0 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src1, src2);
+    src3 = __lsx_vldx(_src, src_stride3);
+    _src += src_stride4;
+    src4 = __lsx_vld(_src, 0);
+    DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src5, src6);
+    _src += src_stride3;
+    DUP4_ARG2(__lsx_vxori_b, src0, 128, src1, 128, src2, 128, src3, 128,
+              src0, src1, src2, src3);
+    DUP2_ARG2(__lsx_vxori_b, src4, 128, src5, 128, src4, src5);
+    src6 = __lsx_vxori_b(src6, 128);
+
+    src0 = HORIZ_8TAP_FILT(src0, src0, mask0, mask1, mask2, mask3, filt_hz0,
+                           filt_hz1, filt_hz2, filt_hz3);
+    src1 = HORIZ_8TAP_FILT(src1, src1, mask0, mask1, mask2, mask3, filt_hz0,
+                           filt_hz1, filt_hz2, filt_hz3);
+    src2 = HORIZ_8TAP_FILT(src2, src2, mask0, mask1, mask2, mask3, filt_hz0,
+                           filt_hz1, filt_hz2, filt_hz3);
+    src3 = HORIZ_8TAP_FILT(src3, src3, mask0, mask1, mask2, mask3, filt_hz0,
+                           filt_hz1, filt_hz2, filt_hz3);
+    src4 = HORIZ_8TAP_FILT(src4, src4, mask0, mask1, mask2, mask3, filt_hz0,
+                           filt_hz1, filt_hz2, filt_hz3);
+    src5 = HORIZ_8TAP_FILT(src5, src5, mask0, mask1, mask2, mask3, filt_hz0,
+                           filt_hz1, filt_hz2, filt_hz3);
+    src6 = HORIZ_8TAP_FILT(src6, src6, mask0, mask1, mask2, mask3, filt_hz0,
+                           filt_hz1, filt_hz2, filt_hz3);
+
+    DUP4_ARG2(__lsx_vldrepl_h, filter_vert, 0, filter_vert, 2, filter_vert, 4,
+              filter_vert, 6, filt_vt0, filt_vt1, filt_vt2, filt_vt3);
+    DUP4_ARG2(__lsx_vpackev_b, src1, src0, src3, src2, src5, src4,
+              src2, src1, tmp0, tmp1, tmp2, tmp4);
+    DUP2_ARG2(__lsx_vpackev_b, src4, src3, src6, src5, tmp5, tmp6);
+
+    for (;loop_cnt--;) {
+        src7 = __lsx_vld(_src, 0);
+        DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src8, src9);
+        src10 = __lsx_vldx(_src, src_stride3);
+        _src += src_stride4;
+
+        DUP4_ARG2(__lsx_vxori_b, src7, 128, src8, 128, src9, 128, src10, 128,
+                  src7, src8, src9, src10);
+        src7 = HORIZ_8TAP_FILT(src7, src7, mask0, mask1, mask2, mask3, filt_hz0,
+                               filt_hz1, filt_hz2, filt_hz3);
+        tmp3 = __lsx_vpackev_b(src7, src6);
+        out0 = FILT_8TAP_DPADD_S_H(tmp0, tmp1, tmp2, tmp3, filt_vt0, filt_vt1,
+                                   filt_vt2, filt_vt3);
+        src8 = HORIZ_8TAP_FILT(src8, src8, mask0, mask1, mask2, mask3, filt_hz0,
+                               filt_hz1, filt_hz2, filt_hz3);
+        src0 = __lsx_vpackev_b(src8, src7);
+        out1 = FILT_8TAP_DPADD_S_H(tmp4, tmp5, tmp6, src0, filt_vt0, filt_vt1,
+                                   filt_vt2, filt_vt3);
+        src9 = HORIZ_8TAP_FILT(src9, src9, mask0, mask1, mask2, mask3, filt_hz0,
+                               filt_hz1, filt_hz2, filt_hz3);
+        src1 = __lsx_vpackev_b(src9, src8);
+        src3 = FILT_8TAP_DPADD_S_H(tmp1, tmp2, tmp3, src1, filt_vt0, filt_vt1,
+                                   filt_vt2, filt_vt3);
+        src10 = HORIZ_8TAP_FILT(src10, src10, mask0, mask1, mask2, mask3, filt_hz0,
+                               filt_hz1, filt_hz2, filt_hz3);
+        src2 = __lsx_vpackev_b(src10, src9);
+        src4 = FILT_8TAP_DPADD_S_H(tmp5, tmp6, src0, src2, filt_vt0, filt_vt1,
+                                   filt_vt2, filt_vt3);
+        DUP2_ARG3(__lsx_vssrarni_b_h, out1, out0, 7, src4, src3, 7, out0, out1);
+        DUP2_ARG2(__lsx_vxori_b, out0, 128, out1, 128, out0, out1);
+        src5 = __lsx_vldrepl_d(dst_tmp, 0);
+        dst_tmp += dst_stride;
+        src7 = __lsx_vldrepl_d(dst_tmp, 0);
+        dst_tmp += dst_stride;
+        src8 = __lsx_vldrepl_d(dst_tmp, 0);
+        dst_tmp += dst_stride;
+        src9 = __lsx_vldrepl_d(dst_tmp, 0);
+        dst_tmp += dst_stride;
+        DUP2_ARG2(__lsx_vilvl_d, src7, src5, src9, src8, src5, src7);
+        DUP2_ARG2(__lsx_vavgr_bu, out0, src5, out1, src7, out0, out1);
+        __lsx_vstelm_d(out0, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(out0, dst, 0, 1);
+        dst += dst_stride;
+        __lsx_vstelm_d(out1, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(out1, dst, 0, 1);
+        dst += dst_stride;
+
+        src6 = src10;
+        tmp0 = tmp2;
+        tmp1 = tmp3;
+        tmp2 = src1;
+        tmp4 = tmp6;
+        tmp5 = src0;
+        tmp6 = src2;
+    }
+}
+
+static void common_hv_8ht_8vt_and_aver_dst_16w_lsx(const uint8_t *src,
+                                                   int32_t src_stride,
+                                                   uint8_t *dst,
+                                                   int32_t dst_stride,
+                                                   const int8_t *filter_horiz,
+                                                   const int8_t *filter_vert,
+                                                   int32_t height)
+{
+    int32_t multiple8_cnt;
+
+    for (multiple8_cnt = 2; multiple8_cnt--;) {
+        common_hv_8ht_8vt_and_aver_dst_8w_lsx(src, src_stride, dst, dst_stride,
+                                              filter_horiz, filter_vert,
+                                              height);
+
+        src += 8;
+        dst += 8;
+    }
+}
+
+static void common_hv_8ht_8vt_and_aver_dst_32w_lsx(const uint8_t *src,
+                                                   int32_t src_stride,
+                                                   uint8_t *dst,
+                                                   int32_t dst_stride,
+                                                   const int8_t *filter_horiz,
+                                                   const int8_t *filter_vert,
+                                                   int32_t height)
+{
+    int32_t multiple8_cnt;
+
+    for (multiple8_cnt = 4; multiple8_cnt--;) {
+        common_hv_8ht_8vt_and_aver_dst_8w_lsx(src, src_stride, dst, dst_stride,
+                                              filter_horiz, filter_vert,
+                                              height);
+
+        src += 8;
+        dst += 8;
+    }
+}
+
+static void common_hv_8ht_8vt_and_aver_dst_64w_lsx(const uint8_t *src,
+                                                   int32_t src_stride,
+                                                   uint8_t *dst,
+                                                   int32_t dst_stride,
+                                                   const int8_t *filter_horiz,
+                                                   const int8_t *filter_vert,
+                                                   int32_t height)
+{
+    int32_t multiple8_cnt;
+
+    for (multiple8_cnt = 8; multiple8_cnt--;) {
+        common_hv_8ht_8vt_and_aver_dst_8w_lsx(src, src_stride, dst, dst_stride,
+                                              filter_horiz, filter_vert,
+                                              height);
+
+        src += 8;
+        dst += 8;
+    }
+}
+
+static void avg_width8_lsx(const uint8_t *src, int32_t src_stride,
+                           uint8_t *dst, int32_t dst_stride,
+                           int32_t height)
+{
+    int32_t cnt = height >> 2;
+    uint8_t *dst_tmp = dst;
+    __m128i src0, src1, dst0, dst1;
+    __m128i tmp0, tmp1, tmp2, tmp3;
+
+    for (;cnt--;) {
+        tmp0 = __lsx_vldrepl_d(src, 0);
+        src += src_stride;
+        tmp1 = __lsx_vldrepl_d(src, 0);
+        src += src_stride;
+        tmp2 = __lsx_vldrepl_d(src, 0);
+        src += src_stride;
+        tmp3 = __lsx_vldrepl_d(src, 0);
+        src += src_stride;
+        DUP2_ARG2(__lsx_vilvl_d, tmp1, tmp0, tmp3, tmp2, src0, src1);
+        tmp0 = __lsx_vldrepl_d(dst_tmp, 0);
+        dst_tmp += dst_stride;
+        tmp1 = __lsx_vldrepl_d(dst_tmp, 0);
+        dst_tmp += dst_stride;
+        tmp2 = __lsx_vldrepl_d(dst_tmp, 0);
+        dst_tmp += dst_stride;
+        tmp3 = __lsx_vldrepl_d(dst_tmp, 0);
+        dst_tmp += dst_stride;
+        DUP2_ARG2(__lsx_vilvl_d, tmp1, tmp0, tmp3, tmp2, dst0, dst1);
+        DUP2_ARG2(__lsx_vavgr_bu, src0, dst0, src1, dst1, dst0, dst1);
+        __lsx_vstelm_d(dst0, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(dst0, dst, 0, 1);
+        dst += dst_stride;
+        __lsx_vstelm_d(dst1, dst, 0, 0);
+        dst += dst_stride;
+        __lsx_vstelm_d(dst1, dst, 0, 1);
+        dst += dst_stride;
+    }
+}
+
+static void avg_width16_lsx(const uint8_t *src, int32_t src_stride,
+                            uint8_t *dst, int32_t dst_stride,
+                            int32_t height)
+{
+    int32_t cnt = height >> 2;
+    __m128i src0, src1, src2, src3;
+    __m128i dst0, dst1, dst2, dst3;
+    int32_t src_stride2 = src_stride << 1;
+    int32_t src_stride3 = src_stride + src_stride2;
+    int32_t src_stride4 = src_stride2 << 1;
+    int32_t dst_stride2 = dst_stride << 1;
+    int32_t dst_stride3 = dst_stride2 + dst_stride;
+    int32_t dst_stride4 = dst_stride2 << 1;
+    uint8_t* _src = (uint8_t*)src;
+
+    for (;cnt--;) {
+        src0 = __lsx_vld(_src, 0);
+        DUP2_ARG2(__lsx_vldx, _src, src_stride, _src, src_stride2, src1, src2);
+        src3 = __lsx_vldx(_src, src_stride3);
+        _src += src_stride4;
+
+        dst0 = __lsx_vld(dst, 0);
+        DUP2_ARG2(__lsx_vldx, dst, dst_stride, dst, dst_stride2,
+                  dst1, dst2);
+        dst3 = __lsx_vldx(dst, dst_stride3);
+        DUP4_ARG2(__lsx_vavgr_bu, src0, dst0, src1, dst1,
+                  src2, dst2, src3, dst3, dst0, dst1, dst2, dst3);
+        __lsx_vst(dst0, dst, 0);
+        __lsx_vstx(dst1, dst, dst_stride);
+        __lsx_vstx(dst2, dst, dst_stride2);
+        __lsx_vstx(dst3, dst, dst_stride3);
+        dst += dst_stride4;
+    }
+}
+
+static void avg_width32_lsx(const uint8_t *src, int32_t src_stride,
+                            uint8_t *dst, int32_t dst_stride,
+                            int32_t height)
+{
+    int32_t cnt = height >> 2;
+    uint8_t *src_tmp1 = (uint8_t*)src;
+    uint8_t *src_tmp2 = src_tmp1 + 16;
+    uint8_t *dst_tmp1, *dst_tmp2;
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m128i dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    int32_t src_stride2 = src_stride << 1;
+    int32_t src_stride3 = src_stride + src_stride2;
+    int32_t src_stride4 = src_stride2 << 1;
+    int32_t dst_stride2 = dst_stride << 1;
+    int32_t dst_stride3 = dst_stride2 + dst_stride;
+    int32_t dst_stride4 = dst_stride2 << 1;
+
+    dst_tmp1 = dst;
+    dst_tmp2 = dst + 16;
+    for (;cnt--;) {
+        src0 = __lsx_vld(src_tmp1, 0);
+        DUP2_ARG2(__lsx_vldx, src_tmp1, src_stride, src_tmp1, src_stride2,
+                  src2, src4);
+        src6 = __lsx_vldx(src_tmp1, src_stride3);
+        src_tmp1 += src_stride4;
+
+        src1 = __lsx_vld(src_tmp2, 0);
+        DUP2_ARG2(__lsx_vldx, src_tmp2, src_stride, src_tmp2, src_stride2,
+                  src3, src5);
+        src7 = __lsx_vldx(src_tmp2, src_stride3);
+        src_tmp2 += src_stride4;
+
+        dst0 = __lsx_vld(dst_tmp1, 0);
+        DUP2_ARG2(__lsx_vldx, dst_tmp1, dst_stride, dst_tmp1, dst_stride2,
+                  dst2, dst4);
+        dst6 = __lsx_vldx(dst_tmp1, dst_stride3);
+        dst1 = __lsx_vld(dst_tmp2, 0);
+        DUP2_ARG2(__lsx_vldx, dst_tmp2, dst_stride, dst_tmp2, dst_stride2,
+                  dst3, dst5);
+        dst7 = __lsx_vldx(dst_tmp2, dst_stride3);
+
+        DUP4_ARG2(__lsx_vavgr_bu, src0, dst0, src1, dst1,
+                  src2, dst2, src3, dst3, dst0, dst1, dst2, dst3);
+        DUP4_ARG2(__lsx_vavgr_bu, src4, dst4, src5, dst5,
+                  src6, dst6, src7, dst7, dst4, dst5, dst6, dst7);
+        __lsx_vst(dst0, dst_tmp1, 0);
+        __lsx_vstx(dst2, dst_tmp1, dst_stride);
+        __lsx_vstx(dst4, dst_tmp1, dst_stride2);
+        __lsx_vstx(dst6, dst_tmp1, dst_stride3);
+        dst_tmp1 += dst_stride4;
+        __lsx_vst(dst1, dst_tmp2, 0);
+        __lsx_vstx(dst3, dst_tmp2, dst_stride);
+        __lsx_vstx(dst5, dst_tmp2, dst_stride2);
+        __lsx_vstx(dst7, dst_tmp2, dst_stride3);
+        dst_tmp2 += dst_stride4;
+    }
+}
+
+static void avg_width64_lsx(const uint8_t *src, int32_t src_stride,
+                            uint8_t *dst, int32_t dst_stride,
+                            int32_t height)
+{
+    int32_t cnt = height >> 2;
+    uint8_t *dst_tmp = dst;
+    __m128i src0, src1, src2, src3, src4, src5, src6, src7;
+    __m128i src8, src9, src10, src11, src12, src13, src14, src15;
+    __m128i dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    __m128i dst8, dst9, dst10, dst11, dst12, dst13, dst14, dst15;
+
+    for (;cnt--;) {
+        DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48,
+                  src0, src1, src2, src3);
+        src += src_stride;
+        DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48,
+                  src4, src5, src6, src7);
+        src += src_stride;
+        DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48,
+                  src8, src9, src10, src11);
+        src += src_stride;
+        DUP4_ARG2(__lsx_vld, src, 0, src, 16, src, 32, src, 48,
+                  src12, src13, src14, src15);
+        src += src_stride;
+        DUP4_ARG2(__lsx_vld, dst_tmp, 0, dst_tmp, 16, dst_tmp, 32, dst_tmp, 48,
+                  dst0, dst1, dst2, dst3);
+        dst_tmp += dst_stride;
+        DUP4_ARG2(__lsx_vld, dst_tmp, 0, dst_tmp, 16, dst_tmp, 32, dst_tmp, 48,
+                  dst4, dst5, dst6, dst7);
+        dst_tmp += dst_stride;
+        DUP4_ARG2(__lsx_vld, dst_tmp, 0, dst_tmp, 16, dst_tmp, 32, dst_tmp, 48,
+                  dst8, dst9, dst10, dst11);
+        dst_tmp += dst_stride;
+        DUP4_ARG2(__lsx_vld, dst_tmp, 0, dst_tmp, 16, dst_tmp, 32, dst_tmp, 48,
+                  dst12, dst13, dst14, dst15);
+        dst_tmp += dst_stride;
+        DUP4_ARG2(__lsx_vavgr_bu, src0, dst0, src1, dst1,
+                  src2, dst2, src3, dst3, dst0, dst1, dst2, dst3);
+        DUP4_ARG2(__lsx_vavgr_bu, src4, dst4, src5, dst5,
+                  src6, dst6, src7, dst7, dst4, dst5, dst6, dst7);
+        DUP4_ARG2(__lsx_vavgr_bu, src8, dst8, src9, dst9, src10,
+                  dst10, src11, dst11, dst8, dst9, dst10, dst11);
+        DUP4_ARG2(__lsx_vavgr_bu, src12, dst12, src13, dst13, src14,
+                  dst14, src15, dst15, dst12, dst13, dst14, dst15);
+        __lsx_vst(dst0, dst, 0);
+        __lsx_vst(dst1, dst, 16);
+        __lsx_vst(dst2, dst, 32);
+        __lsx_vst(dst3, dst, 48);
+        dst += dst_stride;
+        __lsx_vst(dst4, dst, 0);
+        __lsx_vst(dst5, dst, 16);
+        __lsx_vst(dst6, dst, 32);
+        __lsx_vst(dst7, dst, 48);
+        dst += dst_stride;
+        __lsx_vst(dst8, dst, 0);
+        __lsx_vst(dst9, dst, 16);
+        __lsx_vst(dst10, dst, 32);
+        __lsx_vst(dst11, dst, 48);
+        dst += dst_stride;
+        __lsx_vst(dst12, dst, 0);
+        __lsx_vst(dst13, dst, 16);
+        __lsx_vst(dst14, dst, 32);
+        __lsx_vst(dst15, dst, 48);
+        dst += dst_stride;
+    }
+}
+
+static const int8_t vp9_subpel_filters_lsx[3][15][8] = {
+    [FILTER_8TAP_REGULAR] = {
+         {0, 1, -5, 126, 8, -3, 1, 0},
+         {-1, 3, -10, 122, 18, -6, 2, 0},
+         {-1, 4, -13, 118, 27, -9, 3, -1},
+         {-1, 4, -16, 112, 37, -11, 4, -1},
+         {-1, 5, -18, 105, 48, -14, 4, -1},
+         {-1, 5, -19, 97, 58, -16, 5, -1},
+         {-1, 6, -19, 88, 68, -18, 5, -1},
+         {-1, 6, -19, 78, 78, -19, 6, -1},
+         {-1, 5, -18, 68, 88, -19, 6, -1},
+         {-1, 5, -16, 58, 97, -19, 5, -1},
+         {-1, 4, -14, 48, 105, -18, 5, -1},
+         {-1, 4, -11, 37, 112, -16, 4, -1},
+         {-1, 3, -9, 27, 118, -13, 4, -1},
+         {0, 2, -6, 18, 122, -10, 3, -1},
+         {0, 1, -3, 8, 126, -5, 1, 0},
+    }, [FILTER_8TAP_SHARP] = {
+        {-1, 3, -7, 127, 8, -3, 1, 0},
+        {-2, 5, -13, 125, 17, -6, 3, -1},
+        {-3, 7, -17, 121, 27, -10, 5, -2},
+        {-4, 9, -20, 115, 37, -13, 6, -2},
+        {-4, 10, -23, 108, 48, -16, 8, -3},
+        {-4, 10, -24, 100, 59, -19, 9, -3},
+        {-4, 11, -24, 90, 70, -21, 10, -4},
+        {-4, 11, -23, 80, 80, -23, 11, -4},
+        {-4, 10, -21, 70, 90, -24, 11, -4},
+        {-3, 9, -19, 59, 100, -24, 10, -4},
+        {-3, 8, -16, 48, 108, -23, 10, -4},
+        {-2, 6, -13, 37, 115, -20, 9, -4},
+        {-2, 5, -10, 27, 121, -17, 7, -3},
+        {-1, 3, -6, 17, 125, -13, 5, -2},
+        {0, 1, -3, 8, 127, -7, 3, -1},
+    }, [FILTER_8TAP_SMOOTH] = {
+        {-3, -1, 32, 64, 38, 1, -3, 0},
+        {-2, -2, 29, 63, 41, 2, -3, 0},
+        {-2, -2, 26, 63, 43, 4, -4, 0},
+        {-2, -3, 24, 62, 46, 5, -4, 0},
+        {-2, -3, 21, 60, 49, 7, -4, 0},
+        {-1, -4, 18, 59, 51, 9, -4, 0},
+        {-1, -4, 16, 57, 53, 12, -4, -1},
+        {-1, -4, 14, 55, 55, 14, -4, -1},
+        {-1, -4, 12, 53, 57, 16, -4, -1},
+        {0, -4, 9, 51, 59, 18, -4, -1},
+        {0, -4, 7, 49, 60, 21, -3, -2},
+        {0, -4, 5, 46, 62, 24, -3, -2},
+        {0, -4, 4, 43, 63, 26, -2, -2},
+        {0, -3, 2, 41, 63, 29, -2, -2},
+        {0, -3, 1, 38, 64, 32, -1, -3},
+    }
+};
+
+#define VP9_8TAP_LOONGARCH_LSX_FUNC(SIZE, type, type_idx)                      \
+void ff_put_8tap_##type##_##SIZE##h_lsx(uint8_t *dst, ptrdiff_t dststride,     \
+                                        const uint8_t *src,                    \
+                                        ptrdiff_t srcstride,                   \
+                                        int h, int mx, int my)                 \
+{                                                                              \
+    const int8_t *filter = vp9_subpel_filters_lsx[type_idx][mx-1];             \
+                                                                               \
+    common_hz_8t_##SIZE##w_lsx(src, srcstride, dst, dststride, filter, h);     \
+}                                                                              \
+                                                                               \
+void ff_put_8tap_##type##_##SIZE##v_lsx(uint8_t *dst, ptrdiff_t dststride,     \
+                                        const uint8_t *src,                    \
+                                        ptrdiff_t srcstride,                   \
+                                        int h, int mx, int my)                 \
+{                                                                              \
+    const int8_t *filter = vp9_subpel_filters_lsx[type_idx][my-1];             \
+                                                                               \
+    common_vt_8t_##SIZE##w_lsx(src, srcstride, dst, dststride, filter, h);     \
+}                                                                              \
+                                                                               \
+void ff_put_8tap_##type##_##SIZE##hv_lsx(uint8_t *dst, ptrdiff_t dststride,    \
+                                         const uint8_t *src,                   \
+                                         ptrdiff_t srcstride,                  \
+                                         int h, int mx, int my)                \
+{                                                                              \
+    const int8_t *hfilter = vp9_subpel_filters_lsx[type_idx][mx-1];            \
+    const int8_t *vfilter = vp9_subpel_filters_lsx[type_idx][my-1];            \
+                                                                               \
+    common_hv_8ht_8vt_##SIZE##w_lsx(src, srcstride, dst, dststride, hfilter,   \
+                                    vfilter, h);                               \
+}                                                                              \
+                                                                               \
+void ff_avg_8tap_##type##_##SIZE##h_lsx(uint8_t *dst, ptrdiff_t dststride,     \
+                                        const uint8_t *src,                    \
+                                        ptrdiff_t srcstride,                   \
+                                        int h, int mx, int my)                 \
+{                                                                              \
+    const int8_t *filter = vp9_subpel_filters_lsx[type_idx][mx-1];             \
+                                                                               \
+    common_hz_8t_and_aver_dst_##SIZE##w_lsx(src, srcstride, dst,               \
+                                            dststride, filter, h);             \
+}                                                                              \
+                                                                               \
+void ff_avg_8tap_##type##_##SIZE##v_lsx(uint8_t *dst, ptrdiff_t dststride,     \
+                                        const uint8_t *src,                    \
+                                        ptrdiff_t srcstride,                   \
+                                        int h, int mx, int my)                 \
+{                                                                              \
+    const int8_t *filter = vp9_subpel_filters_lsx[type_idx][my-1];             \
+                                                                               \
+    common_vt_8t_and_aver_dst_##SIZE##w_lsx(src, srcstride, dst, dststride,    \
+                                            filter, h);                        \
+}                                                                              \
+                                                                               \
+void ff_avg_8tap_##type##_##SIZE##hv_lsx(uint8_t *dst, ptrdiff_t dststride,    \
+                                         const uint8_t *src,                   \
+                                         ptrdiff_t srcstride,                  \
+                                         int h, int mx, int my)                \
+{                                                                              \
+    const int8_t *hfilter = vp9_subpel_filters_lsx[type_idx][mx-1];            \
+    const int8_t *vfilter = vp9_subpel_filters_lsx[type_idx][my-1];            \
+                                                                               \
+    common_hv_8ht_8vt_and_aver_dst_##SIZE##w_lsx(src, srcstride, dst,          \
+                                                 dststride, hfilter,           \
+                                                 vfilter, h);                  \
+}
+
+#define VP9_COPY_LOONGARCH_LSX_FUNC(SIZE)                          \
+void ff_copy##SIZE##_lsx(uint8_t *dst, ptrdiff_t dststride,        \
+                         const uint8_t *src, ptrdiff_t srcstride,  \
+                         int h, int mx, int my)                    \
+{                                                                  \
+                                                                   \
+    copy_width##SIZE##_lsx(src, srcstride, dst, dststride, h);     \
+}                                                                  \
+void ff_avg##SIZE##_lsx(uint8_t *dst, ptrdiff_t dststride,         \
+                        const uint8_t *src, ptrdiff_t srcstride,   \
+                        int h, int mx, int my)                     \
+{                                                                  \
+                                                                   \
+    avg_width##SIZE##_lsx(src, srcstride, dst, dststride, h);      \
+}
+
+VP9_8TAP_LOONGARCH_LSX_FUNC(64, regular, FILTER_8TAP_REGULAR);
+VP9_8TAP_LOONGARCH_LSX_FUNC(32, regular, FILTER_8TAP_REGULAR);
+VP9_8TAP_LOONGARCH_LSX_FUNC(16, regular, FILTER_8TAP_REGULAR);
+VP9_8TAP_LOONGARCH_LSX_FUNC(8, regular, FILTER_8TAP_REGULAR);
+VP9_8TAP_LOONGARCH_LSX_FUNC(4, regular, FILTER_8TAP_REGULAR);
+
+VP9_8TAP_LOONGARCH_LSX_FUNC(64, sharp, FILTER_8TAP_SHARP);
+VP9_8TAP_LOONGARCH_LSX_FUNC(32, sharp, FILTER_8TAP_SHARP);
+VP9_8TAP_LOONGARCH_LSX_FUNC(16, sharp, FILTER_8TAP_SHARP);
+VP9_8TAP_LOONGARCH_LSX_FUNC(8, sharp, FILTER_8TAP_SHARP);
+VP9_8TAP_LOONGARCH_LSX_FUNC(4, sharp, FILTER_8TAP_SHARP);
+
+VP9_8TAP_LOONGARCH_LSX_FUNC(64, smooth, FILTER_8TAP_SMOOTH);
+VP9_8TAP_LOONGARCH_LSX_FUNC(32, smooth, FILTER_8TAP_SMOOTH);
+VP9_8TAP_LOONGARCH_LSX_FUNC(16, smooth, FILTER_8TAP_SMOOTH);
+VP9_8TAP_LOONGARCH_LSX_FUNC(8, smooth, FILTER_8TAP_SMOOTH);
+VP9_8TAP_LOONGARCH_LSX_FUNC(4, smooth, FILTER_8TAP_SMOOTH);
+
+VP9_COPY_LOONGARCH_LSX_FUNC(64);
+VP9_COPY_LOONGARCH_LSX_FUNC(32);
+VP9_COPY_LOONGARCH_LSX_FUNC(16);
+VP9_COPY_LOONGARCH_LSX_FUNC(8);
+
+#undef VP9_8TAP_LOONGARCH_LSX_FUNC
+#undef VP9_COPY_LOONGARCH_LSX_FUNC
diff --git a/libavcodec/loongarch/vp9dsp_init_loongarch.c b/libavcodec/loongarch/vp9dsp_init_loongarch.c
new file mode 100644
index 0000000000..e49625ad5f
--- /dev/null
+++ b/libavcodec/loongarch/vp9dsp_init_loongarch.c
@@ -0,0 +1,130 @@
+/*
+ * Copyright (c) 2021 Loongson Technology Corporation Limited
+ * Contributed by Hao Chen <chenhao@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "libavutil/loongarch/cpu.h"
+#include "libavutil/attributes.h"
+#include "libavcodec/vp9dsp.h"
+#include "vp9dsp_loongarch.h"
+
+#define init_subpel1(idx1, idx2, idxh, idxv, sz, dir, type)  \
+    dsp->mc[idx1][FILTER_8TAP_SMOOTH ][idx2][idxh][idxv] =   \
+        ff_##type##_8tap_smooth_##sz##dir##_lsx;             \
+    dsp->mc[idx1][FILTER_8TAP_REGULAR][idx2][idxh][idxv] =   \
+        ff_##type##_8tap_regular_##sz##dir##_lsx;            \
+    dsp->mc[idx1][FILTER_8TAP_SHARP  ][idx2][idxh][idxv] =   \
+        ff_##type##_8tap_sharp_##sz##dir##_lsx;
+
+#define init_subpel2(idx, idxh, idxv, dir, type)      \
+    init_subpel1(0, idx, idxh, idxv, 64, dir, type);  \
+    init_subpel1(1, idx, idxh, idxv, 32, dir, type);  \
+    init_subpel1(2, idx, idxh, idxv, 16, dir, type);  \
+    init_subpel1(3, idx, idxh, idxv,  8, dir, type);  \
+    init_subpel1(4, idx, idxh, idxv,  4, dir, type);
+
+#define init_subpel3(idx, type)         \
+    init_subpel2(idx, 1, 0, h, type);   \
+    init_subpel2(idx, 0, 1, v, type);   \
+    init_subpel2(idx, 1, 1, hv, type);
+
+#define init_fpel(idx1, idx2, sz, type)                                    \
+    dsp->mc[idx1][FILTER_8TAP_SMOOTH ][idx2][0][0] = ff_##type##sz##_lsx;  \
+    dsp->mc[idx1][FILTER_8TAP_REGULAR][idx2][0][0] = ff_##type##sz##_lsx;  \
+    dsp->mc[idx1][FILTER_8TAP_SHARP  ][idx2][0][0] = ff_##type##sz##_lsx;  \
+    dsp->mc[idx1][FILTER_BILINEAR    ][idx2][0][0] = ff_##type##sz##_lsx;
+
+#define init_copy(idx, sz)                    \
+    init_fpel(idx, 0, sz, copy);              \
+    init_fpel(idx, 1, sz, avg);
+
+#define init_intra_pred1_lsx(tx, sz)                            \
+    dsp->intra_pred[tx][VERT_PRED]    = ff_vert_##sz##_lsx;     \
+    dsp->intra_pred[tx][HOR_PRED]     = ff_hor_##sz##_lsx;      \
+    dsp->intra_pred[tx][DC_PRED]      = ff_dc_##sz##_lsx;       \
+    dsp->intra_pred[tx][LEFT_DC_PRED] = ff_dc_left_##sz##_lsx;  \
+    dsp->intra_pred[tx][TOP_DC_PRED]  = ff_dc_top_##sz##_lsx;   \
+    dsp->intra_pred[tx][DC_128_PRED]  = ff_dc_128_##sz##_lsx;   \
+    dsp->intra_pred[tx][DC_127_PRED]  = ff_dc_127_##sz##_lsx;   \
+    dsp->intra_pred[tx][DC_129_PRED]  = ff_dc_129_##sz##_lsx;   \
+    dsp->intra_pred[tx][TM_VP8_PRED]  = ff_tm_##sz##_lsx;       \
+
+#define init_intra_pred2_lsx(tx, sz)                            \
+    dsp->intra_pred[tx][DC_PRED]      = ff_dc_##sz##_lsx;       \
+    dsp->intra_pred[tx][LEFT_DC_PRED] = ff_dc_left_##sz##_lsx;  \
+    dsp->intra_pred[tx][TOP_DC_PRED]  = ff_dc_top_##sz##_lsx;   \
+    dsp->intra_pred[tx][TM_VP8_PRED]  = ff_tm_##sz##_lsx;       \
+
+#define init_idct(tx, nm)                        \
+    dsp->itxfm_add[tx][DCT_DCT]   =              \
+    dsp->itxfm_add[tx][ADST_DCT]  =              \
+    dsp->itxfm_add[tx][DCT_ADST]  =              \
+    dsp->itxfm_add[tx][ADST_ADST] = nm##_add_lsx;
+
+#define init_itxfm(tx, sz)                                     \
+    dsp->itxfm_add[tx][DCT_DCT] = ff_idct_idct_##sz##_add_lsx;
+
+av_cold void ff_vp9dsp_init_loongarch(VP9DSPContext *dsp, int bpp)
+{
+    int cpu_flags = av_get_cpu_flags();
+    if (have_lsx(cpu_flags))
+        if (bpp == 8) {
+            init_subpel3(0, put);
+            init_subpel3(1, avg);
+            init_copy(0, 64);
+            init_copy(1, 32);
+            init_copy(2, 16);
+            init_copy(3, 8);
+            init_intra_pred1_lsx(TX_16X16, 16x16);
+            init_intra_pred1_lsx(TX_32X32, 32x32);
+            init_intra_pred2_lsx(TX_4X4, 4x4);
+            init_intra_pred2_lsx(TX_8X8, 8x8);
+            init_itxfm(TX_8X8, 8x8);
+            init_itxfm(TX_16X16, 16x16);
+            init_idct(TX_32X32, ff_idct_idct_32x32);
+            dsp->loop_filter_8[0][0] = ff_loop_filter_h_4_8_lsx;
+            dsp->loop_filter_8[0][1] = ff_loop_filter_v_4_8_lsx;
+            dsp->loop_filter_8[1][0] = ff_loop_filter_h_8_8_lsx;
+            dsp->loop_filter_8[1][1] = ff_loop_filter_v_8_8_lsx;
+            dsp->loop_filter_8[2][0] = ff_loop_filter_h_16_8_lsx;
+            dsp->loop_filter_8[2][1] = ff_loop_filter_v_16_8_lsx;
+
+            dsp->loop_filter_16[0] = ff_loop_filter_h_16_16_lsx;
+            dsp->loop_filter_16[1] = ff_loop_filter_v_16_16_lsx;
+
+            dsp->loop_filter_mix2[0][0][0] = ff_loop_filter_h_44_16_lsx;
+            dsp->loop_filter_mix2[0][0][1] = ff_loop_filter_v_44_16_lsx;
+            dsp->loop_filter_mix2[0][1][0] = ff_loop_filter_h_48_16_lsx;
+            dsp->loop_filter_mix2[0][1][1] = ff_loop_filter_v_48_16_lsx;
+            dsp->loop_filter_mix2[1][0][0] = ff_loop_filter_h_84_16_lsx;
+            dsp->loop_filter_mix2[1][0][1] = ff_loop_filter_v_84_16_lsx;
+            dsp->loop_filter_mix2[1][1][0] = ff_loop_filter_h_88_16_lsx;
+            dsp->loop_filter_mix2[1][1][1] = ff_loop_filter_v_88_16_lsx;
+        }
+}
+
+#undef init_subpel1
+#undef init_subpel2
+#undef init_subpel3
+#undef init_copy
+#undef init_fpel
+#undef init_intra_pred1_lsx
+#undef init_intra_pred2_lsx
+#undef init_idct
+#undef init_itxfm
diff --git a/libavcodec/loongarch/vp9dsp_loongarch.h b/libavcodec/loongarch/vp9dsp_loongarch.h
new file mode 100644
index 0000000000..3cc918a18c
--- /dev/null
+++ b/libavcodec/loongarch/vp9dsp_loongarch.h
@@ -0,0 +1,182 @@
+/*
+ * Copyright (c) 2021 Loongson Technology Corporation Limited
+ * Contributed by Hao Chen <chenhao@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef AVCODEC_LOONGARCH_VP9DSP_LOONGARCH_H
+#define AVCODEC_LOONGARCH_VP9DSP_LOONGARCH_H
+
+#define VP9_8TAP_LOONGARCH_LSX_FUNC(SIZE, type, type_idx)                    \
+void ff_put_8tap_##type##_##SIZE##h_lsx(uint8_t *dst, ptrdiff_t dststride,   \
+                                        const uint8_t *src,                  \
+                                        ptrdiff_t srcstride,                 \
+                                        int h, int mx, int my);              \
+                                                                             \
+void ff_put_8tap_##type##_##SIZE##v_lsx(uint8_t *dst, ptrdiff_t dststride,   \
+                                        const uint8_t *src,                  \
+                                        ptrdiff_t srcstride,                 \
+                                        int h, int mx, int my);              \
+                                                                             \
+void ff_put_8tap_##type##_##SIZE##hv_lsx(uint8_t *dst, ptrdiff_t dststride,  \
+                                         const uint8_t *src,                 \
+                                         ptrdiff_t srcstride,                \
+                                         int h, int mx, int my);             \
+                                                                             \
+void ff_avg_8tap_##type##_##SIZE##h_lsx(uint8_t *dst, ptrdiff_t dststride,   \
+                                        const uint8_t *src,                  \
+                                        ptrdiff_t srcstride,                 \
+                                        int h, int mx, int my);              \
+                                                                             \
+void ff_avg_8tap_##type##_##SIZE##v_lsx(uint8_t *dst, ptrdiff_t dststride,   \
+                                        const uint8_t *src,                  \
+                                        ptrdiff_t srcstride,                 \
+                                        int h, int mx, int my);              \
+                                                                             \
+void ff_avg_8tap_##type##_##SIZE##hv_lsx(uint8_t *dst, ptrdiff_t dststride,  \
+                                         const uint8_t *src,                 \
+                                         ptrdiff_t srcstride,                \
+                                         int h, int mx, int my);
+
+#define VP9_COPY_LOONGARCH_LSX_FUNC(SIZE)                          \
+void ff_copy##SIZE##_lsx(uint8_t *dst, ptrdiff_t dststride,        \
+                         const uint8_t *src, ptrdiff_t srcstride,  \
+                         int h, int mx, int my);                   \
+                                                                   \
+void ff_avg##SIZE##_lsx(uint8_t *dst, ptrdiff_t dststride,         \
+                        const uint8_t *src, ptrdiff_t srcstride,   \
+                        int h, int mx, int my);
+
+VP9_8TAP_LOONGARCH_LSX_FUNC(64, regular, FILTER_8TAP_REGULAR);
+VP9_8TAP_LOONGARCH_LSX_FUNC(32, regular, FILTER_8TAP_REGULAR);
+VP9_8TAP_LOONGARCH_LSX_FUNC(16, regular, FILTER_8TAP_REGULAR);
+VP9_8TAP_LOONGARCH_LSX_FUNC(8, regular, FILTER_8TAP_REGULAR);
+VP9_8TAP_LOONGARCH_LSX_FUNC(4, regular, FILTER_8TAP_REGULAR);
+
+VP9_8TAP_LOONGARCH_LSX_FUNC(64, sharp, FILTER_8TAP_SHARP);
+VP9_8TAP_LOONGARCH_LSX_FUNC(32, sharp, FILTER_8TAP_SHARP);
+VP9_8TAP_LOONGARCH_LSX_FUNC(16, sharp, FILTER_8TAP_SHARP);
+VP9_8TAP_LOONGARCH_LSX_FUNC(8, sharp, FILTER_8TAP_SHARP);
+VP9_8TAP_LOONGARCH_LSX_FUNC(4, sharp, FILTER_8TAP_SHARP);
+
+VP9_8TAP_LOONGARCH_LSX_FUNC(64, smooth, FILTER_8TAP_SMOOTH);
+VP9_8TAP_LOONGARCH_LSX_FUNC(32, smooth, FILTER_8TAP_SMOOTH);
+VP9_8TAP_LOONGARCH_LSX_FUNC(16, smooth, FILTER_8TAP_SMOOTH);
+VP9_8TAP_LOONGARCH_LSX_FUNC(8, smooth, FILTER_8TAP_SMOOTH);
+VP9_8TAP_LOONGARCH_LSX_FUNC(4, smooth, FILTER_8TAP_SMOOTH);
+
+VP9_COPY_LOONGARCH_LSX_FUNC(64);
+VP9_COPY_LOONGARCH_LSX_FUNC(32);
+VP9_COPY_LOONGARCH_LSX_FUNC(16);
+VP9_COPY_LOONGARCH_LSX_FUNC(8);
+
+#undef VP9_8TAP_LOONGARCH_LSX_FUNC
+#undef VP9_COPY_LOONGARCH_LSX_FUNC
+
+void ff_vert_16x16_lsx(uint8_t *dst, ptrdiff_t stride, const uint8_t *left,
+                       const uint8_t *top);
+void ff_vert_32x32_lsx(uint8_t *dst, ptrdiff_t stride, const uint8_t *left,
+                       const uint8_t *top);
+void ff_hor_16x16_lsx(uint8_t *dst, ptrdiff_t stride, const uint8_t *left,
+                      const uint8_t *top);
+void ff_hor_32x32_lsx(uint8_t *dst, ptrdiff_t stride, const uint8_t *left,
+                      const uint8_t *top);
+void ff_dc_4x4_lsx(uint8_t *dst, ptrdiff_t stride, const uint8_t *left,
+                   const uint8_t *top);
+void ff_dc_8x8_lsx(uint8_t *dst, ptrdiff_t stride, const uint8_t *left,
+                   const uint8_t *top);
+void ff_dc_16x16_lsx(uint8_t *dst, ptrdiff_t stride, const uint8_t *left,
+                     const uint8_t *top);
+void ff_dc_32x32_lsx(uint8_t *dst, ptrdiff_t stride, const uint8_t *left,
+                     const uint8_t *top);
+void ff_dc_left_4x4_lsx(uint8_t *dst, ptrdiff_t stride, const uint8_t *left,
+                        const uint8_t *top);
+void ff_dc_left_8x8_lsx(uint8_t *dst, ptrdiff_t stride, const uint8_t *left,
+                        const uint8_t *top);
+void ff_dc_left_16x16_lsx(uint8_t *dst, ptrdiff_t stride,
+                          const uint8_t *left, const uint8_t *top);
+void ff_dc_left_32x32_lsx(uint8_t *dst, ptrdiff_t stride,
+                          const uint8_t *left, const uint8_t *top);
+void ff_dc_top_4x4_lsx(uint8_t *dst, ptrdiff_t stride, const uint8_t *left,
+                       const uint8_t *top);
+void ff_dc_top_8x8_lsx(uint8_t *dst, ptrdiff_t stride, const uint8_t *left,
+                       const uint8_t *top);
+void ff_dc_top_16x16_lsx(uint8_t *dst, ptrdiff_t stride,
+                         const uint8_t *left, const uint8_t *top);
+void ff_dc_top_32x32_lsx(uint8_t *dst, ptrdiff_t stride,
+                         const uint8_t *left, const uint8_t *top);
+void ff_dc_128_16x16_lsx(uint8_t *dst, ptrdiff_t stride,
+                         const uint8_t *left, const uint8_t *top);
+void ff_dc_128_32x32_lsx(uint8_t *dst, ptrdiff_t stride,
+                         const uint8_t *left, const uint8_t *top);
+void ff_dc_127_16x16_lsx(uint8_t *dst, ptrdiff_t stride,
+                         const uint8_t *left, const uint8_t *top);
+void ff_dc_127_32x32_lsx(uint8_t *dst, ptrdiff_t stride,
+                         const uint8_t *left, const uint8_t *top);
+void ff_dc_129_16x16_lsx(uint8_t *dst, ptrdiff_t stride,
+                         const uint8_t *left, const uint8_t *top);
+void ff_dc_129_32x32_lsx(uint8_t *dst, ptrdiff_t stride,
+                         const uint8_t *left, const uint8_t *top);
+void ff_tm_4x4_lsx(uint8_t *dst, ptrdiff_t stride, const uint8_t *left,
+                   const uint8_t *top);
+void ff_tm_8x8_lsx(uint8_t *dst, ptrdiff_t stride, const uint8_t *left,
+                   const uint8_t *top);
+void ff_tm_16x16_lsx(uint8_t *dst, ptrdiff_t stride, const uint8_t *left,
+                     const uint8_t *top);
+void ff_tm_32x32_lsx(uint8_t *dst, ptrdiff_t stride, const uint8_t *left,
+                     const uint8_t *top);
+void ff_loop_filter_h_16_8_lsx(uint8_t *dst, ptrdiff_t stride, int32_t e,
+                               int32_t i, int32_t h);
+void ff_loop_filter_v_16_8_lsx(uint8_t *dst, ptrdiff_t stride, int32_t e,
+                               int32_t i, int32_t h);
+void ff_loop_filter_h_4_8_lsx(uint8_t *dst, ptrdiff_t stride, int32_t e,
+                              int32_t i, int32_t h);
+void ff_loop_filter_v_4_8_lsx(uint8_t *dst, ptrdiff_t stride, int32_t e,
+                              int32_t i, int32_t h);
+void ff_loop_filter_h_44_16_lsx(uint8_t *dst, ptrdiff_t stride, int32_t e,
+                                int32_t i, int32_t h);
+void ff_loop_filter_v_44_16_lsx(uint8_t *dst, ptrdiff_t stride, int32_t e,
+                                int32_t i, int32_t h);
+void ff_loop_filter_h_8_8_lsx(uint8_t *dst, ptrdiff_t stride, int32_t e,
+                              int32_t i, int32_t h);
+void ff_loop_filter_v_8_8_lsx(uint8_t *dst, ptrdiff_t stride, int32_t e,
+                              int32_t i, int32_t h);
+void ff_loop_filter_h_88_16_lsx(uint8_t *dst, ptrdiff_t stride, int32_t e,
+                                int32_t i, int32_t h);
+void ff_loop_filter_v_88_16_lsx(uint8_t *dst, ptrdiff_t stride, int32_t e,
+                                int32_t i, int32_t h);
+void ff_loop_filter_h_84_16_lsx(uint8_t *dst, ptrdiff_t stride, int32_t e,
+                                int32_t i, int32_t h);
+void ff_loop_filter_v_84_16_lsx(uint8_t *dst, ptrdiff_t stride, int32_t e,
+                                int32_t i, int32_t h);
+void ff_loop_filter_h_48_16_lsx(uint8_t *dst, ptrdiff_t stride, int32_t e,
+                                int32_t i, int32_t h);
+void ff_loop_filter_v_48_16_lsx(uint8_t *dst, ptrdiff_t stride, int32_t e,
+                                int32_t i, int32_t h);
+void ff_loop_filter_h_16_16_lsx(uint8_t *dst, ptrdiff_t stride, int32_t e,
+                                int32_t i, int32_t h);
+void ff_loop_filter_v_16_16_lsx(uint8_t *dst, ptrdiff_t stride, int32_t e,
+                                int32_t i, int32_t h);
+void ff_idct_idct_8x8_add_lsx(uint8_t *dst, ptrdiff_t stride,
+                              int16_t *block, int eob);
+void ff_idct_idct_16x16_add_lsx(uint8_t *dst, ptrdiff_t stride,
+                                int16_t *block, int eob);
+void ff_idct_idct_32x32_add_lsx(uint8_t *dst, ptrdiff_t stride,
+                                int16_t *block, int eob);
+
+#endif /* AVCODEC_LOONGARCH_VP9DSP_LOONGARCH_H */
diff --git a/libavcodec/mips/Makefile b/libavcodec/mips/Makefile
index c5b54d55c0..81a73a4d0e 100644
--- a/libavcodec/mips/Makefile
+++ b/libavcodec/mips/Makefile
@@ -57,7 +57,8 @@ MSA-OBJS-$(CONFIG_VP8_DECODER)            += mips/vp8_mc_msa.o             \
                                              mips/vp8_lpf_msa.o
 MSA-OBJS-$(CONFIG_VP3DSP)                 += mips/vp3dsp_idct_msa.o
 MSA-OBJS-$(CONFIG_H264DSP)                += mips/h264dsp_msa.o            \
-                                             mips/h264idct_msa.o
+                                             mips/h264idct_msa.o           \
+                                             mips/h264_deblock_msa.o
 MSA-OBJS-$(CONFIG_H264QPEL)               += mips/h264qpel_msa.o
 MSA-OBJS-$(CONFIG_H264CHROMA)             += mips/h264chroma_msa.o
 MSA-OBJS-$(CONFIG_H264PRED)               += mips/h264pred_msa.o
@@ -71,6 +72,8 @@ MSA-OBJS-$(CONFIG_IDCTDSP)                += mips/idctdsp_msa.o           \
 MSA-OBJS-$(CONFIG_MPEGVIDEO)              += mips/mpegvideo_msa.o
 MSA-OBJS-$(CONFIG_MPEGVIDEOENC)           += mips/mpegvideoencdsp_msa.o
 MSA-OBJS-$(CONFIG_ME_CMP)                 += mips/me_cmp_msa.o
+MSA-OBJS-$(CONFIG_VC1_DECODER)            += mips/vc1dsp_msa.o
+
 MMI-OBJS                                  += mips/constants.o
 MMI-OBJS-$(CONFIG_H264DSP)                += mips/h264dsp_mmi.o
 MMI-OBJS-$(CONFIG_H264CHROMA)             += mips/h264chroma_mmi.o
diff --git a/libavcodec/mips/aacdec_mips.c b/libavcodec/mips/aacdec_mips.c
index 253cdeb80b..d02245dbfa 100644
--- a/libavcodec/mips/aacdec_mips.c
+++ b/libavcodec/mips/aacdec_mips.c
@@ -237,9 +237,9 @@ static void apply_ltp_mips(AACContext *ac, SingleChannelElement *sce)
 
         if (ltp->lag < 1024)
             num_samples = ltp->lag + 1024;
-            j = (2048 - num_samples) >> 2;
-            k = (2048 - num_samples) & 3;
-            p_predTime = &predTime[num_samples];
+        j = (2048 - num_samples) >> 2;
+        k = (2048 - num_samples) & 3;
+        p_predTime = &predTime[num_samples];
 
         for (i = 0; i < num_samples; i++)
             predTime[i] = sce->ltp_state[i + 2048 - ltp->lag] * ltp->coef;
@@ -340,7 +340,7 @@ static void update_ltp_mips(AACContext *ac, SingleChannelElement *sce)
     float *saved_ltp = sce->coeffs;
     const float *lwindow = ics->use_kb_window[0] ? ff_aac_kbd_long_1024 : ff_sine_1024;
     const float *swindow = ics->use_kb_window[0] ? ff_aac_kbd_short_128 : ff_sine_128;
-    float temp0, temp1, temp2, temp3, temp4, temp5, temp6, temp7;
+    uint32_t temp0, temp1, temp2, temp3, temp4, temp5, temp6, temp7;
 
     if (ics->window_sequence[0] == EIGHT_SHORT_SEQUENCE) {
         float *p_saved_ltp = saved_ltp + 576;
diff --git a/libavcodec/mips/aacpsdsp_mips.c b/libavcodec/mips/aacpsdsp_mips.c
index 83fdc2f9db..ae628a9815 100644
--- a/libavcodec/mips/aacpsdsp_mips.c
+++ b/libavcodec/mips/aacpsdsp_mips.c
@@ -293,16 +293,17 @@ static void ps_decorrelate_mips(float (*out)[2], float (*delay)[2],
     float phi_fract0 = phi_fract[0];
     float phi_fract1 = phi_fract[1];
     float temp0, temp1, temp2, temp3, temp4, temp5, temp6, temp7, temp8, temp9;
+    float f1, f2, f3;
 
     float *p_delay_end = (p_delay + (len << 1));
 
     /* merged 2 loops */
+    f1 = 0.65143905753106;
+    f2 = 0.56471812200776;
+    f3 = 0.48954165955695;
     __asm__ volatile(
         ".set    push                                                    \n\t"
         ".set    noreorder                                               \n\t"
-        "li.s    %[ag0],        0.65143905753106                         \n\t"
-        "li.s    %[ag1],        0.56471812200776                         \n\t"
-        "li.s    %[ag2],        0.48954165955695                         \n\t"
         "mul.s   %[ag0],        %[ag0],        %[g_decay_slope]          \n\t"
         "mul.s   %[ag1],        %[ag1],        %[g_decay_slope]          \n\t"
         "mul.s   %[ag2],        %[ag2],        %[g_decay_slope]          \n\t"
@@ -378,10 +379,10 @@ static void ps_decorrelate_mips(float (*out)[2], float (*delay)[2],
           [temp3]"=&f"(temp3), [temp4]"=&f"(temp4), [temp5]"=&f"(temp5),
           [temp6]"=&f"(temp6), [temp7]"=&f"(temp7), [temp8]"=&f"(temp8),
           [temp9]"=&f"(temp9), [p_delay]"+r"(p_delay), [p_ap_delay]"+r"(p_ap_delay),
-          [p_Q_fract]"+r"(p_Q_fract), [p_t_gain]"+r"(p_t_gain), [p_out]"+r"(p_out),
-          [ag0]"=&f"(ag0), [ag1]"=&f"(ag1), [ag2]"=&f"(ag2)
+          [p_Q_fract]"+r"(p_Q_fract), [p_t_gain]"+r"(p_t_gain), [p_out]"+r"(p_out)
         : [phi_fract0]"f"(phi_fract0), [phi_fract1]"f"(phi_fract1),
-          [p_delay_end]"r"(p_delay_end), [g_decay_slope]"f"(g_decay_slope)
+          [p_delay_end]"r"(p_delay_end), [g_decay_slope]"f"(g_decay_slope),
+          [ag0]"f"(f1), [ag1]"f"(f2), [ag2]"f"(f3)
         : "memory"
     );
 }
diff --git a/libavcodec/mips/aacpsy_mips.h b/libavcodec/mips/aacpsy_mips.h
index a1fe5ccea9..7d27d32f18 100644
--- a/libavcodec/mips/aacpsy_mips.h
+++ b/libavcodec/mips/aacpsy_mips.h
@@ -135,11 +135,11 @@ static void psy_hp_filter_mips(const float *firbuf, float *hpfsmpl, const float
     float coeff3 = psy_fir_coeffs[7];
     float coeff4 = psy_fir_coeffs[9];
 
+    float f1 = 32768.0;
     __asm__ volatile (
         ".set push                                          \n\t"
         ".set noreorder                                     \n\t"
 
-        "li.s   $f12,       32768                           \n\t"
         "1:                                                 \n\t"
         "lwc1   $f0,        40(%[fb])                       \n\t"
         "lwc1   $f1,        4(%[fb])                        \n\t"
@@ -203,14 +203,14 @@ static void psy_hp_filter_mips(const float *firbuf, float *hpfsmpl, const float
         "madd.s %[sum2],    %[sum2],    $f9,    %[coeff4]   \n\t"
         "madd.s %[sum4],    %[sum4],    $f6,    %[coeff4]   \n\t"
         "madd.s %[sum3],    %[sum3],    $f3,    %[coeff4]   \n\t"
-        "mul.s  %[sum1],    %[sum1],    $f12                \n\t"
-        "mul.s  %[sum2],    %[sum2],    $f12                \n\t"
+        "mul.s  %[sum1],    %[sum1],    %[f1]               \n\t"
+        "mul.s  %[sum2],    %[sum2],    %[f1]               \n\t"
         "madd.s %[sum4],    %[sum4],    $f11,   %[coeff4]   \n\t"
         "madd.s %[sum3],    %[sum3],    $f8,    %[coeff4]   \n\t"
         "swc1   %[sum1],    0(%[hp])                        \n\t"
         "swc1   %[sum2],    4(%[hp])                        \n\t"
-        "mul.s  %[sum4],    %[sum4],    $f12                \n\t"
-        "mul.s  %[sum3],    %[sum3],    $f12                \n\t"
+        "mul.s  %[sum4],    %[sum4],    %[f1]               \n\t"
+        "mul.s  %[sum3],    %[sum3],    %[f1]               \n\t"
         "swc1   %[sum4],    12(%[hp])                       \n\t"
         "swc1   %[sum3],    8(%[hp])                        \n\t"
         "bne    %[fb],      %[fb_end],  1b                  \n\t"
@@ -223,9 +223,9 @@ static void psy_hp_filter_mips(const float *firbuf, float *hpfsmpl, const float
           [fb]"+r"(fb), [hp]"+r"(hp)
         : [coeff0]"f"(coeff0), [coeff1]"f"(coeff1),
           [coeff2]"f"(coeff2), [coeff3]"f"(coeff3),
-          [coeff4]"f"(coeff4), [fb_end]"r"(fb_end)
+          [coeff4]"f"(coeff4), [fb_end]"r"(fb_end), [f1]"f"(f1)
         : "$f0", "$f1", "$f2", "$f3", "$f4", "$f5", "$f6",
-          "$f7", "$f8", "$f9", "$f10", "$f11", "$f12",
+          "$f7", "$f8", "$f9", "$f10", "$f11",
           "memory"
     );
 }
diff --git a/libavcodec/mips/aacsbr_mips.c b/libavcodec/mips/aacsbr_mips.c
index 56aa4e8682..fe9c08a3de 100644
--- a/libavcodec/mips/aacsbr_mips.c
+++ b/libavcodec/mips/aacsbr_mips.c
@@ -333,7 +333,7 @@ static void sbr_hf_assemble_mips(float Y1[38][64][2],
     int indexnoise = ch_data->f_indexnoise;
     int indexsine  = ch_data->f_indexsine;
     float *g_temp1, *q_temp1, *pok, *pok1;
-    float temp1, temp2, temp3, temp4;
+    uint32_t temp1, temp2, temp3, temp4;
     int size = m_max;
 
     if (sbr->reset) {
diff --git a/libavcodec/mips/blockdsp_init_mips.c b/libavcodec/mips/blockdsp_init_mips.c
index 55ac1c3e99..c6964fa74e 100644
--- a/libavcodec/mips/blockdsp_init_mips.c
+++ b/libavcodec/mips/blockdsp_init_mips.c
@@ -19,36 +19,26 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
+#include "libavutil/mips/cpu.h"
 #include "blockdsp_mips.h"
 
-#if HAVE_MSA
-static av_cold void blockdsp_init_msa(BlockDSPContext *c)
+void ff_blockdsp_init_mips(BlockDSPContext *c)
 {
-    c->clear_block = ff_clear_block_msa;
-    c->clear_blocks = ff_clear_blocks_msa;
+    int cpu_flags = av_get_cpu_flags();
 
-    c->fill_block_tab[0] = ff_fill_block16_msa;
-    c->fill_block_tab[1] = ff_fill_block8_msa;
-}
-#endif  // #if HAVE_MSA
+    if (have_mmi(cpu_flags)) {
+        c->clear_block = ff_clear_block_mmi;
+        c->clear_blocks = ff_clear_blocks_mmi;
 
-#if HAVE_MMI
-static av_cold void blockdsp_init_mmi(BlockDSPContext *c)
-{
-    c->clear_block = ff_clear_block_mmi;
-    c->clear_blocks = ff_clear_blocks_mmi;
+        c->fill_block_tab[0] = ff_fill_block16_mmi;
+        c->fill_block_tab[1] = ff_fill_block8_mmi;
+    }
 
-    c->fill_block_tab[0] = ff_fill_block16_mmi;
-    c->fill_block_tab[1] = ff_fill_block8_mmi;
-}
-#endif /* HAVE_MMI */
+    if (have_msa(cpu_flags)) {
+        c->clear_block = ff_clear_block_msa;
+        c->clear_blocks = ff_clear_blocks_msa;
 
-void ff_blockdsp_init_mips(BlockDSPContext *c)
-{
-#if HAVE_MMI
-    blockdsp_init_mmi(c);
-#endif /* HAVE_MMI */
-#if HAVE_MSA
-    blockdsp_init_msa(c);
-#endif  // #if HAVE_MSA
+        c->fill_block_tab[0] = ff_fill_block16_msa;
+        c->fill_block_tab[1] = ff_fill_block8_msa;
+    }
 }
diff --git a/libavcodec/mips/blockdsp_mmi.c b/libavcodec/mips/blockdsp_mmi.c
index 68641e2544..8b5c7e955c 100644
--- a/libavcodec/mips/blockdsp_mmi.c
+++ b/libavcodec/mips/blockdsp_mmi.c
@@ -76,8 +76,8 @@ void ff_clear_block_mmi(int16_t *block)
     double ftmp[2];
 
     __asm__ volatile (
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
-        "xor        %[ftmp1],   %[ftmp1],       %[ftmp1]                \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "pxor       %[ftmp1],   %[ftmp1],       %[ftmp1]                \n\t"
         MMI_SQC1(%[ftmp0], %[ftmp1], %[block], 0x00)
         MMI_SQC1(%[ftmp0], %[ftmp1], %[block], 0x10)
         MMI_SQC1(%[ftmp0], %[ftmp1], %[block], 0x20)
@@ -97,8 +97,8 @@ void ff_clear_blocks_mmi(int16_t *block)
     double ftmp[2];
 
     __asm__ volatile (
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
-        "xor        %[ftmp1],   %[ftmp1],       %[ftmp1]                \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "pxor       %[ftmp1],   %[ftmp1],       %[ftmp1]                \n\t"
         MMI_SQC1(%[ftmp0], %[ftmp1], %[block], 0x00)
         MMI_SQC1(%[ftmp0], %[ftmp1], %[block], 0x10)
         MMI_SQC1(%[ftmp0], %[ftmp1], %[block], 0x20)
diff --git a/libavcodec/mips/cabac.h b/libavcodec/mips/cabac.h
index 2a05e5ab3c..e968f3d89e 100644
--- a/libavcodec/mips/cabac.h
+++ b/libavcodec/mips/cabac.h
@@ -2,7 +2,8 @@
  * Loongson SIMD optimized h264chroma
  *
  * Copyright (c) 2018 Loongson Technology Corporation Limited
- * Copyright (c) 2018 Shiyou Yin <yinshiyou-hf@loongson.cn>
+ * Contributed by Shiyou Yin <yinshiyou-hf@loongson.cn>
+ *                Gu Xiwei(guxiwei-hf@loongson.cn)
  *
  * This file is part of FFmpeg.
  *
@@ -36,7 +37,7 @@ static av_always_inline int get_cabac_inline(CABACContext *c,
     __asm__ volatile (
         "lbu          %[bit],        0(%[state])                   \n\t"
         "and          %[tmp0],       %[c_range],     0xC0          \n\t"
-        PTR_ADDU     "%[tmp0],       %[tmp0],        %[tmp0]       \n\t"
+        PTR_SLL      "%[tmp0],       %[tmp0],        0x01          \n\t"
         PTR_ADDU     "%[tmp0],       %[tmp0],        %[tables]     \n\t"
         PTR_ADDU     "%[tmp0],       %[tmp0],        %[bit]        \n\t"
         /* tmp1: RangeLPS */
@@ -44,18 +45,11 @@ static av_always_inline int get_cabac_inline(CABACContext *c,
 
         PTR_SUBU     "%[c_range],    %[c_range],     %[tmp1]       \n\t"
         PTR_SLL      "%[tmp0],       %[c_range],     0x11          \n\t"
-        PTR_SUBU     "%[tmp0],       %[tmp0],        %[c_low]      \n\t"
-
-        /* tmp2: lps_mask */
-        PTR_SRA      "%[tmp2],       %[tmp0],        0x1F          \n\t"
-        /* If tmp0 < 0, lps_mask ==  0xffffffff*/
-        /* If tmp0 >= 0, lps_mask ==  0x00000000*/
+        "slt          %[tmp2],       %[tmp0],        %[c_low]      \n\t"
         "beqz         %[tmp2],       1f                            \n\t"
-        PTR_SLL      "%[tmp0],       %[c_range],     0x11          \n\t"
+        "move         %[c_range],    %[tmp1]                       \n\t"
+        "not          %[bit],        %[bit]                        \n\t"
         PTR_SUBU     "%[c_low],      %[c_low],       %[tmp0]       \n\t"
-        PTR_SUBU     "%[tmp0],       %[tmp1],        %[c_range]    \n\t"
-        PTR_ADDU     "%[c_range],    %[c_range],     %[tmp0]       \n\t"
-        "xor          %[bit],        %[bit],         %[tmp2]       \n\t"
 
         "1:                                                        \n\t"
         /* tmp1: *state */
@@ -70,23 +64,29 @@ static av_always_inline int get_cabac_inline(CABACContext *c,
         PTR_SLL      "%[c_range],    %[c_range],     %[tmp2]       \n\t"
         PTR_SLL      "%[c_low],      %[c_low],       %[tmp2]       \n\t"
 
-        "and          %[tmp0],       %[c_low],       %[cabac_mask] \n\t"
-        "bnez         %[tmp0],       1f                            \n\t"
-        PTR_ADDIU    "%[tmp0],       %[c_low],       -0x01         \n\t"
+        "and          %[tmp1],       %[c_low],       %[cabac_mask] \n\t"
+        "bnez         %[tmp1],       1f                            \n\t"
+        PTR_ADDIU    "%[tmp0],       %[c_low],       -0X01         \n\t"
         "xor          %[tmp0],       %[c_low],       %[tmp0]       \n\t"
         PTR_SRA      "%[tmp0],       %[tmp0],        0x0f          \n\t"
         PTR_ADDU     "%[tmp0],       %[tmp0],        %[tables]     \n\t"
+        /* tmp2: ff_h264_norm_shift[x >> (CABAC_BITS - 1)] */
         "lbu          %[tmp2],       %[norm_off](%[tmp0])          \n\t"
-#if CABAC_BITS == 16
-        "lbu          %[tmp0],       0(%[c_bytestream])            \n\t"
-        "lbu          %[tmp1],       1(%[c_bytestream])            \n\t"
-        PTR_SLL      "%[tmp0],       %[tmp0],        0x09          \n\t"
-        PTR_SLL      "%[tmp1],       %[tmp1],        0x01          \n\t"
-        PTR_ADDU     "%[tmp0],       %[tmp0],        %[tmp1]       \n\t"
+#if HAVE_BIGENDIAN
+        "lhu          %[tmp0],       0(%[c_bytestream])            \n\t"
 #else
-        "lbu          %[tmp0],       0(%[c_bytestream])            \n\t"
-        PTR_SLL      "%[tmp0],       %[tmp0],        0x01          \n\t"
+        "lhu          %[tmp0],       0(%[c_bytestream])            \n\t"
+#if HAVE_MIPS32R2 || HAVE_MIPS64R2
+        "wsbh         %[tmp0],       %[tmp0]                       \n\t"
+#else
+        "and          %[tmp1],      %[tmp0],         0xff00ff00    \n\t"
+        "srl          %[tmp1],      %[tmp1],         8             \n\t"
+        "and          %[tmp0],      %[tmp0],         0x00ff00ff    \n\t"
+        "sll          %[tmp0],      %[tmp0],         8             \n\t"
+        "or           %[tmp0],      %[tmp0],         %[tmp1]       \n\t"
 #endif
+#endif
+        PTR_SLL      "%[tmp0],       %[tmp0],        0x01          \n\t"
         PTR_SUBU     "%[tmp0],       %[tmp0],        %[cabac_mask] \n\t"
 
         "li           %[tmp1],       0x07                          \n\t"
@@ -94,10 +94,13 @@ static av_always_inline int get_cabac_inline(CABACContext *c,
         PTR_SLL      "%[tmp0],       %[tmp0],        %[tmp1]       \n\t"
         PTR_ADDU     "%[c_low],      %[c_low],       %[tmp0]       \n\t"
 
-#if !UNCHECKED_BITSTREAM_READER
-        "bge          %[c_bytestream], %[c_bytestream_end], 1f     \n\t"
+#if UNCHECKED_BITSTREAM_READER
+        PTR_ADDIU    "%[c_bytestream], %[c_bytestream],     0x02                 \n\t"
+#else
+        "slt          %[tmp0],         %[c_bytestream],     %[c_bytestream_end]  \n\t"
+        PTR_ADDIU    "%[tmp2],         %[c_bytestream],     0x02                 \n\t"
+        "movn         %[c_bytestream], %[tmp2],             %[tmp0]              \n\t"
 #endif
-        PTR_ADDIU    "%[c_bytestream], %[c_bytestream],     0X02   \n\t"
         "1:                                                        \n\t"
     : [bit]"=&r"(bit), [tmp0]"=&r"(tmp0), [tmp1]"=&r"(tmp1), [tmp2]"=&r"(tmp2),
       [c_range]"+&r"(c->range), [c_low]"+&r"(c->low),
@@ -109,11 +112,116 @@ static av_always_inline int get_cabac_inline(CABACContext *c,
       [lps_off]"i"(H264_LPS_RANGE_OFFSET),
       [mlps_off]"i"(H264_MLPS_STATE_OFFSET + 128),
       [norm_off]"i"(H264_NORM_SHIFT_OFFSET),
-      [cabac_mask]"i"(CABAC_MASK)
+      [cabac_mask]"r"(CABAC_MASK)
     : "memory"
     );
 
     return bit;
 }
 
+#define get_cabac_bypass get_cabac_bypass_mips
+static av_always_inline int get_cabac_bypass_mips(CABACContext *c)
+{
+    mips_reg tmp0, tmp1;
+    int res = 0;
+    __asm__ volatile(
+        PTR_SLL    "%[c_low],        %[c_low],        0x01                \n\t"
+        "and        %[tmp0],         %[c_low],        %[cabac_mask]       \n\t"
+        "bnez       %[tmp0],         1f                                   \n\t"
+#if HAVE_BIGENDIAN
+        "lhu        %[tmp1],         0(%[c_bytestream])                   \n\t"
+#else
+        "lhu        %[tmp1],         0(%[c_bytestream])                   \n\t"
+#if HAVE_MIPS32R2 || HAVE_MIPS64R2
+        "wsbh       %[tmp1],         %[tmp1]                              \n\t"
+#else
+        "and        %[tmp0],         %[tmp1],         0xff00ff00          \n\t"
+        "srl        %[tmp0],         %[tmp0],         8                   \n\t"
+        "and        %[tmp1],         %[tmp1],         0x00ff00ff          \n\t"
+        "sll        %[tmp1],         %[tmp1],         8                   \n\t"
+        "or         %[tmp1],         %[tmp1],         %[tmp0]             \n\t"
+#endif
+#endif
+        PTR_SLL    "%[tmp1],         %[tmp1],         0x01                \n\t"
+        PTR_SUBU   "%[tmp1],         %[tmp1],         %[cabac_mask]       \n\t"
+        PTR_ADDU   "%[c_low],        %[c_low],        %[tmp1]             \n\t"
+#if UNCHECKED_BITSTREAM_READER
+        PTR_ADDIU  "%[c_bytestream], %[c_bytestream], 0x02                \n\t"
+#else
+        "slt        %[tmp0],         %[c_bytestream], %[c_bytestream_end] \n\t"
+        PTR_ADDIU  "%[tmp1],         %[c_bytestream], 0x02                \n\t"
+        "movn       %[c_bytestream], %[tmp1],         %[tmp0]             \n\t"
+#endif
+        "1:                                                               \n\t"
+        PTR_SLL    "%[tmp1],         %[c_range],      0x11                \n\t"
+        "slt        %[tmp0],         %[c_low],        %[tmp1]             \n\t"
+        PTR_SUBU   "%[tmp1],         %[c_low],        %[tmp1]             \n\t"
+        "movz       %[res],          %[one],          %[tmp0]             \n\t"
+        "movz       %[c_low],        %[tmp1],         %[tmp0]             \n\t"
+        : [tmp0]"=&r"(tmp0), [tmp1]"=&r"(tmp1), [res]"+&r"(res),
+          [c_range]"+&r"(c->range), [c_low]"+&r"(c->low),
+          [c_bytestream]"+&r"(c->bytestream)
+        : [cabac_mask]"r"(CABAC_MASK),
+#if !UNCHECKED_BITSTREAM_READER
+          [c_bytestream_end]"r"(c->bytestream_end),
+#endif
+          [one]"r"(0x01)
+        : "memory"
+    );
+    return res;
+}
+
+#define get_cabac_bypass_sign get_cabac_bypass_sign_mips
+static av_always_inline int get_cabac_bypass_sign_mips(CABACContext *c, int val)
+{
+    mips_reg tmp0, tmp1;
+    int res = val;
+    __asm__ volatile(
+        PTR_SLL    "%[c_low],        %[c_low],        0x01                \n\t"
+        "and        %[tmp0],         %[c_low],        %[cabac_mask]       \n\t"
+        "bnez       %[tmp0],         1f                                   \n\t"
+#if HAVE_BIGENDIAN
+        "lhu        %[tmp1],         0(%[c_bytestream])                   \n\t"
+#else
+        "lhu        %[tmp1],         0(%[c_bytestream])                   \n\t"
+#if HAVE_MIPS32R2 || HAVE_MIPS64R2
+        "wsbh       %[tmp1],         %[tmp1]                              \n\t"
+#else
+        "and        %[tmp0],         %[tmp1],         0xff00ff00          \n\t"
+        "srl        %[tmp0],         %[tmp0],         8                   \n\t"
+        "and        %[tmp1],         %[tmp1],         0x00ff00ff          \n\t"
+        "sll        %[tmp1],         %[tmp1],         8                   \n\t"
+        "or         %[tmp1],         %[tmp1],         %[tmp0]             \n\t"
+#endif
+#endif
+        PTR_SLL    "%[tmp1],         %[tmp1],         0x01                \n\t"
+        PTR_SUBU   "%[tmp1],         %[tmp1],         %[cabac_mask]       \n\t"
+        PTR_ADDU   "%[c_low],        %[c_low],        %[tmp1]             \n\t"
+#if UNCHECKED_BITSTREAM_READER
+        PTR_ADDIU  "%[c_bytestream], %[c_bytestream], 0x02                \n\t"
+#else
+        "slt        %[tmp0],         %[c_bytestream], %[c_bytestream_end] \n\t"
+        PTR_ADDIU  "%[tmp1],         %[c_bytestream], 0x02                \n\t"
+        "movn       %[c_bytestream], %[tmp1],         %[tmp0]             \n\t"
+#endif
+        "1:                                                               \n\t"
+        PTR_SLL    "%[tmp1],         %[c_range],      0x11                \n\t"
+        "slt        %[tmp0],         %[c_low],        %[tmp1]             \n\t"
+        PTR_SUBU   "%[tmp1],         %[c_low],        %[tmp1]             \n\t"
+        "movz       %[c_low],        %[tmp1],         %[tmp0]             \n\t"
+        PTR_SUBU   "%[tmp1],         %[zero],         %[res]              \n\t"
+        "movn       %[res],          %[tmp1],         %[tmp0]             \n\t"
+        : [tmp0]"=&r"(tmp0), [tmp1]"=&r"(tmp1), [res]"+&r"(res),
+          [c_range]"+&r"(c->range), [c_low]"+&r"(c->low),
+          [c_bytestream]"+&r"(c->bytestream)
+        : [cabac_mask]"r"(CABAC_MASK),
+#if !UNCHECKED_BITSTREAM_READER
+          [c_bytestream_end]"r"(c->bytestream_end),
+#endif
+          [zero]"r"(0x0)
+        : "memory"
+    );
+
+    return res;
+}
 #endif /* AVCODEC_MIPS_CABAC_H */
diff --git a/libavcodec/mips/constants.c b/libavcodec/mips/constants.c
index a7c4a5ccf6..5df6b5225a 100644
--- a/libavcodec/mips/constants.c
+++ b/libavcodec/mips/constants.c
@@ -19,50 +19,48 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
-#include "config.h"
-#include "libavutil/mem.h"
+#include "libavutil/intfloat.h"
 #include "constants.h"
 
-DECLARE_ALIGNED(8, const uint64_t, ff_pw_1) =       {0x0001000100010001ULL};
-DECLARE_ALIGNED(8, const uint64_t, ff_pw_2) =       {0x0002000200020002ULL};
-DECLARE_ALIGNED(8, const uint64_t, ff_pw_3) =       {0x0003000300030003ULL};
-DECLARE_ALIGNED(8, const uint64_t, ff_pw_4) =       {0x0004000400040004ULL};
-DECLARE_ALIGNED(8, const uint64_t, ff_pw_5) =       {0x0005000500050005ULL};
-DECLARE_ALIGNED(8, const uint64_t, ff_pw_6) =       {0x0006000600060006ULL};
-DECLARE_ALIGNED(8, const uint64_t, ff_pw_8) =       {0x0008000800080008ULL};
-DECLARE_ALIGNED(8, const uint64_t, ff_pw_9) =       {0x0009000900090009ULL};
-DECLARE_ALIGNED(8, const uint64_t, ff_pw_10) =      {0x000A000A000A000AULL};
-DECLARE_ALIGNED(8, const uint64_t, ff_pw_12) =      {0x000C000C000C000CULL};
-DECLARE_ALIGNED(8, const uint64_t, ff_pw_15) =      {0x000F000F000F000FULL};
-DECLARE_ALIGNED(8, const uint64_t, ff_pw_16) =      {0x0010001000100010ULL};
-DECLARE_ALIGNED(8, const uint64_t, ff_pw_17) =      {0x0011001100110011ULL};
-DECLARE_ALIGNED(8, const uint64_t, ff_pw_18) =      {0x0012001200120012ULL};
-DECLARE_ALIGNED(8, const uint64_t, ff_pw_20) =      {0x0014001400140014ULL};
-DECLARE_ALIGNED(8, const uint64_t, ff_pw_22) =      {0x0016001600160016ULL};
-DECLARE_ALIGNED(8, const uint64_t, ff_pw_28) =      {0x001C001C001C001CULL};
-DECLARE_ALIGNED(8, const uint64_t, ff_pw_32) =      {0x0020002000200020ULL};
-DECLARE_ALIGNED(8, const uint64_t, ff_pw_53) =      {0x0035003500350035ULL};
-DECLARE_ALIGNED(8, const uint64_t, ff_pw_64) =      {0x0040004000400040ULL};
-DECLARE_ALIGNED(8, const uint64_t, ff_pw_128) =     {0x0080008000800080ULL};
-DECLARE_ALIGNED(8, const uint64_t, ff_pw_512) =     {0x0200020002000200ULL};
-DECLARE_ALIGNED(8, const uint64_t, ff_pw_m8tom5) =  {0xFFFBFFFAFFF9FFF8ULL};
-DECLARE_ALIGNED(8, const uint64_t, ff_pw_m4tom1) =  {0xFFFFFFFEFFFDFFFCULL};
-DECLARE_ALIGNED(8, const uint64_t, ff_pw_1to4) =    {0x0004000300020001ULL};
-DECLARE_ALIGNED(8, const uint64_t, ff_pw_5to8) =    {0x0008000700060005ULL};
-DECLARE_ALIGNED(8, const uint64_t, ff_pw_0to3) =    {0x0003000200010000ULL};
-DECLARE_ALIGNED(8, const uint64_t, ff_pw_4to7) =    {0x0007000600050004ULL};
-DECLARE_ALIGNED(8, const uint64_t, ff_pw_8tob) =    {0x000b000a00090008ULL};
-DECLARE_ALIGNED(8, const uint64_t, ff_pw_ctof) =    {0x000f000e000d000cULL};
-
-DECLARE_ALIGNED(8, const uint64_t, ff_pb_1) =       {0x0101010101010101ULL};
-DECLARE_ALIGNED(8, const uint64_t, ff_pb_3) =       {0x0303030303030303ULL};
-DECLARE_ALIGNED(8, const uint64_t, ff_pb_80) =      {0x8080808080808080ULL};
-DECLARE_ALIGNED(8, const uint64_t, ff_pb_A1) =      {0xA1A1A1A1A1A1A1A1ULL};
-DECLARE_ALIGNED(8, const uint64_t, ff_pb_FE) =      {0xFEFEFEFEFEFEFEFEULL};
-
-DECLARE_ALIGNED(8, const uint64_t, ff_rnd) =        {0x0004000400040004ULL};
-DECLARE_ALIGNED(8, const uint64_t, ff_rnd2) =       {0x0040004000400040ULL};
-DECLARE_ALIGNED(8, const uint64_t, ff_rnd3) =       {0x0020002000200020ULL};
-
-DECLARE_ALIGNED(8, const uint64_t, ff_wm1010) =     {0xFFFF0000FFFF0000ULL};
-DECLARE_ALIGNED(8, const uint64_t, ff_d40000) =     {0x0000000000040000ULL};
+const union av_intfloat64 ff_pw_1 =      {0x0001000100010001ULL};
+const union av_intfloat64 ff_pw_2 =      {0x0002000200020002ULL};
+const union av_intfloat64 ff_pw_3 =      {0x0003000300030003ULL};
+const union av_intfloat64 ff_pw_4 =      {0x0004000400040004ULL};
+const union av_intfloat64 ff_pw_5 =      {0x0005000500050005ULL};
+const union av_intfloat64 ff_pw_6 =      {0x0006000600060006ULL};
+const union av_intfloat64 ff_pw_8 =      {0x0008000800080008ULL};
+const union av_intfloat64 ff_pw_9 =      {0x0009000900090009ULL};
+const union av_intfloat64 ff_pw_10 =     {0x000A000A000A000AULL};
+const union av_intfloat64 ff_pw_12 =     {0x000C000C000C000CULL};
+const union av_intfloat64 ff_pw_15 =     {0x000F000F000F000FULL};
+const union av_intfloat64 ff_pw_16 =     {0x0010001000100010ULL};
+const union av_intfloat64 ff_pw_17 =     {0x0011001100110011ULL};
+const union av_intfloat64 ff_pw_18 =     {0x0012001200120012ULL};
+const union av_intfloat64 ff_pw_20 =     {0x0014001400140014ULL};
+const union av_intfloat64 ff_pw_22 =     {0x0016001600160016ULL};
+const union av_intfloat64 ff_pw_28 =     {0x001C001C001C001CULL};
+const union av_intfloat64 ff_pw_32 =     {0x0020002000200020ULL};
+const union av_intfloat64 ff_pw_53 =     {0x0035003500350035ULL};
+const union av_intfloat64 ff_pw_64 =     {0x0040004000400040ULL};
+const union av_intfloat64 ff_pw_128 =    {0x0080008000800080ULL};
+const union av_intfloat64 ff_pw_512 =    {0x0200020002000200ULL};
+const union av_intfloat64 ff_pw_m8tom5 = {0xFFFBFFFAFFF9FFF8ULL};
+const union av_intfloat64 ff_pw_m4tom1 = {0xFFFFFFFEFFFDFFFCULL};
+const union av_intfloat64 ff_pw_1to4 =   {0x0004000300020001ULL};
+const union av_intfloat64 ff_pw_5to8 =   {0x0008000700060005ULL};
+const union av_intfloat64 ff_pw_0to3 =   {0x0003000200010000ULL};
+const union av_intfloat64 ff_pw_4to7 =   {0x0007000600050004ULL};
+const union av_intfloat64 ff_pw_8tob =   {0x000b000a00090008ULL};
+const union av_intfloat64 ff_pw_ctof =   {0x000f000e000d000cULL};
+const union av_intfloat64 ff_pw_32_1 =   {0x0000000100000001ULL};
+const union av_intfloat64 ff_pw_32_4 =   {0x0000000400000004ULL};
+const union av_intfloat64 ff_pw_32_64 =  {0x0000004000000040ULL};
+const union av_intfloat64 ff_pb_1 =      {0x0101010101010101ULL};
+const union av_intfloat64 ff_pb_3 =      {0x0303030303030303ULL};
+const union av_intfloat64 ff_pb_80 =     {0x8080808080808080ULL};
+const union av_intfloat64 ff_pb_A1 =     {0xA1A1A1A1A1A1A1A1ULL};
+const union av_intfloat64 ff_pb_FE =     {0xFEFEFEFEFEFEFEFEULL};
+const union av_intfloat64 ff_rnd =       {0x0004000400040004ULL};
+const union av_intfloat64 ff_rnd2 =      {0x0040004000400040ULL};
+const union av_intfloat64 ff_rnd3 =      {0x0020002000200020ULL};
+const union av_intfloat64 ff_ff_wm1010 = {0xFFFF0000FFFF0000ULL};
diff --git a/libavcodec/mips/constants.h b/libavcodec/mips/constants.h
index 2604559950..df54b3016f 100644
--- a/libavcodec/mips/constants.h
+++ b/libavcodec/mips/constants.h
@@ -22,50 +22,48 @@
 #ifndef AVCODEC_MIPS_CONSTANTS_H
 #define AVCODEC_MIPS_CONSTANTS_H
 
-#include <stdint.h>
-
-extern const uint64_t ff_pw_1;
-extern const uint64_t ff_pw_2;
-extern const uint64_t ff_pw_3;
-extern const uint64_t ff_pw_4;
-extern const uint64_t ff_pw_5;
-extern const uint64_t ff_pw_6;
-extern const uint64_t ff_pw_8;
-extern const uint64_t ff_pw_9;
-extern const uint64_t ff_pw_10;
-extern const uint64_t ff_pw_12;
-extern const uint64_t ff_pw_15;
-extern const uint64_t ff_pw_16;
-extern const uint64_t ff_pw_17;
-extern const uint64_t ff_pw_18;
-extern const uint64_t ff_pw_20;
-extern const uint64_t ff_pw_22;
-extern const uint64_t ff_pw_28;
-extern const uint64_t ff_pw_32;
-extern const uint64_t ff_pw_53;
-extern const uint64_t ff_pw_64;
-extern const uint64_t ff_pw_128;
-extern const uint64_t ff_pw_512;
-extern const uint64_t ff_pw_m8tom5;
-extern const uint64_t ff_pw_m4tom1;
-extern const uint64_t ff_pw_1to4;
-extern const uint64_t ff_pw_5to8;
-extern const uint64_t ff_pw_0to3;
-extern const uint64_t ff_pw_4to7;
-extern const uint64_t ff_pw_8tob;
-extern const uint64_t ff_pw_ctof;
-
-extern const uint64_t ff_pb_1;
-extern const uint64_t ff_pb_3;
-extern const uint64_t ff_pb_80;
-extern const uint64_t ff_pb_A1;
-extern const uint64_t ff_pb_FE;
-
-extern const uint64_t ff_rnd;
-extern const uint64_t ff_rnd2;
-extern const uint64_t ff_rnd3;
-
-extern const uint64_t ff_wm1010;
-extern const uint64_t ff_d40000;
+extern const union av_intfloat64 ff_pw_1;
+extern const union av_intfloat64 ff_pw_2;
+extern const union av_intfloat64 ff_pw_3;
+extern const union av_intfloat64 ff_pw_4;
+extern const union av_intfloat64 ff_pw_5;
+extern const union av_intfloat64 ff_pw_6;
+extern const union av_intfloat64 ff_pw_8;
+extern const union av_intfloat64 ff_pw_9;
+extern const union av_intfloat64 ff_pw_10;
+extern const union av_intfloat64 ff_pw_12;
+extern const union av_intfloat64 ff_pw_15;
+extern const union av_intfloat64 ff_pw_16;
+extern const union av_intfloat64 ff_pw_17;
+extern const union av_intfloat64 ff_pw_18;
+extern const union av_intfloat64 ff_pw_20;
+extern const union av_intfloat64 ff_pw_22;
+extern const union av_intfloat64 ff_pw_28;
+extern const union av_intfloat64 ff_pw_32;
+extern const union av_intfloat64 ff_pw_53;
+extern const union av_intfloat64 ff_pw_64;
+extern const union av_intfloat64 ff_pw_128;
+extern const union av_intfloat64 ff_pw_512;
+extern const union av_intfloat64 ff_pw_m8tom5;
+extern const union av_intfloat64 ff_pw_m4tom1;
+extern const union av_intfloat64 ff_pw_1to4;
+extern const union av_intfloat64 ff_pw_5to8;
+extern const union av_intfloat64 ff_pw_0to3;
+extern const union av_intfloat64 ff_pw_4to7;
+extern const union av_intfloat64 ff_pw_8tob;
+extern const union av_intfloat64 ff_pw_ctof;
+extern const union av_intfloat64 ff_pw_32_1;
+extern const union av_intfloat64 ff_pw_32_4;
+extern const union av_intfloat64 ff_pw_32_64;
+extern const union av_intfloat64 ff_pb_1;
+extern const union av_intfloat64 ff_pb_3;
+extern const union av_intfloat64 ff_pb_80;
+extern const union av_intfloat64 ff_pb_A1;
+extern const union av_intfloat64 ff_pb_FE;
+extern const union av_intfloat64 ff_rnd;
+extern const union av_intfloat64 ff_rnd2;
+extern const union av_intfloat64 ff_rnd3;
+extern const union av_intfloat64 ff_wm1010;
+extern const union av_intfloat64 ff_d40000;
 
 #endif /* AVCODEC_MIPS_CONSTANTS_H */
diff --git a/libavcodec/mips/fft_mips.c b/libavcodec/mips/fft_mips.c
index 03dcbad4d8..69abdc8a08 100644
--- a/libavcodec/mips/fft_mips.c
+++ b/libavcodec/mips/fft_mips.c
@@ -71,6 +71,7 @@ static void ff_fft_calc_mips(FFTContext *s, FFTComplex *z)
     float temp, temp1, temp3, temp4;
     FFTComplex * tmpz_n2, * tmpz_n34, * tmpz_n4;
     FFTComplex * tmpz_n2_i, * tmpz_n34_i, * tmpz_n4_i, * tmpz_i;
+    float f1 = 0.7071067812;
 
     num_transforms = (21845 >> (17 - s->nbits)) | 1;
 
@@ -148,7 +149,6 @@ static void ff_fft_calc_mips(FFTContext *s, FFTComplex *z)
             "swc1  %[pom2], 4(%[tmpz])                      \n\t"  // tmpz[0].im = tmpz[0].im + tmp6;
             "lwc1  %[pom1], 16(%[tmpz])                     \n\t"
             "lwc1  %[pom3], 20(%[tmpz])                     \n\t"
-            "li.s  %[pom],  0.7071067812                    \n\t"  // float pom = 0.7071067812f;
             "add.s %[temp1],%[tmp1],    %[tmp2]             \n\t"
             "sub.s %[temp], %[pom1],    %[tmp8]             \n\t"
             "add.s %[pom2], %[pom3],    %[tmp7]             \n\t"
@@ -159,10 +159,10 @@ static void ff_fft_calc_mips(FFTContext *s, FFTComplex *z)
             "add.s %[pom1], %[pom1],    %[tmp8]             \n\t"
             "sub.s %[pom3], %[pom3],    %[tmp7]             \n\t"
             "add.s %[tmp3], %[tmp3],    %[tmp4]             \n\t"
-            "mul.s %[tmp5], %[pom],     %[temp1]            \n\t"  // tmp5 = pom * (tmp1 + tmp2);
-            "mul.s %[tmp7], %[pom],     %[temp3]            \n\t"  // tmp7 = pom * (tmp3 - tmp4);
-            "mul.s %[tmp6], %[pom],     %[temp4]            \n\t"  // tmp6 = pom * (tmp2 - tmp1);
-            "mul.s %[tmp8], %[pom],     %[tmp3]             \n\t"  // tmp8 = pom * (tmp3 + tmp4);
+            "mul.s %[tmp5], %[f1],      %[temp1]            \n\t"  // tmp5 = pom * (tmp1 + tmp2);
+            "mul.s %[tmp7], %[f1],      %[temp3]            \n\t"  // tmp7 = pom * (tmp3 - tmp4);
+            "mul.s %[tmp6], %[f1],      %[temp4]            \n\t"  // tmp6 = pom * (tmp2 - tmp1);
+            "mul.s %[tmp8], %[f1],      %[tmp3]             \n\t"  // tmp8 = pom * (tmp3 + tmp4);
             "swc1  %[pom1], 16(%[tmpz])                     \n\t"  // tmpz[2].re = tmpz[2].re + tmp8;
             "swc1  %[pom3], 20(%[tmpz])                     \n\t"  // tmpz[2].im = tmpz[2].im - tmp7;
             "add.s %[tmp1], %[tmp5],    %[tmp7]             \n\t"  // tmp1 = tmp5 + tmp7;
@@ -193,7 +193,7 @@ static void ff_fft_calc_mips(FFTContext *s, FFTComplex *z)
               [tmp3]"=&f"(tmp3), [tmp2]"=&f"(tmp2), [tmp4]"=&f"(tmp4), [tmp5]"=&f"(tmp5),  [tmp7]"=&f"(tmp7),
               [tmp6]"=&f"(tmp6), [tmp8]"=&f"(tmp8), [pom3]"=&f"(pom3),[temp]"=&f"(temp), [temp1]"=&f"(temp1),
               [temp3]"=&f"(temp3), [temp4]"=&f"(temp4)
-            : [tmpz]"r"(tmpz)
+            : [tmpz]"r"(tmpz), [f1]"f"(f1)
             : "memory"
         );
     }
diff --git a/libavcodec/mips/h263dsp_init_mips.c b/libavcodec/mips/h263dsp_init_mips.c
index 09bd93707d..a73eb12d87 100644
--- a/libavcodec/mips/h263dsp_init_mips.c
+++ b/libavcodec/mips/h263dsp_init_mips.c
@@ -18,19 +18,15 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
+#include "libavutil/mips/cpu.h"
 #include "h263dsp_mips.h"
 
-#if HAVE_MSA
-static av_cold void h263dsp_init_msa(H263DSPContext *c)
-{
-    c->h263_h_loop_filter = ff_h263_h_loop_filter_msa;
-    c->h263_v_loop_filter = ff_h263_v_loop_filter_msa;
-}
-#endif  // #if HAVE_MSA
-
 av_cold void ff_h263dsp_init_mips(H263DSPContext *c)
 {
-#if HAVE_MSA
-    h263dsp_init_msa(c);
-#endif  // #if HAVE_MSA
+    int cpu_flags = av_get_cpu_flags();
+
+    if (have_msa(cpu_flags)){
+        c->h263_h_loop_filter = ff_h263_h_loop_filter_msa;
+        c->h263_v_loop_filter = ff_h263_v_loop_filter_msa;
+    }
 }
diff --git a/libavcodec/mips/h264_deblock_msa.c b/libavcodec/mips/h264_deblock_msa.c
new file mode 100644
index 0000000000..4fed55c504
--- /dev/null
+++ b/libavcodec/mips/h264_deblock_msa.c
@@ -0,0 +1,153 @@
+/*
+ * MIPS SIMD optimized H.264 deblocking code
+ *
+ * Copyright (c) 2020 Loongson Technology Corporation Limited
+ *                    Gu Xiwei <guxiwei-hf@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "libavcodec/bit_depth_template.c"
+#include "h264dsp_mips.h"
+#include "libavutil/mips/generic_macros_msa.h"
+#include "libavcodec/mips/h264dsp_mips.h"
+
+#define h264_loop_filter_strength_iteration_msa(edges, step, mask_mv, dir, \
+                                                d_idx, mask_dir)           \
+do {                                                                       \
+    int b_idx = 0; \
+    int step_x4 = step << 2; \
+    int d_idx_12 = d_idx + 12; \
+    int d_idx_52 = d_idx + 52; \
+    int d_idx_x4 = d_idx << 2; \
+    int d_idx_x4_48 = d_idx_x4 + 48; \
+    int dir_x32  = dir * 32; \
+    uint8_t *ref_t = (uint8_t*)ref; \
+    uint8_t *mv_t  = (uint8_t*)mv; \
+    uint8_t *nnz_t = (uint8_t*)nnz; \
+    uint8_t *bS_t  = (uint8_t*)bS; \
+    mask_mv <<= 3; \
+    for (; b_idx < edges; b_idx += step) { \
+        out &= mask_dir; \
+        if (!(mask_mv & b_idx)) { \
+            if (bidir) { \
+                ref_2 = LD_SB(ref_t + d_idx_12); \
+                ref_3 = LD_SB(ref_t + d_idx_52); \
+                ref_0 = LD_SB(ref_t + 12); \
+                ref_1 = LD_SB(ref_t + 52); \
+                ref_2 = (v16i8)__msa_ilvr_w((v4i32)ref_3, (v4i32)ref_2); \
+                ref_0 = (v16i8)__msa_ilvr_w((v4i32)ref_0, (v4i32)ref_0); \
+                ref_1 = (v16i8)__msa_ilvr_w((v4i32)ref_1, (v4i32)ref_1); \
+                ref_3 = (v16i8)__msa_shf_h((v8i16)ref_2, 0x4e); \
+                ref_0 -= ref_2; \
+                ref_1 -= ref_3; \
+                ref_0 = (v16i8)__msa_or_v((v16u8)ref_0, (v16u8)ref_1); \
+\
+                tmp_2 = LD_SH(mv_t + d_idx_x4_48);   \
+                tmp_3 = LD_SH(mv_t + 48); \
+                tmp_4 = LD_SH(mv_t + 208); \
+                tmp_5 = tmp_2 - tmp_3; \
+                tmp_6 = tmp_2 - tmp_4; \
+                SAT_SH2_SH(tmp_5, tmp_6, 7); \
+                tmp_0 = __msa_pckev_b((v16i8)tmp_6, (v16i8)tmp_5); \
+                tmp_0 += cnst_1; \
+                tmp_0 = (v16i8)__msa_subs_u_b((v16u8)tmp_0, (v16u8)cnst_0);\
+                tmp_0 = (v16i8)__msa_sat_s_h((v8i16)tmp_0, 7); \
+                tmp_0 = __msa_pckev_b(tmp_0, tmp_0); \
+                out   = (v16i8)__msa_or_v((v16u8)ref_0, (v16u8)tmp_0); \
+\
+                tmp_2 = LD_SH(mv_t + 208 + d_idx_x4); \
+                tmp_5 = tmp_2 - tmp_3; \
+                tmp_6 = tmp_2 - tmp_4; \
+                SAT_SH2_SH(tmp_5, tmp_6, 7); \
+                tmp_1 = __msa_pckev_b((v16i8)tmp_6, (v16i8)tmp_5); \
+                tmp_1 += cnst_1; \
+                tmp_1 = (v16i8)__msa_subs_u_b((v16u8)tmp_1, (v16u8)cnst_0); \
+                tmp_1 = (v16i8)__msa_sat_s_h((v8i16)tmp_1, 7); \
+                tmp_1 = __msa_pckev_b(tmp_1, tmp_1); \
+\
+                tmp_1 = (v16i8)__msa_shf_h((v8i16)tmp_1, 0x4e); \
+                out   = (v16i8)__msa_or_v((v16u8)out, (v16u8)tmp_1); \
+                tmp_0 = (v16i8)__msa_shf_h((v8i16)out, 0x4e); \
+                out   = (v16i8)__msa_min_u_b((v16u8)out, (v16u8)tmp_0); \
+            } else { \
+                ref_0 = LD_SB(ref_t + d_idx_12); \
+                ref_3 = LD_SB(ref_t + 12); \
+                tmp_2 = LD_SH(mv_t + d_idx_x4_48); \
+                tmp_3 = LD_SH(mv_t + 48); \
+                tmp_4 = tmp_3 - tmp_2; \
+                tmp_1 = (v16i8)__msa_sat_s_h(tmp_4, 7); \
+                tmp_1 = __msa_pckev_b(tmp_1, tmp_1); \
+                tmp_1 += cnst_1; \
+                out   = (v16i8)__msa_subs_u_b((v16u8)tmp_1, (v16u8)cnst_0); \
+                out   = (v16i8)__msa_sat_s_h((v8i16)out, 7); \
+                out   = __msa_pckev_b(out, out); \
+                ref_0 = ref_3 - ref_0; \
+                out   = (v16i8)__msa_or_v((v16u8)out, (v16u8)ref_0); \
+            } \
+        } \
+        tmp_0 = LD_SB(nnz_t + 12); \
+        tmp_1 = LD_SB(nnz_t + d_idx_12); \
+        tmp_0 = (v16i8)__msa_or_v((v16u8)tmp_0, (v16u8)tmp_1); \
+        tmp_0 = (v16i8)__msa_min_u_b((v16u8)tmp_0, (v16u8)cnst_2); \
+        out   = (v16i8)__msa_min_u_b((v16u8)out, (v16u8)cnst_2); \
+        tmp_0 = (v16i8)((v8i16)tmp_0 << 1); \
+        tmp_0 = (v16i8)__msa_max_u_b((v16u8)out, (v16u8)tmp_0); \
+        tmp_0 = __msa_ilvr_b(zero, tmp_0); \
+        ST_D1(tmp_0, 0, bS_t + dir_x32); \
+        ref_t += step; \
+        mv_t  += step_x4; \
+        nnz_t += step; \
+        bS_t  += step; \
+    } \
+} while(0)
+
+void ff_h264_loop_filter_strength_msa(int16_t bS[2][4][4], uint8_t nnz[40],
+                                      int8_t ref[2][40], int16_t mv[2][40][2],
+                                      int bidir, int edges, int step,
+                                      int mask_mv0, int mask_mv1, int field)
+{
+    v16i8 out;
+    v16i8 ref_0, ref_1, ref_2, ref_3;
+    v16i8 tmp_0, tmp_1;
+    v8i16 tmp_2, tmp_3, tmp_4, tmp_5, tmp_6;
+    v16i8 cnst_0, cnst_1, cnst_2;
+    v16i8 zero = { 0 };
+    v16i8 one  = __msa_fill_b(0xff);
+    if (field) {
+        cnst_0 = (v16i8)__msa_fill_h(0x206);
+        cnst_1 = (v16i8)__msa_fill_h(0x103);
+        cnst_2 = (v16i8)__msa_fill_h(0x101);
+    } else {
+        cnst_0 = __msa_fill_b(0x6);
+        cnst_1 = __msa_fill_b(0x3);
+        cnst_2 = __msa_fill_b(0x1);
+    }
+    step  <<= 3;
+    edges <<= 3;
+
+    h264_loop_filter_strength_iteration_msa(edges, step, mask_mv1, 1, -8, zero);
+    h264_loop_filter_strength_iteration_msa(32, 8, mask_mv0, 0, -1, one);
+
+    LD_SB2((int8_t*)bS, 16, tmp_0, tmp_1);
+    tmp_2 = (v8i16)__msa_ilvl_d((v2i64)tmp_0, (v2i64)tmp_0);
+    tmp_3 = (v8i16)__msa_ilvl_d((v2i64)tmp_1, (v2i64)tmp_1);
+    TRANSPOSE4x4_SH_SH(tmp_0, tmp_2, tmp_1, tmp_3, tmp_2, tmp_3, tmp_4, tmp_5);
+    tmp_0 = (v16i8)__msa_ilvr_d((v2i64)tmp_3, (v2i64)tmp_2);
+    tmp_1 = (v16i8)__msa_ilvr_d((v2i64)tmp_5, (v2i64)tmp_4);
+    ST_SB2(tmp_0, tmp_1, (int8_t*)bS, 16);
+}
diff --git a/libavcodec/mips/h264chroma_init_mips.c b/libavcodec/mips/h264chroma_init_mips.c
index ae817e47ae..755cc0401f 100644
--- a/libavcodec/mips/h264chroma_init_mips.c
+++ b/libavcodec/mips/h264chroma_init_mips.c
@@ -19,45 +19,33 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
+#include "libavutil/mips/cpu.h"
 #include "h264chroma_mips.h"
 
-#if HAVE_MSA
-static av_cold void h264chroma_init_msa(H264ChromaContext *c, int bit_depth)
-{
-    const int high_bit_depth = bit_depth > 8;
-
-    if (!high_bit_depth) {
-        c->put_h264_chroma_pixels_tab[0] = ff_put_h264_chroma_mc8_msa;
-        c->put_h264_chroma_pixels_tab[1] = ff_put_h264_chroma_mc4_msa;
-        c->put_h264_chroma_pixels_tab[2] = ff_put_h264_chroma_mc2_msa;
-
-        c->avg_h264_chroma_pixels_tab[0] = ff_avg_h264_chroma_mc8_msa;
-        c->avg_h264_chroma_pixels_tab[1] = ff_avg_h264_chroma_mc4_msa;
-        c->avg_h264_chroma_pixels_tab[2] = ff_avg_h264_chroma_mc2_msa;
-    }
-}
-#endif  // #if HAVE_MSA
 
-#if HAVE_MMI
-static av_cold void h264chroma_init_mmi(H264ChromaContext *c, int bit_depth)
+av_cold void ff_h264chroma_init_mips(H264ChromaContext *c, int bit_depth)
 {
+    int cpu_flags = av_get_cpu_flags();
     int high_bit_depth = bit_depth > 8;
 
-    if (!high_bit_depth) {
-        c->put_h264_chroma_pixels_tab[0] = ff_put_h264_chroma_mc8_mmi;
-        c->avg_h264_chroma_pixels_tab[0] = ff_avg_h264_chroma_mc8_mmi;
-        c->put_h264_chroma_pixels_tab[1] = ff_put_h264_chroma_mc4_mmi;
-        c->avg_h264_chroma_pixels_tab[1] = ff_avg_h264_chroma_mc4_mmi;
+    if (have_mmi(cpu_flags)) {
+        if (!high_bit_depth) {
+            c->put_h264_chroma_pixels_tab[0] = ff_put_h264_chroma_mc8_mmi;
+            c->avg_h264_chroma_pixels_tab[0] = ff_avg_h264_chroma_mc8_mmi;
+            c->put_h264_chroma_pixels_tab[1] = ff_put_h264_chroma_mc4_mmi;
+            c->avg_h264_chroma_pixels_tab[1] = ff_avg_h264_chroma_mc4_mmi;
+        }
     }
-}
-#endif /* HAVE_MMI */
 
-av_cold void ff_h264chroma_init_mips(H264ChromaContext *c, int bit_depth)
-{
-#if HAVE_MMI
-    h264chroma_init_mmi(c, bit_depth);
-#endif /* HAVE_MMI */
-#if HAVE_MSA
-    h264chroma_init_msa(c, bit_depth);
-#endif  // #if HAVE_MSA
+    if (have_msa(cpu_flags)) {
+        if (!high_bit_depth) {
+            c->put_h264_chroma_pixels_tab[0] = ff_put_h264_chroma_mc8_msa;
+            c->put_h264_chroma_pixels_tab[1] = ff_put_h264_chroma_mc4_msa;
+            c->put_h264_chroma_pixels_tab[2] = ff_put_h264_chroma_mc2_msa;
+
+            c->avg_h264_chroma_pixels_tab[0] = ff_avg_h264_chroma_mc8_msa;
+            c->avg_h264_chroma_pixels_tab[1] = ff_avg_h264_chroma_mc4_msa;
+            c->avg_h264_chroma_pixels_tab[2] = ff_avg_h264_chroma_mc2_msa;
+        }
+    }
 }
diff --git a/libavcodec/mips/h264chroma_mmi.c b/libavcodec/mips/h264chroma_mmi.c
index 739dd7d4d6..ec35c5a72e 100644
--- a/libavcodec/mips/h264chroma_mmi.c
+++ b/libavcodec/mips/h264chroma_mmi.c
@@ -29,12 +29,14 @@
 void ff_put_h264_chroma_mc8_mmi(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
         int h, int x, int y)
 {
-    int A = 64, B, C, D, E;
     double ftmp[12];
-    uint64_t tmp[1];
+    union mmi_intfloat64 A, B, C, D, E;
+    DECLARE_VAR_ALL64;
+
+    A.i = 64;
 
     if (!(x || y)) {
-        /* x=0, y=0, A=64 */
+        /* x=0, y=0, A.i=64 */
         __asm__ volatile (
             "1:                                                        \n\t"
             MMI_ULDC1(%[ftmp0], %[src], 0x00)
@@ -57,7 +59,8 @@ void ff_put_h264_chroma_mc8_mmi(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
             MMI_SDC1(%[ftmp3], %[dst], 0x00)
             PTR_ADDU   "%[dst],     %[dst],         %[stride]          \n\t"
             "bnez       %[h],       1b                                 \n\t"
-            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+            : RESTRICT_ASM_ALL64
+              [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
               [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
               [dst]"+&r"(dst),              [src]"+&r"(src),
               [h]"+&r"(h)
@@ -66,14 +69,13 @@ void ff_put_h264_chroma_mc8_mmi(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
         );
     } else if (x && y) {
         /* x!=0, y!=0 */
-        D = x * y;
-        B = (x << 3) - D;
-        C = (y << 3) - D;
-        A = 64 - D - B - C;
+        D.i = x * y;
+        B.i = (x << 3) - D.i;
+        C.i = (y << 3) - D.i;
+        A.i = 64 - D.i - B.i - C.i;
 
         __asm__ volatile (
-            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]           \n\t"
-            "dli        %[tmp0],    0x06                               \n\t"
+            "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]           \n\t"
             "pshufh     %[A],       %[A],           %[ftmp0]           \n\t"
             "pshufh     %[B],       %[B],           %[ftmp0]           \n\t"
             "mtc1       %[tmp0],    %[ftmp9]                           \n\t"
@@ -152,28 +154,28 @@ void ff_put_h264_chroma_mc8_mmi(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
             MMI_SDC1(%[ftmp3], %[dst], 0x00)
             PTR_ADDU   "%[dst],     %[dst],         %[stride]          \n\t"
             "bnez       %[h],       1b                                 \n\t"
-            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+            : RESTRICT_ASM_ALL64
+              [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
               [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
               [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
               [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
               [ftmp8]"=&f"(ftmp[8]),        [ftmp9]"=&f"(ftmp[9]),
               [ftmp10]"=&f"(ftmp[10]),      [ftmp11]"=&f"(ftmp[11]),
-              [tmp0]"=&r"(tmp[0]),
               [dst]"+&r"(dst),              [src]"+&r"(src),
               [h]"+&r"(h)
-            : [stride]"r"((mips_reg)stride),[ff_pw_32]"f"(ff_pw_32),
-              [A]"f"(A),                    [B]"f"(B),
-              [C]"f"(C),                    [D]"f"(D)
+            : [stride]"r"((mips_reg)stride),[ff_pw_32]"f"(ff_pw_32.f),
+              [A]"f"(A.f),                  [B]"f"(B.f),
+              [C]"f"(C.f),                  [D]"f"(D.f),
+              [tmp0]"r"(0x06)
             : "memory"
         );
     } else if (x) {
         /* x!=0, y==0 */
-        E = x << 3;
-        A = 64 - E;
+        E.i = x << 3;
+        A.i = 64 - E.i;
 
         __asm__ volatile (
-            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]           \n\t"
-            "dli        %[tmp0],    0x06                               \n\t"
+            "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]           \n\t"
             "pshufh     %[A],       %[A],           %[ftmp0]           \n\t"
             "pshufh     %[E],       %[E],           %[ftmp0]           \n\t"
             "mtc1       %[tmp0],    %[ftmp7]                           \n\t"
@@ -203,26 +205,25 @@ void ff_put_h264_chroma_mc8_mmi(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
             MMI_SDC1(%[ftmp1], %[dst], 0x00)
             PTR_ADDU   "%[dst],     %[dst],         %[stride]          \n\t"
             "bnez       %[h],       1b                                 \n\t"
-            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+            : RESTRICT_ASM_ALL64
+              [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
               [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
               [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
               [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
-              [tmp0]"=&r"(tmp[0]),
               [dst]"+&r"(dst),              [src]"+&r"(src),
               [h]"+&r"(h)
             : [stride]"r"((mips_reg)stride),
-              [ff_pw_32]"f"(ff_pw_32),
-              [A]"f"(A),                    [E]"f"(E)
+              [ff_pw_32]"f"(ff_pw_32.f),    [tmp0]"r"(0x06),
+              [A]"f"(A.f),                  [E]"f"(E.f)
             : "memory"
         );
     } else {
         /* x==0, y!=0 */
-        E = y << 3;
-        A = 64 - E;
+        E.i = y << 3;
+        A.i = 64 - E.i;
 
         __asm__ volatile (
-            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]           \n\t"
-            "dli        %[tmp0],    0x06                               \n\t"
+            "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]           \n\t"
             "pshufh     %[A],       %[A],           %[ftmp0]           \n\t"
             "pshufh     %[E],       %[E],           %[ftmp0]           \n\t"
             "mtc1       %[tmp0],    %[ftmp7]                           \n\t"
@@ -272,16 +273,17 @@ void ff_put_h264_chroma_mc8_mmi(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
             MMI_SDC1(%[ftmp2], %[dst], 0x00)
             PTR_ADDU   "%[dst],     %[dst],         %[stride]          \n\t"
             "bnez       %[h],       1b                                 \n\t"
-            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+            : RESTRICT_ASM_ALL64
+              [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
               [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
               [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
               [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
-              [ftmp8]"=&f"(ftmp[8]),        [tmp0]"=&r"(tmp[0]),
+              [ftmp8]"=&f"(ftmp[8]),
               [dst]"+&r"(dst),              [src]"+&r"(src),
               [h]"+&r"(h)
             : [stride]"r"((mips_reg)stride),
-              [ff_pw_32]"f"(ff_pw_32),
-              [A]"f"(A),                    [E]"f"(E)
+              [ff_pw_32]"f"(ff_pw_32.f),    [A]"f"(A.f),
+              [E]"f"(E.f),                  [tmp0]"r"(0x06)
             : "memory"
         );
     }
@@ -290,12 +292,14 @@ void ff_put_h264_chroma_mc8_mmi(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
 void ff_avg_h264_chroma_mc8_mmi(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
         int h, int x, int y)
 {
-    int A = 64, B, C, D, E;
     double ftmp[10];
-    uint64_t tmp[1];
+    union mmi_intfloat64 A, B, C, D, E;
+    DECLARE_VAR_ALL64;
+
+    A.i = 64;
 
     if(!(x || y)){
-        /* x=0, y=0, A=64 */
+        /* x=0, y=0, A.i=64 */
         __asm__ volatile (
             "1:                                                         \n\t"
             MMI_ULDC1(%[ftmp0], %[src], 0x00)
@@ -314,7 +318,8 @@ void ff_avg_h264_chroma_mc8_mmi(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
             PTR_ADDU   "%[dst],     %[dst],         %[stride]           \n\t"
             "addi       %[h],       %[h],           -0x02               \n\t"
             "bnez       %[h],       1b                                  \n\t"
-            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+            : RESTRICT_ASM_ALL64
+              [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
               [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
               [dst]"+&r"(dst),              [src]"+&r"(src),
               [h]"+&r"(h)
@@ -323,13 +328,12 @@ void ff_avg_h264_chroma_mc8_mmi(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
         );
     } else if (x && y) {
         /* x!=0, y!=0 */
-        D = x * y;
-        B = (x << 3) - D;
-        C = (y << 3) - D;
-        A = 64 - D - B - C;
+        D.i = x * y;
+        B.i = (x << 3) - D.i;
+        C.i = (y << 3) - D.i;
+        A.i = 64 - D.i - B.i - C.i;
         __asm__ volatile (
-            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]       \n\t"
-            "dli        %[tmp0],    0x06                           \n\t"
+            "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]       \n\t"
             "pshufh     %[A],       %[A],           %[ftmp0]       \n\t"
             "pshufh     %[B],       %[B],           %[ftmp0]       \n\t"
             "mtc1       %[tmp0],    %[ftmp9]                       \n\t"
@@ -378,26 +382,26 @@ void ff_avg_h264_chroma_mc8_mmi(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
             MMI_SDC1(%[ftmp1], %[dst], 0x00)
             PTR_ADDU   "%[dst],     %[dst],         %[stride]      \n\t"
             "bnez       %[h],       1b                             \n\t"
-            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+            : RESTRICT_ASM_ALL64
+              [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
               [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
               [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
               [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
               [ftmp8]"=&f"(ftmp[8]),        [ftmp9]"=&f"(ftmp[9]),
-              [tmp0]"=&r"(tmp[0]),
               [dst]"+&r"(dst),              [src]"+&r"(src),
               [h]"+&r"(h)
-            : [stride]"r"((mips_reg)stride),[ff_pw_32]"f"(ff_pw_32),
-              [A]"f"(A),                    [B]"f"(B),
-              [C]"f"(C),                    [D]"f"(D)
+            : [stride]"r"((mips_reg)stride),[ff_pw_32]"f"(ff_pw_32.f),
+              [A]"f"(A.f),                  [B]"f"(B.f),
+              [C]"f"(C.f),                  [D]"f"(D.f),
+              [tmp0]"r"(0x06)
             : "memory"
         );
     } else if (x) {
         /* x!=0, y==0 */
-        E = x << 3;
-        A = 64 - E;
+        E.i = x << 3;
+        A.i = 64 - E.i;
         __asm__ volatile (
-            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]       \n\t"
-            "dli        %[tmp0],    0x06                           \n\t"
+            "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]       \n\t"
             "pshufh     %[A],       %[A],           %[ftmp0]       \n\t"
             "pshufh     %[E],       %[E],           %[ftmp0]       \n\t"
             "mtc1       %[tmp0],    %[ftmp7]                       \n\t"
@@ -429,25 +433,24 @@ void ff_avg_h264_chroma_mc8_mmi(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
             MMI_SDC1(%[ftmp1], %[dst], 0x00)
             PTR_ADDU   "%[dst],     %[dst],         %[stride]      \n\t"
             "bnez       %[h],       1b                             \n\t"
-            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+            : RESTRICT_ASM_ALL64
+              [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
               [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
               [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
               [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
-              [tmp0]"=&r"(tmp[0]),
               [dst]"+&r"(dst),              [src]"+&r"(src),
               [h]"+&r"(h)
             : [stride]"r"((mips_reg)stride),
-              [ff_pw_32]"f"(ff_pw_32),
-              [A]"f"(A),                    [E]"f"(E)
+              [ff_pw_32]"f"(ff_pw_32.f),    [tmp0]"r"(0x06),
+              [A]"f"(A.f),                  [E]"f"(E.f)
             : "memory"
         );
     } else {
         /* x==0, y!=0 */
-        E = y << 3;
-        A = 64 - E;
+        E.i = y << 3;
+        A.i = 64 - E.i;
         __asm__ volatile (
-            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]       \n\t"
-            "dli        %[tmp0],    0x06                           \n\t"
+            "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]       \n\t"
             "pshufh     %[A],       %[A],           %[ftmp0]       \n\t"
             "pshufh     %[E],       %[E],           %[ftmp0]       \n\t"
             "mtc1       %[tmp0],    %[ftmp7]                       \n\t"
@@ -469,8 +472,8 @@ void ff_avg_h264_chroma_mc8_mmi(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
             "pmullh     %[ftmp6],   %[ftmp6],       %[E]           \n\t"
             "paddh      %[ftmp2],   %[ftmp4],       %[ftmp6]       \n\t"
 
-            "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_32]    \n\t"
-            "paddh      %[ftmp2],   %[ftmp2],       %[ff_pw_32]    \n\t"
+            "paddh      %[ftmp1],   %[ftmp1],       %[ff_pw_32]  \n\t"
+            "paddh      %[ftmp2],   %[ftmp2],       %[ff_pw_32]  \n\t"
             "psrlh      %[ftmp1],   %[ftmp1],       %[ftmp7]       \n\t"
             "psrlh      %[ftmp2],   %[ftmp2],       %[ftmp7]       \n\t"
             "packushb   %[ftmp1],   %[ftmp1],       %[ftmp2]       \n\t"
@@ -479,16 +482,16 @@ void ff_avg_h264_chroma_mc8_mmi(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
             MMI_SDC1(%[ftmp1], %[dst], 0x00)
             PTR_ADDU   "%[dst],     %[dst],         %[stride]      \n\t"
             "bnez       %[h],       1b                             \n\t"
-            : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
+            : RESTRICT_ASM_ALL64
+              [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
               [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
               [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
               [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
-              [tmp0]"=&r"(tmp[0]),
               [dst]"+&r"(dst),              [src]"+&r"(src),
               [h]"+&r"(h)
             : [stride]"r"((mips_reg)stride),
-              [ff_pw_32]"f"(ff_pw_32),
-              [A]"f"(A),                    [E]"f"(E)
+              [ff_pw_32]"f"(ff_pw_32.f),    [tmp0]"r"(0x06),
+              [A]"f"(A.f),                  [E]"f"(E.f)
             : "memory"
         );
     }
@@ -497,20 +500,19 @@ void ff_avg_h264_chroma_mc8_mmi(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
 void ff_put_h264_chroma_mc4_mmi(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
         int h, int x, int y)
 {
-    const int A = (8 - x) * (8 - y);
-    const int B = x * (8 - y);
-    const int C = (8 - x) * y;
-    const int D = x * y;
-    const int E = B + C;
     double ftmp[8];
-    uint64_t tmp[1];
     mips_reg addr[1];
+    union mmi_intfloat64 A, B, C, D, E;
     DECLARE_VAR_LOW32;
+    A.i = (8 - x) * (8 - y);
+    B.i = x * (8 - y);
+    C.i = (8 - x) * y;
+    D.i = x * y;
+    E.i = B.i + C.i;
 
-    if (D) {
+    if (D.i) {
         __asm__ volatile (
-            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
-            "dli        %[tmp0],    0x06                                \n\t"
+            "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
             "pshufh     %[A],       %[A],           %[ftmp0]            \n\t"
             "pshufh     %[B],       %[B],           %[ftmp0]            \n\t"
             "mtc1       %[tmp0],    %[ftmp7]                            \n\t"
@@ -547,20 +549,19 @@ void ff_put_h264_chroma_mc4_mmi(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
               [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
               [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
               [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
-              [tmp0]"=&r"(tmp[0]),
               RESTRICT_ASM_LOW32
               [dst]"+&r"(dst),              [src]"+&r"(src),
               [h]"+&r"(h)
-            : [stride]"r"((mips_reg)stride),[ff_pw_32]"f"(ff_pw_32),
-              [A]"f"(A),                    [B]"f"(B),
-              [C]"f"(C),                    [D]"f"(D)
+            : [stride]"r"((mips_reg)stride),[ff_pw_32]"f"(ff_pw_32.f),
+              [A]"f"(A.f),                  [B]"f"(B.f),
+              [C]"f"(C.f),                  [D]"f"(D.f),
+              [tmp0]"r"(0x06)
             : "memory"
         );
-    } else if (E) {
-        const int step = C ? stride : 1;
+    } else if (E.i) {
+        const int step = C.i ? stride : 1;
         __asm__ volatile (
-            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
-            "dli        %[tmp0],    0x06                                \n\t"
+            "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
             "pshufh     %[A],       %[A],           %[ftmp0]            \n\t"
             "pshufh     %[E],       %[E],           %[ftmp0]            \n\t"
             "mtc1       %[tmp0],    %[ftmp5]                            \n\t"
@@ -585,14 +586,13 @@ void ff_put_h264_chroma_mc4_mmi(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
             : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
               [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
               [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
-              [tmp0]"=&r"(tmp[0]),
               RESTRICT_ASM_LOW32
               [addr0]"=&r"(addr[0]),
               [dst]"+&r"(dst),              [src]"+&r"(src),
               [h]"+&r"(h)
             : [stride]"r"((mips_reg)stride),[step]"r"((mips_reg)step),
-              [ff_pw_32]"f"(ff_pw_32),
-              [A]"f"(A),                    [E]"f"(E)
+              [ff_pw_32]"f"(ff_pw_32.f),    [tmp0]"r"(0x06),
+              [A]"f"(A.f),                  [E]"f"(E.f)
             : "memory"
         );
     } else {
@@ -621,20 +621,19 @@ void ff_put_h264_chroma_mc4_mmi(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
 void ff_avg_h264_chroma_mc4_mmi(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
         int h, int x, int y)
 {
-    const int A = (8 - x) *(8 - y);
-    const int B = x * (8 - y);
-    const int C = (8 - x) * y;
-    const int D = x * y;
-    const int E = B + C;
     double ftmp[8];
-    uint64_t tmp[1];
     mips_reg addr[1];
+    union mmi_intfloat64 A, B, C, D, E;
     DECLARE_VAR_LOW32;
+    A.i = (8 - x) *(8 - y);
+    B.i = x * (8 - y);
+    C.i = (8 - x) * y;
+    D.i = x * y;
+    E.i = B.i + C.i;
 
-    if (D) {
+    if (D.i) {
         __asm__ volatile (
-            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
-            "dli        %[tmp0],    0x06                                \n\t"
+            "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
             "pshufh     %[A],       %[A],           %[ftmp0]            \n\t"
             "pshufh     %[B],       %[B],           %[ftmp0]            \n\t"
             "mtc1       %[tmp0],    %[ftmp7]                            \n\t"
@@ -673,20 +672,19 @@ void ff_avg_h264_chroma_mc4_mmi(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
               [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
               [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
               [ftmp6]"=&f"(ftmp[6]),        [ftmp7]"=&f"(ftmp[7]),
-              [tmp0]"=&r"(tmp[0]),
               RESTRICT_ASM_LOW32
               [dst]"+&r"(dst),              [src]"+&r"(src),
               [h]"+&r"(h)
-            : [stride]"r"((mips_reg)stride),[ff_pw_32]"f"(ff_pw_32),
-              [A]"f"(A),                    [B]"f"(B),
-              [C]"f"(C),                    [D]"f"(D)
+            : [stride]"r"((mips_reg)stride),[ff_pw_32]"f"(ff_pw_32.f),
+              [A]"f"(A.f),                  [B]"f"(B.f),
+              [C]"f"(C.f),                  [D]"f"(D.f),
+              [tmp0]"r"(0x06)
             : "memory"
         );
-    } else if (E) {
-        const int step = C ? stride : 1;
+    } else if (E.i) {
+        const int step = C.i ? stride : 1;
         __asm__ volatile (
-            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
-            "dli        %[tmp0],    0x06                                \n\t"
+            "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
             "pshufh     %[A],       %[A],           %[ftmp0]            \n\t"
             "pshufh     %[E],       %[E],           %[ftmp0]            \n\t"
             "mtc1       %[tmp0],    %[ftmp5]                            \n\t"
@@ -713,14 +711,13 @@ void ff_avg_h264_chroma_mc4_mmi(uint8_t *dst, uint8_t *src, ptrdiff_t stride,
             : [ftmp0]"=&f"(ftmp[0]),        [ftmp1]"=&f"(ftmp[1]),
               [ftmp2]"=&f"(ftmp[2]),        [ftmp3]"=&f"(ftmp[3]),
               [ftmp4]"=&f"(ftmp[4]),        [ftmp5]"=&f"(ftmp[5]),
-              [tmp0]"=&r"(tmp[0]),
               RESTRICT_ASM_LOW32
               [addr0]"=&r"(addr[0]),
               [dst]"+&r"(dst),              [src]"+&r"(src),
               [h]"+&r"(h)
             : [stride]"r"((mips_reg)stride),[step]"r"((mips_reg)step),
-              [ff_pw_32]"f"(ff_pw_32),
-              [A]"f"(A),                    [E]"f"(E)
+              [ff_pw_32]"f"(ff_pw_32.f),    [tmp0]"r"(0x06),
+              [A]"f"(A.f),                  [E]"f"(E.f)
             : "memory"
         );
     } else {
diff --git a/libavcodec/mips/h264dsp_init_mips.c b/libavcodec/mips/h264dsp_init_mips.c
index dc08a25800..02669e0059 100644
--- a/libavcodec/mips/h264dsp_init_mips.c
+++ b/libavcodec/mips/h264dsp_init_mips.c
@@ -19,129 +19,120 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
+#include "libavutil/mips/cpu.h"
 #include "h264dsp_mips.h"
 
-#if HAVE_MSA
-static av_cold void h264dsp_init_msa(H264DSPContext *c,
-                                     const int bit_depth,
-                                     const int chroma_format_idc)
-{
-    if (8 == bit_depth) {
-        c->h264_v_loop_filter_luma = ff_h264_v_lpf_luma_inter_msa;
-        c->h264_h_loop_filter_luma = ff_h264_h_lpf_luma_inter_msa;
-        c->h264_h_loop_filter_luma_mbaff =
-            ff_h264_h_loop_filter_luma_mbaff_msa;
-        c->h264_v_loop_filter_luma_intra = ff_h264_v_lpf_luma_intra_msa;
-        c->h264_h_loop_filter_luma_intra = ff_h264_h_lpf_luma_intra_msa;
-        c->h264_h_loop_filter_luma_mbaff_intra =
-            ff_h264_h_loop_filter_luma_mbaff_intra_msa;
-        c->h264_v_loop_filter_chroma = ff_h264_v_lpf_chroma_inter_msa;
-
-        if (chroma_format_idc <= 1)
-            c->h264_h_loop_filter_chroma = ff_h264_h_lpf_chroma_inter_msa;
-        else
-            c->h264_h_loop_filter_chroma =
-                ff_h264_h_loop_filter_chroma422_msa;
-
-        if (chroma_format_idc > 1)
-            c->h264_h_loop_filter_chroma_mbaff =
-                ff_h264_h_loop_filter_chroma422_mbaff_msa;
-
-        c->h264_v_loop_filter_chroma_intra =
-            ff_h264_v_lpf_chroma_intra_msa;
-
-        if (chroma_format_idc <= 1)
-            c->h264_h_loop_filter_chroma_intra =
-                ff_h264_h_lpf_chroma_intra_msa;
-
-        /* Weighted MC */
-        c->weight_h264_pixels_tab[0] = ff_weight_h264_pixels16_8_msa;
-        c->weight_h264_pixels_tab[1] = ff_weight_h264_pixels8_8_msa;
-        c->weight_h264_pixels_tab[2] = ff_weight_h264_pixels4_8_msa;
-
-        c->biweight_h264_pixels_tab[0] = ff_biweight_h264_pixels16_8_msa;
-        c->biweight_h264_pixels_tab[1] = ff_biweight_h264_pixels8_8_msa;
-        c->biweight_h264_pixels_tab[2] = ff_biweight_h264_pixels4_8_msa;
-
-        c->h264_idct_add = ff_h264_idct_add_msa;
-        c->h264_idct8_add = ff_h264_idct8_addblk_msa;
-        c->h264_idct_dc_add = ff_h264_idct4x4_addblk_dc_msa;
-        c->h264_idct8_dc_add = ff_h264_idct8_dc_addblk_msa;
-        c->h264_idct_add16 = ff_h264_idct_add16_msa;
-        c->h264_idct8_add4 = ff_h264_idct8_add4_msa;
-
-        if (chroma_format_idc <= 1)
-            c->h264_idct_add8 = ff_h264_idct_add8_msa;
-        else
-            c->h264_idct_add8 = ff_h264_idct_add8_422_msa;
-
-        c->h264_idct_add16intra = ff_h264_idct_add16_intra_msa;
-        c->h264_luma_dc_dequant_idct = ff_h264_deq_idct_luma_dc_msa;
-    }  // if (8 == bit_depth)
-}
-#endif  // #if HAVE_MSA
-
-#if HAVE_MMI
-static av_cold void h264dsp_init_mmi(H264DSPContext * c, const int bit_depth,
-        const int chroma_format_idc)
+av_cold void ff_h264dsp_init_mips(H264DSPContext *c, const int bit_depth,
+                                  const int chroma_format_idc)
 {
-    if (bit_depth == 8) {
-        c->h264_add_pixels4_clear = ff_h264_add_pixels4_8_mmi;
-        c->h264_idct_add = ff_h264_idct_add_8_mmi;
-        c->h264_idct8_add = ff_h264_idct8_add_8_mmi;
-        c->h264_idct_dc_add = ff_h264_idct_dc_add_8_mmi;
-        c->h264_idct8_dc_add = ff_h264_idct8_dc_add_8_mmi;
-        c->h264_idct_add16 = ff_h264_idct_add16_8_mmi;
-        c->h264_idct_add16intra = ff_h264_idct_add16intra_8_mmi;
-        c->h264_idct8_add4 = ff_h264_idct8_add4_8_mmi;
-
-        if (chroma_format_idc <= 1)
-            c->h264_idct_add8 = ff_h264_idct_add8_8_mmi;
-        else
-            c->h264_idct_add8 = ff_h264_idct_add8_422_8_mmi;
-
-        c->h264_luma_dc_dequant_idct = ff_h264_luma_dc_dequant_idct_8_mmi;
+    int cpu_flags = av_get_cpu_flags();
+
+    if (have_mmi(cpu_flags)) {
+        if (bit_depth == 8) {
+            c->h264_add_pixels4_clear = ff_h264_add_pixels4_8_mmi;
+            c->h264_idct_add = ff_h264_idct_add_8_mmi;
+            c->h264_idct8_add = ff_h264_idct8_add_8_mmi;
+            c->h264_idct_dc_add = ff_h264_idct_dc_add_8_mmi;
+            c->h264_idct8_dc_add = ff_h264_idct8_dc_add_8_mmi;
+            c->h264_idct_add16 = ff_h264_idct_add16_8_mmi;
+            c->h264_idct_add16intra = ff_h264_idct_add16intra_8_mmi;
+            c->h264_idct8_add4 = ff_h264_idct8_add4_8_mmi;
+
+            if (chroma_format_idc <= 1)
+                c->h264_idct_add8 = ff_h264_idct_add8_8_mmi;
+            else
+                c->h264_idct_add8 = ff_h264_idct_add8_422_8_mmi;
+
+            c->h264_luma_dc_dequant_idct = ff_h264_luma_dc_dequant_idct_8_mmi;
+
+            if (chroma_format_idc <= 1)
+                c->h264_chroma_dc_dequant_idct =
+                    ff_h264_chroma_dc_dequant_idct_8_mmi;
+            else
+                c->h264_chroma_dc_dequant_idct =
+                    ff_h264_chroma422_dc_dequant_idct_8_mmi;
+
+            c->weight_h264_pixels_tab[0] = ff_h264_weight_pixels16_8_mmi;
+            c->weight_h264_pixels_tab[1] = ff_h264_weight_pixels8_8_mmi;
+            c->weight_h264_pixels_tab[2] = ff_h264_weight_pixels4_8_mmi;
+
+            c->biweight_h264_pixels_tab[0] = ff_h264_biweight_pixels16_8_mmi;
+            c->biweight_h264_pixels_tab[1] = ff_h264_biweight_pixels8_8_mmi;
+            c->biweight_h264_pixels_tab[2] = ff_h264_biweight_pixels4_8_mmi;
+
+            c->h264_v_loop_filter_chroma       = ff_deblock_v_chroma_8_mmi;
+            c->h264_v_loop_filter_chroma_intra = ff_deblock_v_chroma_intra_8_mmi;
+
+            if (chroma_format_idc <= 1) {
+                c->h264_h_loop_filter_chroma =
+                    ff_deblock_h_chroma_8_mmi;
+                c->h264_h_loop_filter_chroma_intra =
+                    ff_deblock_h_chroma_intra_8_mmi;
+            }
+
+            c->h264_v_loop_filter_luma = ff_deblock_v_luma_8_mmi;
+            c->h264_v_loop_filter_luma_intra = ff_deblock_v_luma_intra_8_mmi;
+            c->h264_h_loop_filter_luma = ff_deblock_h_luma_8_mmi;
+            c->h264_h_loop_filter_luma_intra = ff_deblock_h_luma_intra_8_mmi;
+        }
+    }
 
+    if (have_msa(cpu_flags)) {
         if (chroma_format_idc <= 1)
-            c->h264_chroma_dc_dequant_idct =
-                ff_h264_chroma_dc_dequant_idct_8_mmi;
-        else
-            c->h264_chroma_dc_dequant_idct =
-                ff_h264_chroma422_dc_dequant_idct_8_mmi;
-
-        c->weight_h264_pixels_tab[0] = ff_h264_weight_pixels16_8_mmi;
-        c->weight_h264_pixels_tab[1] = ff_h264_weight_pixels8_8_mmi;
-        c->weight_h264_pixels_tab[2] = ff_h264_weight_pixels4_8_mmi;
-
-        c->biweight_h264_pixels_tab[0] = ff_h264_biweight_pixels16_8_mmi;
-        c->biweight_h264_pixels_tab[1] = ff_h264_biweight_pixels8_8_mmi;
-        c->biweight_h264_pixels_tab[2] = ff_h264_biweight_pixels4_8_mmi;
-
-        c->h264_v_loop_filter_chroma       = ff_deblock_v_chroma_8_mmi;
-        c->h264_v_loop_filter_chroma_intra = ff_deblock_v_chroma_intra_8_mmi;
-
-        if (chroma_format_idc <= 1) {
-            c->h264_h_loop_filter_chroma =
-                ff_deblock_h_chroma_8_mmi;
-            c->h264_h_loop_filter_chroma_intra =
-                ff_deblock_h_chroma_intra_8_mmi;
+            c->h264_loop_filter_strength = ff_h264_loop_filter_strength_msa;
+        if (bit_depth == 8) {
+            /* TODO: Presently, MMI version of h264_v_loop_filter_luma are
+               more effective than MSA. To be refined. */
+            //c->h264_v_loop_filter_luma = ff_h264_v_lpf_luma_inter_msa;
+            c->h264_h_loop_filter_luma = ff_h264_h_lpf_luma_inter_msa;
+            c->h264_h_loop_filter_luma_mbaff =
+                ff_h264_h_loop_filter_luma_mbaff_msa;
+            c->h264_v_loop_filter_luma_intra = ff_h264_v_lpf_luma_intra_msa;
+            c->h264_h_loop_filter_luma_intra = ff_h264_h_lpf_luma_intra_msa;
+            c->h264_h_loop_filter_luma_mbaff_intra =
+                ff_h264_h_loop_filter_luma_mbaff_intra_msa;
+            c->h264_v_loop_filter_chroma = ff_h264_v_lpf_chroma_inter_msa;
+
+            if (chroma_format_idc <= 1)
+                c->h264_h_loop_filter_chroma = ff_h264_h_lpf_chroma_inter_msa;
+            else
+                c->h264_h_loop_filter_chroma =
+                    ff_h264_h_loop_filter_chroma422_msa;
+
+            if (chroma_format_idc > 1)
+                c->h264_h_loop_filter_chroma_mbaff =
+                    ff_h264_h_loop_filter_chroma422_mbaff_msa;
+
+            c->h264_v_loop_filter_chroma_intra =
+                ff_h264_v_lpf_chroma_intra_msa;
+
+            if (chroma_format_idc <= 1)
+                c->h264_h_loop_filter_chroma_intra =
+                    ff_h264_h_lpf_chroma_intra_msa;
+
+            /* Weighted MC */
+            c->weight_h264_pixels_tab[0] = ff_weight_h264_pixels16_8_msa;
+            c->weight_h264_pixels_tab[1] = ff_weight_h264_pixels8_8_msa;
+            c->weight_h264_pixels_tab[2] = ff_weight_h264_pixels4_8_msa;
+
+            c->biweight_h264_pixels_tab[0] = ff_biweight_h264_pixels16_8_msa;
+            c->biweight_h264_pixels_tab[1] = ff_biweight_h264_pixels8_8_msa;
+            c->biweight_h264_pixels_tab[2] = ff_biweight_h264_pixels4_8_msa;
+
+            c->h264_idct_add = ff_h264_idct_add_msa;
+            c->h264_idct8_add = ff_h264_idct8_addblk_msa;
+            c->h264_idct_dc_add = ff_h264_idct4x4_addblk_dc_msa;
+            c->h264_idct8_dc_add = ff_h264_idct8_dc_addblk_msa;
+            c->h264_idct_add16 = ff_h264_idct_add16_msa;
+            c->h264_idct8_add4 = ff_h264_idct8_add4_msa;
+
+            if (chroma_format_idc <= 1)
+                c->h264_idct_add8 = ff_h264_idct_add8_msa;
+            else
+                c->h264_idct_add8 = ff_h264_idct_add8_422_msa;
+
+            c->h264_idct_add16intra = ff_h264_idct_add16_intra_msa;
+            c->h264_luma_dc_dequant_idct = ff_h264_deq_idct_luma_dc_msa;
         }
-
-        c->h264_v_loop_filter_luma = ff_deblock_v_luma_8_mmi;
-        c->h264_v_loop_filter_luma_intra = ff_deblock_v_luma_intra_8_mmi;
-        c->h264_h_loop_filter_luma = ff_deblock_h_luma_8_mmi;
-        c->h264_h_loop_filter_luma_intra = ff_deblock_h_luma_intra_8_mmi;
     }
 }
-#endif /* HAVE_MMI */
-
-av_cold void ff_h264dsp_init_mips(H264DSPContext *c, const int bit_depth,
-                                  const int chroma_format_idc)
-{
-#if HAVE_MMI
-    h264dsp_init_mmi(c, bit_depth, chroma_format_idc);
-#endif /* HAVE_MMI */
-#if HAVE_MSA
-    h264dsp_init_msa(c, bit_depth, chroma_format_idc);
-#endif  // #if HAVE_MSA
-}
diff --git a/libavcodec/mips/h264dsp_mips.h b/libavcodec/mips/h264dsp_mips.h
index 21b7de06f0..5847ef36fe 100644
--- a/libavcodec/mips/h264dsp_mips.h
+++ b/libavcodec/mips/h264dsp_mips.h
@@ -25,21 +25,21 @@
 #include "libavcodec/h264dec.h"
 #include "constants.h"
 
-void ff_h264_h_lpf_luma_inter_msa(uint8_t *src, int stride,
+void ff_h264_h_lpf_luma_inter_msa(uint8_t *src, ptrdiff_t stride,
                                   int alpha, int beta, int8_t *tc0);
-void ff_h264_v_lpf_luma_inter_msa(uint8_t *src, int stride,
+void ff_h264_v_lpf_luma_inter_msa(uint8_t *src, ptrdiff_t stride,
                                   int alpha, int beta, int8_t *tc0);
-void ff_h264_h_lpf_chroma_inter_msa(uint8_t *src, int stride,
+void ff_h264_h_lpf_chroma_inter_msa(uint8_t *src, ptrdiff_t stride,
                                     int alpha, int beta, int8_t *tc0);
-void ff_h264_v_lpf_chroma_inter_msa(uint8_t *src, int stride,
+void ff_h264_v_lpf_chroma_inter_msa(uint8_t *src, ptrdiff_t stride,
                                     int alpha, int beta, int8_t *tc0);
-void ff_h264_h_loop_filter_chroma422_msa(uint8_t *src, int32_t stride,
+void ff_h264_h_loop_filter_chroma422_msa(uint8_t *src, ptrdiff_t stride,
                                          int32_t alpha, int32_t beta,
                                          int8_t *tc0);
-void ff_h264_h_loop_filter_chroma422_mbaff_msa(uint8_t *src, int32_t stride,
+void ff_h264_h_loop_filter_chroma422_mbaff_msa(uint8_t *src, ptrdiff_t stride,
                                                int32_t alpha, int32_t beta,
                                                int8_t *tc0);
-void ff_h264_h_loop_filter_luma_mbaff_msa(uint8_t *src, int32_t stride,
+void ff_h264_h_loop_filter_luma_mbaff_msa(uint8_t *src, ptrdiff_t stride,
                                           int32_t alpha, int32_t beta,
                                           int8_t *tc0);
 
@@ -67,15 +67,15 @@ void ff_h264_idct8_add4_msa(uint8_t *dst, const int *blk_offset,
                             int16_t *blk, int dst_stride,
                             const uint8_t nnzc[15 * 8]);
 
-void ff_h264_h_lpf_luma_intra_msa(uint8_t *src, int stride,
+void ff_h264_h_lpf_luma_intra_msa(uint8_t *src, ptrdiff_t stride,
                                   int alpha, int beta);
-void ff_h264_v_lpf_luma_intra_msa(uint8_t *src, int stride,
+void ff_h264_v_lpf_luma_intra_msa(uint8_t *src, ptrdiff_t stride,
                                   int alpha, int beta);
-void ff_h264_h_lpf_chroma_intra_msa(uint8_t *src, int stride,
+void ff_h264_h_lpf_chroma_intra_msa(uint8_t *src, ptrdiff_t stride,
                                     int alpha, int beta);
-void ff_h264_v_lpf_chroma_intra_msa(uint8_t *src, int stride,
+void ff_h264_v_lpf_chroma_intra_msa(uint8_t *src, ptrdiff_t stride,
                                     int alpha, int beta);
-void ff_h264_h_loop_filter_luma_mbaff_intra_msa(uint8_t *src, int stride,
+void ff_h264_h_loop_filter_luma_mbaff_intra_msa(uint8_t *src, ptrdiff_t stride,
                                                 int alpha, int beta);
 
 void ff_biweight_h264_pixels16_8_msa(uint8_t *dst, uint8_t *src,
@@ -319,6 +319,10 @@ void ff_vp8_pred8x8_129_dc_8_msa(uint8_t *src, ptrdiff_t stride);
 void ff_vp8_pred16x16_127_dc_8_msa(uint8_t *src, ptrdiff_t stride);
 void ff_vp8_pred16x16_129_dc_8_msa(uint8_t *src, ptrdiff_t stride);
 
+void ff_h264_loop_filter_strength_msa(int16_t bS[2][4][4], uint8_t nnz[40],
+        int8_t ref[2][40], int16_t mv[2][40][2], int bidir, int edges,
+        int step, int mask_mv0, int mask_mv1, int field);
+
 void ff_h264_add_pixels4_8_mmi(uint8_t *_dst, int16_t *_src, int stride);
 void ff_h264_idct_add_8_mmi(uint8_t *dst, int16_t *block, int stride);
 void ff_h264_idct8_add_8_mmi(uint8_t *dst, int16_t *block, int stride);
@@ -357,23 +361,23 @@ void ff_h264_biweight_pixels4_8_mmi(uint8_t *dst, uint8_t *src,
 
 void ff_deblock_v_chroma_8_mmi(uint8_t *pix, ptrdiff_t stride, int alpha, int beta,
         int8_t *tc0);
-void ff_deblock_v_chroma_intra_8_mmi(uint8_t *pix, int stride, int alpha,
+void ff_deblock_v_chroma_intra_8_mmi(uint8_t *pix, ptrdiff_t stride, int alpha,
         int beta);
-void ff_deblock_h_chroma_8_mmi(uint8_t *pix, int stride, int alpha, int beta,
+void ff_deblock_h_chroma_8_mmi(uint8_t *pix, ptrdiff_t stride, int alpha, int beta,
         int8_t *tc0);
-void ff_deblock_h_chroma_intra_8_mmi(uint8_t *pix, int stride, int alpha,
+void ff_deblock_h_chroma_intra_8_mmi(uint8_t *pix, ptrdiff_t stride, int alpha,
         int beta);
-void ff_deblock_v_luma_8_mmi(uint8_t *pix, int stride, int alpha, int beta,
+void ff_deblock_v_luma_8_mmi(uint8_t *pix, ptrdiff_t stride, int alpha, int beta,
         int8_t *tc0);
-void ff_deblock_v_luma_intra_8_mmi(uint8_t *pix, int stride, int alpha,
+void ff_deblock_v_luma_intra_8_mmi(uint8_t *pix, ptrdiff_t stride, int alpha,
         int beta);
-void ff_deblock_h_luma_8_mmi(uint8_t *pix, int stride, int alpha, int beta,
+void ff_deblock_h_luma_8_mmi(uint8_t *pix, ptrdiff_t stride, int alpha, int beta,
         int8_t *tc0);
-void ff_deblock_h_luma_intra_8_mmi(uint8_t *pix, int stride, int alpha,
+void ff_deblock_h_luma_intra_8_mmi(uint8_t *pix, ptrdiff_t stride, int alpha,
         int beta);
-void ff_deblock_v8_luma_8_mmi(uint8_t *pix, int stride, int alpha, int beta,
+void ff_deblock_v8_luma_8_mmi(uint8_t *pix, ptrdiff_t stride, int alpha, int beta,
         int8_t *tc0);
-void ff_deblock_v8_luma_intra_8_mmi(uint8_t *pix, int stride, int alpha,
+void ff_deblock_v8_luma_intra_8_mmi(uint8_t *pix, ptrdiff_t stride, int alpha,
         int beta);
 
 void ff_put_h264_qpel16_mc00_mmi(uint8_t *dst, const uint8_t *src,
diff --git a/libavcodec/mips/h264dsp_mmi.c b/libavcodec/mips/h264dsp_mmi.c
index ac65a20db0..51612a9db6 100644
--- a/libavcodec/mips/h264dsp_mmi.c
+++ b/libavcodec/mips/h264dsp_mmi.c
@@ -33,11 +33,14 @@ void ff_h264_add_pixels4_8_mmi(uint8_t *dst, int16_t *src, int stride)
     DECLARE_VAR_LOW32;
 
     __asm__ volatile (
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
         MMI_LDC1(%[ftmp1], %[src], 0x00)
         MMI_LDC1(%[ftmp2], %[src], 0x08)
         MMI_LDC1(%[ftmp3], %[src], 0x10)
         MMI_LDC1(%[ftmp4], %[src], 0x18)
+        /* memset(src, 0, 32); */
+        MMI_SQC1(%[ftmp0], %[ftmp0], %[src], 0x00)
+        MMI_SQC1(%[ftmp0], %[ftmp0], %[src], 0x10)
         MMI_ULWC1(%[ftmp5], %[dst0], 0x00)
         MMI_ULWC1(%[ftmp6], %[dst1], 0x00)
         MMI_ULWC1(%[ftmp7], %[dst2], 0x00)
@@ -58,11 +61,6 @@ void ff_h264_add_pixels4_8_mmi(uint8_t *dst, int16_t *src, int stride)
         MMI_SWC1(%[ftmp2], %[dst1], 0x00)
         MMI_SWC1(%[ftmp3], %[dst2], 0x00)
         MMI_SWC1(%[ftmp4], %[dst3], 0x00)
-
-        /* memset(src, 0, 32); */
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
-        "gssqc1     %[ftmp0],   %[ftmp0],       0x00(%[src])            \n\t"
-        "gssqc1     %[ftmp0],   %[ftmp0],       0x10(%[src])            \n\t"
         : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
           [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
           [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
@@ -85,15 +83,19 @@ void ff_h264_idct_add_8_mmi(uint8_t *dst, int16_t *block, int stride)
     DECLARE_VAR_ADDRT;
 
     __asm__ volatile (
-        "dli        %[tmp0],    0x01                                    \n\t"
         MMI_LDC1(%[ftmp0], %[block], 0x00)
-        "mtc1       %[tmp0],    %[ftmp8]                                \n\t"
         MMI_LDC1(%[ftmp1], %[block], 0x08)
-        "dli        %[tmp0],    0x06                                    \n\t"
         MMI_LDC1(%[ftmp2], %[block], 0x10)
+        MMI_LDC1(%[ftmp3], %[block], 0x18)
+        /* memset(block, 0, 32) */
+        "pxor       %[ftmp4],   %[ftmp4],       %[ftmp4]                \n\t"
+        MMI_SQC1(%[ftmp4], %[ftmp4], %[block], 0x00)
+        MMI_SQC1(%[ftmp4], %[ftmp4], %[block], 0x10)
+        "dli        %[tmp0],    0x01                                    \n\t"
+        "mtc1       %[tmp0],    %[ftmp8]                                \n\t"
+        "dli        %[tmp0],    0x06                                    \n\t"
         "mtc1       %[tmp0],    %[ftmp9]                                \n\t"
         "psrah      %[ftmp4],   %[ftmp1],       %[ftmp8]                \n\t"
-        MMI_LDC1(%[ftmp3], %[block], 0x18)
         "psrah      %[ftmp5],   %[ftmp3],       %[ftmp8]                \n\t"
         "psubh      %[ftmp4],   %[ftmp4],       %[ftmp3]                \n\t"
         "paddh      %[ftmp5],   %[ftmp5],       %[ftmp1]                \n\t"
@@ -121,15 +123,11 @@ void ff_h264_idct_add_8_mmi(uint8_t *dst, int16_t *block, int stride)
         "paddh      %[ftmp10],  %[ftmp3],       %[ftmp1]                \n\t"
         "psubh      %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
         "paddh      %[ftmp11],  %[ftmp4],       %[ftmp5]                \n\t"
-        "xor        %[ftmp7],   %[ftmp7],       %[ftmp7]                \n\t"
         "psubh      %[ftmp5],   %[ftmp5],       %[ftmp4]                \n\t"
-        MMI_SDC1(%[ftmp7], %[block], 0x00)
-        MMI_SDC1(%[ftmp7], %[block], 0x08)
-        MMI_SDC1(%[ftmp7], %[block], 0x10)
-        MMI_SDC1(%[ftmp7], %[block], 0x18)
         MMI_ULWC1(%[ftmp2], %[dst], 0x00)
-        "psrah      %[ftmp3],   %[ftmp10],      %[ftmp9]                \n\t"
         MMI_LWXC1(%[ftmp0], %[dst], %[stride], 0x00)
+        "pxor       %[ftmp7],   %[ftmp7],       %[ftmp7]                \n\t"
+        "psrah      %[ftmp3],   %[ftmp10],      %[ftmp9]                \n\t"
         "psrah      %[ftmp4],   %[ftmp11],      %[ftmp9]                \n\t"
         "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp7]                \n\t"
         "punpcklbh  %[ftmp0],   %[ftmp0],       %[ftmp7]                \n\t"
@@ -153,11 +151,6 @@ void ff_h264_idct_add_8_mmi(uint8_t *dst, int16_t *block, int stride)
         MMI_SWC1(%[ftmp2], %[dst], 0x00)
         "packushb   %[ftmp0],   %[ftmp0],       %[ftmp7]                \n\t"
         MMI_SWXC1(%[ftmp0], %[dst], %[stride], 0x00)
-
-        /* memset(block, 0, 32) */
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
-        "gssqc1     %[ftmp0],   %[ftmp0],       0x00(%[block])          \n\t"
-        "gssqc1     %[ftmp0],   %[ftmp0],       0x10(%[block])          \n\t"
         : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
           [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
           [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
@@ -168,7 +161,7 @@ void ff_h264_idct_add_8_mmi(uint8_t *dst, int16_t *block, int stride)
           RESTRICT_ASM_ADDRT
           [tmp0]"=&r"(tmp[0])
         : [dst]"r"(dst),                    [block]"r"(block),
-          [stride]"r"((mips_reg)stride),    [ff_pw_32]"f"(ff_pw_32)
+          [stride]"r"((mips_reg)stride),    [ff_pw_32]"f"(ff_pw_32.f)
         : "memory"
     );
 
@@ -184,7 +177,7 @@ void ff_h264_idct8_add_8_mmi(uint8_t *dst, int16_t *block, int stride)
 
     __asm__ volatile (
         "lhu        %[tmp0],    0x00(%[block])                          \n\t"
-        PTR_ADDI   "$29,        $29,            -0x20                   \n\t"
+        PTR_ADDI   "$sp,        $sp,            -0x20                   \n\t"
         PTR_ADDIU  "%[tmp0],    %[tmp0],        0x20                    \n\t"
         MMI_LDC1(%[ftmp1], %[block], 0x10)
         "sh         %[tmp0],    0x00(%[block])                          \n\t"
@@ -261,8 +254,8 @@ void ff_h264_idct8_add_8_mmi(uint8_t *dst, int16_t *block, int stride)
         "punpckhwd  %[ftmp3],   %[ftmp6],       %[ftmp0]                \n\t"
         "punpcklwd  %[ftmp6],   %[ftmp6],       %[ftmp0]                \n\t"
         MMI_LDC1(%[ftmp0], %[block], 0x00)
-        MMI_SDC1(%[ftmp7], $29, 0x00)
-        MMI_SDC1(%[ftmp1], $29, 0x10)
+        MMI_SDC1(%[ftmp7], $sp, 0x00)
+        MMI_SDC1(%[ftmp1], $sp, 0x10)
         "dmfc1      %[tmp1],    %[ftmp6]                                \n\t"
         "dmfc1      %[tmp3],    %[ftmp3]                                \n\t"
         "punpckhhw  %[ftmp3],   %[ftmp5],       %[ftmp2]                \n\t"
@@ -273,8 +266,8 @@ void ff_h264_idct8_add_8_mmi(uint8_t *dst, int16_t *block, int stride)
         "punpcklwd  %[ftmp5],   %[ftmp5],       %[ftmp4]                \n\t"
         "punpckhwd  %[ftmp4],   %[ftmp3],       %[ftmp2]                \n\t"
         "punpcklwd  %[ftmp3],   %[ftmp3],       %[ftmp2]                \n\t"
-        MMI_SDC1(%[ftmp5], $29, 0x08)
-        MMI_SDC1(%[ftmp0], $29, 0x18)
+        MMI_SDC1(%[ftmp5], $sp, 0x08)
+        MMI_SDC1(%[ftmp0], $sp, 0x18)
         "dmfc1      %[tmp2],    %[ftmp3]                                \n\t"
         "dmfc1      %[tmp4],    %[ftmp4]                                \n\t"
         MMI_LDC1(%[ftmp1], %[block], 0x18)
@@ -366,7 +359,7 @@ void ff_h264_idct8_add_8_mmi(uint8_t *dst, int16_t *block, int stride)
         PTR_ADDIU  "%[addr0],   %[dst],         0x04                    \n\t"
         "mov.d      %[ftmp7],   %[ftmp10]                               \n\t"
         "dmtc1      %[tmp3],    %[ftmp6]                                \n\t"
-        MMI_LDC1(%[ftmp1], $29, 0x10)
+        MMI_LDC1(%[ftmp1], $sp, 0x10)
         "dmtc1      %[tmp1],    %[ftmp3]                                \n\t"
         "mov.d      %[ftmp4],   %[ftmp1]                                \n\t"
         "psrah      %[ftmp1],   %[ftmp1],       %[ftmp8]                \n\t"
@@ -399,7 +392,7 @@ void ff_h264_idct8_add_8_mmi(uint8_t *dst, int16_t *block, int stride)
         "psrah      %[ftmp0],   %[ftmp3],       %[ftmp8]                \n\t"
         "paddh      %[ftmp2],   %[ftmp2],       %[ftmp3]                \n\t"
         "psubh      %[ftmp0],   %[ftmp0],       %[ftmp7]                \n\t"
-        MMI_LDC1(%[ftmp3], $29, 0x00)
+        MMI_LDC1(%[ftmp3], $sp, 0x00)
         "dmtc1      %[tmp5],    %[ftmp7]                                \n\t"
         "paddh      %[ftmp7],   %[ftmp7],       %[ftmp3]                \n\t"
         "paddh      %[ftmp3],   %[ftmp3],       %[ftmp3]                \n\t"
@@ -421,11 +414,11 @@ void ff_h264_idct8_add_8_mmi(uint8_t *dst, int16_t *block, int stride)
         "paddh      %[ftmp1],   %[ftmp1],       %[ftmp7]                \n\t"
         "psubh      %[ftmp3],   %[ftmp3],       %[ftmp6]                \n\t"
         "paddh      %[ftmp7],   %[ftmp7],       %[ftmp7]                \n\t"
-        MMI_SDC1(%[ftmp3], $29, 0x00)
+        MMI_SDC1(%[ftmp3], $sp, 0x00)
         "psubh      %[ftmp7],   %[ftmp7],       %[ftmp1]                \n\t"
-        MMI_SDC1(%[ftmp0], $29, 0x10)
+        MMI_SDC1(%[ftmp0], $sp, 0x10)
         "dmfc1      %[tmp1],    %[ftmp2]                                \n\t"
-        "xor        %[ftmp2],   %[ftmp2],       %[ftmp2]                \n\t"
+        "pxor       %[ftmp2],   %[ftmp2],       %[ftmp2]                \n\t"
         MMI_SDC1(%[ftmp2], %[block], 0x00)
         MMI_SDC1(%[ftmp2], %[block], 0x08)
         MMI_SDC1(%[ftmp2], %[block], 0x10)
@@ -470,8 +463,8 @@ void ff_h264_idct8_add_8_mmi(uint8_t *dst, int16_t *block, int stride)
         "packushb   %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
         MMI_SWC1(%[ftmp3], %[dst], 0x00)
         MMI_SWXC1(%[ftmp0], %[dst], %[stride], 0x00)
-        MMI_LDC1(%[ftmp5], $29, 0x00)
-        MMI_LDC1(%[ftmp4], $29, 0x10)
+        MMI_LDC1(%[ftmp5], $sp, 0x00)
+        MMI_LDC1(%[ftmp4], $sp, 0x10)
         "dmtc1      %[tmp1],    %[ftmp6]                                \n\t"
         PTR_ADDU   "%[dst],     %[dst],         %[stride]               \n\t"
         PTR_ADDU   "%[dst],     %[dst],         %[stride]               \n\t"
@@ -503,7 +496,7 @@ void ff_h264_idct8_add_8_mmi(uint8_t *dst, int16_t *block, int stride)
         MMI_SWXC1(%[ftmp0], %[dst], %[stride], 0x00)
         "dmtc1      %[tmp4],    %[ftmp1]                                \n\t"
         "dmtc1      %[tmp2],    %[ftmp6]                                \n\t"
-        MMI_LDC1(%[ftmp4], $29, 0x18)
+        MMI_LDC1(%[ftmp4], $sp, 0x18)
         "mov.d      %[ftmp5],   %[ftmp4]                                \n\t"
         "psrah      %[ftmp4],   %[ftmp4],       %[ftmp8]                \n\t"
         "psrah      %[ftmp7],   %[ftmp11],      %[ftmp8]                \n\t"
@@ -535,7 +528,7 @@ void ff_h264_idct8_add_8_mmi(uint8_t *dst, int16_t *block, int stride)
         "psrah      %[ftmp7],   %[ftmp6],       %[ftmp8]                \n\t"
         "paddh      %[ftmp0],   %[ftmp0],       %[ftmp6]                \n\t"
         "psubh      %[ftmp7],   %[ftmp7],       %[ftmp3]                \n\t"
-        MMI_LDC1(%[ftmp6], $29, 0x08)
+        MMI_LDC1(%[ftmp6], $sp, 0x08)
         "dmtc1      %[tmp6],    %[ftmp3]                                \n\t"
         "paddh      %[ftmp3],   %[ftmp3],       %[ftmp6]                \n\t"
         "paddh      %[ftmp6],   %[ftmp6],       %[ftmp6]                \n\t"
@@ -557,11 +550,11 @@ void ff_h264_idct8_add_8_mmi(uint8_t *dst, int16_t *block, int stride)
         "paddh      %[ftmp4],   %[ftmp4],       %[ftmp3]                \n\t"
         "psubh      %[ftmp6],   %[ftmp6],       %[ftmp1]                \n\t"
         "paddh      %[ftmp3],   %[ftmp3],       %[ftmp3]                \n\t"
-        MMI_SDC1(%[ftmp6], $29, 0x08)
+        MMI_SDC1(%[ftmp6], $sp, 0x08)
         "psubh      %[ftmp3],   %[ftmp3],       %[ftmp4]                \n\t"
-        MMI_SDC1(%[ftmp7], $29, 0x18)
+        MMI_SDC1(%[ftmp7], $sp, 0x18)
         "dmfc1      %[tmp2],    %[ftmp0]                                \n\t"
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
         MMI_ULWC1(%[ftmp6], %[addr0], 0x00)
         MMI_LWXC1(%[ftmp7], %[addr0], %[stride], 0x00)
         "psrah      %[ftmp2],   %[ftmp2],       %[ftmp10]               \n\t"
@@ -588,8 +581,8 @@ void ff_h264_idct8_add_8_mmi(uint8_t *dst, int16_t *block, int stride)
         "packushb   %[ftmp7],   %[ftmp7],       %[ftmp0]                \n\t"
         MMI_SWC1(%[ftmp6], %[addr0], 0x00)
         MMI_SWXC1(%[ftmp7], %[addr0], %[stride], 0x00)
-        MMI_LDC1(%[ftmp2], $29, 0x08)
-        MMI_LDC1(%[ftmp5], $29, 0x18)
+        MMI_LDC1(%[ftmp2], $sp, 0x08)
+        MMI_LDC1(%[ftmp5], $sp, 0x18)
         PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
         "dmtc1      %[tmp2],    %[ftmp1]                                \n\t"
         PTR_ADDU   "%[addr0],   %[addr0],       %[stride]               \n\t"
@@ -619,18 +612,7 @@ void ff_h264_idct8_add_8_mmi(uint8_t *dst, int16_t *block, int stride)
         "packushb   %[ftmp7],   %[ftmp7],       %[ftmp0]                \n\t"
         MMI_SWC1(%[ftmp6], %[addr0], 0x00)
         MMI_SWXC1(%[ftmp7], %[addr0], %[stride], 0x00)
-        PTR_ADDIU  "$29,        $29,            0x20                    \n\t"
-
-        /* memset(block, 0, 128) */
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
-        "gssqc1     %[ftmp0],   %[ftmp0],       0x00(%[block])          \n\t"
-        "gssqc1     %[ftmp0],   %[ftmp0],       0x10(%[block])          \n\t"
-        "gssqc1     %[ftmp0],   %[ftmp0],       0x20(%[block])          \n\t"
-        "gssqc1     %[ftmp0],   %[ftmp0],       0x30(%[block])          \n\t"
-        "gssqc1     %[ftmp0],   %[ftmp0],       0x40(%[block])          \n\t"
-        "gssqc1     %[ftmp0],   %[ftmp0],       0x50(%[block])          \n\t"
-        "gssqc1     %[ftmp0],   %[ftmp0],       0x60(%[block])          \n\t"
-        "gssqc1     %[ftmp0],   %[ftmp0],       0x70(%[block])          \n\t"
+        PTR_ADDIU  "$sp,        $sp,            0x20                    \n\t"
         : [ftmp0]"=&f"(ftmp[0]),            [ftmp1]"=&f"(ftmp[1]),
           [ftmp2]"=&f"(ftmp[2]),            [ftmp3]"=&f"(ftmp[3]),
           [ftmp4]"=&f"(ftmp[4]),            [ftmp5]"=&f"(ftmp[5]),
@@ -648,7 +630,7 @@ void ff_h264_idct8_add_8_mmi(uint8_t *dst, int16_t *block, int stride)
           [addr0]"=&r"(addr[0])
         : [dst]"r"(dst),                    [block]"r"(block),
           [stride]"r"((mips_reg)stride)
-        : "$29","memory"
+        : "memory"
     );
 
 }
@@ -663,7 +645,7 @@ void ff_h264_idct_dc_add_8_mmi(uint8_t *dst, int16_t *block, int stride)
 
     __asm__ volatile (
         "mtc1       %[dc],      %[ftmp5]                                \n\t"
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
         "pshufh     %[ftmp5],   %[ftmp5],       %[ftmp0]                \n\t"
         MMI_ULWC1(%[ftmp1], %[dst0], 0x00)
         MMI_ULWC1(%[ftmp2], %[dst1], 0x00)
@@ -707,7 +689,7 @@ void ff_h264_idct8_dc_add_8_mmi(uint8_t *dst, int16_t *block, int stride)
 
     __asm__ volatile (
         "mtc1       %[dc],      %[ftmp5]                                \n\t"
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
         "pshufh     %[ftmp5],   %[ftmp5],       %[ftmp0]                \n\t"
         MMI_LDC1(%[ftmp1], %[dst0], 0x00)
         MMI_LDC1(%[ftmp2], %[dst1], 0x00)
@@ -946,7 +928,7 @@ void ff_h264_luma_dc_dequant_idct_8_mmi(int16_t *output, int16_t *input,
         "packsswh   %[ftmp0],   %[ftmp0],       %[ftmp1]                \n\t"
         "packsswh   %[ftmp2],   %[ftmp2],       %[ftmp5]                \n\t"
         "dmfc1      %[tmp1],    %[ftmp0]                                \n\t"
-        "dsrl       %[ftmp0],   %[ftmp0],       %[ftmp9]                \n\t"
+        "ssrld      %[ftmp0],   %[ftmp0],       %[ftmp9]                \n\t"
         "mfc1       %[input],   %[ftmp0]                                \n\t"
         "sh         %[tmp1],    0x00(%[output])                         \n\t"
         "sh         %[input],   0x80(%[output])                         \n\t"
@@ -955,7 +937,7 @@ void ff_h264_luma_dc_dequant_idct_8_mmi(int16_t *output, int16_t *input,
         "sh         %[tmp1],    0x20(%[output])                         \n\t"
         "sh         %[input],   0xa0(%[output])                         \n\t"
         "dmfc1      %[tmp1],    %[ftmp2]                                \n\t"
-        "dsrl       %[ftmp2],   %[ftmp2],       %[ftmp9]                \n\t"
+        "ssrld      %[ftmp2],   %[ftmp2],       %[ftmp9]                \n\t"
         "mfc1       %[input],   %[ftmp2]                                \n\t"
         "sh         %[tmp1],    0x40(%[output])                         \n\t"
         "sh         %[input],   0xc0(%[output])                         \n\t"
@@ -980,7 +962,7 @@ void ff_h264_luma_dc_dequant_idct_8_mmi(int16_t *output, int16_t *input,
         "packsswh   %[ftmp3],   %[ftmp3],       %[ftmp1]                \n\t"
         "packsswh   %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
         "dmfc1      %[tmp1],    %[ftmp3]                                \n\t"
-        "dsrl       %[ftmp3],   %[ftmp3],       %[ftmp9]                \n\t"
+        "ssrld      %[ftmp3],   %[ftmp3],       %[ftmp9]                \n\t"
         "mfc1       %[input],   %[ftmp3]                                \n\t"
         "sh         %[tmp1],    0x100(%[output])                        \n\t"
         "sh         %[input],   0x180(%[output])                        \n\t"
@@ -989,7 +971,7 @@ void ff_h264_luma_dc_dequant_idct_8_mmi(int16_t *output, int16_t *input,
         "sh         %[tmp1],    0x120(%[output])                        \n\t"
         "sh         %[input],   0x1a0(%[output])                        \n\t"
         "dmfc1      %[tmp1],    %[ftmp4]                                \n\t"
-        "dsrl       %[ftmp4],   %[ftmp4],       %[ftmp9]                \n\t"
+        "ssrld      %[ftmp4],   %[ftmp4],       %[ftmp9]                \n\t"
         "mfc1       %[input],   %[ftmp4]                                \n\t"
         "sh         %[tmp1],    0x140(%[output])                        \n\t"
         "sh         %[input],   0x1c0(%[output])                        \n\t"
@@ -1033,7 +1015,7 @@ void ff_h264_luma_dc_dequant_idct_8_mmi(int16_t *output, int16_t *input,
         "packsswh   %[ftmp0],   %[ftmp0],       %[ftmp1]                \n\t"
         "packsswh   %[ftmp2],   %[ftmp2],       %[ftmp5]                \n\t"
         "dmfc1      %[tmp1],    %[ftmp0]                                \n\t"
-        "dsrl       %[ftmp0],   %[ftmp0],       %[ftmp9]                \n\t"
+        "ssrld      %[ftmp0],   %[ftmp0],       %[ftmp9]                \n\t"
         "sh         %[tmp1],    0x00(%[output])                         \n\t"
         "mfc1       %[input],   %[ftmp0]                                \n\t"
         "dsrl       %[tmp1],    %[tmp1],        0x10                    \n\t"
@@ -1042,7 +1024,7 @@ void ff_h264_luma_dc_dequant_idct_8_mmi(int16_t *output, int16_t *input,
         PTR_SRL    "%[input],   %[input],       0x10                    \n\t"
         "dmfc1      %[tmp1],    %[ftmp2]                                \n\t"
         "sh         %[input],   0xa0(%[output])                         \n\t"
-        "dsrl       %[ftmp2],   %[ftmp2],       %[ftmp9]                \n\t"
+        "ssrld      %[ftmp2],   %[ftmp2],       %[ftmp9]                \n\t"
         "sh         %[tmp1],    0x40(%[output])                         \n\t"
         "mfc1       %[input],   %[ftmp2]                                \n\t"
         "dsrl       %[tmp1],    %[tmp1],        0x10                    \n\t"
@@ -1067,7 +1049,7 @@ void ff_h264_luma_dc_dequant_idct_8_mmi(int16_t *output, int16_t *input,
         "packsswh   %[ftmp3],   %[ftmp3],       %[ftmp1]                \n\t"
         "packsswh   %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
         "dmfc1      %[tmp1],    %[ftmp3]                                \n\t"
-        "dsrl       %[ftmp3],   %[ftmp3],       %[ftmp9]                \n\t"
+        "ssrld      %[ftmp3],   %[ftmp3],       %[ftmp9]                \n\t"
         "mfc1       %[input],   %[ftmp3]                                \n\t"
         "sh         %[tmp1],    0x100(%[output])                        \n\t"
         "sh         %[input],   0x180(%[output])                        \n\t"
@@ -1076,7 +1058,7 @@ void ff_h264_luma_dc_dequant_idct_8_mmi(int16_t *output, int16_t *input,
         "sh         %[tmp1],    0x120(%[output])                        \n\t"
         "sh         %[input],   0x1a0(%[output])                        \n\t"
         "dmfc1      %[tmp1],    %[ftmp4]                                \n\t"
-        "dsrl       %[ftmp4],   %[ftmp4],       %[ftmp9]                \n\t"
+        "ssrld      %[ftmp4],   %[ftmp4],       %[ftmp9]                \n\t"
         "mfc1       %[input],   %[ftmp4]                                \n\t"
         "sh         %[tmp1],    0x140(%[output])                        \n\t"
         "sh         %[input],   0x1c0(%[output])                        \n\t"
@@ -1095,7 +1077,7 @@ void ff_h264_luma_dc_dequant_idct_8_mmi(int16_t *output, int16_t *input,
           RESTRICT_ASM_ALL64
           [output]"+&r"(output),            [input]"+&r"(input),
           [qmul]"+&r"(qmul)
-        : [ff_pw_1]"f"(ff_pw_1)
+        : [ff_pw_1]"f"(ff_pw_1.f)
         : "memory"
     );
 }
@@ -1161,7 +1143,7 @@ void ff_h264_weight_pixels16_8_mmi(uint8_t *block, ptrdiff_t stride, int height,
 
     for (y=0; y<height; y++, block+=stride) {
         __asm__ volatile (
-            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+            "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
             MMI_LDC1(%[ftmp1], %[block0], 0x00)
             MMI_LDC1(%[ftmp2], %[block1], 0x00)
             "mtc1       %[weight],  %[ftmp3]                            \n\t"
@@ -1215,7 +1197,7 @@ void ff_h264_biweight_pixels16_8_mmi(uint8_t *dst, uint8_t *src,
 
     for (y=0; y<height; y++, dst+=stride, src+=stride) {
         __asm__ volatile (
-            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+            "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
             MMI_LDC1(%[ftmp1], %[src0], 0x00)
             MMI_LDC1(%[ftmp2], %[dst0], 0x00)
             "mtc1       %[weights], %[ftmp3]                            \n\t"
@@ -1288,7 +1270,7 @@ void ff_h264_weight_pixels8_8_mmi(uint8_t *block, ptrdiff_t stride, int height,
 
     for (y=0; y<height; y++, block+=stride) {
         __asm__ volatile (
-            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+            "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
             MMI_LDC1(%[ftmp1], %[block], 0x00)
             "mtc1       %[weight],  %[ftmp2]                            \n\t"
             "mtc1       %[offset],  %[ftmp3]                            \n\t"
@@ -1329,7 +1311,7 @@ void ff_h264_biweight_pixels8_8_mmi(uint8_t *dst, uint8_t *src,
 
     for (y=0; y<height; y++, dst+=stride, src+=stride) {
         __asm__ volatile (
-            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+            "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
             MMI_LDC1(%[ftmp1], %[src], 0x00)
             MMI_LDC1(%[ftmp2], %[dst], 0x00)
             "mtc1       %[weights], %[ftmp3]                            \n\t"
@@ -1383,7 +1365,7 @@ void ff_h264_weight_pixels4_8_mmi(uint8_t *block, ptrdiff_t stride, int height,
 
     for (y=0; y<height; y++, block+=stride) {
         __asm__ volatile (
-            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+            "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
             MMI_ULWC1(%[ftmp1], %[block], 0x00)
             "mtc1       %[weight],  %[ftmp2]                            \n\t"
             "mtc1       %[offset],  %[ftmp3]                            \n\t"
@@ -1419,7 +1401,7 @@ void ff_h264_biweight_pixels4_8_mmi(uint8_t *dst, uint8_t *src,
 
     for (y=0; y<height; y++, dst+=stride, src+=stride) {
         __asm__ volatile (
-            "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+            "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
             MMI_ULWC1(%[ftmp1], %[src], 0x00)
             MMI_ULWC1(%[ftmp2], %[dst], 0x00)
             "mtc1       %[weight],  %[ftmp3]                            \n\t"
@@ -1451,7 +1433,7 @@ void ff_h264_biweight_pixels4_8_mmi(uint8_t *dst, uint8_t *src,
     }
 }
 
-void ff_deblock_v8_luma_8_mmi(uint8_t *pix, int stride, int alpha, int beta,
+void ff_deblock_v8_luma_8_mmi(uint8_t *pix, ptrdiff_t stride, int alpha, int beta,
         int8_t *tc0)
 {
     double ftmp[12];
@@ -1462,7 +1444,7 @@ void ff_deblock_v8_luma_8_mmi(uint8_t *pix, int stride, int alpha, int beta,
 
     __asm__ volatile (
         PTR_ADDU   "%[addr0],   %[stride],      %[stride]               \n\t"
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
         PTR_ADDU   "%[addr1],   %[stride],      %[addr0]                \n\t"
         "addi       %[alpha],   %[alpha],       -0x01                   \n\t"
         PTR_SUBU   "%[addr1],   $0,             %[addr1]                \n\t"
@@ -1480,18 +1462,18 @@ void ff_deblock_v8_luma_8_mmi(uint8_t *pix, int stride, int alpha, int beta,
         "packushb   %[ftmp6],   %[ftmp6],       %[ftmp6]                \n\t"
         "psubusb    %[ftmp7],   %[ftmp3],       %[ftmp2]                \n\t"
         "psubusb    %[ftmp8],   %[ftmp2],       %[ftmp3]                \n\t"
-        "or         %[ftmp8],   %[ftmp8],       %[ftmp7]                \n\t"
+        "por        %[ftmp8],   %[ftmp8],       %[ftmp7]                \n\t"
         "psubusb    %[ftmp7],   %[ftmp2],       %[ftmp1]                \n\t"
         "psubusb    %[ftmp8],   %[ftmp8],       %[ftmp5]                \n\t"
         "psubusb    %[ftmp5],   %[ftmp1],       %[ftmp2]                \n\t"
-        "or         %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
+        "por        %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
         "psubusb    %[ftmp7],   %[ftmp3],       %[ftmp4]                \n\t"
         "psubusb    %[ftmp5],   %[ftmp5],       %[ftmp6]                \n\t"
-        "or         %[ftmp8],   %[ftmp8],       %[ftmp5]                \n\t"
+        "por        %[ftmp8],   %[ftmp8],       %[ftmp5]                \n\t"
         "psubusb    %[ftmp5],   %[ftmp4],       %[ftmp3]                \n\t"
-        "or         %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
+        "por        %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
         "psubusb    %[ftmp5],   %[ftmp5],       %[ftmp6]                \n\t"
-        "or         %[ftmp8],   %[ftmp8],       %[ftmp5]                \n\t"
+        "por        %[ftmp8],   %[ftmp8],       %[ftmp5]                \n\t"
         "pcmpeqb    %[ftmp8],   %[ftmp8],       %[ftmp0]                \n\t"
         "pcmpeqb    %[ftmp4],   %[ftmp4],       %[ftmp4]                \n\t"
         MMI_ULWC1(%[ftmp5], %[tc0], 0x00)
@@ -1499,21 +1481,21 @@ void ff_deblock_v8_luma_8_mmi(uint8_t *pix, int stride, int alpha, int beta,
         "punpcklbh  %[ftmp9],   %[ftmp5],       %[ftmp5]                \n\t"
         "pcmpgtb    %[ftmp5],   %[ftmp9],       %[ftmp4]                \n\t"
         MMI_LDC1(%[ftmp4], %[addr1], 0x00)
-        "and        %[ftmp10],  %[ftmp5],       %[ftmp8]                \n\t"
+        "pand       %[ftmp10],  %[ftmp5],       %[ftmp8]                \n\t"
         "psubusb    %[ftmp8],   %[ftmp4],       %[ftmp2]                \n\t"
         "psubusb    %[ftmp7],   %[ftmp2],       %[ftmp4]                \n\t"
         "psubusb    %[ftmp8],   %[ftmp8],       %[ftmp6]                \n\t"
         "psubusb    %[ftmp7],   %[ftmp7],       %[ftmp6]                \n\t"
         "pcmpeqb    %[ftmp7],   %[ftmp7],       %[ftmp8]                \n\t"
-        "and        %[ftmp7],   %[ftmp7],       %[ftmp10]               \n\t"
-        "and        %[ftmp5],   %[ftmp10],      %[ftmp9]                \n\t"
+        "pand       %[ftmp7],   %[ftmp7],       %[ftmp10]               \n\t"
+        "pand       %[ftmp5],   %[ftmp10],      %[ftmp9]                \n\t"
         "psubb      %[ftmp8],   %[ftmp5],       %[ftmp7]                \n\t"
-        "and        %[ftmp7],   %[ftmp7],       %[ftmp5]                \n\t"
+        "pand       %[ftmp7],   %[ftmp7],       %[ftmp5]                \n\t"
         "pavgb      %[ftmp5],   %[ftmp2],       %[ftmp3]                \n\t"
         MMI_LDC1(%[ftmp11], %[addr1], 0x00)
         "pavgb      %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
-        "xor        %[ftmp5],   %[ftmp5],       %[ftmp11]               \n\t"
-        "and        %[ftmp5],   %[ftmp5],       %[ff_pb_1]              \n\t"
+        "pxor       %[ftmp5],   %[ftmp5],       %[ftmp11]               \n\t"
+        "pand       %[ftmp5],   %[ftmp5],       %[ff_pb_1]              \n\t"
         "psubusb    %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
         "psubusb    %[ftmp5],   %[ftmp1],       %[ftmp7]                \n\t"
         "paddusb    %[ftmp7],   %[ftmp7],       %[ftmp1]                \n\t"
@@ -1526,26 +1508,26 @@ void ff_deblock_v8_luma_8_mmi(uint8_t *pix, int stride, int alpha, int beta,
         "psubusb    %[ftmp4],   %[ftmp4],       %[ftmp6]                \n\t"
         "psubusb    %[ftmp7],   %[ftmp7],       %[ftmp6]                \n\t"
         "pcmpeqb    %[ftmp7],   %[ftmp7],       %[ftmp4]                \n\t"
-        "and        %[ftmp7],   %[ftmp7],       %[ftmp10]               \n\t"
+        "pand       %[ftmp7],   %[ftmp7],       %[ftmp10]               \n\t"
         "psubb      %[ftmp8],   %[ftmp8],       %[ftmp7]                \n\t"
-        "and        %[ftmp6],   %[ftmp9],       %[ftmp7]                \n\t"
+        "pand       %[ftmp6],   %[ftmp9],       %[ftmp7]                \n\t"
         MMI_LDXC1(%[ftmp4], %[pix], %[stride], 0x00)
         "pavgb      %[ftmp7],   %[ftmp2],       %[ftmp3]                \n\t"
         MMI_LDXC1(%[ftmp11], %[pix], %[addr0], 0x00)
         "pavgb      %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
-        "xor        %[ftmp7],   %[ftmp7],       %[ftmp11]               \n\t"
-        "and        %[ftmp7],   %[ftmp7],       %[ff_pb_1]              \n\t"
+        "pxor       %[ftmp7],   %[ftmp7],       %[ftmp11]               \n\t"
+        "pand       %[ftmp7],   %[ftmp7],       %[ff_pb_1]              \n\t"
         "psubusb    %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
         "psubusb    %[ftmp7],   %[ftmp4],       %[ftmp6]                \n\t"
         "paddusb    %[ftmp6],   %[ftmp6],       %[ftmp4]                \n\t"
         "pmaxub     %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
         "pminub     %[ftmp5],   %[ftmp5],       %[ftmp6]                \n\t"
         MMI_SDXC1(%[ftmp5], %[pix], %[stride], 0x00)
-        "xor        %[ftmp6],   %[ftmp2],       %[ftmp3]                \n\t"
+        "pxor       %[ftmp6],   %[ftmp2],       %[ftmp3]                \n\t"
         "pcmpeqb    %[ftmp5],   %[ftmp5],       %[ftmp5]                \n\t"
-        "and        %[ftmp6],   %[ftmp6],       %[ff_pb_1]              \n\t"
-        "xor        %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
-        "xor        %[ftmp5],   %[ftmp5],       %[ftmp2]                \n\t"
+        "pand       %[ftmp6],   %[ftmp6],       %[ff_pb_1]              \n\t"
+        "pxor       %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
+        "pxor       %[ftmp5],   %[ftmp5],       %[ftmp2]                \n\t"
         "pavgb      %[ftmp4],   %[ftmp4],       %[ftmp1]                \n\t"
         "pavgb      %[ftmp4],   %[ftmp4],       %[ff_pb_3]              \n\t"
         "pavgb      %[ftmp5],   %[ftmp5],       %[ftmp3]                \n\t"
@@ -1573,13 +1555,13 @@ void ff_deblock_v8_luma_8_mmi(uint8_t *pix, int stride, int alpha, int beta,
           [addr0]"=&r"(addr[0]),            [addr1]"=&r"(addr[1])
         : [pix]"r"(pix),                    [stride]"r"((mips_reg)stride),
           [alpha]"r"((mips_reg)alpha),      [beta]"r"((mips_reg)beta),
-          [tc0]"r"(tc0),                    [ff_pb_1]"f"(ff_pb_1),
-          [ff_pb_3]"f"(ff_pb_3),            [ff_pb_A1]"f"(ff_pb_A1)
+          [tc0]"r"(tc0),                    [ff_pb_1]"f"(ff_pb_1.f),
+          [ff_pb_3]"f"(ff_pb_3.f),          [ff_pb_A1]"f"(ff_pb_A1.f)
         : "memory"
     );
 }
 
-static void deblock_v8_luma_intra_8_mmi(uint8_t *pix, int stride, int alpha,
+static void deblock_v8_luma_intra_8_mmi(uint8_t *pix, ptrdiff_t stride, int alpha,
         int beta)
 {
     DECLARE_ALIGNED(8, const uint64_t, stack[0x0a]);
@@ -1591,12 +1573,12 @@ static void deblock_v8_luma_intra_8_mmi(uint8_t *pix, int stride, int alpha,
 
     __asm__ volatile (
         "ori        %[tmp0],    $0,             0x01                    \n\t"
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
         "mtc1       %[tmp0],    %[ftmp9]                                \n\t"
         PTR_SLL    "%[addr0],   %[stride],      0x02                    \n\t"
         PTR_ADDU   "%[addr2],   %[stride],      %[stride]               \n\t"
         PTR_ADDIU  "%[alpha],   %[alpha],       -0x01                   \n\t"
-        PTR_SLL    "%[ftmp11],  %[ftmp9],       %[ftmp9]                \n\t"
+        "sslld      %[ftmp11],  %[ftmp9],       %[ftmp9]                \n\t"
         "bltz       %[alpha],   1f                                      \n\t"
         PTR_ADDU   "%[addr1],   %[addr2],       %[stride]               \n\t"
         PTR_ADDIU  "%[beta],    %[beta],        -0x01                   \n\t"
@@ -1615,20 +1597,20 @@ static void deblock_v8_luma_intra_8_mmi(uint8_t *pix, int stride, int alpha,
         "psubusb    %[ftmp7],   %[ftmp3],       %[ftmp2]                \n\t"
         "psubusb    %[ftmp8],   %[ftmp2],       %[ftmp3]                \n\t"
         "packushb   %[ftmp6],   %[ftmp6],       %[ftmp6]                \n\t"
-        "or         %[ftmp8],   %[ftmp8],       %[ftmp7]                \n\t"
+        "por        %[ftmp8],   %[ftmp8],       %[ftmp7]                \n\t"
         MMI_SDC1(%[ftmp5], %[stack], 0x10)
         "psubusb    %[ftmp8],   %[ftmp8],       %[ftmp5]                \n\t"
         "psubusb    %[ftmp7],   %[ftmp2],       %[ftmp1]                \n\t"
         "psubusb    %[ftmp5],   %[ftmp1],       %[ftmp2]                \n\t"
-        "or         %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
+        "por        %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
         "psubusb    %[ftmp5],   %[ftmp5],       %[ftmp6]                \n\t"
-        "or         %[ftmp8],   %[ftmp8],       %[ftmp5]                \n\t"
+        "por        %[ftmp8],   %[ftmp8],       %[ftmp5]                \n\t"
         "psubusb    %[ftmp7],   %[ftmp3],       %[ftmp4]                \n\t"
         "psubusb    %[ftmp5],   %[ftmp4],       %[ftmp3]                \n\t"
-        "or         %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
+        "por        %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
         "psubusb    %[ftmp5],   %[ftmp5],       %[ftmp6]                \n\t"
-        "or         %[ftmp8],   %[ftmp8],       %[ftmp5]                \n\t"
-        "xor        %[ftmp7],   %[ftmp7],       %[ftmp7]                \n\t"
+        "por        %[ftmp8],   %[ftmp8],       %[ftmp5]                \n\t"
+        "pxor       %[ftmp7],   %[ftmp7],       %[ftmp7]                \n\t"
         MMI_LDC1(%[ftmp5], %[stack], 0x10)
         "pcmpeqb    %[ftmp8],   %[ftmp8],       %[ftmp7]                \n\t"
         "ldc1       %[ftmp10],  %[ff_pb_1]                              \n\t"
@@ -1641,14 +1623,14 @@ static void deblock_v8_luma_intra_8_mmi(uint8_t *pix, int stride, int alpha,
         "psubusb    %[ftmp7],   %[ftmp7],       %[ftmp5]                \n\t"
         MMI_LDC1(%[ftmp15], %[stack], 0x20)
         "pcmpeqb    %[ftmp7],   %[ftmp7],       %[ftmp8]                \n\t"
-        "and        %[ftmp7],   %[ftmp7],       %[ftmp15]               \n\t"
+        "pand       %[ftmp7],   %[ftmp7],       %[ftmp15]               \n\t"
         MMI_LDXC1(%[ftmp15], %[addr0], %[stride], 0x00)
         "psubusb    %[ftmp8],   %[ftmp15],      %[ftmp2]                \n\t"
         "psubusb    %[ftmp5],   %[ftmp2],       %[ftmp15]               \n\t"
         "psubusb    %[ftmp8],   %[ftmp8],       %[ftmp6]                \n\t"
         "psubusb    %[ftmp5],   %[ftmp5],       %[ftmp6]                \n\t"
         "pcmpeqb    %[ftmp5],   %[ftmp5],       %[ftmp8]                \n\t"
-        "and        %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
+        "pand       %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
         MMI_LDXC1(%[ftmp14], %[pix], %[addr2], 0x00)
         MMI_SDC1(%[ftmp5], %[stack], 0x30)
         "psubusb    %[ftmp8],   %[ftmp14],      %[ftmp3]                \n\t"
@@ -1656,7 +1638,7 @@ static void deblock_v8_luma_intra_8_mmi(uint8_t *pix, int stride, int alpha,
         "psubusb    %[ftmp8],   %[ftmp8],       %[ftmp6]                \n\t"
         "psubusb    %[ftmp5],   %[ftmp5],       %[ftmp6]                \n\t"
         "pcmpeqb    %[ftmp5],   %[ftmp5],       %[ftmp8]                \n\t"
-        "and        %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
+        "pand       %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
         MMI_SDC1(%[ftmp5], %[stack], 0x40)
         "pavgb      %[ftmp5],   %[ftmp15],      %[ftmp1]                \n\t"
         "pavgb      %[ftmp6],   %[ftmp2],       %[ftmp3]                \n\t"
@@ -1669,36 +1651,36 @@ static void deblock_v8_luma_intra_8_mmi(uint8_t *pix, int stride, int alpha,
         MMI_SDC1(%[ftmp7], %[stack], 0x00)
         "psrlh      %[ftmp7],   %[ftmp7],       %[ftmp9]                \n\t"
         "pavgb      %[ftmp7],   %[ftmp7],       %[ftmp0]                \n\t"
-        "xor        %[ftmp7],   %[ftmp7],       %[ftmp5]                \n\t"
-        "and        %[ftmp7],   %[ftmp7],       %[ftmp10]               \n\t"
+        "pxor       %[ftmp7],   %[ftmp7],       %[ftmp5]                \n\t"
+        "pand       %[ftmp7],   %[ftmp7],       %[ftmp10]               \n\t"
         "psubb      %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
         "pavgb      %[ftmp6],   %[ftmp15],      %[ftmp4]                \n\t"
         "psubb      %[ftmp7],   %[ftmp15],      %[ftmp4]                \n\t"
         "paddb      %[ftmp8],   %[ftmp8],       %[ftmp8]                \n\t"
         "psubb      %[ftmp8],   %[ftmp8],       %[ftmp7]                \n\t"
-        "and        %[ftmp7],   %[ftmp7],       %[ftmp10]               \n\t"
+        "pand       %[ftmp7],   %[ftmp7],       %[ftmp10]               \n\t"
         "psubb      %[ftmp6],   %[ftmp6],       %[ftmp7]                \n\t"
         MMI_LDC1(%[ftmp13], %[stack], 0x10)
         "pavgb      %[ftmp6],   %[ftmp6],       %[ftmp1]                \n\t"
         "psrlh      %[ftmp8],   %[ftmp8],       %[ftmp11]               \n\t"
         "pavgb      %[ftmp6],   %[ftmp6],       %[ftmp13]               \n\t"
         "pavgb      %[ftmp8],   %[ftmp8],       %[ftmp0]                \n\t"
-        "xor        %[ftmp8],   %[ftmp8],       %[ftmp6]                \n\t"
-        "and        %[ftmp8],   %[ftmp8],       %[ftmp10]               \n\t"
+        "pxor       %[ftmp8],   %[ftmp8],       %[ftmp6]                \n\t"
+        "pand       %[ftmp8],   %[ftmp8],       %[ftmp10]               \n\t"
         "psubb      %[ftmp6],   %[ftmp6],       %[ftmp8]                \n\t"
-        "xor        %[ftmp8],   %[ftmp2],       %[ftmp4]                \n\t"
+        "pxor       %[ftmp8],   %[ftmp2],       %[ftmp4]                \n\t"
         "pavgb      %[ftmp7],   %[ftmp2],       %[ftmp4]                \n\t"
-        "and        %[ftmp8],   %[ftmp8],       %[ftmp10]               \n\t"
+        "pand       %[ftmp8],   %[ftmp8],       %[ftmp10]               \n\t"
         "psubb      %[ftmp7],   %[ftmp7],       %[ftmp8]                \n\t"
         MMI_LDC1(%[ftmp13], %[stack], 0x30)
         "pavgb      %[ftmp7],   %[ftmp7],       %[ftmp1]                \n\t"
         MMI_LDC1(%[ftmp12], %[stack], 0x20)
-        "xor        %[ftmp6],   %[ftmp6],       %[ftmp7]                \n\t"
-        "xor        %[ftmp7],   %[ftmp7],       %[ftmp2]                \n\t"
-        "and        %[ftmp6],   %[ftmp6],       %[ftmp13]               \n\t"
-        "and        %[ftmp7],   %[ftmp7],       %[ftmp12]               \n\t"
-        "xor        %[ftmp6],   %[ftmp6],       %[ftmp7]                \n\t"
-        "xor        %[ftmp6],   %[ftmp6],       %[ftmp2]                \n\t"
+        "pxor       %[ftmp6],   %[ftmp6],       %[ftmp7]                \n\t"
+        "pxor       %[ftmp7],   %[ftmp7],       %[ftmp2]                \n\t"
+        "pand       %[ftmp6],   %[ftmp6],       %[ftmp13]               \n\t"
+        "pand       %[ftmp7],   %[ftmp7],       %[ftmp12]               \n\t"
+        "pxor       %[ftmp6],   %[ftmp6],       %[ftmp7]                \n\t"
+        "pxor       %[ftmp6],   %[ftmp6],       %[ftmp2]                \n\t"
         MMI_SDXC1(%[ftmp6], %[addr0], %[addr1], 0x00)
         MMI_LDC1(%[ftmp6], %[addr0], 0x00)
         "paddb      %[ftmp7],   %[ftmp15],      %[ftmp6]                \n\t"
@@ -1709,16 +1691,16 @@ static void deblock_v8_luma_intra_8_mmi(uint8_t *pix, int stride, int alpha,
         "paddb      %[ftmp7],   %[ftmp7],       %[ftmp12]               \n\t"
         "psrlh      %[ftmp7],   %[ftmp7],       %[ftmp11]               \n\t"
         "pavgb      %[ftmp7],   %[ftmp7],       %[ftmp0]                \n\t"
-        "xor        %[ftmp7],   %[ftmp7],       %[ftmp6]                \n\t"
-        "and        %[ftmp7],   %[ftmp7],       %[ftmp10]               \n\t"
+        "pxor       %[ftmp7],   %[ftmp7],       %[ftmp6]                \n\t"
+        "pand       %[ftmp7],   %[ftmp7],       %[ftmp10]               \n\t"
         MMI_LDC1(%[ftmp12], %[stack], 0x30)
         "psubb      %[ftmp6],   %[ftmp6],       %[ftmp7]                \n\t"
-        "xor        %[ftmp5],   %[ftmp5],       %[ftmp1]                \n\t"
-        "xor        %[ftmp6],   %[ftmp6],       %[ftmp15]               \n\t"
-        "and        %[ftmp5],   %[ftmp5],       %[ftmp12]               \n\t"
-        "and        %[ftmp6],   %[ftmp6],       %[ftmp12]               \n\t"
-        "xor        %[ftmp5],   %[ftmp5],       %[ftmp1]                \n\t"
-        "xor        %[ftmp6],   %[ftmp6],       %[ftmp15]               \n\t"
+        "pxor       %[ftmp5],   %[ftmp5],       %[ftmp1]                \n\t"
+        "pxor       %[ftmp6],   %[ftmp6],       %[ftmp15]               \n\t"
+        "pand       %[ftmp5],   %[ftmp5],       %[ftmp12]               \n\t"
+        "pand       %[ftmp6],   %[ftmp6],       %[ftmp12]               \n\t"
+        "pxor       %[ftmp5],   %[ftmp5],       %[ftmp1]                \n\t"
+        "pxor       %[ftmp6],   %[ftmp6],       %[ftmp15]               \n\t"
         MMI_SDXC1(%[ftmp5], %[addr0], %[addr2], 0x00)
         MMI_SDXC1(%[ftmp6], %[addr0], %[stride], 0x00)
         "pavgb      %[ftmp5],   %[ftmp14],      %[ftmp4]                \n\t"
@@ -1732,36 +1714,36 @@ static void deblock_v8_luma_intra_8_mmi(uint8_t *pix, int stride, int alpha,
         MMI_SDC1(%[ftmp7], %[stack], 0x00)
         "psrlh      %[ftmp7],   %[ftmp7],       %[ftmp9]                \n\t"
         "pavgb      %[ftmp7],   %[ftmp7],       %[ftmp0]                \n\t"
-        "xor        %[ftmp7],   %[ftmp7],       %[ftmp5]                \n\t"
-        "and        %[ftmp7],   %[ftmp7],       %[ftmp10]               \n\t"
+        "pxor       %[ftmp7],   %[ftmp7],       %[ftmp5]                \n\t"
+        "pand       %[ftmp7],   %[ftmp7],       %[ftmp10]               \n\t"
         "psubb      %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
         "pavgb      %[ftmp6],   %[ftmp14],      %[ftmp1]                \n\t"
         "paddb      %[ftmp8],   %[ftmp8],       %[ftmp8]                \n\t"
         "psubb      %[ftmp7],   %[ftmp14],      %[ftmp1]                \n\t"
         "psubb      %[ftmp8],   %[ftmp8],       %[ftmp7]                \n\t"
-        "and        %[ftmp7],   %[ftmp7],       %[ftmp10]               \n\t"
+        "pand       %[ftmp7],   %[ftmp7],       %[ftmp10]               \n\t"
         "psubb      %[ftmp6],   %[ftmp6],       %[ftmp7]                \n\t"
         MMI_LDC1(%[ftmp12], %[stack], 0x10)
         "pavgb      %[ftmp6],   %[ftmp6],       %[ftmp4]                \n\t"
         "pavgb      %[ftmp6],   %[ftmp6],       %[ftmp12]               \n\t"
         "psrlh      %[ftmp8],   %[ftmp8],       %[ftmp11]               \n\t"
         "pavgb      %[ftmp8],   %[ftmp8],       %[ftmp0]                \n\t"
-        "xor        %[ftmp8],   %[ftmp8],       %[ftmp6]                \n\t"
-        "and        %[ftmp8],   %[ftmp8],       %[ftmp10]               \n\t"
+        "pxor       %[ftmp8],   %[ftmp8],       %[ftmp6]                \n\t"
+        "pand       %[ftmp8],   %[ftmp8],       %[ftmp10]               \n\t"
         "psubb      %[ftmp6],   %[ftmp6],       %[ftmp8]                \n\t"
-        "xor        %[ftmp8],   %[ftmp3],       %[ftmp1]                \n\t"
+        "pxor       %[ftmp8],   %[ftmp3],       %[ftmp1]                \n\t"
         "pavgb      %[ftmp7],   %[ftmp3],       %[ftmp1]                \n\t"
-        "and        %[ftmp8],   %[ftmp8],       %[ftmp10]               \n\t"
+        "pand       %[ftmp8],   %[ftmp8],       %[ftmp10]               \n\t"
         MMI_LDC1(%[ftmp12], %[stack], 0x40)
         "psubb      %[ftmp7],   %[ftmp7],       %[ftmp8]                \n\t"
         MMI_LDC1(%[ftmp13], %[stack], 0x20)
         "pavgb      %[ftmp7],   %[ftmp7],       %[ftmp4]                \n\t"
-        "xor        %[ftmp6],   %[ftmp6],       %[ftmp7]                \n\t"
-        "xor        %[ftmp7],   %[ftmp7],       %[ftmp3]                \n\t"
-        "and        %[ftmp6],   %[ftmp6],       %[ftmp12]               \n\t"
-        "and        %[ftmp7],   %[ftmp7],       %[ftmp13]               \n\t"
-        "xor        %[ftmp6],   %[ftmp6],       %[ftmp7]                \n\t"
-        "xor        %[ftmp6],   %[ftmp6],       %[ftmp3]                \n\t"
+        "pxor       %[ftmp6],   %[ftmp6],       %[ftmp7]                \n\t"
+        "pxor       %[ftmp7],   %[ftmp7],       %[ftmp3]                \n\t"
+        "pand       %[ftmp6],   %[ftmp6],       %[ftmp12]               \n\t"
+        "pand       %[ftmp7],   %[ftmp7],       %[ftmp13]               \n\t"
+        "pxor       %[ftmp6],   %[ftmp6],       %[ftmp7]                \n\t"
+        "pxor       %[ftmp6],   %[ftmp6],       %[ftmp3]                \n\t"
         MMI_SDC1(%[ftmp6], %[pix], 0x00)
         MMI_LDXC1(%[ftmp6], %[pix], %[addr1], 0x00)
         "paddb      %[ftmp7],   %[ftmp14],      %[ftmp6]                \n\t"
@@ -1772,16 +1754,16 @@ static void deblock_v8_luma_intra_8_mmi(uint8_t *pix, int stride, int alpha,
         "paddb      %[ftmp7],   %[ftmp7],       %[ftmp12]               \n\t"
         "psrlh      %[ftmp7],   %[ftmp7],       %[ftmp11]               \n\t"
         "pavgb      %[ftmp7],   %[ftmp7],       %[ftmp0]                \n\t"
-        "xor        %[ftmp7],   %[ftmp7],       %[ftmp6]                \n\t"
-        "and        %[ftmp7],   %[ftmp7],       %[ftmp10]               \n\t"
+        "pxor       %[ftmp7],   %[ftmp7],       %[ftmp6]                \n\t"
+        "pand       %[ftmp7],   %[ftmp7],       %[ftmp10]               \n\t"
         MMI_LDC1(%[ftmp12], %[stack], 0x40)
         "psubb      %[ftmp6],   %[ftmp6],       %[ftmp7]                \n\t"
-        "xor        %[ftmp5],   %[ftmp5],       %[ftmp4]                \n\t"
-        "xor        %[ftmp6],   %[ftmp6],       %[ftmp14]               \n\t"
-        "and        %[ftmp5],   %[ftmp5],       %[ftmp12]               \n\t"
-        "and        %[ftmp6],   %[ftmp6],       %[ftmp12]               \n\t"
-        "xor        %[ftmp5],   %[ftmp5],       %[ftmp4]                \n\t"
-        "xor        %[ftmp6],   %[ftmp6],       %[ftmp14]               \n\t"
+        "pxor       %[ftmp5],   %[ftmp5],       %[ftmp4]                \n\t"
+        "pxor       %[ftmp6],   %[ftmp6],       %[ftmp14]               \n\t"
+        "pand       %[ftmp5],   %[ftmp5],       %[ftmp12]               \n\t"
+        "pand       %[ftmp6],   %[ftmp6],       %[ftmp12]               \n\t"
+        "pxor       %[ftmp5],   %[ftmp5],       %[ftmp4]                \n\t"
+        "pxor       %[ftmp6],   %[ftmp6],       %[ftmp14]               \n\t"
         MMI_SDXC1(%[ftmp5], %[pix], %[stride], 0x00)
         MMI_SDXC1(%[ftmp6], %[pix], %[addr2], 0x00)
         "1:                                                             \n\t"
@@ -1825,7 +1807,7 @@ void ff_deblock_v_chroma_8_mmi(uint8_t *pix, ptrdiff_t stride, int alpha,
         MMI_LDC1(%[ftmp3], %[pix], 0x00)
         MMI_LDXC1(%[ftmp4], %[pix], %[stride], 0x00)
 
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
         "mtc1       %[alpha],   %[ftmp5]                                \n\t"
         "mtc1       %[beta],    %[ftmp6]                                \n\t"
         "pshufh     %[ftmp5],   %[ftmp5],       %[ftmp0]                \n\t"
@@ -1834,29 +1816,29 @@ void ff_deblock_v_chroma_8_mmi(uint8_t *pix, ptrdiff_t stride, int alpha,
         "packushb   %[ftmp6],   %[ftmp6],       %[ftmp6]                \n\t"
         "psubusb    %[ftmp7],   %[ftmp3],       %[ftmp2]                \n\t"
         "psubusb    %[ftmp8],   %[ftmp2],       %[ftmp3]                \n\t"
-        "or         %[ftmp8],   %[ftmp8],       %[ftmp7]                \n\t"
+        "por        %[ftmp8],   %[ftmp8],       %[ftmp7]                \n\t"
         "psubusb    %[ftmp8],   %[ftmp8],       %[ftmp5]                \n\t"
         "psubusb    %[ftmp7],   %[ftmp2],       %[ftmp1]                \n\t"
         "psubusb    %[ftmp5],   %[ftmp1],       %[ftmp2]                \n\t"
-        "or         %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
+        "por        %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
         "psubusb    %[ftmp5],   %[ftmp5],       %[ftmp6]                \n\t"
-        "or         %[ftmp8],   %[ftmp8],       %[ftmp5]                \n\t"
+        "por        %[ftmp8],   %[ftmp8],       %[ftmp5]                \n\t"
         "psubusb    %[ftmp7],   %[ftmp3],       %[ftmp4]                \n\t"
         "psubusb    %[ftmp5],   %[ftmp4],       %[ftmp3]                \n\t"
-        "or         %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
+        "por        %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
         "psubusb    %[ftmp5],   %[ftmp5],       %[ftmp6]                \n\t"
-        "or         %[ftmp8],   %[ftmp8],       %[ftmp5]                \n\t"
-        "xor        %[ftmp7],   %[ftmp7],       %[ftmp7]                \n\t"
+        "por        %[ftmp8],   %[ftmp8],       %[ftmp5]                \n\t"
+        "pxor       %[ftmp7],   %[ftmp7],       %[ftmp7]                \n\t"
         "pcmpeqb    %[ftmp8],   %[ftmp8],       %[ftmp7]                \n\t"
         MMI_ULWC1(%[ftmp7], %[tc0], 0x00)
         "punpcklbh  %[ftmp7],   %[ftmp7],       %[ftmp7]                \n\t"
-        "and        %[ftmp8],   %[ftmp8],       %[ftmp7]                \n\t"
+        "pand       %[ftmp8],   %[ftmp8],       %[ftmp7]                \n\t"
         "pcmpeqb    %[ftmp5],   %[ftmp5],       %[ftmp5]                \n\t"
-        "xor        %[ftmp6],   %[ftmp2],       %[ftmp3]                \n\t"
-        "xor        %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
-        "and        %[ftmp6],   %[ftmp6],       %[ff_pb_1]              \n\t"
+        "pxor       %[ftmp6],   %[ftmp2],       %[ftmp3]                \n\t"
+        "pxor       %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
+        "pand       %[ftmp6],   %[ftmp6],       %[ff_pb_1]              \n\t"
         "pavgb      %[ftmp4],   %[ftmp4],       %[ftmp1]                \n\t"
-        "xor        %[ftmp5],   %[ftmp5],       %[ftmp2]                \n\t"
+        "pxor       %[ftmp5],   %[ftmp5],       %[ftmp2]                \n\t"
         "pavgb      %[ftmp4],   %[ftmp4],       %[ff_pb_3]              \n\t"
         "pavgb      %[ftmp5],   %[ftmp5],       %[ftmp3]                \n\t"
         "pavgb      %[ftmp4],   %[ftmp4],       %[ftmp6]                \n\t"
@@ -1883,13 +1865,13 @@ void ff_deblock_v_chroma_8_mmi(uint8_t *pix, ptrdiff_t stride, int alpha,
           [addr0]"=&r"(addr[0])
         : [pix]"r"(pix),                    [stride]"r"((mips_reg)stride),
           [alpha]"r"(alpha),                [beta]"r"(beta),
-          [tc0]"r"(tc0),                    [ff_pb_1]"f"(ff_pb_1),
-          [ff_pb_3]"f"(ff_pb_3),            [ff_pb_A1]"f"(ff_pb_A1)
+          [tc0]"r"(tc0),                    [ff_pb_1]"f"(ff_pb_1.f),
+          [ff_pb_3]"f"(ff_pb_3.f),          [ff_pb_A1]"f"(ff_pb_A1.f)
         : "memory"
     );
 }
 
-void ff_deblock_v_chroma_intra_8_mmi(uint8_t *pix, int stride, int alpha,
+void ff_deblock_v_chroma_intra_8_mmi(uint8_t *pix, ptrdiff_t stride, int alpha,
         int beta)
 {
     double ftmp[9];
@@ -1908,7 +1890,7 @@ void ff_deblock_v_chroma_intra_8_mmi(uint8_t *pix, int stride, int alpha,
         MMI_LDC1(%[ftmp3], %[pix], 0x00)
         MMI_LDXC1(%[ftmp4], %[pix], %[stride], 0x00)
 
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
         "mtc1       %[alpha],   %[ftmp5]                                \n\t"
         "mtc1       %[beta],    %[ftmp6]                                \n\t"
         "pshufh     %[ftmp5],   %[ftmp5],       %[ftmp0]                \n\t"
@@ -1917,36 +1899,36 @@ void ff_deblock_v_chroma_intra_8_mmi(uint8_t *pix, int stride, int alpha,
         "packushb   %[ftmp6],   %[ftmp6],       %[ftmp6]                \n\t"
         "psubusb    %[ftmp7],   %[ftmp3],       %[ftmp2]                \n\t"
         "psubusb    %[ftmp8],   %[ftmp2],       %[ftmp3]                \n\t"
-        "or         %[ftmp8],   %[ftmp8],       %[ftmp7]                \n\t"
+        "por        %[ftmp8],   %[ftmp8],       %[ftmp7]                \n\t"
         "psubusb    %[ftmp8],   %[ftmp8],       %[ftmp5]                \n\t"
         "psubusb    %[ftmp7],   %[ftmp2],       %[ftmp1]                \n\t"
         "psubusb    %[ftmp5],   %[ftmp1],       %[ftmp2]                \n\t"
-        "or         %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
+        "por        %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
         "psubusb    %[ftmp5],   %[ftmp5],       %[ftmp6]                \n\t"
-        "or         %[ftmp8],   %[ftmp8],       %[ftmp5]                \n\t"
+        "por        %[ftmp8],   %[ftmp8],       %[ftmp5]                \n\t"
         "psubusb    %[ftmp7],   %[ftmp3],       %[ftmp4]                \n\t"
         "psubusb    %[ftmp5],   %[ftmp4],       %[ftmp3]                \n\t"
-        "or         %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
+        "por        %[ftmp5],   %[ftmp5],       %[ftmp7]                \n\t"
         "psubusb    %[ftmp5],   %[ftmp5],       %[ftmp6]                \n\t"
-        "or         %[ftmp8],   %[ftmp8],       %[ftmp5]                \n\t"
-        "xor        %[ftmp7],   %[ftmp7],       %[ftmp7]                \n\t"
+        "por        %[ftmp8],   %[ftmp8],       %[ftmp5]                \n\t"
+        "pxor       %[ftmp7],   %[ftmp7],       %[ftmp7]                \n\t"
         "pcmpeqb    %[ftmp8],   %[ftmp8],       %[ftmp7]                \n\t"
         "mov.d      %[ftmp6],   %[ftmp2]                                \n\t"
         "mov.d      %[ftmp7],   %[ftmp3]                                \n\t"
-        "xor        %[ftmp5],   %[ftmp2],       %[ftmp4]                \n\t"
-        "and        %[ftmp5],   %[ftmp5],       %[ff_pb_1]              \n\t"
+        "pxor       %[ftmp5],   %[ftmp2],       %[ftmp4]                \n\t"
+        "pand       %[ftmp5],   %[ftmp5],       %[ff_pb_1]              \n\t"
         "pavgb      %[ftmp2],   %[ftmp2],       %[ftmp4]                \n\t"
         "psubusb    %[ftmp2],   %[ftmp2],       %[ftmp5]                \n\t"
         "pavgb      %[ftmp2],   %[ftmp2],       %[ftmp1]                \n\t"
-        "xor        %[ftmp5],   %[ftmp3],       %[ftmp1]                \n\t"
-        "and        %[ftmp5],   %[ftmp5],       %[ff_pb_1]              \n\t"
+        "pxor       %[ftmp5],   %[ftmp3],       %[ftmp1]                \n\t"
+        "pand       %[ftmp5],   %[ftmp5],       %[ff_pb_1]              \n\t"
         "pavgb      %[ftmp3],   %[ftmp3],       %[ftmp1]                \n\t"
         "psubusb    %[ftmp3],   %[ftmp3],       %[ftmp5]                \n\t"
         "pavgb      %[ftmp3],   %[ftmp3],       %[ftmp4]                \n\t"
         "psubb      %[ftmp2],   %[ftmp2],       %[ftmp6]                \n\t"
         "psubb      %[ftmp3],   %[ftmp3],       %[ftmp7]                \n\t"
-        "and        %[ftmp2],   %[ftmp2],       %[ftmp8]                \n\t"
-        "and        %[ftmp3],   %[ftmp3],       %[ftmp8]                \n\t"
+        "pand       %[ftmp2],   %[ftmp2],       %[ftmp8]                \n\t"
+        "pand       %[ftmp3],   %[ftmp3],       %[ftmp8]                \n\t"
         "paddb      %[ftmp2],   %[ftmp2],       %[ftmp6]                \n\t"
         "paddb      %[ftmp3],   %[ftmp3],       %[ftmp7]                \n\t"
 
@@ -1962,12 +1944,12 @@ void ff_deblock_v_chroma_intra_8_mmi(uint8_t *pix, int stride, int alpha,
           [addr0]"=&r"(addr[0])
         : [pix]"r"(pix),                    [stride]"r"((mips_reg)stride),
           [alpha]"r"(alpha),                [beta]"r"(beta),
-          [ff_pb_1]"f"(ff_pb_1)
+          [ff_pb_1]"f"(ff_pb_1.f)
         : "memory"
     );
 }
 
-void ff_deblock_h_chroma_8_mmi(uint8_t *pix, int stride, int alpha, int beta,
+void ff_deblock_h_chroma_8_mmi(uint8_t *pix, ptrdiff_t stride, int alpha, int beta,
         int8_t *tc0)
 {
     double ftmp[11];
@@ -2013,7 +1995,7 @@ void ff_deblock_h_chroma_8_mmi(uint8_t *pix, int stride, int alpha, int beta,
         "mov.d      %[ftmp9],   %[ftmp0]                                \n\t"
         "mov.d      %[ftmp10],  %[ftmp3]                                \n\t"
 
-        "xor        %[ftmp8],   %[ftmp8],       %[ftmp8]                \n\t"
+        "pxor       %[ftmp8],   %[ftmp8],       %[ftmp8]                \n\t"
         "mtc1       %[alpha],   %[ftmp4]                                \n\t"
         "mtc1       %[beta],    %[ftmp5]                                \n\t"
         "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp8]                \n\t"
@@ -2022,29 +2004,29 @@ void ff_deblock_h_chroma_8_mmi(uint8_t *pix, int stride, int alpha, int beta,
         "packushb   %[ftmp5],   %[ftmp5],       %[ftmp5]                \n\t"
         "psubusb    %[ftmp6],   %[ftmp2],       %[ftmp1]                \n\t"
         "psubusb    %[ftmp7],   %[ftmp1],       %[ftmp2]                \n\t"
-        "or         %[ftmp7],   %[ftmp7],       %[ftmp6]                \n\t"
+        "por        %[ftmp7],   %[ftmp7],       %[ftmp6]                \n\t"
         "psubusb    %[ftmp7],   %[ftmp7],       %[ftmp4]                \n\t"
         "psubusb    %[ftmp6],   %[ftmp1],       %[ftmp0]                \n\t"
         "psubusb    %[ftmp4],   %[ftmp0],       %[ftmp1]                \n\t"
-        "or         %[ftmp4],   %[ftmp4],       %[ftmp6]                \n\t"
+        "por        %[ftmp4],   %[ftmp4],       %[ftmp6]                \n\t"
         "psubusb    %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
-        "or         %[ftmp7],   %[ftmp7],       %[ftmp4]                \n\t"
+        "por        %[ftmp7],   %[ftmp7],       %[ftmp4]                \n\t"
         "psubusb    %[ftmp6],   %[ftmp2],       %[ftmp3]                \n\t"
         "psubusb    %[ftmp4],   %[ftmp3],       %[ftmp2]                \n\t"
-        "or         %[ftmp4],   %[ftmp4],       %[ftmp6]                \n\t"
+        "por        %[ftmp4],   %[ftmp4],       %[ftmp6]                \n\t"
         "psubusb    %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
-        "or         %[ftmp7],   %[ftmp7],       %[ftmp4]                \n\t"
-        "xor        %[ftmp6],   %[ftmp6],       %[ftmp6]                \n\t"
+        "por        %[ftmp7],   %[ftmp7],       %[ftmp4]                \n\t"
+        "pxor       %[ftmp6],   %[ftmp6],       %[ftmp6]                \n\t"
         "pcmpeqb    %[ftmp7],   %[ftmp7],       %[ftmp6]                \n\t"
         MMI_ULWC1(%[ftmp6], %[tc0], 0x00)
         "punpcklbh  %[ftmp6],   %[ftmp6],       %[ftmp6]                \n\t"
-        "and        %[ftmp7],   %[ftmp7],       %[ftmp6]                \n\t"
+        "pand       %[ftmp7],   %[ftmp7],       %[ftmp6]                \n\t"
         "pcmpeqb    %[ftmp4],   %[ftmp4],       %[ftmp4]                \n\t"
-        "xor        %[ftmp5],   %[ftmp1],       %[ftmp2]                \n\t"
-        "xor        %[ftmp3],   %[ftmp3],       %[ftmp4]                \n\t"
-        "and        %[ftmp5],   %[ftmp5],       %[ff_pb_1]              \n\t"
+        "pxor       %[ftmp5],   %[ftmp1],       %[ftmp2]                \n\t"
+        "pxor       %[ftmp3],   %[ftmp3],       %[ftmp4]                \n\t"
+        "pand       %[ftmp5],   %[ftmp5],       %[ff_pb_1]              \n\t"
         "pavgb      %[ftmp3],   %[ftmp3],       %[ftmp0]                \n\t"
-        "xor        %[ftmp4],   %[ftmp4],       %[ftmp1]                \n\t"
+        "pxor       %[ftmp4],   %[ftmp4],       %[ftmp1]                \n\t"
         "pavgb      %[ftmp3],   %[ftmp3],       %[ff_pb_3]              \n\t"
         "pavgb      %[ftmp4],   %[ftmp4],       %[ftmp2]                \n\t"
         "pavgb      %[ftmp3],   %[ftmp3],       %[ftmp5]                \n\t"
@@ -2101,13 +2083,13 @@ void ff_deblock_h_chroma_8_mmi(uint8_t *pix, int stride, int alpha, int beta,
           [pix]"+&r"(pix)
         : [alpha]"r"(alpha),                [beta]"r"(beta),
           [stride]"r"((mips_reg)stride),    [tc0]"r"(tc0),
-          [ff_pb_1]"f"(ff_pb_1),            [ff_pb_3]"f"(ff_pb_3),
-          [ff_pb_A1]"f"(ff_pb_A1)
+          [ff_pb_1]"f"(ff_pb_1.f),          [ff_pb_3]"f"(ff_pb_3.f),
+          [ff_pb_A1]"f"(ff_pb_A1.f)
         : "memory"
     );
 }
 
-void ff_deblock_h_chroma_intra_8_mmi(uint8_t *pix, int stride, int alpha,
+void ff_deblock_h_chroma_intra_8_mmi(uint8_t *pix, ptrdiff_t stride, int alpha,
         int beta)
 {
     double ftmp[11];
@@ -2151,7 +2133,7 @@ void ff_deblock_h_chroma_intra_8_mmi(uint8_t *pix, int stride, int alpha,
         "punpcklwd  %[ftmp0],   %[ftmp0],       %[ftmp4]                \n\t"
         "punpcklwd  %[ftmp2],   %[ftmp2],       %[ftmp6]                \n\t"
 
-        "xor        %[ftmp8],   %[ftmp8],       %[ftmp8]                \n\t"
+        "pxor       %[ftmp8],   %[ftmp8],       %[ftmp8]                \n\t"
         "mtc1       %[alpha],   %[ftmp4]                                \n\t"
         "mtc1       %[beta],    %[ftmp5]                                \n\t"
         "pshufh     %[ftmp4],   %[ftmp4],       %[ftmp8]                \n\t"
@@ -2160,36 +2142,36 @@ void ff_deblock_h_chroma_intra_8_mmi(uint8_t *pix, int stride, int alpha,
         "packushb   %[ftmp5],   %[ftmp5],       %[ftmp5]                \n\t"
         "psubusb    %[ftmp6],   %[ftmp2],       %[ftmp1]                \n\t"
         "psubusb    %[ftmp7],   %[ftmp1],       %[ftmp2]                \n\t"
-        "or         %[ftmp7],   %[ftmp7],       %[ftmp6]                \n\t"
+        "por        %[ftmp7],   %[ftmp7],       %[ftmp6]                \n\t"
         "psubusb    %[ftmp7],   %[ftmp7],       %[ftmp4]                \n\t"
         "psubusb    %[ftmp6],   %[ftmp1],       %[ftmp0]                \n\t"
         "psubusb    %[ftmp4],   %[ftmp0],       %[ftmp1]                \n\t"
-        "or         %[ftmp4],   %[ftmp4],       %[ftmp6]                \n\t"
+        "por        %[ftmp4],   %[ftmp4],       %[ftmp6]                \n\t"
         "psubusb    %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
-        "or         %[ftmp7],   %[ftmp7],       %[ftmp4]                \n\t"
+        "por        %[ftmp7],   %[ftmp7],       %[ftmp4]                \n\t"
         "psubusb    %[ftmp6],   %[ftmp2],       %[ftmp3]                \n\t"
         "psubusb    %[ftmp4],   %[ftmp3],       %[ftmp2]                \n\t"
-        "or         %[ftmp4],   %[ftmp4],       %[ftmp6]                \n\t"
+        "por        %[ftmp4],   %[ftmp4],       %[ftmp6]                \n\t"
         "psubusb    %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
-        "or         %[ftmp7],   %[ftmp7],       %[ftmp4]                \n\t"
-        "xor        %[ftmp6],   %[ftmp6],       %[ftmp6]                \n\t"
+        "por        %[ftmp7],   %[ftmp7],       %[ftmp4]                \n\t"
+        "pxor       %[ftmp6],   %[ftmp6],       %[ftmp6]                \n\t"
         "pcmpeqb    %[ftmp7],   %[ftmp7],       %[ftmp6]                \n\t"
         "mov.d      %[ftmp5],   %[ftmp1]                                \n\t"
         "mov.d      %[ftmp6],   %[ftmp2]                                \n\t"
-        "xor        %[ftmp4],   %[ftmp1],       %[ftmp3]                \n\t"
-        "and        %[ftmp4],   %[ftmp4],       %[ff_pb_1]              \n\t"
+        "pxor       %[ftmp4],   %[ftmp1],       %[ftmp3]                \n\t"
+        "pand       %[ftmp4],   %[ftmp4],       %[ff_pb_1]              \n\t"
         "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
         "psubusb    %[ftmp1],   %[ftmp1],       %[ftmp4]                \n\t"
         "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp0]                \n\t"
-        "xor        %[ftmp4],   %[ftmp2],       %[ftmp0]                \n\t"
-        "and        %[ftmp4],   %[ftmp4],       %[ff_pb_1]              \n\t"
+        "pxor       %[ftmp4],   %[ftmp2],       %[ftmp0]                \n\t"
+        "pand       %[ftmp4],   %[ftmp4],       %[ff_pb_1]              \n\t"
         "pavgb      %[ftmp2],   %[ftmp2],       %[ftmp0]                \n\t"
         "psubusb    %[ftmp2],   %[ftmp2],       %[ftmp4]                \n\t"
         "pavgb      %[ftmp2],   %[ftmp2],       %[ftmp3]                \n\t"
         "psubb      %[ftmp1],   %[ftmp1],       %[ftmp5]                \n\t"
         "psubb      %[ftmp2],   %[ftmp2],       %[ftmp6]                \n\t"
-        "and        %[ftmp1],   %[ftmp1],       %[ftmp7]                \n\t"
-        "and        %[ftmp2],   %[ftmp2],       %[ftmp7]                \n\t"
+        "pand       %[ftmp1],   %[ftmp1],       %[ftmp7]                \n\t"
+        "pand       %[ftmp2],   %[ftmp2],       %[ftmp7]                \n\t"
         "paddb      %[ftmp1],   %[ftmp1],       %[ftmp5]                \n\t"
         "paddb      %[ftmp2],   %[ftmp2],       %[ftmp6]                \n\t"
 
@@ -2235,12 +2217,12 @@ void ff_deblock_h_chroma_intra_8_mmi(uint8_t *pix, int stride, int alpha,
           [addr4]"=&r"(addr[4]),            [addr5]"=&r"(addr[5]),
           [pix]"+&r"(pix)
         : [alpha]"r"(alpha),                [beta]"r"(beta),
-          [stride]"r"((mips_reg)stride),    [ff_pb_1]"f"(ff_pb_1)
+          [stride]"r"((mips_reg)stride),    [ff_pb_1]"f"(ff_pb_1.f)
         : "memory"
     );
 }
 
-void ff_deblock_v_luma_8_mmi(uint8_t *pix, int stride, int alpha, int beta,
+void ff_deblock_v_luma_8_mmi(uint8_t *pix, ptrdiff_t stride, int alpha, int beta,
         int8_t *tc0)
 {
     if ((tc0[0] & tc0[1]) >= 0)
@@ -2249,14 +2231,14 @@ void ff_deblock_v_luma_8_mmi(uint8_t *pix, int stride, int alpha, int beta,
         ff_deblock_v8_luma_8_mmi(pix + 8, stride, alpha, beta, tc0 + 2);
 }
 
-void ff_deblock_v_luma_intra_8_mmi(uint8_t *pix, int stride, int alpha,
+void ff_deblock_v_luma_intra_8_mmi(uint8_t *pix, ptrdiff_t stride, int alpha,
         int beta)
 {
     deblock_v8_luma_intra_8_mmi(pix + 0, stride, alpha, beta);
     deblock_v8_luma_intra_8_mmi(pix + 8, stride, alpha, beta);
 }
 
-void ff_deblock_h_luma_8_mmi(uint8_t *pix, int stride, int alpha, int beta,
+void ff_deblock_h_luma_8_mmi(uint8_t *pix, ptrdiff_t stride, int alpha, int beta,
         int8_t *tc0)
 {
     DECLARE_ALIGNED(8, const uint64_t, stack[0x0d]);
@@ -2475,7 +2457,7 @@ void ff_deblock_h_luma_8_mmi(uint8_t *pix, int stride, int alpha, int beta,
     );
 }
 
-void ff_deblock_h_luma_intra_8_mmi(uint8_t *pix, int stride, int alpha,
+void ff_deblock_h_luma_intra_8_mmi(uint8_t *pix, ptrdiff_t stride, int alpha,
         int beta)
 {
     DECLARE_ALIGNED(8, const uint64_t, ptmp[0x11]);
diff --git a/libavcodec/mips/h264dsp_msa.c b/libavcodec/mips/h264dsp_msa.c
index 89fe399469..9d815f8faa 100644
--- a/libavcodec/mips/h264dsp_msa.c
+++ b/libavcodec/mips/h264dsp_msa.c
@@ -21,7 +21,7 @@
 #include "libavutil/mips/generic_macros_msa.h"
 #include "h264dsp_mips.h"
 
-static void avc_wgt_4x2_msa(uint8_t *data, int32_t stride,
+static void avc_wgt_4x2_msa(uint8_t *data, ptrdiff_t stride,
                             int32_t log2_denom, int32_t src_weight,
                             int32_t offset_in)
 {
@@ -48,8 +48,9 @@ static void avc_wgt_4x2_msa(uint8_t *data, int32_t stride,
     ST_W2(src0, 0, 1, data, stride);
 }
 
-static void avc_wgt_4x4_msa(uint8_t *data, int32_t stride, int32_t log2_denom,
-                            int32_t src_weight, int32_t offset_in)
+static void avc_wgt_4x4_msa(uint8_t *data, ptrdiff_t stride,
+                            int32_t log2_denom, int32_t src_weight,
+                            int32_t offset_in)
 {
     uint32_t tp0, tp1, tp2, tp3, offset_val;
     v16u8 src0 = { 0 };
@@ -74,8 +75,9 @@ static void avc_wgt_4x4_msa(uint8_t *data, int32_t stride, int32_t log2_denom,
     ST_W4(src0, 0, 1, 2, 3, data, stride);
 }
 
-static void avc_wgt_4x8_msa(uint8_t *data, int32_t stride, int32_t log2_denom,
-                            int32_t src_weight, int32_t offset_in)
+static void avc_wgt_4x8_msa(uint8_t *data, ptrdiff_t stride,
+                            int32_t log2_denom, int32_t src_weight,
+                            int32_t offset_in)
 {
     uint32_t tp0, tp1, tp2, tp3, offset_val;
     v16u8 src0 = { 0 }, src1 = { 0 };
@@ -105,8 +107,9 @@ static void avc_wgt_4x8_msa(uint8_t *data, int32_t stride, int32_t log2_denom,
     ST_W8(src0, src1, 0, 1, 2, 3, 0, 1, 2, 3, data, stride);
 }
 
-static void avc_wgt_8x4_msa(uint8_t *data, int32_t stride, int32_t log2_denom,
-                            int32_t src_weight, int32_t offset_in)
+static void avc_wgt_8x4_msa(uint8_t *data, ptrdiff_t stride,
+                            int32_t log2_denom, int32_t src_weight,
+                            int32_t offset_in)
 {
     uint32_t offset_val;
     uint64_t tp0, tp1, tp2, tp3;
@@ -136,7 +139,7 @@ static void avc_wgt_8x4_msa(uint8_t *data, int32_t stride, int32_t log2_denom,
     ST_D4(src0, src1, 0, 1, 0, 1, data, stride);
 }
 
-static void avc_wgt_8x8_msa(uint8_t *data, int32_t stride, int32_t log2_denom,
+static void avc_wgt_8x8_msa(uint8_t *data, ptrdiff_t stride, int32_t log2_denom,
                             int32_t src_weight, int32_t offset_in)
 {
     uint32_t offset_val;
@@ -178,8 +181,9 @@ static void avc_wgt_8x8_msa(uint8_t *data, int32_t stride, int32_t log2_denom,
     ST_D8(src0, src1, src2, src3, 0, 1, 0, 1, 0, 1, 0, 1, data, stride);
 }
 
-static void avc_wgt_8x16_msa(uint8_t *data, int32_t stride, int32_t log2_denom,
-                             int32_t src_weight, int32_t offset_in)
+static void avc_wgt_8x16_msa(uint8_t *data, ptrdiff_t stride,
+                             int32_t log2_denom, int32_t src_weight,
+                             int32_t offset_in)
 {
     uint32_t offset_val, cnt;
     uint64_t tp0, tp1, tp2, tp3;
@@ -223,7 +227,7 @@ static void avc_wgt_8x16_msa(uint8_t *data, int32_t stride, int32_t log2_denom,
     }
 }
 
-static void avc_biwgt_4x2_msa(uint8_t *src, uint8_t *dst, int32_t stride,
+static void avc_biwgt_4x2_msa(uint8_t *src, uint8_t *dst, ptrdiff_t stride,
                               int32_t log2_denom, int32_t src_weight,
                               int32_t dst_weight, int32_t offset_in)
 {
@@ -256,7 +260,7 @@ static void avc_biwgt_4x2_msa(uint8_t *src, uint8_t *dst, int32_t stride,
     ST_W2(dst0, 0, 1, dst, stride);
 }
 
-static void avc_biwgt_4x4_msa(uint8_t *src, uint8_t *dst, int32_t stride,
+static void avc_biwgt_4x4_msa(uint8_t *src, uint8_t *dst, ptrdiff_t stride,
                               int32_t log2_denom, int32_t src_weight,
                               int32_t dst_weight, int32_t offset_in)
 {
@@ -290,7 +294,7 @@ static void avc_biwgt_4x4_msa(uint8_t *src, uint8_t *dst, int32_t stride,
     ST_W4(dst0, 0, 1, 2, 3, dst, stride);
 }
 
-static void avc_biwgt_4x8_msa(uint8_t *src, uint8_t *dst, int32_t stride,
+static void avc_biwgt_4x8_msa(uint8_t *src, uint8_t *dst, ptrdiff_t stride,
                               int32_t log2_denom, int32_t src_weight,
                               int32_t dst_weight, int32_t offset_in)
 {
@@ -330,7 +334,7 @@ static void avc_biwgt_4x8_msa(uint8_t *src, uint8_t *dst, int32_t stride,
     ST_W8(dst0, dst1, 0, 1, 2, 3, 0, 1, 2, 3, dst, stride);
 }
 
-static void avc_biwgt_8x4_msa(uint8_t *src, uint8_t *dst, int32_t stride,
+static void avc_biwgt_8x4_msa(uint8_t *src, uint8_t *dst, ptrdiff_t stride,
                               int32_t log2_denom, int32_t src_weight,
                               int32_t dst_weight, int32_t offset_in)
 {
@@ -368,7 +372,7 @@ static void avc_biwgt_8x4_msa(uint8_t *src, uint8_t *dst, int32_t stride,
     ST_D4(dst0, dst1, 0, 1, 0, 1, dst, stride);
 }
 
-static void avc_biwgt_8x8_msa(uint8_t *src, uint8_t *dst, int32_t stride,
+static void avc_biwgt_8x8_msa(uint8_t *src, uint8_t *dst, ptrdiff_t stride,
                               int32_t log2_denom, int32_t src_weight,
                               int32_t dst_weight, int32_t offset_in)
 {
@@ -413,14 +417,13 @@ static void avc_biwgt_8x8_msa(uint8_t *src, uint8_t *dst, int32_t stride,
     tmp7 = __msa_dpadd_s_h(offset, wgt, vec7);
     SRA_4V(tmp0, tmp1, tmp2, tmp3, denom);
     SRA_4V(tmp4, tmp5, tmp6, tmp7, denom);
-    CLIP_SH4_0_255(tmp0, tmp1, tmp2, tmp3);
-    CLIP_SH4_0_255(tmp4, tmp5, tmp6, tmp7);
+    CLIP_SH8_0_255(tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7);
     PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, dst0, dst1);
     PCKEV_B2_UB(tmp5, tmp4, tmp7, tmp6, dst2, dst3);
     ST_D8(dst0, dst1, dst2, dst3, 0, 1, 0, 1, 0, 1, 0, 1, dst, stride);
 }
 
-static void avc_biwgt_8x16_msa(uint8_t *src, uint8_t *dst, int32_t stride,
+static void avc_biwgt_8x16_msa(uint8_t *src, uint8_t *dst, ptrdiff_t stride,
                                int32_t log2_denom, int32_t src_weight,
                                int32_t dst_weight, int32_t offset_in)
 {
@@ -475,8 +478,7 @@ static void avc_biwgt_8x16_msa(uint8_t *src, uint8_t *dst, int32_t stride,
 
         SRA_4V(temp0, temp1, temp2, temp3, denom);
         SRA_4V(temp4, temp5, temp6, temp7, denom);
-        CLIP_SH4_0_255(temp0, temp1, temp2, temp3);
-        CLIP_SH4_0_255(temp4, temp5, temp6, temp7);
+        CLIP_SH8_0_255(temp0, temp1, temp2, temp3, temp4, temp5, temp6, temp7);
         PCKEV_B4_UB(temp1, temp0, temp3, temp2, temp5, temp4, temp7, temp6,
                     dst0, dst1, dst2, dst3);
         ST_D8(dst0, dst1, dst2, dst3, 0, 1, 0, 1, 0, 1, 0, 1, dst, stride);
@@ -531,7 +533,7 @@ static void avc_biwgt_8x16_msa(uint8_t *src, uint8_t *dst, int32_t stride,
     temp = p1_or_q1_org_in << 1;                              \
     clip3 = clip3 - temp;                                     \
     clip3 = __msa_ave_s_h(p2_or_q2_org_in, clip3);            \
-    clip3 = CLIP_SH(clip3, negate_tc_in, tc_in);              \
+    CLIP_SH(clip3, negate_tc_in, tc_in);                      \
     p1_or_q1_out = p1_or_q1_org_in + clip3;                   \
 }
 
@@ -549,7 +551,7 @@ static void avc_biwgt_8x16_msa(uint8_t *src, uint8_t *dst, int32_t stride,
     delta = q0_sub_p0 + p1_sub_q1;                              \
     delta >>= 3;                                                \
                                                                 \
-    delta = CLIP_SH(delta, negate_threshold_in, threshold_in);  \
+    CLIP_SH(delta, negate_threshold_in, threshold_in);          \
                                                                 \
     p0_or_q0_out = p0_or_q0_org_in + delta;                     \
     q0_or_p0_out = q0_or_p0_org_in - delta;                     \
@@ -598,7 +600,7 @@ static void avc_biwgt_8x16_msa(uint8_t *src, uint8_t *dst, int32_t stride,
     delta = q0_sub_p0 + p1_sub_q1;                                       \
     delta = __msa_srari_h(delta, 3);                                     \
                                                                          \
-    delta = CLIP_SH(delta, -tc, tc);                                     \
+    CLIP_SH(delta, -tc, tc);                                             \
                                                                          \
     ILVR_B2_SH(zeros, src1, zeros, src2, res0_r, res1_r);                \
                                                                          \
@@ -620,7 +622,7 @@ static void avc_biwgt_8x16_msa(uint8_t *src, uint8_t *dst, int32_t stride,
                                                              \
     out0 = (v16u8) __msa_ilvr_b((v16i8) in1, (v16i8) in0);   \
     out1 = (v16u8) __msa_sldi_b(zero_m, (v16i8) out0, 2);    \
-    SLDI_B2_0_UB(out1, out2, out2, out3, 2);                 \
+    SLDI_B2_UB(zero_m, out1, zero_m, out2, 2, out2, out3);   \
 }
 
 #define AVC_LPF_H_2BYTE_CHROMA_422(src, stride, tc_val, alpha, beta, res)  \
@@ -662,7 +664,7 @@ static void avc_biwgt_8x16_msa(uint8_t *src, uint8_t *dst, int32_t stride,
     q0_sub_p0 <<= 2;                                                       \
     delta = q0_sub_p0 + p1_sub_q1;                                         \
     delta = __msa_srari_h(delta, 3);                                       \
-    delta = CLIP_SH(delta, -tc, tc);                                       \
+    CLIP_SH(delta, -tc, tc);                                               \
                                                                            \
     ILVR_B2_SH(zeros, src1, zeros, src2, res0_r, res1_r);                  \
                                                                            \
@@ -681,7 +683,7 @@ static void avc_biwgt_8x16_msa(uint8_t *src, uint8_t *dst, int32_t stride,
 static void avc_loopfilter_luma_intra_edge_hor_msa(uint8_t *data,
                                                    uint8_t alpha_in,
                                                    uint8_t beta_in,
-                                                   uint32_t img_width)
+                                                   ptrdiff_t img_width)
 {
     v16u8 p0_asub_q0, p1_asub_p0, q1_asub_q0;
     v16u8 is_less_than, is_less_than_beta, is_less_than_alpha;
@@ -814,7 +816,7 @@ static void avc_loopfilter_luma_intra_edge_hor_msa(uint8_t *data,
 static void avc_loopfilter_luma_intra_edge_ver_msa(uint8_t *data,
                                                    uint8_t alpha_in,
                                                    uint8_t beta_in,
-                                                   uint32_t img_width)
+                                                   ptrdiff_t img_width)
 {
     uint8_t *src = data - 4;
     v16u8 alpha, beta, p0_asub_q0;
@@ -971,7 +973,8 @@ static void avc_loopfilter_luma_intra_edge_ver_msa(uint8_t *data,
     }
 }
 
-static void avc_h_loop_filter_luma_mbaff_intra_msa(uint8_t *src, int32_t stride,
+static void avc_h_loop_filter_luma_mbaff_intra_msa(uint8_t *src,
+                                                   ptrdiff_t stride,
                                                    int32_t alpha_in,
                                                    int32_t beta_in)
 {
@@ -1025,7 +1028,8 @@ static void avc_h_loop_filter_luma_mbaff_intra_msa(uint8_t *src, int32_t stride,
 
     ILVR_W2_SB(tmp2, tmp0, tmp3, tmp1, src6, src3);
     ILVL_W2_SB(tmp2, tmp0, tmp3, tmp1, src1, src5);
-    SLDI_B4_0_SB(src6, src1, src3, src5, src0, src2, src4, src7, 8);
+    SLDI_B4_SB(zeros, src6, zeros, src1, zeros, src3, zeros, src5,
+               8, src0, src2, src4, src7);
 
     p0_asub_q0 = __msa_asub_u_b((v16u8) src2, (v16u8) src3);
     p1_asub_p0 = __msa_asub_u_b((v16u8) src1, (v16u8) src2);
@@ -1116,10 +1120,10 @@ static void avc_h_loop_filter_luma_mbaff_intra_msa(uint8_t *src, int32_t stride,
     ILVRL_H2_SH(zeros, dst2_x, tmp2, tmp3);
 
     ILVR_W2_UB(tmp2, tmp0, tmp3, tmp1, dst0, dst4);
-    SLDI_B2_0_UB(dst0, dst4, dst1, dst5, 8);
+    SLDI_B2_UB(zeros, dst0, zeros, dst4, 8, dst1, dst5);
     dst2_x = (v16u8) __msa_ilvl_w((v4i32) tmp2, (v4i32) tmp0);
     dst2_y = (v16u8) __msa_ilvl_w((v4i32) tmp3, (v4i32) tmp1);
-    SLDI_B2_0_UB(dst2_x, dst2_y, dst3_x, dst3_y, 8);
+    SLDI_B2_UB(zeros, dst2_x, zeros, dst2_y, 8, dst3_x, dst3_y);
 
     out0 = __msa_copy_u_w((v4i32) dst0, 0);
     out1 = __msa_copy_u_h((v8i16) dst0, 2);
@@ -1172,7 +1176,7 @@ static void avc_h_loop_filter_luma_mbaff_intra_msa(uint8_t *src, int32_t stride,
 static void avc_loopfilter_cb_or_cr_intra_edge_hor_msa(uint8_t *data_cb_or_cr,
                                                        uint8_t alpha_in,
                                                        uint8_t beta_in,
-                                                       uint32_t img_width)
+                                                       ptrdiff_t img_width)
 {
     v16u8 alpha, beta;
     v16u8 is_less_than;
@@ -1221,7 +1225,7 @@ static void avc_loopfilter_cb_or_cr_intra_edge_hor_msa(uint8_t *data_cb_or_cr,
 static void avc_loopfilter_cb_or_cr_intra_edge_ver_msa(uint8_t *data_cb_or_cr,
                                                        uint8_t alpha_in,
                                                        uint8_t beta_in,
-                                                       uint32_t img_width)
+                                                       ptrdiff_t img_width)
 {
     v8i16 tmp1;
     v16u8 alpha, beta, is_less_than;
@@ -1280,284 +1284,160 @@ static void avc_loopfilter_cb_or_cr_intra_edge_ver_msa(uint8_t *data_cb_or_cr,
     }
 }
 
-static void avc_loopfilter_luma_inter_edge_ver_msa(uint8_t *data,
-                                                   uint8_t bs0, uint8_t bs1,
-                                                   uint8_t bs2, uint8_t bs3,
-                                                   uint8_t tc0, uint8_t tc1,
-                                                   uint8_t tc2, uint8_t tc3,
-                                                   uint8_t alpha_in,
-                                                   uint8_t beta_in,
-                                                   uint32_t img_width)
+static void avc_loopfilter_luma_inter_edge_ver_msa(uint8_t* pPix, uint32_t iStride,
+                                                   uint8_t iAlpha, uint8_t iBeta,
+                                                   uint8_t* pTc)
 {
-    v16u8 tmp_vec, bs = { 0 };
-
-    tmp_vec = (v16u8) __msa_fill_b(bs0);
-    bs = (v16u8) __msa_insve_w((v4i32) bs, 0, (v4i32) tmp_vec);
-    tmp_vec = (v16u8) __msa_fill_b(bs1);
-    bs = (v16u8) __msa_insve_w((v4i32) bs, 1, (v4i32) tmp_vec);
-    tmp_vec = (v16u8) __msa_fill_b(bs2);
-    bs = (v16u8) __msa_insve_w((v4i32) bs, 2, (v4i32) tmp_vec);
-    tmp_vec = (v16u8) __msa_fill_b(bs3);
-    bs = (v16u8) __msa_insve_w((v4i32) bs, 3, (v4i32) tmp_vec);
-
-    if (!__msa_test_bz_v(bs)) {
-        uint8_t *src = data - 4;
-        v16u8 p3_org, p2_org, p1_org, p0_org, q0_org, q1_org, q2_org, q3_org;
-        v16u8 p0_asub_q0, p1_asub_p0, q1_asub_q0, alpha, beta;
-        v16u8 is_less_than, is_less_than_beta, is_less_than_alpha;
-        v16u8 is_bs_greater_than0;
-        v16u8 tc = { 0 };
-        v16i8 zero = { 0 };
-
-        tmp_vec = (v16u8) __msa_fill_b(tc0);
-        tc = (v16u8) __msa_insve_w((v4i32) tc, 0, (v4i32) tmp_vec);
-        tmp_vec = (v16u8) __msa_fill_b(tc1);
-        tc = (v16u8) __msa_insve_w((v4i32) tc, 1, (v4i32) tmp_vec);
-        tmp_vec = (v16u8) __msa_fill_b(tc2);
-        tc = (v16u8) __msa_insve_w((v4i32) tc, 2, (v4i32) tmp_vec);
-        tmp_vec = (v16u8) __msa_fill_b(tc3);
-        tc = (v16u8) __msa_insve_w((v4i32) tc, 3, (v4i32) tmp_vec);
-
-        is_bs_greater_than0 = (zero < bs);
-
-        {
-            v16u8 row0, row1, row2, row3, row4, row5, row6, row7;
-            v16u8 row8, row9, row10, row11, row12, row13, row14, row15;
-
-            LD_UB8(src, img_width,
-                   row0, row1, row2, row3, row4, row5, row6, row7);
-            src += (8 * img_width);
-            LD_UB8(src, img_width,
-                   row8, row9, row10, row11, row12, row13, row14, row15);
-
-            TRANSPOSE16x8_UB_UB(row0, row1, row2, row3, row4, row5, row6, row7,
-                                row8, row9, row10, row11,
-                                row12, row13, row14, row15,
-                                p3_org, p2_org, p1_org, p0_org,
-                                q0_org, q1_org, q2_org, q3_org);
-        }
-
-        p0_asub_q0 = __msa_asub_u_b(p0_org, q0_org);
-        p1_asub_p0 = __msa_asub_u_b(p1_org, p0_org);
-        q1_asub_q0 = __msa_asub_u_b(q1_org, q0_org);
-
-        alpha = (v16u8) __msa_fill_b(alpha_in);
-        beta = (v16u8) __msa_fill_b(beta_in);
-
-        is_less_than_alpha = (p0_asub_q0 < alpha);
-        is_less_than_beta = (p1_asub_p0 < beta);
-        is_less_than = is_less_than_beta & is_less_than_alpha;
-        is_less_than_beta = (q1_asub_q0 < beta);
-        is_less_than = is_less_than_beta & is_less_than;
-        is_less_than = is_less_than & is_bs_greater_than0;
-
-        if (!__msa_test_bz_v(is_less_than)) {
-            v16i8 negate_tc, sign_negate_tc;
-            v16u8 p0, q0, p2_asub_p0, q2_asub_q0;
-            v8i16 tc_r, tc_l, negate_tc_r, i16_negatetc_l;
-            v8i16 p1_org_r, p0_org_r, q0_org_r, q1_org_r;
-            v8i16 p1_org_l, p0_org_l, q0_org_l, q1_org_l;
-            v8i16 p0_r, q0_r, p0_l, q0_l;
-
-            negate_tc = zero - (v16i8) tc;
-            sign_negate_tc = __msa_clti_s_b(negate_tc, 0);
-
-            ILVRL_B2_SH(sign_negate_tc, negate_tc, negate_tc_r, i16_negatetc_l);
-
-            UNPCK_UB_SH(tc, tc_r, tc_l);
-            UNPCK_UB_SH(p1_org, p1_org_r, p1_org_l);
-            UNPCK_UB_SH(p0_org, p0_org_r, p0_org_l);
-            UNPCK_UB_SH(q0_org, q0_org_r, q0_org_l);
-
-            p2_asub_p0 = __msa_asub_u_b(p2_org, p0_org);
-            is_less_than_beta = (p2_asub_p0 < beta);
-            is_less_than_beta = is_less_than_beta & is_less_than;
-
-            if (!__msa_test_bz_v(is_less_than_beta)) {
-                v16u8 p1;
-                v8i16 p1_r = { 0 };
-                v8i16 p1_l = { 0 };
-                v8i16 p2_org_r = (v8i16) __msa_ilvr_b(zero, (v16i8) p2_org);
-                v8i16 p2_org_l = (v8i16) __msa_ilvl_b(zero, (v16i8) p2_org);
-
-                AVC_LPF_P1_OR_Q1(p0_org_r, q0_org_r, p1_org_r, p2_org_r,
-                                 negate_tc_r, tc_r, p1_r);
-                AVC_LPF_P1_OR_Q1(p0_org_l, q0_org_l, p1_org_l, p2_org_l,
-                                 i16_negatetc_l, tc_l, p1_l);
-
-                p1 = (v16u8) __msa_pckev_b((v16i8) p1_l, (v16i8) p1_r);
-                p1_org = __msa_bmnz_v(p1_org, p1, is_less_than_beta);
-
-                is_less_than_beta = __msa_andi_b(is_less_than_beta, 1);
-                tc = tc + is_less_than_beta;
-            }
-
-            q2_asub_q0 = __msa_asub_u_b(q2_org, q0_org);
-            is_less_than_beta = (q2_asub_q0 < beta);
-            is_less_than_beta = is_less_than_beta & is_less_than;
-
-            q1_org_r = (v8i16) __msa_ilvr_b(zero, (v16i8) q1_org);
-            q1_org_l = (v8i16) __msa_ilvl_b(zero, (v16i8) q1_org);
-
-            if (!__msa_test_bz_v(is_less_than_beta)) {
-                v16u8 q1;
-                v8i16 q1_r = { 0 };
-                v8i16 q1_l = { 0 };
-                v8i16 q2_org_r = (v8i16) __msa_ilvr_b(zero, (v16i8) q2_org);
-                v8i16 q2_org_l = (v8i16) __msa_ilvl_b(zero, (v16i8) q2_org);
-
-                AVC_LPF_P1_OR_Q1(p0_org_r, q0_org_r, q1_org_r, q2_org_r,
-                                 negate_tc_r, tc_r, q1_r);
-                AVC_LPF_P1_OR_Q1(p0_org_l, q0_org_l, q1_org_l, q2_org_l,
-                                 i16_negatetc_l, tc_l, q1_l);
-
-                q1 = (v16u8) __msa_pckev_b((v16i8) q1_l, (v16i8) q1_r);
-                q1_org = __msa_bmnz_v(q1_org, q1, is_less_than_beta);
-
-                is_less_than_beta = __msa_andi_b(is_less_than_beta, 1);
-                tc = tc + is_less_than_beta;
-            }
-
-            {
-                v8i16 threshold_r, negate_thresh_r;
-                v8i16 threshold_l, negate_thresh_l;
-                v16i8 negate_thresh, sign_negate_thresh;
-
-                negate_thresh = zero - (v16i8) tc;
-                sign_negate_thresh = __msa_clti_s_b(negate_thresh, 0);
-
-                ILVR_B2_SH(zero, tc, sign_negate_thresh, negate_thresh,
-                           threshold_r, negate_thresh_r);
-
-                AVC_LPF_P0Q0(q0_org_r, p0_org_r, p1_org_r, q1_org_r,
-                             negate_thresh_r, threshold_r, p0_r, q0_r);
-
-                threshold_l = (v8i16) __msa_ilvl_b(zero, (v16i8) tc);
-                negate_thresh_l = (v8i16) __msa_ilvl_b(sign_negate_thresh,
-                                                       negate_thresh);
-
-                AVC_LPF_P0Q0(q0_org_l, p0_org_l, p1_org_l, q1_org_l,
-                             negate_thresh_l, threshold_l, p0_l, q0_l);
-            }
-
-            PCKEV_B2_UB(p0_l, p0_r, q0_l, q0_r, p0, q0);
-
-            p0_org = __msa_bmnz_v(p0_org, p0, is_less_than);
-            q0_org = __msa_bmnz_v(q0_org, q0, is_less_than);
-
-        {
-            v16i8 tp0, tp1, tp2, tp3;
-            v8i16 tmp2, tmp5;
-            v4i32 tmp3, tmp4, tmp6, tmp7;
-            uint32_t out0, out2;
-            uint16_t out1, out3;
-
-            src = data - 3;
-
-            ILVRL_B2_SB(p1_org, p2_org, tp0, tp2);
-            ILVRL_B2_SB(q0_org, p0_org, tp1, tp3);
-            ILVRL_B2_SH(q2_org, q1_org, tmp2, tmp5);
-
-            ILVRL_H2_SW(tp1, tp0, tmp3, tmp4);
-            ILVRL_H2_SW(tp3, tp2, tmp6, tmp7);
-
-            out0 = __msa_copy_u_w(tmp3, 0);
-            out1 = __msa_copy_u_h(tmp2, 0);
-            out2 = __msa_copy_u_w(tmp3, 1);
-            out3 = __msa_copy_u_h(tmp2, 1);
-
-            SW(out0, src);
-            SH(out1, (src + 4));
-            src += img_width;
-            SW(out2, src);
-            SH(out3, (src + 4));
-
-            out0 = __msa_copy_u_w(tmp3, 2);
-            out1 = __msa_copy_u_h(tmp2, 2);
-            out2 = __msa_copy_u_w(tmp3, 3);
-            out3 = __msa_copy_u_h(tmp2, 3);
-
-            src += img_width;
-            SW(out0, src);
-            SH(out1, (src + 4));
-            src += img_width;
-            SW(out2, src);
-            SH(out3, (src + 4));
-
-            out0 = __msa_copy_u_w(tmp4, 0);
-            out1 = __msa_copy_u_h(tmp2, 4);
-            out2 = __msa_copy_u_w(tmp4, 1);
-            out3 = __msa_copy_u_h(tmp2, 5);
-
-            src += img_width;
-            SW(out0, src);
-            SH(out1, (src + 4));
-            src += img_width;
-            SW(out2, src);
-            SH(out3, (src + 4));
-
-            out0 = __msa_copy_u_w(tmp4, 2);
-            out1 = __msa_copy_u_h(tmp2, 6);
-            out2 = __msa_copy_u_w(tmp4, 3);
-            out3 = __msa_copy_u_h(tmp2, 7);
-
-            src += img_width;
-            SW(out0, src);
-            SH(out1, (src + 4));
-            src += img_width;
-            SW(out2, src);
-            SH(out3, (src + 4));
-
-            out0 = __msa_copy_u_w(tmp6, 0);
-            out1 = __msa_copy_u_h(tmp5, 0);
-            out2 = __msa_copy_u_w(tmp6, 1);
-            out3 = __msa_copy_u_h(tmp5, 1);
-
-            src += img_width;
-            SW(out0, src);
-            SH(out1, (src + 4));
-            src += img_width;
-            SW(out2, src);
-            SH(out3, (src + 4));
-
-            out0 = __msa_copy_u_w(tmp6, 2);
-            out1 = __msa_copy_u_h(tmp5, 2);
-            out2 = __msa_copy_u_w(tmp6, 3);
-            out3 = __msa_copy_u_h(tmp5, 3);
-
-            src += img_width;
-            SW(out0, src);
-            SH(out1, (src + 4));
-            src += img_width;
-            SW(out2, src);
-            SH(out3, (src + 4));
-
-            out0 = __msa_copy_u_w(tmp7, 0);
-            out1 = __msa_copy_u_h(tmp5, 4);
-            out2 = __msa_copy_u_w(tmp7, 1);
-            out3 = __msa_copy_u_h(tmp5, 5);
-
-            src += img_width;
-            SW(out0, src);
-            SH(out1, (src + 4));
-            src += img_width;
-            SW(out2, src);
-            SH(out3, (src + 4));
-
-            out0 = __msa_copy_u_w(tmp7, 2);
-            out1 = __msa_copy_u_h(tmp5, 6);
-            out2 = __msa_copy_u_w(tmp7, 3);
-            out3 = __msa_copy_u_h(tmp5, 7);
-
-            src += img_width;
-            SW(out0, src);
-            SH(out1, (src + 4));
-            src += img_width;
-            SW(out2, src);
-            SH(out3, (src + 4));
-        }
-        }
-    }
+    v16u8 p0, p1, p2, q0, q1, q2;
+    v16i8 iTc, negiTc, negTc, flags, f;
+    v8i16 p0_l, p0_r, p1_l, p1_r, p2_l, p2_r, q0_l, q0_r, q1_l, q1_r, q2_l, q2_r;
+    v8i16 tc_l, tc_r, negTc_l, negTc_r;
+    v8i16 iTc_l, iTc_r, negiTc_l, negiTc_r;
+    // Use for temporary variable
+    v8i16 t0, t1, t2, t3;
+    v16u8 alpha, beta;
+    v16u8 bDetaP0Q0, bDetaP1P0, bDetaQ1Q0, bDetaP2P0, bDetaQ2Q0;
+    v16i8 const_1_b = __msa_ldi_b(1);
+    v8i16 const_1_h = __msa_ldi_h(1);
+    v8i16 const_4_h = __msa_ldi_h(4);
+    v8i16 const_not_255_h = __msa_ldi_h(~255);
+    v16i8 zero = { 0 };
+    v16i8 tc = { pTc[0  >> 2], pTc[1  >> 2], pTc[2  >> 2], pTc[3  >> 2],
+                 pTc[4  >> 2], pTc[5  >> 2], pTc[6  >> 2], pTc[7  >> 2],
+                 pTc[8  >> 2], pTc[9  >> 2], pTc[10 >> 2], pTc[11 >> 2],
+                 pTc[12 >> 2], pTc[13 >> 2], pTc[14 >> 2], pTc[15 >> 2] };
+    negTc = zero - tc;
+    iTc = tc;
+
+    // Load data from pPix
+    LD_SH8(pPix - 3, iStride, t0, t1, t2, t3, q1_l, q1_r, q2_l, q2_r);
+    LD_SH8(pPix + 8 * iStride - 3, iStride, p0_l, p0_r, p1_l, p1_r,
+           p2_l, p2_r, q0_l, q0_r);
+    TRANSPOSE16x8_UB_UB(t0, t1, t2, t3, q1_l, q1_r, q2_l, q2_r,
+                        p0_l, p0_r, p1_l, p1_r, p2_l, p2_r, q0_l, q0_r,
+                        p2, p1, p0, q0, q1, q2, alpha, beta);
+
+    alpha = (v16u8)__msa_fill_b(iAlpha);
+    beta  = (v16u8)__msa_fill_b(iBeta);
+
+    bDetaP0Q0 = __msa_asub_u_b(p0, q0);
+    bDetaP1P0 = __msa_asub_u_b(p1, p0);
+    bDetaQ1Q0 = __msa_asub_u_b(q1, q0);
+    bDetaP2P0 = __msa_asub_u_b(p2, p0);
+    bDetaQ2Q0 = __msa_asub_u_b(q2, q0);
+    bDetaP0Q0 = (v16u8)__msa_clt_u_b(bDetaP0Q0, alpha);
+    bDetaP1P0 = (v16u8)__msa_clt_u_b(bDetaP1P0, beta);
+    bDetaQ1Q0 = (v16u8)__msa_clt_u_b(bDetaQ1Q0, beta);
+    bDetaP2P0 = (v16u8)__msa_clt_u_b(bDetaP2P0, beta);
+    bDetaQ2Q0 = (v16u8)__msa_clt_u_b(bDetaQ2Q0, beta);
+
+    // Unsigned extend p0, p1, p2, q0, q1, q2 from 8 bits to 16 bits
+    ILVRL_B2_SH(zero, p0, p0_r, p0_l);
+    ILVRL_B2_SH(zero, p1, p1_r, p1_l);
+    ILVRL_B2_SH(zero, p2, p2_r, p2_l);
+    ILVRL_B2_SH(zero, q0, q0_r, q0_l);
+    ILVRL_B2_SH(zero, q1, q1_r, q1_l);
+    ILVRL_B2_SH(zero, q2, q2_r, q2_l);
+    // Signed extend tc, negTc from 8 bits to 16 bits
+    flags = __msa_clt_s_b(tc, zero);
+    ILVRL_B2(v8i16, flags, tc, tc_r, tc_l);
+    flags = __msa_clt_s_b(negTc, zero);
+    ILVRL_B2(v8i16, flags, negTc, negTc_r, negTc_l);
+
+    f = (v16i8)bDetaP0Q0 & (v16i8)bDetaP1P0 & (v16i8)bDetaQ1Q0;
+    flags = f & (v16i8)bDetaP2P0;
+    flags = __msa_ceq_b(flags, zero);
+    iTc += ((~flags) & const_1_b);
+    flags = f & (v16i8)bDetaQ2Q0;
+    flags = __msa_ceq_b(flags, zero);
+    iTc += ((~flags) & const_1_b);
+    negiTc = zero - iTc;
+    // Signed extend iTc, negiTc from 8 bits to 16 bits
+    flags = __msa_clt_s_b(iTc, zero);
+    ILVRL_B2(v8i16, flags, iTc, iTc_r, iTc_l);
+    flags = __msa_clt_s_b(negiTc, zero);
+    ILVRL_B2(v8i16, flags, negiTc, negiTc_r, negiTc_l);
+
+    // Calculate the left part
+    // p1
+    t0 = (p2_l + ((p0_l + q0_l + const_1_h) >> 1) - (p1_l << 1)) >> 1;
+    t0 = __msa_max_s_h(negTc_l, t0);
+    t0 = __msa_min_s_h(tc_l, t0);
+    t1 = p1_l + t0;
+    // q1
+    t0 = (q2_l + ((p0_l + q0_l + const_1_h) >> 1) - (q1_l << 1)) >> 1;
+    t0 = __msa_max_s_h(negTc_l, t0);
+    t0 = __msa_min_s_h(tc_l, t0);
+    t2 = q1_l + t0;
+    // iDeta
+    t0 = (((q0_l - p0_l) << 2) + (p1_l - q1_l) + const_4_h) >> 3;
+    t0 = __msa_max_s_h(negiTc_l, t0);
+    t0 = __msa_min_s_h(iTc_l, t0);
+    p1_l = t1;
+    q1_l = t2;
+    // p0
+    t1 = p0_l + t0;
+    t2 = t1 & const_not_255_h;
+    t3 = __msa_cle_s_h((v8i16)zero, t1);
+    flags = (v16i8)__msa_ceq_h(t2, (v8i16)zero);
+    p0_l = (t1 & (v8i16)flags) + (t3 & (v8i16)(~flags));
+    // q0
+    t1 = q0_l - t0;
+    t2 = t1 & const_not_255_h;
+    t3 = __msa_cle_s_h((v8i16)zero, t1);
+    flags = (v16i8)__msa_ceq_h(t2, (v8i16)zero);
+    q0_l = (t1 & (v8i16)flags) + (t3 & (v8i16)(~flags));
+
+    // Calculate the right part
+    // p1
+    t0 = (p2_r + ((p0_r + q0_r + const_1_h) >> 1) - (p1_r << 1)) >> 1;
+    t0 = __msa_max_s_h(negTc_r, t0);
+    t0 = __msa_min_s_h(tc_r, t0);
+    t1 = p1_r + t0;
+    // q1
+    t0 = (q2_r + ((p0_r + q0_r + const_1_h) >> 1) - (q1_r << 1)) >> 1;
+    t0 = __msa_max_s_h(negTc_r, t0);
+    t0 = __msa_min_s_h(tc_r, t0);
+    t2 = q1_r + t0;
+    // iDeta
+    t0 = (((q0_r - p0_r) << 2) + (p1_r - q1_r) + const_4_h) >> 3;
+    t0 = __msa_max_s_h(negiTc_r, t0);
+    t0 = __msa_min_s_h(iTc_r, t0);
+    p1_r = t1;
+    q1_r = t2;
+    // p0
+    t1 = p0_r + t0;
+    t2 = t1 & const_not_255_h;
+    t3 = __msa_cle_s_h((v8i16)zero, t1);
+    flags = (v16i8)__msa_ceq_h(t2, (v8i16)zero);
+    p0_r = (t1 & (v8i16)flags) + (t3 & (v8i16)(~flags));
+    // q0
+    t1 = q0_r - t0;
+    t2 = t1 & const_not_255_h;
+    t3 = __msa_cle_s_h((v8i16)zero, t1);
+    flags = (v16i8)__msa_ceq_h(t2, (v8i16)zero);
+    q0_r = (t1 & (v8i16)flags) + (t3 & (v8i16)(~flags));
+
+    // Combined left and right
+    PCKEV_B4(v8i16, p1_l, p1_r, p0_l, p0_r, q0_l, q0_r, q1_l, q1_r,
+             t0, t1, t2, t3);
+    flags = (v16i8)__msa_cle_s_b(zero, tc);
+    flags &= f;
+    p0 = (v16u8)(((v16i8)t1 & flags) + (p0 & (~flags)));
+    q0 = (v16u8)(((v16i8)t2 & flags) + (q0 & (~flags)));
+    // Using t1, t2 as temporary flags
+    t1 = (v8i16)(flags & (~(__msa_ceq_b((v16i8)bDetaP2P0, zero))));
+    p1 = (v16u8)(t0 & t1) + (p1 & (v16u8)(~t1));
+    t2 = (v8i16)(flags & (~(__msa_ceq_b((v16i8)bDetaQ2Q0, zero))));
+    q1 = (v16u8)(t3 & t2) + (q1 & (v16u8)(~t2));
+
+    ILVRL_B2_SH(p0, p1, t0, t1);
+    ILVRL_B2_SH(q1, q0, t2, t3);
+    ILVRL_H2_UB(t2, t0, p1, p0);
+    ILVRL_H2_UB(t3, t1, q0, q1);
+    // Store data to pPix
+    ST_W8(p1, p0, 0, 1, 2, 3, 0, 1, 2, 3, pPix - 2, iStride);
+    ST_W8(q0, q1, 0, 1, 2, 3, 0, 1, 2, 3, pPix + 8 * iStride - 2, iStride);
 }
 
 static void avc_loopfilter_luma_inter_edge_hor_msa(uint8_t *data,
@@ -1567,7 +1447,7 @@ static void avc_loopfilter_luma_inter_edge_hor_msa(uint8_t *data,
                                                    uint8_t tc2, uint8_t tc3,
                                                    uint8_t alpha_in,
                                                    uint8_t beta_in,
-                                                   uint32_t image_width)
+                                                   ptrdiff_t image_width)
 {
     v16u8 tmp_vec;
     v16u8 bs = { 0 };
@@ -1716,7 +1596,7 @@ static void avc_loopfilter_luma_inter_edge_hor_msa(uint8_t *data,
     }
 }
 
-static void avc_h_loop_filter_luma_mbaff_msa(uint8_t *in, int32_t stride,
+static void avc_h_loop_filter_luma_mbaff_msa(uint8_t *in, ptrdiff_t stride,
                                              int32_t alpha_in, int32_t beta_in,
                                              int8_t *tc0)
 {
@@ -1741,7 +1621,7 @@ static void avc_h_loop_filter_luma_mbaff_msa(uint8_t *in, int32_t stride,
     v8i16 tc, tc_orig_r, tc_plus1;
     v16u8 is_tc_orig1, is_tc_orig2, tc_orig = { 0 };
     v8i16 p0_ilvr_q0, p0_add_q0, q0_sub_p0, p1_sub_q1;
-    v8u16 src2_r, src3_r;
+    v8i16 src2_r, src3_r;
     v8i16 p2_r, p1_r, q2_r, q1_r;
     v16u8 p2, q2, p0, q0;
     v4i32 dst0, dst1;
@@ -1839,8 +1719,8 @@ static void avc_h_loop_filter_luma_mbaff_msa(uint8_t *in, int32_t stride,
     tc_orig_r = (v8i16) __msa_ilvr_b(zeros, (v16i8) tc_orig);
     tc = tc_orig_r;
 
-    p2_r = CLIP_SH(p2_r, -tc_orig_r, tc_orig_r);
-    q2_r = CLIP_SH(q2_r, -tc_orig_r, tc_orig_r);
+    CLIP_SH(p2_r, -tc_orig_r, tc_orig_r);
+    CLIP_SH(q2_r, -tc_orig_r, tc_orig_r);
 
     p2_r += p1_r;
     q2_r += q1_r;
@@ -1872,14 +1752,13 @@ static void avc_h_loop_filter_luma_mbaff_msa(uint8_t *in, int32_t stride,
                                               (v16i8) is_less_than_beta2);
     tc = (v8i16) __msa_bmnz_v((v16u8) tc, (v16u8) tc_plus1, is_less_than_beta2);
 
-    q0_sub_p0 = CLIP_SH(q0_sub_p0, -tc, tc);
+    CLIP_SH(q0_sub_p0, -tc, tc);
 
-    ILVR_B2_UH(zeros, src2, zeros, src3, src2_r, src3_r);
+    ILVR_B2_SH(zeros, src2, zeros, src3, src2_r, src3_r);
     src2_r += q0_sub_p0;
     src3_r -= q0_sub_p0;
 
-    src2_r = (v8u16) CLIP_SH_0_255(src2_r);
-    src3_r = (v8u16) CLIP_SH_0_255(src3_r);
+    CLIP_SH2_0_255(src2_r, src3_r);
 
     PCKEV_B2_UB(src2_r, src2_r, src3_r, src3_r, p0, q0);
 
@@ -1943,7 +1822,7 @@ static void avc_loopfilter_cb_or_cr_inter_edge_hor_msa(uint8_t *data,
                                                        uint8_t tc2, uint8_t tc3,
                                                        uint8_t alpha_in,
                                                        uint8_t beta_in,
-                                                       uint32_t img_width)
+                                                       ptrdiff_t img_width)
 {
     v16u8 alpha, beta;
     v8i16 tmp_vec;
@@ -2029,7 +1908,7 @@ static void avc_loopfilter_cb_or_cr_inter_edge_ver_msa(uint8_t *data,
                                                        uint8_t tc2, uint8_t tc3,
                                                        uint8_t alpha_in,
                                                        uint8_t beta_in,
-                                                       uint32_t img_width)
+                                                       ptrdiff_t img_width)
 {
     uint8_t *src;
     v16u8 alpha, beta;
@@ -2117,7 +1996,7 @@ static void avc_loopfilter_cb_or_cr_inter_edge_ver_msa(uint8_t *data,
     }
 }
 
-static void avc_h_loop_filter_chroma422_msa(uint8_t *src, int32_t stride,
+static void avc_h_loop_filter_chroma422_msa(uint8_t *src, ptrdiff_t stride,
                                             int32_t alpha_in, int32_t beta_in,
                                             int8_t *tc0)
 {
@@ -2141,7 +2020,8 @@ static void avc_h_loop_filter_chroma422_msa(uint8_t *src, int32_t stride,
     }
 }
 
-static void avc_h_loop_filter_chroma422_mbaff_msa(uint8_t *src, int32_t stride,
+static void avc_h_loop_filter_chroma422_mbaff_msa(uint8_t *src,
+                                                  ptrdiff_t stride,
                                                   int32_t alpha_in,
                                                   int32_t beta_in,
                                                   int8_t *tc0)
@@ -2173,29 +2053,30 @@ static void avc_h_loop_filter_chroma422_mbaff_msa(uint8_t *src, int32_t stride,
     }
 }
 
-void ff_h264_h_lpf_luma_inter_msa(uint8_t *data, int img_width,
+void ff_h264_h_lpf_luma_inter_msa(uint8_t *data, ptrdiff_t img_width,
                                   int alpha, int beta, int8_t *tc)
 {
-    uint8_t bs0 = 1;
-    uint8_t bs1 = 1;
-    uint8_t bs2 = 1;
-    uint8_t bs3 = 1;
-
-    if (tc[0] < 0)
-        bs0 = 0;
-    if (tc[1] < 0)
-        bs1 = 0;
-    if (tc[2] < 0)
-        bs2 = 0;
-    if (tc[3] < 0)
-        bs3 = 0;
-
-    avc_loopfilter_luma_inter_edge_ver_msa(data, bs0, bs1, bs2, bs3,
-                                           tc[0], tc[1], tc[2], tc[3],
-                                           alpha, beta, img_width);
+//    uint8_t bs0 = 1;
+//    uint8_t bs1 = 1;
+//    uint8_t bs2 = 1;
+//    uint8_t bs3 = 1;
+//
+//    if (tc[0] < 0)
+//        bs0 = 0;
+//    if (tc[1] < 0)
+//        bs1 = 0;
+//    if (tc[2] < 0)
+//        bs2 = 0;
+//    if (tc[3] < 0)
+//        bs3 = 0;
+//
+//    avc_loopfilter_luma_inter_edge_ver_msa(data, bs0, bs1, bs2, bs3,
+//                                           tc[0], tc[1], tc[2], tc[3],
+//                                           alpha, beta, img_width);
+    avc_loopfilter_luma_inter_edge_ver_msa(data, img_width, alpha, beta, tc);
 }
 
-void ff_h264_v_lpf_luma_inter_msa(uint8_t *data, int img_width,
+void ff_h264_v_lpf_luma_inter_msa(uint8_t *data, ptrdiff_t img_width,
                                   int alpha, int beta, int8_t *tc)
 {
 
@@ -2218,7 +2099,7 @@ void ff_h264_v_lpf_luma_inter_msa(uint8_t *data, int img_width,
                                            alpha, beta, img_width);
 }
 
-void ff_h264_h_lpf_chroma_inter_msa(uint8_t *data, int img_width,
+void ff_h264_h_lpf_chroma_inter_msa(uint8_t *data, ptrdiff_t img_width,
                                     int alpha, int beta, int8_t *tc)
 {
     uint8_t bs0 = 1;
@@ -2240,7 +2121,7 @@ void ff_h264_h_lpf_chroma_inter_msa(uint8_t *data, int img_width,
                                                alpha, beta, img_width);
 }
 
-void ff_h264_v_lpf_chroma_inter_msa(uint8_t *data, int img_width,
+void ff_h264_v_lpf_chroma_inter_msa(uint8_t *data, ptrdiff_t img_width,
                                     int alpha, int beta, int8_t *tc)
 {
     uint8_t bs0 = 1;
@@ -2262,40 +2143,40 @@ void ff_h264_v_lpf_chroma_inter_msa(uint8_t *data, int img_width,
                                                alpha, beta, img_width);
 }
 
-void ff_h264_h_lpf_luma_intra_msa(uint8_t *data, int img_width,
+void ff_h264_h_lpf_luma_intra_msa(uint8_t *data, ptrdiff_t img_width,
                                   int alpha, int beta)
 {
     avc_loopfilter_luma_intra_edge_ver_msa(data, (uint8_t) alpha,
                                            (uint8_t) beta,
-                                           (unsigned int) img_width);
+                                           img_width);
 }
 
-void ff_h264_v_lpf_luma_intra_msa(uint8_t *data, int img_width,
+void ff_h264_v_lpf_luma_intra_msa(uint8_t *data, ptrdiff_t img_width,
                                   int alpha, int beta)
 {
     avc_loopfilter_luma_intra_edge_hor_msa(data, (uint8_t) alpha,
                                            (uint8_t) beta,
-                                           (unsigned int) img_width);
+                                           img_width);
 }
 
-void ff_h264_h_lpf_chroma_intra_msa(uint8_t *data, int img_width,
+void ff_h264_h_lpf_chroma_intra_msa(uint8_t *data, ptrdiff_t img_width,
                                     int alpha, int beta)
 {
     avc_loopfilter_cb_or_cr_intra_edge_ver_msa(data, (uint8_t) alpha,
                                                (uint8_t) beta,
-                                               (unsigned int) img_width);
+                                               img_width);
 }
 
-void ff_h264_v_lpf_chroma_intra_msa(uint8_t *data, int img_width,
+void ff_h264_v_lpf_chroma_intra_msa(uint8_t *data, ptrdiff_t img_width,
                                     int alpha, int beta)
 {
     avc_loopfilter_cb_or_cr_intra_edge_hor_msa(data, (uint8_t) alpha,
                                                (uint8_t) beta,
-                                               (unsigned int) img_width);
+                                               img_width);
 }
 
 void ff_h264_h_loop_filter_chroma422_msa(uint8_t *src,
-                                         int32_t ystride,
+                                         ptrdiff_t ystride,
                                          int32_t alpha, int32_t beta,
                                          int8_t *tc0)
 {
@@ -2303,7 +2184,7 @@ void ff_h264_h_loop_filter_chroma422_msa(uint8_t *src,
 }
 
 void ff_h264_h_loop_filter_chroma422_mbaff_msa(uint8_t *src,
-                                               int32_t ystride,
+                                               ptrdiff_t ystride,
                                                int32_t alpha,
                                                int32_t beta,
                                                int8_t *tc0)
@@ -2312,7 +2193,7 @@ void ff_h264_h_loop_filter_chroma422_mbaff_msa(uint8_t *src,
 }
 
 void ff_h264_h_loop_filter_luma_mbaff_msa(uint8_t *src,
-                                          int32_t ystride,
+                                          ptrdiff_t ystride,
                                           int32_t alpha,
                                           int32_t beta,
                                           int8_t *tc0)
@@ -2321,7 +2202,7 @@ void ff_h264_h_loop_filter_luma_mbaff_msa(uint8_t *src,
 }
 
 void ff_h264_h_loop_filter_luma_mbaff_intra_msa(uint8_t *src,
-                                                int32_t ystride,
+                                                ptrdiff_t ystride,
                                                 int32_t alpha,
                                                 int32_t beta)
 {
@@ -2509,10 +2390,8 @@ void ff_biweight_h264_pixels16_8_msa(uint8_t *dst, uint8_t *src,
     SRA_4V(tmp4, tmp5, tmp6, tmp7, denom);
     SRA_4V(tmp8, tmp9, tmp10, tmp11, denom);
     SRA_4V(tmp12, tmp13, tmp14, tmp15, denom);
-    CLIP_SH4_0_255(tmp0, tmp1, tmp2, tmp3);
-    CLIP_SH4_0_255(tmp4, tmp5, tmp6, tmp7);
-    CLIP_SH4_0_255(tmp8, tmp9, tmp10, tmp11);
-    CLIP_SH4_0_255(tmp12, tmp13, tmp14, tmp15);
+    CLIP_SH8_0_255(tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7);
+    CLIP_SH8_0_255(tmp8, tmp9, tmp10, tmp11, tmp12, tmp13, tmp14, tmp15);
     PCKEV_B4_UB(tmp1, tmp0, tmp3, tmp2, tmp5, tmp4, tmp7, tmp6, dst0, dst1,
                 dst2, dst3);
     PCKEV_B4_UB(tmp9, tmp8, tmp11, tmp10, tmp13, tmp12, tmp15, tmp14, dst4,
@@ -2553,10 +2432,8 @@ void ff_biweight_h264_pixels16_8_msa(uint8_t *dst, uint8_t *src,
         SRA_4V(tmp4, tmp5, tmp6, tmp7, denom);
         SRA_4V(tmp8, tmp9, tmp10, tmp11, denom);
         SRA_4V(tmp12, tmp13, tmp14, tmp15, denom);
-        CLIP_SH4_0_255(tmp0, tmp1, tmp2, tmp3);
-        CLIP_SH4_0_255(tmp4, tmp5, tmp6, tmp7);
-        CLIP_SH4_0_255(tmp8, tmp9, tmp10, tmp11);
-        CLIP_SH4_0_255(tmp12, tmp13, tmp14, tmp15);
+        CLIP_SH8_0_255(tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7);
+        CLIP_SH8_0_255(tmp8, tmp9, tmp10, tmp11, tmp12, tmp13, tmp14, tmp15);
         PCKEV_B4_UB(tmp1, tmp0, tmp3, tmp2, tmp5, tmp4, tmp7, tmp6, dst0, dst1,
                     dst2, dst3);
         PCKEV_B4_UB(tmp9, tmp8, tmp11, tmp10, tmp13, tmp12, tmp15, tmp14, dst4,
diff --git a/libavcodec/mips/h264idct_msa.c b/libavcodec/mips/h264idct_msa.c
index 7851bfdf4b..9a1a757045 100644
--- a/libavcodec/mips/h264idct_msa.c
+++ b/libavcodec/mips/h264idct_msa.c
@@ -233,8 +233,7 @@ static void avc_idct8_addblk_msa(uint8_t *dst, int16_t *src, int32_t dst_stride)
          res0, res1, res2, res3);
     ADD4(res4, tmp4, res5, tmp5, res6, tmp6, res7, tmp7,
          res4, res5, res6, res7);
-    CLIP_SH4_0_255(res0, res1, res2, res3);
-    CLIP_SH4_0_255(res4, res5, res6, res7);
+    CLIP_SH8_0_255(res0, res1, res2, res3, res4, res5, res6, res7);
     PCKEV_B4_SB(res1, res0, res3, res2, res5, res4, res7, res6,
                 dst0, dst1, dst2, dst3);
     ST_D8(dst0, dst1, dst2, dst3, 0, 1, 0, 1, 0, 1, 0, 1, dst, dst_stride)
@@ -263,8 +262,8 @@ static void avc_idct8_dc_addblk_msa(uint8_t *dst, int16_t *src,
          dst0_r, dst1_r, dst2_r, dst3_r);
     ADD4(dst4_r, dc, dst5_r, dc, dst6_r, dc, dst7_r, dc,
          dst4_r, dst5_r, dst6_r, dst7_r);
-    CLIP_SH4_0_255(dst0_r, dst1_r, dst2_r, dst3_r);
-    CLIP_SH4_0_255(dst4_r, dst5_r, dst6_r, dst7_r);
+    CLIP_SH8_0_255(dst0_r, dst1_r, dst2_r, dst3_r,
+                   dst4_r, dst5_r, dst6_r, dst7_r);
     PCKEV_B4_SB(dst1_r, dst0_r, dst3_r, dst2_r, dst5_r, dst4_r, dst7_r, dst6_r,
                 dst0, dst1, dst2, dst3);
     ST_D8(dst0, dst1, dst2, dst3, 0, 1, 0, 1, 0, 1, 0, 1, dst, dst_stride)
@@ -272,7 +271,7 @@ static void avc_idct8_dc_addblk_msa(uint8_t *dst, int16_t *src,
 
 void ff_h264_idct_add_msa(uint8_t *dst, int16_t *src, int32_t dst_stride)
 {
-    uint32_t src0_m, src1_m, src2_m, src3_m, out0_m, out1_m, out2_m, out3_m;
+    uint32_t out0_m, out1_m, out2_m, out3_m;
     v16i8 dst0_m = { 0 };
     v16i8 dst1_m = { 0 };
     v8i16 hres0, hres1, hres2, hres3, vres0, vres1, vres2, vres3;
@@ -280,23 +279,23 @@ void ff_h264_idct_add_msa(uint8_t *dst, int16_t *src, int32_t dst_stride)
     const v8i16 src0 = LD_SH(src);
     const v8i16 src2 = LD_SH(src + 8);
     const v8i16 zero = { 0 };
-    const uint8_t *dst1 = dst + dst_stride;
-    const uint8_t *dst2 = dst + 2 * dst_stride;
-    const uint8_t *dst3 = dst + 3 * dst_stride;
+    const uint8_t *dst1 = dst  + dst_stride;
+    const uint8_t *dst2 = dst1 + dst_stride;
+    const uint8_t *dst3 = dst2 + dst_stride;
 
     ILVL_D2_SH(src0, src0, src2, src2, src1, src3);
     ST_SH2(zero, zero, src, 8);
     AVC_ITRANS_H(src0, src1, src2, src3, hres0, hres1, hres2, hres3);
     TRANSPOSE4x4_SH_SH(hres0, hres1, hres2, hres3, hres0, hres1, hres2, hres3);
     AVC_ITRANS_H(hres0, hres1, hres2, hres3, vres0, vres1, vres2, vres3);
-    src0_m = LW(dst);
-    src1_m = LW(dst1);
-    SRARI_H4_SH(vres0, vres1, vres2, vres3, 6);
-    src2_m = LW(dst2);
-    src3_m = LW(dst3);
+    out0_m = LW(dst);
+    out1_m = LW(dst1);
+    out2_m = LW(dst2);
+    out3_m = LW(dst3);
     ILVR_D2_SH(vres1, vres0, vres3, vres2, inp0_m, inp1_m);
-    INSERT_W2_SB(src0_m, src1_m, dst0_m);
-    INSERT_W2_SB(src2_m, src3_m, dst1_m);
+    SRARI_H2_SH(inp0_m, inp1_m, 6);
+    INSERT_W2_SB(out0_m, out1_m, dst0_m);
+    INSERT_W2_SB(out2_m, out3_m, dst1_m);
     ILVR_B2_SH(zero, dst0_m, zero, dst1_m, res0_m, res1_m);
     ADD2(res0_m, inp0_m, res1_m, inp1_m, res0_m, res1_m);
     CLIP_SH2_0_255(res0_m, res1_m);
@@ -353,17 +352,13 @@ void ff_h264_idct_add16_msa(uint8_t *dst,
     int32_t i;
 
     for (i = 0; i < 16; i++) {
-        int32_t nnz = nzc[scan8[i]];
+        uint8_t *dst_l = dst + blk_offset[i];
+        uint8_t  nnz   = nzc[scan8[i]];
+        int32_t i_16p  = i << 4;
 
         if (nnz) {
-            if (nnz == 1 && ((dctcoef *) block)[i * 16])
-                ff_h264_idct4x4_addblk_dc_msa(dst + blk_offset[i],
-                                              block + i * 16 * sizeof(pixel),
-                                              dst_stride);
-            else
-                ff_h264_idct_add_msa(dst + blk_offset[i],
-                                     block + i * 16 * sizeof(pixel),
-                                     dst_stride);
+            i_16p *= sizeof(pixel);
+            ff_h264_idct_add_msa(dst_l, block + i_16p, dst_stride);
         }
     }
 }
diff --git a/libavcodec/mips/h264pred_init_mips.c b/libavcodec/mips/h264pred_init_mips.c
index 63637b8732..0fd9bb737a 100644
--- a/libavcodec/mips/h264pred_init_mips.c
+++ b/libavcodec/mips/h264pred_init_mips.c
@@ -19,137 +19,121 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
+#include "libavutil/mips/cpu.h"
 #include "config.h"
 #include "h264dsp_mips.h"
 #include "h264pred_mips.h"
 
-#if HAVE_MSA
-static av_cold void h264_pred_init_msa(H264PredContext *h, int codec_id,
-                                       const int bit_depth,
-                                       const int chroma_format_idc)
+av_cold void ff_h264_pred_init_mips(H264PredContext *h, int codec_id,
+                                    int bit_depth,
+                                    const int chroma_format_idc)
 {
-    if (8 == bit_depth) {
-        if (chroma_format_idc == 1) {
-            h->pred8x8[VERT_PRED8x8] = ff_h264_intra_pred_vert_8x8_msa;
-            h->pred8x8[HOR_PRED8x8] = ff_h264_intra_pred_horiz_8x8_msa;
-        }
+    int cpu_flags = av_get_cpu_flags();
 
-        if (codec_id != AV_CODEC_ID_VP7 && codec_id != AV_CODEC_ID_VP8) {
+    if (have_mmi(cpu_flags)) {
+        if (bit_depth == 8) {
             if (chroma_format_idc == 1) {
-                h->pred8x8[PLANE_PRED8x8] = ff_h264_intra_predict_plane_8x8_msa;
-            }
-        }
-        if (codec_id != AV_CODEC_ID_RV40 && codec_id != AV_CODEC_ID_VP7
-            && codec_id != AV_CODEC_ID_VP8) {
-            if (chroma_format_idc == 1) {
-                h->pred8x8[DC_PRED8x8] = ff_h264_intra_predict_dc_4blk_8x8_msa;
-                h->pred8x8[LEFT_DC_PRED8x8] =
-                    ff_h264_intra_predict_hor_dc_8x8_msa;
-                h->pred8x8[TOP_DC_PRED8x8] =
-                    ff_h264_intra_predict_vert_dc_8x8_msa;
-                h->pred8x8[ALZHEIMER_DC_L0T_PRED8x8] =
-                    ff_h264_intra_predict_mad_cow_dc_l0t_8x8_msa;
-                h->pred8x8[ALZHEIMER_DC_0LT_PRED8x8] =
-                    ff_h264_intra_predict_mad_cow_dc_0lt_8x8_msa;
-                h->pred8x8[ALZHEIMER_DC_L00_PRED8x8] =
-                    ff_h264_intra_predict_mad_cow_dc_l00_8x8_msa;
-                h->pred8x8[ALZHEIMER_DC_0L0_PRED8x8] =
-                    ff_h264_intra_predict_mad_cow_dc_0l0_8x8_msa;
-            }
-        } else {
-            if (codec_id == AV_CODEC_ID_VP7 || codec_id == AV_CODEC_ID_VP8) {
-                h->pred8x8[7] = ff_vp8_pred8x8_127_dc_8_msa;
-                h->pred8x8[8] = ff_vp8_pred8x8_129_dc_8_msa;
+                h->pred8x8  [VERT_PRED8x8       ] = ff_pred8x8_vertical_8_mmi;
+                h->pred8x8  [HOR_PRED8x8        ] = ff_pred8x8_horizontal_8_mmi;
+            } else {
+                h->pred8x8  [VERT_PRED8x8       ] = ff_pred8x16_vertical_8_mmi;
+                h->pred8x8  [HOR_PRED8x8        ] = ff_pred8x16_horizontal_8_mmi;
             }
-        }
 
-        if (chroma_format_idc == 1) {
-            h->pred8x8[DC_128_PRED8x8] = ff_h264_intra_pred_dc_128_8x8_msa;
-        }
+            h->pred16x16[DC_PRED8x8             ] = ff_pred16x16_dc_8_mmi;
+            h->pred16x16[VERT_PRED8x8           ] = ff_pred16x16_vertical_8_mmi;
+            h->pred16x16[HOR_PRED8x8            ] = ff_pred16x16_horizontal_8_mmi;
+            h->pred8x8l [TOP_DC_PRED            ] = ff_pred8x8l_top_dc_8_mmi;
+            h->pred8x8l [DC_PRED                ] = ff_pred8x8l_dc_8_mmi;
 
-        h->pred16x16[DC_PRED8x8] = ff_h264_intra_pred_dc_16x16_msa;
-        h->pred16x16[VERT_PRED8x8] = ff_h264_intra_pred_vert_16x16_msa;
-        h->pred16x16[HOR_PRED8x8] = ff_h264_intra_pred_horiz_16x16_msa;
+    #if ARCH_MIPS64
+            switch (codec_id) {
+            case AV_CODEC_ID_SVQ3:
+                h->pred16x16[PLANE_PRED8x8      ] = ff_pred16x16_plane_svq3_8_mmi;
+                break;
+            case AV_CODEC_ID_RV40:
+                h->pred16x16[PLANE_PRED8x8      ] = ff_pred16x16_plane_rv40_8_mmi;
+                break;
+            case AV_CODEC_ID_VP7:
+            case AV_CODEC_ID_VP8:
+                break;
+            default:
+                h->pred16x16[PLANE_PRED8x8      ] = ff_pred16x16_plane_h264_8_mmi;
+                break;
+            }
+    #endif
 
-        switch (codec_id) {
-        case AV_CODEC_ID_SVQ3:
-            ;
-            break;
-        case AV_CODEC_ID_RV40:
-            ;
-            break;
-        case AV_CODEC_ID_VP7:
-        case AV_CODEC_ID_VP8:
-            h->pred16x16[7] = ff_vp8_pred16x16_127_dc_8_msa;
-            h->pred16x16[8] = ff_vp8_pred16x16_129_dc_8_msa;
-            break;
-        default:
-            h->pred16x16[PLANE_PRED8x8] =
-                ff_h264_intra_predict_plane_16x16_msa;
-            break;
+            if (codec_id == AV_CODEC_ID_SVQ3 || codec_id == AV_CODEC_ID_H264) {
+                if (chroma_format_idc == 1) {
+                    h->pred8x8[TOP_DC_PRED8x8   ] = ff_pred8x8_top_dc_8_mmi;
+                    h->pred8x8[DC_PRED8x8       ] = ff_pred8x8_dc_8_mmi;
+                }
+            }
         }
-
-        h->pred16x16[LEFT_DC_PRED8x8] = ff_h264_intra_pred_dc_left_16x16_msa;
-        h->pred16x16[TOP_DC_PRED8x8] = ff_h264_intra_pred_dc_top_16x16_msa;
-        h->pred16x16[DC_128_PRED8x8] = ff_h264_intra_pred_dc_128_16x16_msa;
     }
-}
-#endif  // #if HAVE_MSA
-
-#if HAVE_MMI
-static av_cold void h264_pred_init_mmi(H264PredContext *h, int codec_id,
-        const int bit_depth, const int chroma_format_idc)
-{
-    if (bit_depth == 8) {
-        if (chroma_format_idc == 1) {
-            h->pred8x8  [VERT_PRED8x8       ] = ff_pred8x8_vertical_8_mmi;
-            h->pred8x8  [HOR_PRED8x8        ] = ff_pred8x8_horizontal_8_mmi;
-        } else {
-            h->pred8x8  [VERT_PRED8x8       ] = ff_pred8x16_vertical_8_mmi;
-            h->pred8x8  [HOR_PRED8x8        ] = ff_pred8x16_horizontal_8_mmi;
-        }
 
-        h->pred16x16[DC_PRED8x8             ] = ff_pred16x16_dc_8_mmi;
-        h->pred16x16[VERT_PRED8x8           ] = ff_pred16x16_vertical_8_mmi;
-        h->pred16x16[HOR_PRED8x8            ] = ff_pred16x16_horizontal_8_mmi;
-        h->pred8x8l [TOP_DC_PRED            ] = ff_pred8x8l_top_dc_8_mmi;
-        h->pred8x8l [DC_PRED                ] = ff_pred8x8l_dc_8_mmi;
+    if (have_msa(cpu_flags)) {
+        if (8 == bit_depth) {
+            if (chroma_format_idc == 1) {
+                h->pred8x8[VERT_PRED8x8] = ff_h264_intra_pred_vert_8x8_msa;
+                h->pred8x8[HOR_PRED8x8] = ff_h264_intra_pred_horiz_8x8_msa;
+            }
 
-#if ARCH_MIPS64
-        switch (codec_id) {
-        case AV_CODEC_ID_SVQ3:
-            h->pred16x16[PLANE_PRED8x8      ] = ff_pred16x16_plane_svq3_8_mmi;
-            break;
-        case AV_CODEC_ID_RV40:
-            h->pred16x16[PLANE_PRED8x8      ] = ff_pred16x16_plane_rv40_8_mmi;
-            break;
-        case AV_CODEC_ID_VP7:
-        case AV_CODEC_ID_VP8:
-            break;
-        default:
-            h->pred16x16[PLANE_PRED8x8      ] = ff_pred16x16_plane_h264_8_mmi;
-            break;
-        }
-#endif
+            if (codec_id != AV_CODEC_ID_VP7 && codec_id != AV_CODEC_ID_VP8) {
+                if (chroma_format_idc == 1) {
+                    h->pred8x8[PLANE_PRED8x8] = ff_h264_intra_predict_plane_8x8_msa;
+                }
+            }
+            if (codec_id != AV_CODEC_ID_RV40 && codec_id != AV_CODEC_ID_VP7
+                && codec_id != AV_CODEC_ID_VP8) {
+                if (chroma_format_idc == 1) {
+                    h->pred8x8[DC_PRED8x8] = ff_h264_intra_predict_dc_4blk_8x8_msa;
+                    h->pred8x8[LEFT_DC_PRED8x8] =
+                        ff_h264_intra_predict_hor_dc_8x8_msa;
+                    h->pred8x8[TOP_DC_PRED8x8] =
+                        ff_h264_intra_predict_vert_dc_8x8_msa;
+                    h->pred8x8[ALZHEIMER_DC_L0T_PRED8x8] =
+                        ff_h264_intra_predict_mad_cow_dc_l0t_8x8_msa;
+                    h->pred8x8[ALZHEIMER_DC_0LT_PRED8x8] =
+                        ff_h264_intra_predict_mad_cow_dc_0lt_8x8_msa;
+                    h->pred8x8[ALZHEIMER_DC_L00_PRED8x8] =
+                        ff_h264_intra_predict_mad_cow_dc_l00_8x8_msa;
+                    h->pred8x8[ALZHEIMER_DC_0L0_PRED8x8] =
+                        ff_h264_intra_predict_mad_cow_dc_0l0_8x8_msa;
+                }
+            } else {
+                if (codec_id == AV_CODEC_ID_VP7 || codec_id == AV_CODEC_ID_VP8) {
+                    h->pred8x8[7] = ff_vp8_pred8x8_127_dc_8_msa;
+                    h->pred8x8[8] = ff_vp8_pred8x8_129_dc_8_msa;
+                }
+            }
 
-        if (codec_id == AV_CODEC_ID_SVQ3 || codec_id == AV_CODEC_ID_H264) {
             if (chroma_format_idc == 1) {
-                h->pred8x8[TOP_DC_PRED8x8   ] = ff_pred8x8_top_dc_8_mmi;
-                h->pred8x8[DC_PRED8x8       ] = ff_pred8x8_dc_8_mmi;
+                h->pred8x8[DC_128_PRED8x8] = ff_h264_intra_pred_dc_128_8x8_msa;
+            }
+
+            h->pred16x16[DC_PRED8x8] = ff_h264_intra_pred_dc_16x16_msa;
+            h->pred16x16[VERT_PRED8x8] = ff_h264_intra_pred_vert_16x16_msa;
+            h->pred16x16[HOR_PRED8x8] = ff_h264_intra_pred_horiz_16x16_msa;
+
+            switch (codec_id) {
+            case AV_CODEC_ID_SVQ3:
+            case AV_CODEC_ID_RV40:
+                break;
+            case AV_CODEC_ID_VP7:
+            case AV_CODEC_ID_VP8:
+                h->pred16x16[7] = ff_vp8_pred16x16_127_dc_8_msa;
+                h->pred16x16[8] = ff_vp8_pred16x16_129_dc_8_msa;
+                break;
+            default:
+                h->pred16x16[PLANE_PRED8x8] =
+                    ff_h264_intra_predict_plane_16x16_msa;
+                break;
             }
+
+            h->pred16x16[LEFT_DC_PRED8x8] = ff_h264_intra_pred_dc_left_16x16_msa;
+            h->pred16x16[TOP_DC_PRED8x8] = ff_h264_intra_pred_dc_top_16x16_msa;
+            h->pred16x16[DC_128_PRED8x8] = ff_h264_intra_pred_dc_128_16x16_msa;
         }
     }
 }
-#endif /* HAVE_MMI */
-
-av_cold void ff_h264_pred_init_mips(H264PredContext *h, int codec_id,
-                                    int bit_depth,
-                                    const int chroma_format_idc)
-{
-#if HAVE_MMI
-    h264_pred_init_mmi(h, codec_id, bit_depth, chroma_format_idc);
-#endif /* HAVE_MMI */
-#if HAVE_MSA
-    h264_pred_init_msa(h, codec_id, bit_depth, chroma_format_idc);
-#endif  // #if HAVE_MSA
-}
diff --git a/libavcodec/mips/h264pred_mmi.c b/libavcodec/mips/h264pred_mmi.c
index f4fe0911af..480411f5b5 100644
--- a/libavcodec/mips/h264pred_mmi.c
+++ b/libavcodec/mips/h264pred_mmi.c
@@ -155,14 +155,14 @@ void ff_pred16x16_dc_8_mmi(uint8_t *src, ptrdiff_t stride)
 void ff_pred8x8l_top_dc_8_mmi(uint8_t *src, int has_topleft,
         int has_topright, ptrdiff_t stride)
 {
-    uint32_t dc;
     double ftmp[11];
     mips_reg tmp[3];
+    union av_intfloat64 dc;
     DECLARE_VAR_ALL64;
     DECLARE_VAR_ADDRT;
 
     __asm__ volatile (
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
         MMI_ULDC1(%[ftmp10], %[srcA], 0x00)
         MMI_ULDC1(%[ftmp9], %[src0], 0x00)
         MMI_ULDC1(%[ftmp8], %[src1], 0x00)
@@ -178,7 +178,9 @@ void ff_pred8x8l_top_dc_8_mmi(uint8_t *src, int has_topleft,
 
         "1:                                                             \n\t"
         "bnez       %[has_topright],            2f                      \n\t"
-        "pinsrh_3   %[ftmp2],   %[ftmp2],       %[ftmp4]                \n\t"
+        "dli        %[tmp0],    0xa4                                    \n\t"
+        "mtc1       %[tmp0],    %[ftmp1]                                \n\t"
+        "pshufh     %[ftmp2],   %[ftmp2],       %[ftmp1]                \n\t"
 
         "2:                                                             \n\t"
         "dli        %[tmp0],    0x02                                    \n\t"
@@ -207,12 +209,12 @@ void ff_pred8x8l_top_dc_8_mmi(uint8_t *src, int has_topleft,
           [ftmp10]"=&f"(ftmp[10]),
           [tmp0]"=&r"(tmp[0]),              [tmp1]"=&r"(tmp[1]),
           RESTRICT_ASM_ALL64
-          [dc]"=r"(dc)
+          [dc]"=r"(dc.i)
         : [srcA]"r"((mips_reg)(src-stride-1)),
           [src0]"r"((mips_reg)(src-stride)),
           [src1]"r"((mips_reg)(src-stride+1)),
           [has_topleft]"r"(has_topleft),    [has_topright]"r"(has_topright),
-          [ff_pb_1]"r"(ff_pb_1),            [ff_pw_2]"f"(ff_pw_2)
+          [ff_pb_1]"r"(ff_pb_1.i),          [ff_pw_2]"f"(ff_pw_2.f)
         : "memory"
     );
 
@@ -236,7 +238,7 @@ void ff_pred8x8l_top_dc_8_mmi(uint8_t *src, int has_topleft,
           RESTRICT_ASM_ALL64
           RESTRICT_ASM_ADDRT
           [src]"+&r"(src)
-        : [dc]"f"(dc),                      [stride]"r"((mips_reg)stride)
+        : [dc]"f"(dc.f),                    [stride]"r"((mips_reg)stride)
         : "memory"
     );
 }
@@ -244,9 +246,10 @@ void ff_pred8x8l_top_dc_8_mmi(uint8_t *src, int has_topleft,
 void ff_pred8x8l_dc_8_mmi(uint8_t *src, int has_topleft, int has_topright,
         ptrdiff_t stride)
 {
-    uint32_t dc, dc1, dc2;
+    uint32_t dc1, dc2;
     double ftmp[14];
     mips_reg tmp[1];
+    union av_intfloat64 dc;
 
     const int l0 = ((has_topleft ? src[-1+-1*stride] : src[-1+0*stride]) + 2*src[-1+0*stride] + src[-1+1*stride] + 2) >> 2;
     const int l1 = (src[-1+0*stride] + 2*src[-1+1*stride] + src[-1+2*stride] + 2) >> 2;
@@ -264,7 +267,7 @@ void ff_pred8x8l_dc_8_mmi(uint8_t *src, int has_topleft, int has_topright,
         MMI_ULDC1(%[ftmp4], %[srcA], 0x00)
         MMI_ULDC1(%[ftmp5], %[src0], 0x00)
         MMI_ULDC1(%[ftmp6], %[src1], 0x00)
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
         "dli        %[tmp0],    0x03                                    \n\t"
         "punpcklbh  %[ftmp7],   %[ftmp4],       %[ftmp0]                \n\t"
         "punpckhbh  %[ftmp8],   %[ftmp4],       %[ftmp0]                \n\t"
@@ -320,7 +323,7 @@ void ff_pred8x8l_dc_8_mmi(uint8_t *src, int has_topleft, int has_topright,
     );
 
     dc1 = l0+l1+l2+l3+l4+l5+l6+l7;
-    dc = ((dc1+dc2+8)>>4)*0x01010101U;
+    dc.i = ((dc1+dc2+8)>>4)*0x01010101U;
 
     __asm__ volatile (
         "dli        %[tmp0],    0x02                                    \n\t"
@@ -342,7 +345,7 @@ void ff_pred8x8l_dc_8_mmi(uint8_t *src, int has_topleft, int has_topright,
           RESTRICT_ASM_ALL64
           RESTRICT_ASM_ADDRT
           [src]"+&r"(src)
-        : [dc]"f"(dc),                      [stride]"r"((mips_reg)stride)
+        : [dc]"f"(dc.f),                    [stride]"r"((mips_reg)stride)
         : "memory"
     );
 }
@@ -355,7 +358,7 @@ void ff_pred8x8l_vertical_8_mmi(uint8_t *src, int has_topleft,
     DECLARE_VAR_ALL64;
 
     __asm__ volatile (
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
         MMI_LDC1(%[ftmp3], %[srcA], 0x00)
         MMI_LDC1(%[ftmp4], %[src0], 0x00)
         MMI_LDC1(%[ftmp5], %[src1], 0x00)
@@ -370,7 +373,9 @@ void ff_pred8x8l_vertical_8_mmi(uint8_t *src, int has_topleft,
 
         "1:                                                             \n\t"
         "bnez       %[has_topright],            2f                      \n\t"
-        "pinsrh_3   %[ftmp11],  %[ftmp11],      %[ftmp9]                \n\t"
+        "dli        %[tmp0],    0xa4                                    \n\t"
+        "mtc1       %[tmp0],    %[ftmp1]                                \n\t"
+        "pshufh     %[ftmp11],  %[ftmp11],      %[ftmp1]                \n\t"
 
         "2:                                                             \n\t"
         "dli        %[tmp0],    0x02                                    \n\t"
@@ -526,7 +531,7 @@ void ff_pred8x8_top_dc_8_mmi(uint8_t *src, ptrdiff_t stride)
 
     __asm__ volatile (
         "dli        %[tmp0],    0x02                                    \n\t"
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
         PTR_SUBU   "%[addr0],   %[src],         %[stride]               \n\t"
         MMI_LDC1(%[ftmp1], %[addr0], 0x00)
         "punpcklbh  %[ftmp2],   %[ftmp1],       %[ftmp0]                \n\t"
@@ -636,7 +641,7 @@ void ff_pred8x8_dc_8_mmi(uint8_t *src, ptrdiff_t stride)
         PTR_SRL    "%[addr4],   0x02                                    \n\t"
         PTR_SRL    "%[addr1],   0x02                                    \n\t"
         PTR_SRL    "%[addr2],   0x03                                    \n\t"
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
         "dmtc1      %[addr3],   %[ftmp1]                                \n\t"
         "pshufh     %[ftmp1],   %[ftmp1],       %[ftmp0]                \n\t"
         "dmtc1      %[addr4],   %[ftmp2]                                \n\t"
@@ -753,9 +758,9 @@ static inline void pred16x16_plane_compat_mmi(uint8_t *src, int stride,
         "dmtc1      %[tmp0],    %[ftmp4]                                \n\t"
         MMI_ULDC1(%[ftmp0], %[addr0], -0x01)
         MMI_ULDC1(%[ftmp2], %[addr0],  0x08)
-        "dsrl       %[ftmp1],   %[ftmp0],       %[ftmp4]                \n\t"
-        "dsrl       %[ftmp3],   %[ftmp2],       %[ftmp4]                \n\t"
-        "xor        %[ftmp4],   %[ftmp4],       %[ftmp4]                \n\t"
+        "ssrld      %[ftmp1],   %[ftmp0],       %[ftmp4]                \n\t"
+        "ssrld      %[ftmp3],   %[ftmp2],       %[ftmp4]                \n\t"
+        "pxor       %[ftmp4],   %[ftmp4],       %[ftmp4]                \n\t"
         "punpcklbh  %[ftmp0],   %[ftmp0],       %[ftmp4]                \n\t"
         "punpcklbh  %[ftmp1],   %[ftmp1],       %[ftmp4]                \n\t"
         "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp4]                \n\t"
@@ -911,7 +916,7 @@ static inline void pred16x16_plane_compat_mmi(uint8_t *src, int stride,
         "dmul       %[tmp3],    %[tmp3],        %[tmp2]                 \n\t"
         "dsubu      %[tmp5],    %[tmp5],        %[tmp3]                 \n\t"
 
-        "xor        %[ftmp4],   %[ftmp4],       %[ftmp4]                \n\t"
+        "pxor       %[ftmp4],   %[ftmp4],       %[ftmp4]                \n\t"
         "dmtc1      %[tmp0],    %[ftmp0]                                \n\t"
         "pshufh     %[ftmp0],   %[ftmp0],       %[ftmp4]                \n\t"
         "dmtc1      %[tmp1],    %[ftmp5]                                \n\t"
@@ -961,10 +966,10 @@ static inline void pred16x16_plane_compat_mmi(uint8_t *src, int stride,
           [addr0]"=&r"(addr[0])
         : [src]"r"(src),                    [stride]"r"((mips_reg)stride),
           [svq3]"r"(svq3),                  [rv40]"r"(rv40),
-          [ff_pw_m8tom5]"f"(ff_pw_m8tom5),  [ff_pw_m4tom1]"f"(ff_pw_m4tom1),
-          [ff_pw_1to4]"f"(ff_pw_1to4),      [ff_pw_5to8]"f"(ff_pw_5to8),
-          [ff_pw_0to3]"f"(ff_pw_0to3),      [ff_pw_4to7]"r"(ff_pw_4to7),
-          [ff_pw_8tob]"r"(ff_pw_8tob),      [ff_pw_ctof]"r"(ff_pw_ctof)
+          [ff_pw_m8tom5]"f"(ff_pw_m8tom5.f),[ff_pw_m4tom1]"f"(ff_pw_m4tom1.f),
+          [ff_pw_1to4]"f"(ff_pw_1to4.f),    [ff_pw_5to8]"f"(ff_pw_5to8.f),
+          [ff_pw_0to3]"f"(ff_pw_0to3.f),    [ff_pw_4to7]"r"(ff_pw_4to7.i),
+          [ff_pw_8tob]"r"(ff_pw_8tob.i),    [ff_pw_ctof]"r"(ff_pw_ctof.i)
         : "memory"
     );
 }
diff --git a/libavcodec/mips/h264qpel_init_mips.c b/libavcodec/mips/h264qpel_init_mips.c
index 33bae3093a..ea839f0714 100644
--- a/libavcodec/mips/h264qpel_init_mips.c
+++ b/libavcodec/mips/h264qpel_init_mips.c
@@ -19,231 +19,221 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
+#include "libavutil/mips/cpu.h"
 #include "h264dsp_mips.h"
 
-#if HAVE_MSA
-static av_cold void h264qpel_init_msa(H264QpelContext *c, int bit_depth)
+av_cold void ff_h264qpel_init_mips(H264QpelContext *c, int bit_depth)
 {
-    if (8 == bit_depth) {
-        c->put_h264_qpel_pixels_tab[0][0] = ff_put_h264_qpel16_mc00_msa;
-        c->put_h264_qpel_pixels_tab[0][1] = ff_put_h264_qpel16_mc10_msa;
-        c->put_h264_qpel_pixels_tab[0][2] = ff_put_h264_qpel16_mc20_msa;
-        c->put_h264_qpel_pixels_tab[0][3] = ff_put_h264_qpel16_mc30_msa;
-        c->put_h264_qpel_pixels_tab[0][4] = ff_put_h264_qpel16_mc01_msa;
-        c->put_h264_qpel_pixels_tab[0][5] = ff_put_h264_qpel16_mc11_msa;
-        c->put_h264_qpel_pixels_tab[0][6] = ff_put_h264_qpel16_mc21_msa;
-        c->put_h264_qpel_pixels_tab[0][7] = ff_put_h264_qpel16_mc31_msa;
-        c->put_h264_qpel_pixels_tab[0][8] = ff_put_h264_qpel16_mc02_msa;
-        c->put_h264_qpel_pixels_tab[0][9] = ff_put_h264_qpel16_mc12_msa;
-        c->put_h264_qpel_pixels_tab[0][10] = ff_put_h264_qpel16_mc22_msa;
-        c->put_h264_qpel_pixels_tab[0][11] = ff_put_h264_qpel16_mc32_msa;
-        c->put_h264_qpel_pixels_tab[0][12] = ff_put_h264_qpel16_mc03_msa;
-        c->put_h264_qpel_pixels_tab[0][13] = ff_put_h264_qpel16_mc13_msa;
-        c->put_h264_qpel_pixels_tab[0][14] = ff_put_h264_qpel16_mc23_msa;
-        c->put_h264_qpel_pixels_tab[0][15] = ff_put_h264_qpel16_mc33_msa;
+    int cpu_flags = av_get_cpu_flags();
+
+    if (have_mmi(cpu_flags)) {
+        if (bit_depth == 8) {
+            c->put_h264_qpel_pixels_tab[0][0] = ff_put_h264_qpel16_mc00_mmi;
+            c->put_h264_qpel_pixels_tab[0][1] = ff_put_h264_qpel16_mc10_mmi;
+            c->put_h264_qpel_pixels_tab[0][2] = ff_put_h264_qpel16_mc20_mmi;
+            c->put_h264_qpel_pixels_tab[0][3] = ff_put_h264_qpel16_mc30_mmi;
+            c->put_h264_qpel_pixels_tab[0][4] = ff_put_h264_qpel16_mc01_mmi;
+            c->put_h264_qpel_pixels_tab[0][5] = ff_put_h264_qpel16_mc11_mmi;
+            c->put_h264_qpel_pixels_tab[0][6] = ff_put_h264_qpel16_mc21_mmi;
+            c->put_h264_qpel_pixels_tab[0][7] = ff_put_h264_qpel16_mc31_mmi;
+            c->put_h264_qpel_pixels_tab[0][8] = ff_put_h264_qpel16_mc02_mmi;
+            c->put_h264_qpel_pixels_tab[0][9] = ff_put_h264_qpel16_mc12_mmi;
+            c->put_h264_qpel_pixels_tab[0][10] = ff_put_h264_qpel16_mc22_mmi;
+            c->put_h264_qpel_pixels_tab[0][11] = ff_put_h264_qpel16_mc32_mmi;
+            c->put_h264_qpel_pixels_tab[0][12] = ff_put_h264_qpel16_mc03_mmi;
+            c->put_h264_qpel_pixels_tab[0][13] = ff_put_h264_qpel16_mc13_mmi;
+            c->put_h264_qpel_pixels_tab[0][14] = ff_put_h264_qpel16_mc23_mmi;
+            c->put_h264_qpel_pixels_tab[0][15] = ff_put_h264_qpel16_mc33_mmi;
 
-        c->put_h264_qpel_pixels_tab[1][0] = ff_put_h264_qpel8_mc00_msa;
-        c->put_h264_qpel_pixels_tab[1][1] = ff_put_h264_qpel8_mc10_msa;
-        c->put_h264_qpel_pixels_tab[1][2] = ff_put_h264_qpel8_mc20_msa;
-        c->put_h264_qpel_pixels_tab[1][3] = ff_put_h264_qpel8_mc30_msa;
-        c->put_h264_qpel_pixels_tab[1][4] = ff_put_h264_qpel8_mc01_msa;
-        c->put_h264_qpel_pixels_tab[1][5] = ff_put_h264_qpel8_mc11_msa;
-        c->put_h264_qpel_pixels_tab[1][6] = ff_put_h264_qpel8_mc21_msa;
-        c->put_h264_qpel_pixels_tab[1][7] = ff_put_h264_qpel8_mc31_msa;
-        c->put_h264_qpel_pixels_tab[1][8] = ff_put_h264_qpel8_mc02_msa;
-        c->put_h264_qpel_pixels_tab[1][9] = ff_put_h264_qpel8_mc12_msa;
-        c->put_h264_qpel_pixels_tab[1][10] = ff_put_h264_qpel8_mc22_msa;
-        c->put_h264_qpel_pixels_tab[1][11] = ff_put_h264_qpel8_mc32_msa;
-        c->put_h264_qpel_pixels_tab[1][12] = ff_put_h264_qpel8_mc03_msa;
-        c->put_h264_qpel_pixels_tab[1][13] = ff_put_h264_qpel8_mc13_msa;
-        c->put_h264_qpel_pixels_tab[1][14] = ff_put_h264_qpel8_mc23_msa;
-        c->put_h264_qpel_pixels_tab[1][15] = ff_put_h264_qpel8_mc33_msa;
+            c->put_h264_qpel_pixels_tab[1][0] = ff_put_h264_qpel8_mc00_mmi;
+            c->put_h264_qpel_pixels_tab[1][1] = ff_put_h264_qpel8_mc10_mmi;
+            c->put_h264_qpel_pixels_tab[1][2] = ff_put_h264_qpel8_mc20_mmi;
+            c->put_h264_qpel_pixels_tab[1][3] = ff_put_h264_qpel8_mc30_mmi;
+            c->put_h264_qpel_pixels_tab[1][4] = ff_put_h264_qpel8_mc01_mmi;
+            c->put_h264_qpel_pixels_tab[1][5] = ff_put_h264_qpel8_mc11_mmi;
+            c->put_h264_qpel_pixels_tab[1][6] = ff_put_h264_qpel8_mc21_mmi;
+            c->put_h264_qpel_pixels_tab[1][7] = ff_put_h264_qpel8_mc31_mmi;
+            c->put_h264_qpel_pixels_tab[1][8] = ff_put_h264_qpel8_mc02_mmi;
+            c->put_h264_qpel_pixels_tab[1][9] = ff_put_h264_qpel8_mc12_mmi;
+            c->put_h264_qpel_pixels_tab[1][10] = ff_put_h264_qpel8_mc22_mmi;
+            c->put_h264_qpel_pixels_tab[1][11] = ff_put_h264_qpel8_mc32_mmi;
+            c->put_h264_qpel_pixels_tab[1][12] = ff_put_h264_qpel8_mc03_mmi;
+            c->put_h264_qpel_pixels_tab[1][13] = ff_put_h264_qpel8_mc13_mmi;
+            c->put_h264_qpel_pixels_tab[1][14] = ff_put_h264_qpel8_mc23_mmi;
+            c->put_h264_qpel_pixels_tab[1][15] = ff_put_h264_qpel8_mc33_mmi;
 
-        c->put_h264_qpel_pixels_tab[2][1] = ff_put_h264_qpel4_mc10_msa;
-        c->put_h264_qpel_pixels_tab[2][2] = ff_put_h264_qpel4_mc20_msa;
-        c->put_h264_qpel_pixels_tab[2][3] = ff_put_h264_qpel4_mc30_msa;
-        c->put_h264_qpel_pixels_tab[2][4] = ff_put_h264_qpel4_mc01_msa;
-        c->put_h264_qpel_pixels_tab[2][5] = ff_put_h264_qpel4_mc11_msa;
-        c->put_h264_qpel_pixels_tab[2][6] = ff_put_h264_qpel4_mc21_msa;
-        c->put_h264_qpel_pixels_tab[2][7] = ff_put_h264_qpel4_mc31_msa;
-        c->put_h264_qpel_pixels_tab[2][8] = ff_put_h264_qpel4_mc02_msa;
-        c->put_h264_qpel_pixels_tab[2][9] = ff_put_h264_qpel4_mc12_msa;
-        c->put_h264_qpel_pixels_tab[2][10] = ff_put_h264_qpel4_mc22_msa;
-        c->put_h264_qpel_pixels_tab[2][11] = ff_put_h264_qpel4_mc32_msa;
-        c->put_h264_qpel_pixels_tab[2][12] = ff_put_h264_qpel4_mc03_msa;
-        c->put_h264_qpel_pixels_tab[2][13] = ff_put_h264_qpel4_mc13_msa;
-        c->put_h264_qpel_pixels_tab[2][14] = ff_put_h264_qpel4_mc23_msa;
-        c->put_h264_qpel_pixels_tab[2][15] = ff_put_h264_qpel4_mc33_msa;
+            c->put_h264_qpel_pixels_tab[2][0] = ff_put_h264_qpel4_mc00_mmi;
+            c->put_h264_qpel_pixels_tab[2][1] = ff_put_h264_qpel4_mc10_mmi;
+            c->put_h264_qpel_pixels_tab[2][2] = ff_put_h264_qpel4_mc20_mmi;
+            c->put_h264_qpel_pixels_tab[2][3] = ff_put_h264_qpel4_mc30_mmi;
+            c->put_h264_qpel_pixels_tab[2][4] = ff_put_h264_qpel4_mc01_mmi;
+            c->put_h264_qpel_pixels_tab[2][5] = ff_put_h264_qpel4_mc11_mmi;
+            c->put_h264_qpel_pixels_tab[2][6] = ff_put_h264_qpel4_mc21_mmi;
+            c->put_h264_qpel_pixels_tab[2][7] = ff_put_h264_qpel4_mc31_mmi;
+            c->put_h264_qpel_pixels_tab[2][8] = ff_put_h264_qpel4_mc02_mmi;
+            c->put_h264_qpel_pixels_tab[2][9] = ff_put_h264_qpel4_mc12_mmi;
+            c->put_h264_qpel_pixels_tab[2][10] = ff_put_h264_qpel4_mc22_mmi;
+            c->put_h264_qpel_pixels_tab[2][11] = ff_put_h264_qpel4_mc32_mmi;
+            c->put_h264_qpel_pixels_tab[2][12] = ff_put_h264_qpel4_mc03_mmi;
+            c->put_h264_qpel_pixels_tab[2][13] = ff_put_h264_qpel4_mc13_mmi;
+            c->put_h264_qpel_pixels_tab[2][14] = ff_put_h264_qpel4_mc23_mmi;
+            c->put_h264_qpel_pixels_tab[2][15] = ff_put_h264_qpel4_mc33_mmi;
 
-        c->avg_h264_qpel_pixels_tab[0][0] = ff_avg_h264_qpel16_mc00_msa;
-        c->avg_h264_qpel_pixels_tab[0][1] = ff_avg_h264_qpel16_mc10_msa;
-        c->avg_h264_qpel_pixels_tab[0][2] = ff_avg_h264_qpel16_mc20_msa;
-        c->avg_h264_qpel_pixels_tab[0][3] = ff_avg_h264_qpel16_mc30_msa;
-        c->avg_h264_qpel_pixels_tab[0][4] = ff_avg_h264_qpel16_mc01_msa;
-        c->avg_h264_qpel_pixels_tab[0][5] = ff_avg_h264_qpel16_mc11_msa;
-        c->avg_h264_qpel_pixels_tab[0][6] = ff_avg_h264_qpel16_mc21_msa;
-        c->avg_h264_qpel_pixels_tab[0][7] = ff_avg_h264_qpel16_mc31_msa;
-        c->avg_h264_qpel_pixels_tab[0][8] = ff_avg_h264_qpel16_mc02_msa;
-        c->avg_h264_qpel_pixels_tab[0][9] = ff_avg_h264_qpel16_mc12_msa;
-        c->avg_h264_qpel_pixels_tab[0][10] = ff_avg_h264_qpel16_mc22_msa;
-        c->avg_h264_qpel_pixels_tab[0][11] = ff_avg_h264_qpel16_mc32_msa;
-        c->avg_h264_qpel_pixels_tab[0][12] = ff_avg_h264_qpel16_mc03_msa;
-        c->avg_h264_qpel_pixels_tab[0][13] = ff_avg_h264_qpel16_mc13_msa;
-        c->avg_h264_qpel_pixels_tab[0][14] = ff_avg_h264_qpel16_mc23_msa;
-        c->avg_h264_qpel_pixels_tab[0][15] = ff_avg_h264_qpel16_mc33_msa;
+            c->avg_h264_qpel_pixels_tab[0][0] = ff_avg_h264_qpel16_mc00_mmi;
+            c->avg_h264_qpel_pixels_tab[0][1] = ff_avg_h264_qpel16_mc10_mmi;
+            c->avg_h264_qpel_pixels_tab[0][2] = ff_avg_h264_qpel16_mc20_mmi;
+            c->avg_h264_qpel_pixels_tab[0][3] = ff_avg_h264_qpel16_mc30_mmi;
+            c->avg_h264_qpel_pixels_tab[0][4] = ff_avg_h264_qpel16_mc01_mmi;
+            c->avg_h264_qpel_pixels_tab[0][5] = ff_avg_h264_qpel16_mc11_mmi;
+            c->avg_h264_qpel_pixels_tab[0][6] = ff_avg_h264_qpel16_mc21_mmi;
+            c->avg_h264_qpel_pixels_tab[0][7] = ff_avg_h264_qpel16_mc31_mmi;
+            c->avg_h264_qpel_pixels_tab[0][8] = ff_avg_h264_qpel16_mc02_mmi;
+            c->avg_h264_qpel_pixels_tab[0][9] = ff_avg_h264_qpel16_mc12_mmi;
+            c->avg_h264_qpel_pixels_tab[0][10] = ff_avg_h264_qpel16_mc22_mmi;
+            c->avg_h264_qpel_pixels_tab[0][11] = ff_avg_h264_qpel16_mc32_mmi;
+            c->avg_h264_qpel_pixels_tab[0][12] = ff_avg_h264_qpel16_mc03_mmi;
+            c->avg_h264_qpel_pixels_tab[0][13] = ff_avg_h264_qpel16_mc13_mmi;
+            c->avg_h264_qpel_pixels_tab[0][14] = ff_avg_h264_qpel16_mc23_mmi;
+            c->avg_h264_qpel_pixels_tab[0][15] = ff_avg_h264_qpel16_mc33_mmi;
 
-        c->avg_h264_qpel_pixels_tab[1][0] = ff_avg_h264_qpel8_mc00_msa;
-        c->avg_h264_qpel_pixels_tab[1][1] = ff_avg_h264_qpel8_mc10_msa;
-        c->avg_h264_qpel_pixels_tab[1][2] = ff_avg_h264_qpel8_mc20_msa;
-        c->avg_h264_qpel_pixels_tab[1][3] = ff_avg_h264_qpel8_mc30_msa;
-        c->avg_h264_qpel_pixels_tab[1][4] = ff_avg_h264_qpel8_mc01_msa;
-        c->avg_h264_qpel_pixels_tab[1][5] = ff_avg_h264_qpel8_mc11_msa;
-        c->avg_h264_qpel_pixels_tab[1][6] = ff_avg_h264_qpel8_mc21_msa;
-        c->avg_h264_qpel_pixels_tab[1][7] = ff_avg_h264_qpel8_mc31_msa;
-        c->avg_h264_qpel_pixels_tab[1][8] = ff_avg_h264_qpel8_mc02_msa;
-        c->avg_h264_qpel_pixels_tab[1][9] = ff_avg_h264_qpel8_mc12_msa;
-        c->avg_h264_qpel_pixels_tab[1][10] = ff_avg_h264_qpel8_mc22_msa;
-        c->avg_h264_qpel_pixels_tab[1][11] = ff_avg_h264_qpel8_mc32_msa;
-        c->avg_h264_qpel_pixels_tab[1][12] = ff_avg_h264_qpel8_mc03_msa;
-        c->avg_h264_qpel_pixels_tab[1][13] = ff_avg_h264_qpel8_mc13_msa;
-        c->avg_h264_qpel_pixels_tab[1][14] = ff_avg_h264_qpel8_mc23_msa;
-        c->avg_h264_qpel_pixels_tab[1][15] = ff_avg_h264_qpel8_mc33_msa;
+            c->avg_h264_qpel_pixels_tab[1][0] = ff_avg_h264_qpel8_mc00_mmi;
+            c->avg_h264_qpel_pixels_tab[1][1] = ff_avg_h264_qpel8_mc10_mmi;
+            c->avg_h264_qpel_pixels_tab[1][2] = ff_avg_h264_qpel8_mc20_mmi;
+            c->avg_h264_qpel_pixels_tab[1][3] = ff_avg_h264_qpel8_mc30_mmi;
+            c->avg_h264_qpel_pixels_tab[1][4] = ff_avg_h264_qpel8_mc01_mmi;
+            c->avg_h264_qpel_pixels_tab[1][5] = ff_avg_h264_qpel8_mc11_mmi;
+            c->avg_h264_qpel_pixels_tab[1][6] = ff_avg_h264_qpel8_mc21_mmi;
+            c->avg_h264_qpel_pixels_tab[1][7] = ff_avg_h264_qpel8_mc31_mmi;
+            c->avg_h264_qpel_pixels_tab[1][8] = ff_avg_h264_qpel8_mc02_mmi;
+            c->avg_h264_qpel_pixels_tab[1][9] = ff_avg_h264_qpel8_mc12_mmi;
+            c->avg_h264_qpel_pixels_tab[1][10] = ff_avg_h264_qpel8_mc22_mmi;
+            c->avg_h264_qpel_pixels_tab[1][11] = ff_avg_h264_qpel8_mc32_mmi;
+            c->avg_h264_qpel_pixels_tab[1][12] = ff_avg_h264_qpel8_mc03_mmi;
+            c->avg_h264_qpel_pixels_tab[1][13] = ff_avg_h264_qpel8_mc13_mmi;
+            c->avg_h264_qpel_pixels_tab[1][14] = ff_avg_h264_qpel8_mc23_mmi;
+            c->avg_h264_qpel_pixels_tab[1][15] = ff_avg_h264_qpel8_mc33_mmi;
 
-        c->avg_h264_qpel_pixels_tab[2][0] = ff_avg_h264_qpel4_mc00_msa;
-        c->avg_h264_qpel_pixels_tab[2][1] = ff_avg_h264_qpel4_mc10_msa;
-        c->avg_h264_qpel_pixels_tab[2][2] = ff_avg_h264_qpel4_mc20_msa;
-        c->avg_h264_qpel_pixels_tab[2][3] = ff_avg_h264_qpel4_mc30_msa;
-        c->avg_h264_qpel_pixels_tab[2][4] = ff_avg_h264_qpel4_mc01_msa;
-        c->avg_h264_qpel_pixels_tab[2][5] = ff_avg_h264_qpel4_mc11_msa;
-        c->avg_h264_qpel_pixels_tab[2][6] = ff_avg_h264_qpel4_mc21_msa;
-        c->avg_h264_qpel_pixels_tab[2][7] = ff_avg_h264_qpel4_mc31_msa;
-        c->avg_h264_qpel_pixels_tab[2][8] = ff_avg_h264_qpel4_mc02_msa;
-        c->avg_h264_qpel_pixels_tab[2][9] = ff_avg_h264_qpel4_mc12_msa;
-        c->avg_h264_qpel_pixels_tab[2][10] = ff_avg_h264_qpel4_mc22_msa;
-        c->avg_h264_qpel_pixels_tab[2][11] = ff_avg_h264_qpel4_mc32_msa;
-        c->avg_h264_qpel_pixels_tab[2][12] = ff_avg_h264_qpel4_mc03_msa;
-        c->avg_h264_qpel_pixels_tab[2][13] = ff_avg_h264_qpel4_mc13_msa;
-        c->avg_h264_qpel_pixels_tab[2][14] = ff_avg_h264_qpel4_mc23_msa;
-        c->avg_h264_qpel_pixels_tab[2][15] = ff_avg_h264_qpel4_mc33_msa;
+            c->avg_h264_qpel_pixels_tab[2][0] = ff_avg_h264_qpel4_mc00_mmi;
+            c->avg_h264_qpel_pixels_tab[2][1] = ff_avg_h264_qpel4_mc10_mmi;
+            c->avg_h264_qpel_pixels_tab[2][2] = ff_avg_h264_qpel4_mc20_mmi;
+            c->avg_h264_qpel_pixels_tab[2][3] = ff_avg_h264_qpel4_mc30_mmi;
+            c->avg_h264_qpel_pixels_tab[2][4] = ff_avg_h264_qpel4_mc01_mmi;
+            c->avg_h264_qpel_pixels_tab[2][5] = ff_avg_h264_qpel4_mc11_mmi;
+            c->avg_h264_qpel_pixels_tab[2][6] = ff_avg_h264_qpel4_mc21_mmi;
+            c->avg_h264_qpel_pixels_tab[2][7] = ff_avg_h264_qpel4_mc31_mmi;
+            c->avg_h264_qpel_pixels_tab[2][8] = ff_avg_h264_qpel4_mc02_mmi;
+            c->avg_h264_qpel_pixels_tab[2][9] = ff_avg_h264_qpel4_mc12_mmi;
+            c->avg_h264_qpel_pixels_tab[2][10] = ff_avg_h264_qpel4_mc22_mmi;
+            c->avg_h264_qpel_pixels_tab[2][11] = ff_avg_h264_qpel4_mc32_mmi;
+            c->avg_h264_qpel_pixels_tab[2][12] = ff_avg_h264_qpel4_mc03_mmi;
+            c->avg_h264_qpel_pixels_tab[2][13] = ff_avg_h264_qpel4_mc13_mmi;
+            c->avg_h264_qpel_pixels_tab[2][14] = ff_avg_h264_qpel4_mc23_mmi;
+            c->avg_h264_qpel_pixels_tab[2][15] = ff_avg_h264_qpel4_mc33_mmi;
+        }
     }
-}
-#endif  // #if HAVE_MSA
 
-#if HAVE_MMI
-static av_cold void h264qpel_init_mmi(H264QpelContext *c, int bit_depth)
-{
-    if (8 == bit_depth) {
-        c->put_h264_qpel_pixels_tab[0][0] = ff_put_h264_qpel16_mc00_mmi;
-        c->put_h264_qpel_pixels_tab[0][1] = ff_put_h264_qpel16_mc10_mmi;
-        c->put_h264_qpel_pixels_tab[0][2] = ff_put_h264_qpel16_mc20_mmi;
-        c->put_h264_qpel_pixels_tab[0][3] = ff_put_h264_qpel16_mc30_mmi;
-        c->put_h264_qpel_pixels_tab[0][4] = ff_put_h264_qpel16_mc01_mmi;
-        c->put_h264_qpel_pixels_tab[0][5] = ff_put_h264_qpel16_mc11_mmi;
-        c->put_h264_qpel_pixels_tab[0][6] = ff_put_h264_qpel16_mc21_mmi;
-        c->put_h264_qpel_pixels_tab[0][7] = ff_put_h264_qpel16_mc31_mmi;
-        c->put_h264_qpel_pixels_tab[0][8] = ff_put_h264_qpel16_mc02_mmi;
-        c->put_h264_qpel_pixels_tab[0][9] = ff_put_h264_qpel16_mc12_mmi;
-        c->put_h264_qpel_pixels_tab[0][10] = ff_put_h264_qpel16_mc22_mmi;
-        c->put_h264_qpel_pixels_tab[0][11] = ff_put_h264_qpel16_mc32_mmi;
-        c->put_h264_qpel_pixels_tab[0][12] = ff_put_h264_qpel16_mc03_mmi;
-        c->put_h264_qpel_pixels_tab[0][13] = ff_put_h264_qpel16_mc13_mmi;
-        c->put_h264_qpel_pixels_tab[0][14] = ff_put_h264_qpel16_mc23_mmi;
-        c->put_h264_qpel_pixels_tab[0][15] = ff_put_h264_qpel16_mc33_mmi;
+    if (have_msa(cpu_flags)) {
+        if (bit_depth == 8) {
+            c->put_h264_qpel_pixels_tab[0][0] = ff_put_h264_qpel16_mc00_msa;
+            c->put_h264_qpel_pixels_tab[0][1] = ff_put_h264_qpel16_mc10_msa;
+            c->put_h264_qpel_pixels_tab[0][2] = ff_put_h264_qpel16_mc20_msa;
+            c->put_h264_qpel_pixels_tab[0][3] = ff_put_h264_qpel16_mc30_msa;
+            c->put_h264_qpel_pixels_tab[0][4] = ff_put_h264_qpel16_mc01_msa;
+            c->put_h264_qpel_pixels_tab[0][5] = ff_put_h264_qpel16_mc11_msa;
+            c->put_h264_qpel_pixels_tab[0][6] = ff_put_h264_qpel16_mc21_msa;
+            c->put_h264_qpel_pixels_tab[0][7] = ff_put_h264_qpel16_mc31_msa;
+            c->put_h264_qpel_pixels_tab[0][8] = ff_put_h264_qpel16_mc02_msa;
+            c->put_h264_qpel_pixels_tab[0][9] = ff_put_h264_qpel16_mc12_msa;
+            c->put_h264_qpel_pixels_tab[0][10] = ff_put_h264_qpel16_mc22_msa;
+            c->put_h264_qpel_pixels_tab[0][11] = ff_put_h264_qpel16_mc32_msa;
+            c->put_h264_qpel_pixels_tab[0][12] = ff_put_h264_qpel16_mc03_msa;
+            c->put_h264_qpel_pixels_tab[0][13] = ff_put_h264_qpel16_mc13_msa;
+            c->put_h264_qpel_pixels_tab[0][14] = ff_put_h264_qpel16_mc23_msa;
+            c->put_h264_qpel_pixels_tab[0][15] = ff_put_h264_qpel16_mc33_msa;
 
-        c->put_h264_qpel_pixels_tab[1][0] = ff_put_h264_qpel8_mc00_mmi;
-        c->put_h264_qpel_pixels_tab[1][1] = ff_put_h264_qpel8_mc10_mmi;
-        c->put_h264_qpel_pixels_tab[1][2] = ff_put_h264_qpel8_mc20_mmi;
-        c->put_h264_qpel_pixels_tab[1][3] = ff_put_h264_qpel8_mc30_mmi;
-        c->put_h264_qpel_pixels_tab[1][4] = ff_put_h264_qpel8_mc01_mmi;
-        c->put_h264_qpel_pixels_tab[1][5] = ff_put_h264_qpel8_mc11_mmi;
-        c->put_h264_qpel_pixels_tab[1][6] = ff_put_h264_qpel8_mc21_mmi;
-        c->put_h264_qpel_pixels_tab[1][7] = ff_put_h264_qpel8_mc31_mmi;
-        c->put_h264_qpel_pixels_tab[1][8] = ff_put_h264_qpel8_mc02_mmi;
-        c->put_h264_qpel_pixels_tab[1][9] = ff_put_h264_qpel8_mc12_mmi;
-        c->put_h264_qpel_pixels_tab[1][10] = ff_put_h264_qpel8_mc22_mmi;
-        c->put_h264_qpel_pixels_tab[1][11] = ff_put_h264_qpel8_mc32_mmi;
-        c->put_h264_qpel_pixels_tab[1][12] = ff_put_h264_qpel8_mc03_mmi;
-        c->put_h264_qpel_pixels_tab[1][13] = ff_put_h264_qpel8_mc13_mmi;
-        c->put_h264_qpel_pixels_tab[1][14] = ff_put_h264_qpel8_mc23_mmi;
-        c->put_h264_qpel_pixels_tab[1][15] = ff_put_h264_qpel8_mc33_mmi;
+            c->put_h264_qpel_pixels_tab[1][0] = ff_put_h264_qpel8_mc00_msa;
+            c->put_h264_qpel_pixels_tab[1][1] = ff_put_h264_qpel8_mc10_msa;
+            c->put_h264_qpel_pixels_tab[1][2] = ff_put_h264_qpel8_mc20_msa;
+            c->put_h264_qpel_pixels_tab[1][3] = ff_put_h264_qpel8_mc30_msa;
+            c->put_h264_qpel_pixels_tab[1][4] = ff_put_h264_qpel8_mc01_msa;
+            c->put_h264_qpel_pixels_tab[1][5] = ff_put_h264_qpel8_mc11_msa;
+            c->put_h264_qpel_pixels_tab[1][6] = ff_put_h264_qpel8_mc21_msa;
+            c->put_h264_qpel_pixels_tab[1][7] = ff_put_h264_qpel8_mc31_msa;
+            c->put_h264_qpel_pixels_tab[1][8] = ff_put_h264_qpel8_mc02_msa;
+            c->put_h264_qpel_pixels_tab[1][9] = ff_put_h264_qpel8_mc12_msa;
+            c->put_h264_qpel_pixels_tab[1][10] = ff_put_h264_qpel8_mc22_msa;
+            c->put_h264_qpel_pixels_tab[1][11] = ff_put_h264_qpel8_mc32_msa;
+            c->put_h264_qpel_pixels_tab[1][12] = ff_put_h264_qpel8_mc03_msa;
+            c->put_h264_qpel_pixels_tab[1][13] = ff_put_h264_qpel8_mc13_msa;
+            c->put_h264_qpel_pixels_tab[1][14] = ff_put_h264_qpel8_mc23_msa;
+            c->put_h264_qpel_pixels_tab[1][15] = ff_put_h264_qpel8_mc33_msa;
 
-        c->put_h264_qpel_pixels_tab[2][0] = ff_put_h264_qpel4_mc00_mmi;
-        c->put_h264_qpel_pixels_tab[2][1] = ff_put_h264_qpel4_mc10_mmi;
-        c->put_h264_qpel_pixels_tab[2][2] = ff_put_h264_qpel4_mc20_mmi;
-        c->put_h264_qpel_pixels_tab[2][3] = ff_put_h264_qpel4_mc30_mmi;
-        c->put_h264_qpel_pixels_tab[2][4] = ff_put_h264_qpel4_mc01_mmi;
-        c->put_h264_qpel_pixels_tab[2][5] = ff_put_h264_qpel4_mc11_mmi;
-        c->put_h264_qpel_pixels_tab[2][6] = ff_put_h264_qpel4_mc21_mmi;
-        c->put_h264_qpel_pixels_tab[2][7] = ff_put_h264_qpel4_mc31_mmi;
-        c->put_h264_qpel_pixels_tab[2][8] = ff_put_h264_qpel4_mc02_mmi;
-        c->put_h264_qpel_pixels_tab[2][9] = ff_put_h264_qpel4_mc12_mmi;
-        c->put_h264_qpel_pixels_tab[2][10] = ff_put_h264_qpel4_mc22_mmi;
-        c->put_h264_qpel_pixels_tab[2][11] = ff_put_h264_qpel4_mc32_mmi;
-        c->put_h264_qpel_pixels_tab[2][12] = ff_put_h264_qpel4_mc03_mmi;
-        c->put_h264_qpel_pixels_tab[2][13] = ff_put_h264_qpel4_mc13_mmi;
-        c->put_h264_qpel_pixels_tab[2][14] = ff_put_h264_qpel4_mc23_mmi;
-        c->put_h264_qpel_pixels_tab[2][15] = ff_put_h264_qpel4_mc33_mmi;
+            c->put_h264_qpel_pixels_tab[2][1] = ff_put_h264_qpel4_mc10_msa;
+            c->put_h264_qpel_pixels_tab[2][2] = ff_put_h264_qpel4_mc20_msa;
+            c->put_h264_qpel_pixels_tab[2][3] = ff_put_h264_qpel4_mc30_msa;
+            c->put_h264_qpel_pixels_tab[2][4] = ff_put_h264_qpel4_mc01_msa;
+            c->put_h264_qpel_pixels_tab[2][5] = ff_put_h264_qpel4_mc11_msa;
+            c->put_h264_qpel_pixels_tab[2][6] = ff_put_h264_qpel4_mc21_msa;
+            c->put_h264_qpel_pixels_tab[2][7] = ff_put_h264_qpel4_mc31_msa;
+            c->put_h264_qpel_pixels_tab[2][8] = ff_put_h264_qpel4_mc02_msa;
+            c->put_h264_qpel_pixels_tab[2][9] = ff_put_h264_qpel4_mc12_msa;
+            c->put_h264_qpel_pixels_tab[2][10] = ff_put_h264_qpel4_mc22_msa;
+            c->put_h264_qpel_pixels_tab[2][11] = ff_put_h264_qpel4_mc32_msa;
+            c->put_h264_qpel_pixels_tab[2][12] = ff_put_h264_qpel4_mc03_msa;
+            c->put_h264_qpel_pixels_tab[2][13] = ff_put_h264_qpel4_mc13_msa;
+            c->put_h264_qpel_pixels_tab[2][14] = ff_put_h264_qpel4_mc23_msa;
+            c->put_h264_qpel_pixels_tab[2][15] = ff_put_h264_qpel4_mc33_msa;
 
-        c->avg_h264_qpel_pixels_tab[0][0] = ff_avg_h264_qpel16_mc00_mmi;
-        c->avg_h264_qpel_pixels_tab[0][1] = ff_avg_h264_qpel16_mc10_mmi;
-        c->avg_h264_qpel_pixels_tab[0][2] = ff_avg_h264_qpel16_mc20_mmi;
-        c->avg_h264_qpel_pixels_tab[0][3] = ff_avg_h264_qpel16_mc30_mmi;
-        c->avg_h264_qpel_pixels_tab[0][4] = ff_avg_h264_qpel16_mc01_mmi;
-        c->avg_h264_qpel_pixels_tab[0][5] = ff_avg_h264_qpel16_mc11_mmi;
-        c->avg_h264_qpel_pixels_tab[0][6] = ff_avg_h264_qpel16_mc21_mmi;
-        c->avg_h264_qpel_pixels_tab[0][7] = ff_avg_h264_qpel16_mc31_mmi;
-        c->avg_h264_qpel_pixels_tab[0][8] = ff_avg_h264_qpel16_mc02_mmi;
-        c->avg_h264_qpel_pixels_tab[0][9] = ff_avg_h264_qpel16_mc12_mmi;
-        c->avg_h264_qpel_pixels_tab[0][10] = ff_avg_h264_qpel16_mc22_mmi;
-        c->avg_h264_qpel_pixels_tab[0][11] = ff_avg_h264_qpel16_mc32_mmi;
-        c->avg_h264_qpel_pixels_tab[0][12] = ff_avg_h264_qpel16_mc03_mmi;
-        c->avg_h264_qpel_pixels_tab[0][13] = ff_avg_h264_qpel16_mc13_mmi;
-        c->avg_h264_qpel_pixels_tab[0][14] = ff_avg_h264_qpel16_mc23_mmi;
-        c->avg_h264_qpel_pixels_tab[0][15] = ff_avg_h264_qpel16_mc33_mmi;
+            c->avg_h264_qpel_pixels_tab[0][0] = ff_avg_h264_qpel16_mc00_msa;
+            c->avg_h264_qpel_pixels_tab[0][1] = ff_avg_h264_qpel16_mc10_msa;
+            c->avg_h264_qpel_pixels_tab[0][2] = ff_avg_h264_qpel16_mc20_msa;
+            c->avg_h264_qpel_pixels_tab[0][3] = ff_avg_h264_qpel16_mc30_msa;
+            c->avg_h264_qpel_pixels_tab[0][4] = ff_avg_h264_qpel16_mc01_msa;
+            c->avg_h264_qpel_pixels_tab[0][5] = ff_avg_h264_qpel16_mc11_msa;
+            c->avg_h264_qpel_pixels_tab[0][6] = ff_avg_h264_qpel16_mc21_msa;
+            c->avg_h264_qpel_pixels_tab[0][7] = ff_avg_h264_qpel16_mc31_msa;
+            c->avg_h264_qpel_pixels_tab[0][8] = ff_avg_h264_qpel16_mc02_msa;
+            c->avg_h264_qpel_pixels_tab[0][9] = ff_avg_h264_qpel16_mc12_msa;
+            c->avg_h264_qpel_pixels_tab[0][10] = ff_avg_h264_qpel16_mc22_msa;
+            c->avg_h264_qpel_pixels_tab[0][11] = ff_avg_h264_qpel16_mc32_msa;
+            c->avg_h264_qpel_pixels_tab[0][12] = ff_avg_h264_qpel16_mc03_msa;
+            c->avg_h264_qpel_pixels_tab[0][13] = ff_avg_h264_qpel16_mc13_msa;
+            c->avg_h264_qpel_pixels_tab[0][14] = ff_avg_h264_qpel16_mc23_msa;
+            c->avg_h264_qpel_pixels_tab[0][15] = ff_avg_h264_qpel16_mc33_msa;
 
-        c->avg_h264_qpel_pixels_tab[1][0] = ff_avg_h264_qpel8_mc00_mmi;
-        c->avg_h264_qpel_pixels_tab[1][1] = ff_avg_h264_qpel8_mc10_mmi;
-        c->avg_h264_qpel_pixels_tab[1][2] = ff_avg_h264_qpel8_mc20_mmi;
-        c->avg_h264_qpel_pixels_tab[1][3] = ff_avg_h264_qpel8_mc30_mmi;
-        c->avg_h264_qpel_pixels_tab[1][4] = ff_avg_h264_qpel8_mc01_mmi;
-        c->avg_h264_qpel_pixels_tab[1][5] = ff_avg_h264_qpel8_mc11_mmi;
-        c->avg_h264_qpel_pixels_tab[1][6] = ff_avg_h264_qpel8_mc21_mmi;
-        c->avg_h264_qpel_pixels_tab[1][7] = ff_avg_h264_qpel8_mc31_mmi;
-        c->avg_h264_qpel_pixels_tab[1][8] = ff_avg_h264_qpel8_mc02_mmi;
-        c->avg_h264_qpel_pixels_tab[1][9] = ff_avg_h264_qpel8_mc12_mmi;
-        c->avg_h264_qpel_pixels_tab[1][10] = ff_avg_h264_qpel8_mc22_mmi;
-        c->avg_h264_qpel_pixels_tab[1][11] = ff_avg_h264_qpel8_mc32_mmi;
-        c->avg_h264_qpel_pixels_tab[1][12] = ff_avg_h264_qpel8_mc03_mmi;
-        c->avg_h264_qpel_pixels_tab[1][13] = ff_avg_h264_qpel8_mc13_mmi;
-        c->avg_h264_qpel_pixels_tab[1][14] = ff_avg_h264_qpel8_mc23_mmi;
-        c->avg_h264_qpel_pixels_tab[1][15] = ff_avg_h264_qpel8_mc33_mmi;
+            c->avg_h264_qpel_pixels_tab[1][0] = ff_avg_h264_qpel8_mc00_msa;
+            c->avg_h264_qpel_pixels_tab[1][1] = ff_avg_h264_qpel8_mc10_msa;
+            c->avg_h264_qpel_pixels_tab[1][2] = ff_avg_h264_qpel8_mc20_msa;
+            c->avg_h264_qpel_pixels_tab[1][3] = ff_avg_h264_qpel8_mc30_msa;
+            c->avg_h264_qpel_pixels_tab[1][4] = ff_avg_h264_qpel8_mc01_msa;
+            c->avg_h264_qpel_pixels_tab[1][5] = ff_avg_h264_qpel8_mc11_msa;
+            c->avg_h264_qpel_pixels_tab[1][6] = ff_avg_h264_qpel8_mc21_msa;
+            c->avg_h264_qpel_pixels_tab[1][7] = ff_avg_h264_qpel8_mc31_msa;
+            c->avg_h264_qpel_pixels_tab[1][8] = ff_avg_h264_qpel8_mc02_msa;
+            c->avg_h264_qpel_pixels_tab[1][9] = ff_avg_h264_qpel8_mc12_msa;
+            c->avg_h264_qpel_pixels_tab[1][10] = ff_avg_h264_qpel8_mc22_msa;
+            c->avg_h264_qpel_pixels_tab[1][11] = ff_avg_h264_qpel8_mc32_msa;
+            c->avg_h264_qpel_pixels_tab[1][12] = ff_avg_h264_qpel8_mc03_msa;
+            c->avg_h264_qpel_pixels_tab[1][13] = ff_avg_h264_qpel8_mc13_msa;
+            c->avg_h264_qpel_pixels_tab[1][14] = ff_avg_h264_qpel8_mc23_msa;
+            c->avg_h264_qpel_pixels_tab[1][15] = ff_avg_h264_qpel8_mc33_msa;
 
-        c->avg_h264_qpel_pixels_tab[2][0] = ff_avg_h264_qpel4_mc00_mmi;
-        c->avg_h264_qpel_pixels_tab[2][1] = ff_avg_h264_qpel4_mc10_mmi;
-        c->avg_h264_qpel_pixels_tab[2][2] = ff_avg_h264_qpel4_mc20_mmi;
-        c->avg_h264_qpel_pixels_tab[2][3] = ff_avg_h264_qpel4_mc30_mmi;
-        c->avg_h264_qpel_pixels_tab[2][4] = ff_avg_h264_qpel4_mc01_mmi;
-        c->avg_h264_qpel_pixels_tab[2][5] = ff_avg_h264_qpel4_mc11_mmi;
-        c->avg_h264_qpel_pixels_tab[2][6] = ff_avg_h264_qpel4_mc21_mmi;
-        c->avg_h264_qpel_pixels_tab[2][7] = ff_avg_h264_qpel4_mc31_mmi;
-        c->avg_h264_qpel_pixels_tab[2][8] = ff_avg_h264_qpel4_mc02_mmi;
-        c->avg_h264_qpel_pixels_tab[2][9] = ff_avg_h264_qpel4_mc12_mmi;
-        c->avg_h264_qpel_pixels_tab[2][10] = ff_avg_h264_qpel4_mc22_mmi;
-        c->avg_h264_qpel_pixels_tab[2][11] = ff_avg_h264_qpel4_mc32_mmi;
-        c->avg_h264_qpel_pixels_tab[2][12] = ff_avg_h264_qpel4_mc03_mmi;
-        c->avg_h264_qpel_pixels_tab[2][13] = ff_avg_h264_qpel4_mc13_mmi;
-        c->avg_h264_qpel_pixels_tab[2][14] = ff_avg_h264_qpel4_mc23_mmi;
-        c->avg_h264_qpel_pixels_tab[2][15] = ff_avg_h264_qpel4_mc33_mmi;
+            c->avg_h264_qpel_pixels_tab[2][0] = ff_avg_h264_qpel4_mc00_msa;
+            c->avg_h264_qpel_pixels_tab[2][1] = ff_avg_h264_qpel4_mc10_msa;
+            c->avg_h264_qpel_pixels_tab[2][2] = ff_avg_h264_qpel4_mc20_msa;
+            c->avg_h264_qpel_pixels_tab[2][3] = ff_avg_h264_qpel4_mc30_msa;
+            c->avg_h264_qpel_pixels_tab[2][4] = ff_avg_h264_qpel4_mc01_msa;
+            c->avg_h264_qpel_pixels_tab[2][5] = ff_avg_h264_qpel4_mc11_msa;
+            c->avg_h264_qpel_pixels_tab[2][6] = ff_avg_h264_qpel4_mc21_msa;
+            c->avg_h264_qpel_pixels_tab[2][7] = ff_avg_h264_qpel4_mc31_msa;
+            c->avg_h264_qpel_pixels_tab[2][8] = ff_avg_h264_qpel4_mc02_msa;
+            c->avg_h264_qpel_pixels_tab[2][9] = ff_avg_h264_qpel4_mc12_msa;
+            c->avg_h264_qpel_pixels_tab[2][10] = ff_avg_h264_qpel4_mc22_msa;
+            c->avg_h264_qpel_pixels_tab[2][11] = ff_avg_h264_qpel4_mc32_msa;
+            c->avg_h264_qpel_pixels_tab[2][12] = ff_avg_h264_qpel4_mc03_msa;
+            c->avg_h264_qpel_pixels_tab[2][13] = ff_avg_h264_qpel4_mc13_msa;
+            c->avg_h264_qpel_pixels_tab[2][14] = ff_avg_h264_qpel4_mc23_msa;
+            c->avg_h264_qpel_pixels_tab[2][15] = ff_avg_h264_qpel4_mc33_msa;
+        }
     }
 }
-#endif /* HAVE_MMI */
-
-av_cold void ff_h264qpel_init_mips(H264QpelContext *c, int bit_depth)
-{
-#if HAVE_MMI
-    h264qpel_init_mmi(c, bit_depth);
-#endif /* HAVE_MMI */
-#if HAVE_MSA
-    h264qpel_init_msa(c, bit_depth);
-#endif  // #if HAVE_MSA
-}
diff --git a/libavcodec/mips/h264qpel_mmi.c b/libavcodec/mips/h264qpel_mmi.c
index 13fbebf7f5..3482956e13 100644
--- a/libavcodec/mips/h264qpel_mmi.c
+++ b/libavcodec/mips/h264qpel_mmi.c
@@ -114,7 +114,7 @@ static void put_h264_qpel4_h_lowpass_mmi(uint8_t *dst, const uint8_t *src,
     DECLARE_VAR_LOW32;
 
     __asm__ volatile (
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
         "dli        %[tmp0],    0x04                                    \n\t"
         "1:                                                             \n\t"
         MMI_ULWC1(%[ftmp1], %[src], -0x02)
@@ -155,8 +155,8 @@ static void put_h264_qpel4_h_lowpass_mmi(uint8_t *dst, const uint8_t *src,
           [dst]"+&r"(dst),                  [src]"+&r"(src)
         : [dstStride]"r"((mips_reg)dstStride),
           [srcStride]"r"((mips_reg)srcStride),
-          [ff_pw_20]"f"(ff_pw_20),          [ff_pw_5]"f"(ff_pw_5),
-          [ff_pw_16]"f"(ff_pw_16)
+          [ff_pw_20]"f"(ff_pw_20.f),        [ff_pw_5]"f"(ff_pw_5.f),
+          [ff_pw_16]"f"(ff_pw_16.f)
         : "memory"
     );
 }
@@ -169,7 +169,7 @@ static void put_h264_qpel8_h_lowpass_mmi(uint8_t *dst, const uint8_t *src,
     DECLARE_VAR_ALL64;
 
     __asm__ volatile (
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
         "dli        %[tmp0],    0x08                                    \n\t"
         "1:                                                             \n\t"
         MMI_ULDC1(%[ftmp1], %[src], -0x02)
@@ -225,8 +225,8 @@ static void put_h264_qpel8_h_lowpass_mmi(uint8_t *dst, const uint8_t *src,
           [dst]"+&r"(dst),                  [src]"+&r"(src)
         : [dstStride]"r"((mips_reg)dstStride),
           [srcStride]"r"((mips_reg)srcStride),
-          [ff_pw_20]"f"(ff_pw_20),          [ff_pw_5]"f"(ff_pw_5),
-          [ff_pw_16]"f"(ff_pw_16)
+          [ff_pw_20]"f"(ff_pw_20.f),        [ff_pw_5]"f"(ff_pw_5.f),
+          [ff_pw_16]"f"(ff_pw_16.f)
         : "memory"
     );
 }
@@ -250,7 +250,7 @@ static void avg_h264_qpel4_h_lowpass_mmi(uint8_t *dst, const uint8_t *src,
     DECLARE_VAR_LOW32;
 
     __asm__ volatile (
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
         "dli        %[tmp0],    0x04                                    \n\t"
         "1:                                                             \n\t"
         MMI_ULWC1(%[ftmp1], %[src], -0x02)
@@ -293,8 +293,8 @@ static void avg_h264_qpel4_h_lowpass_mmi(uint8_t *dst, const uint8_t *src,
           [dst]"+&r"(dst),                  [src]"+&r"(src)
         : [dstStride]"r"((mips_reg)dstStride),
           [srcStride]"r"((mips_reg)srcStride),
-          [ff_pw_20]"f"(ff_pw_20),          [ff_pw_5]"f"(ff_pw_5),
-          [ff_pw_16]"f"(ff_pw_16)
+          [ff_pw_20]"f"(ff_pw_20.f),        [ff_pw_5]"f"(ff_pw_5.f),
+          [ff_pw_16]"f"(ff_pw_16.f)
         : "memory"
     );
 }
@@ -307,7 +307,7 @@ static void avg_h264_qpel8_h_lowpass_mmi(uint8_t *dst, const uint8_t *src,
     DECLARE_VAR_ALL64;
 
     __asm__ volatile (
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
         "dli        %[tmp0],    0x08                                    \n\t"
         "1:                                                             \n\t"
         MMI_ULDC1(%[ftmp1], %[src], -0x02)
@@ -365,8 +365,8 @@ static void avg_h264_qpel8_h_lowpass_mmi(uint8_t *dst, const uint8_t *src,
           [dst]"+&r"(dst),                  [src]"+&r"(src)
         : [dstStride]"r"((mips_reg)dstStride),
           [srcStride]"r"((mips_reg)srcStride),
-          [ff_pw_20]"f"(ff_pw_20),          [ff_pw_5]"f"(ff_pw_5),
-          [ff_pw_16]"f"(ff_pw_16)
+          [ff_pw_20]"f"(ff_pw_20.f),        [ff_pw_5]"f"(ff_pw_5.f),
+          [ff_pw_16]"f"(ff_pw_16.f)
         : "memory"
     );
 }
@@ -394,7 +394,7 @@ static void put_h264_qpel4_v_lowpass_mmi(uint8_t *dst, const uint8_t *src,
     __asm__ volatile (
         ".set       push                                                \n\t"
         ".set       noreorder                                           \n\t"
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
         "dli        %[tmp0],    0x02                                    \n\t"
         MMI_LWC1(%[ftmp1], %[src], 0x00)
         "mtc1       %[tmp0],    %[ftmp10]                               \n\t"
@@ -486,7 +486,7 @@ static void put_h264_qpel4_v_lowpass_mmi(uint8_t *dst, const uint8_t *src,
           [dst]"+&r"(dst),                  [src]"+&r"(src)
         : [dstStride]"r"((mips_reg)dstStride),
           [srcStride]"r"((mips_reg)srcStride),
-          [ff_pw_5]"f"(ff_pw_5),            [ff_pw_16]"f"(ff_pw_16)
+          [ff_pw_5]"f"(ff_pw_5.f),          [ff_pw_16]"f"(ff_pw_16.f)
         : "memory"
     );
 }
@@ -516,7 +516,7 @@ static void put_h264_qpel8_v_lowpass_mmi(uint8_t *dst, const uint8_t *src,
             PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
             MMI_LWC1(%[ftmp2], %[src], 0x00)
             PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
-            "xor        %[ftmp7],   %[ftmp7],       %[ftmp7]            \n\t"
+            "pxor       %[ftmp7],   %[ftmp7],       %[ftmp7]            \n\t"
             MMI_LWC1(%[ftmp3], %[src], 0x00)
             PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
             MMI_LWC1(%[ftmp4], %[src], 0x00)
@@ -780,7 +780,7 @@ static void put_h264_qpel8_v_lowpass_mmi(uint8_t *dst, const uint8_t *src,
               [h]"+&r"(h)
             : [dstStride]"r"((mips_reg)dstStride),
               [srcStride]"r"((mips_reg)srcStride),
-              [ff_pw_5]"f"(ff_pw_5),        [ff_pw_16]"f"(ff_pw_16)
+              [ff_pw_5]"f"(ff_pw_5.f),      [ff_pw_16]"f"(ff_pw_16.f)
             : "memory"
         );
 
@@ -812,7 +812,7 @@ static void avg_h264_qpel4_v_lowpass_mmi(uint8_t *dst, const uint8_t *src,
         ".set       push                                                \n\t"
         ".set       noreorder                                           \n\t"
         "dli        %[tmp0],    0x02                                    \n\t"
-        "xor        %[ftmp7],   %[ftmp7],       %[ftmp7]                \n\t"
+        "pxor       %[ftmp7],   %[ftmp7],       %[ftmp7]                \n\t"
         "mtc1       %[tmp0],    %[ftmp9]                                \n\t"
         "dli        %[tmp0],    0x05                                    \n\t"
         MMI_LWC1(%[ftmp0], %[src], 0x00)
@@ -909,7 +909,7 @@ static void avg_h264_qpel4_v_lowpass_mmi(uint8_t *dst, const uint8_t *src,
           [src]"+&r"(src),              [dst]"+&r"(dst)
         : [dstStride]"r"((mips_reg)dstStride),
           [srcStride]"r"((mips_reg)srcStride),
-          [ff_pw_5]"f"(ff_pw_5),        [ff_pw_16]"f"(ff_pw_16)
+          [ff_pw_5]"f"(ff_pw_5.f),      [ff_pw_16]"f"(ff_pw_16.f)
         : "memory"
     );
 }
@@ -930,7 +930,7 @@ static void avg_h264_qpel8_v_lowpass_mmi(uint8_t *dst, const uint8_t *src,
             ".set       push                                            \n\t"
             ".set       noreorder                                       \n\t"
             "dli        %[tmp0],    0x02                                \n\t"
-            "xor        %[ftmp7],   %[ftmp7],       %[ftmp7]            \n\t"
+            "pxor       %[ftmp7],   %[ftmp7],       %[ftmp7]            \n\t"
             "mtc1       %[tmp0],    %[ftmp9]                            \n\t"
             "dli        %[tmp0],    0x05                                \n\t"
             MMI_LWC1(%[ftmp0], %[src], 0x00)
@@ -1235,7 +1235,7 @@ static void avg_h264_qpel8_v_lowpass_mmi(uint8_t *dst, const uint8_t *src,
               [h]"+&r"(h)
             : [dstStride]"r"((mips_reg)dstStride),
               [srcStride]"r"((mips_reg)srcStride),
-              [ff_pw_5]"f"(ff_pw_5),        [ff_pw_16]"f"(ff_pw_16)
+              [ff_pw_5]"f"(ff_pw_5.f),      [ff_pw_16]"f"(ff_pw_16.f)
             : "memory"
         );
 
@@ -1269,7 +1269,7 @@ static void put_h264_qpel4_hv_lowpass_mmi(uint8_t *dst, const uint8_t *src,
     src -= 2*srcStride;
 
     __asm__ volatile (
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
         "dli        %[tmp0],    0x09                                    \n\t"
         "1:                                                             \n\t"
         MMI_ULWC1(%[ftmp1], %[src], -0x02)
@@ -1306,7 +1306,7 @@ static void put_h264_qpel4_hv_lowpass_mmi(uint8_t *dst, const uint8_t *src,
           [tmp]"+&r"(tmp),                  [src]"+&r"(src)
         : [tmpStride]"r"(8),
           [srcStride]"r"((mips_reg)srcStride),
-          [ff_pw_20]"f"(ff_pw_20),          [ff_pw_5]"f"(ff_pw_5)
+          [ff_pw_20]"f"(ff_pw_20.f),        [ff_pw_5]"f"(ff_pw_5.f)
         : "memory"
     );
 
@@ -1347,7 +1347,7 @@ static void put_h264_qpel8or16_hv1_lowpass_mmi(int16_t *tmp,
             MMI_ULWC1(%[ftmp0], %[src], 0x00)
             "mtc1       %[tmp0],    %[ftmp10]                           \n\t"
             PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
-            "xor        %[ftmp7],   %[ftmp7],       %[ftmp7]            \n\t"
+            "pxor       %[ftmp7],   %[ftmp7],       %[ftmp7]            \n\t"
             MMI_ULWC1(%[ftmp1], %[src], 0x00)
             PTR_ADDU   "%[src],     %[src],         %[srcStride]        \n\t"
             MMI_ULWC1(%[ftmp2], %[src], 0x00)
@@ -1567,7 +1567,7 @@ static void put_h264_qpel8or16_hv1_lowpass_mmi(int16_t *tmp,
               [src]"+&r"(src)
             : [tmp]"r"(tmp),                [size]"r"(size),
               [srcStride]"r"((mips_reg)srcStride),
-              [ff_pw_5]"f"(ff_pw_5),        [ff_pw_16]"f"(ff_pw_16)
+              [ff_pw_5]"f"(ff_pw_5.f),      [ff_pw_16]"f"(ff_pw_16.f)
             : "memory"
         );
 
@@ -1684,7 +1684,7 @@ static void put_h264_qpel8_h_lowpass_l2_mmi(uint8_t *dst, const uint8_t *src,
         "dli        %[tmp0],    0x02                                    \n\t"
         "mtc1       %[tmp0],    %[ftmp7]                                \n\t"
         "dli        %[tmp0],    0x05                                    \n\t"
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
         "mtc1       %[tmp0],    %[ftmp8]                                \n\t"
         "1:                                                             \n\t"
         MMI_ULDC1(%[ftmp1], %[src], 0x00)
@@ -1742,7 +1742,7 @@ static void put_h264_qpel8_h_lowpass_l2_mmi(uint8_t *dst, const uint8_t *src,
           [src2]"+&r"(src2),                [h]"+&r"(h)
         : [src2Stride]"r"((mips_reg)src2Stride),
           [dstStride]"r"((mips_reg)dstStride),
-          [ff_pw_5]"f"(ff_pw_5),            [ff_pw_16]"f"(ff_pw_16)
+          [ff_pw_5]"f"(ff_pw_5.f),          [ff_pw_16]"f"(ff_pw_16.f)
         : "memory"
     );
 }
@@ -1833,7 +1833,7 @@ static void avg_h264_qpel4_hv_lowpass_mmi(uint8_t *dst, const uint8_t *src,
     src -= 2*srcStride;
 
     __asm__ volatile (
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
         "dli        %[tmp0],    0x09                                    \n\t"
         "1:                                                             \n\t"
         MMI_ULWC1(%[ftmp1], %[src], -0x02)
@@ -1870,7 +1870,7 @@ static void avg_h264_qpel4_hv_lowpass_mmi(uint8_t *dst, const uint8_t *src,
           [tmp]"+&r"(tmp),                  [src]"+&r"(src)
         : [tmpStride]"r"(8),
           [srcStride]"r"((mips_reg)srcStride),
-          [ff_pw_20]"f"(ff_pw_20),          [ff_pw_5]"f"(ff_pw_5)
+          [ff_pw_20]"f"(ff_pw_20.f),        [ff_pw_5]"f"(ff_pw_5.f)
         : "memory"
     );
 
@@ -2005,7 +2005,7 @@ static void avg_h264_qpel8_h_lowpass_l2_mmi(uint8_t *dst, const uint8_t *src,
         "ori        %[tmp0],    $0,             0x8                     \n\t"
         "mtc1       %[tmp1],    %[ftmp7]                                \n\t"
         "dli        %[tmp1],    0x05                                    \n\t"
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
         "mtc1       %[tmp1],    %[ftmp8]                                \n\t"
         "1:                                                             \n\t"
         MMI_ULDC1(%[ftmp1], %[src], 0x00)
@@ -2065,7 +2065,7 @@ static void avg_h264_qpel8_h_lowpass_l2_mmi(uint8_t *dst, const uint8_t *src,
           [src2]"+&r"(src2)
         : [dstStride]"r"((mips_reg)dstStride),
           [src2Stride]"r"((mips_reg)src2Stride),
-          [ff_pw_5]"f"(ff_pw_5),            [ff_pw_16]"f"(ff_pw_16)
+          [ff_pw_5]"f"(ff_pw_5.f),          [ff_pw_16]"f"(ff_pw_16.f)
         : "memory"
     );
 }
diff --git a/libavcodec/mips/h264qpel_msa.c b/libavcodec/mips/h264qpel_msa.c
index df7e3e2a3f..e435c18750 100644
--- a/libavcodec/mips/h264qpel_msa.c
+++ b/libavcodec/mips/h264qpel_msa.c
@@ -790,8 +790,8 @@ void ff_put_h264_qpel16_mc10_msa(uint8_t *dst, const uint8_t *src,
                      minus5b, res4, res5, res6, res7);
         DPADD_SB4_SH(vec2, vec5, vec8, vec11, plus20b, plus20b, plus20b,
                      plus20b, res4, res5, res6, res7);
-        SLDI_B2_SB(src1, src3, src0, src2, src0, src2, 2);
-        SLDI_B2_SB(src5, src7, src4, src6, src4, src6, 2);
+        SLDI_B4_SB(src1, src0, src3, src2, src5, src4, src7, src6, 2,
+                   src0, src2, src4, src6);
         SRARI_H4_SH(res0, res1, res2, res3, 5);
         SRARI_H4_SH(res4, res5, res6, res7, 5);
         SAT_SH4_SH(res0, res1, res2, res3, 7);
@@ -858,8 +858,8 @@ void ff_put_h264_qpel16_mc30_msa(uint8_t *dst, const uint8_t *src,
                      minus5b, res4, res5, res6, res7);
         DPADD_SB4_SH(vec2, vec5, vec8, vec11, plus20b, plus20b, plus20b,
                      plus20b, res4, res5, res6, res7);
-        SLDI_B2_SB(src1, src3, src0, src2, src0, src2, 3);
-        SLDI_B2_SB(src5, src7, src4, src6, src4, src6, 3);
+        SLDI_B4_SB(src1, src0, src3, src2, src5, src4, src7, src6, 3,
+                   src0, src2, src4, src6);
         SRARI_H4_SH(res0, res1, res2, res3, 5);
         SRARI_H4_SH(res4, res5, res6, res7, 5);
         SAT_SH4_SH(res0, res1, res2, res3, 7);
@@ -911,10 +911,10 @@ void ff_put_h264_qpel8_mc10_msa(uint8_t *dst, const uint8_t *src,
     VSHF_B2_SB(src6, src6, src7, src7, mask2, mask2, vec10, vec11);
     DPADD_SB4_SH(vec8, vec9, vec10, vec11, plus20b, plus20b, plus20b, plus20b,
                  res4, res5, res6, res7);
-    SLDI_B2_SB(src0, src1, src0, src1, src0, src1, 2);
-    SLDI_B2_SB(src2, src3, src2, src3, src2, src3, 2);
-    SLDI_B2_SB(src4, src5, src4, src5, src4, src5, 2);
-    SLDI_B2_SB(src6, src7, src6, src7, src6, src7, 2);
+    SLDI_B4_SB(src0, src0, src1, src1, src2, src2, src3, src3, 2,
+               src0, src1, src2, src3);
+    SLDI_B4_SB(src4, src4, src5, src5, src6, src6, src7, src7, 2,
+               src4, src5, src6, src7);
     PCKEV_D2_SB(src1, src0, src3, src2, src0, src1);
     PCKEV_D2_SB(src5, src4, src7, src6, src4, src5);
     SRARI_H4_SH(res0, res1, res2, res3, 5);
@@ -966,10 +966,10 @@ void ff_put_h264_qpel8_mc30_msa(uint8_t *dst, const uint8_t *src,
     VSHF_B2_SB(src6, src6, src7, src7, mask2, mask2, vec10, vec11);
     DPADD_SB4_SH(vec8, vec9, vec10, vec11, plus20b, plus20b, plus20b, plus20b,
                  res4, res5, res6, res7);
-    SLDI_B2_SB(src0, src1, src0, src1, src0, src1, 3);
-    SLDI_B2_SB(src2, src3, src2, src3, src2, src3, 3);
-    SLDI_B2_SB(src4, src5, src4, src5, src4, src5, 3);
-    SLDI_B2_SB(src6, src7, src6, src7, src6, src7, 3);
+    SLDI_B4_SB(src0, src0, src1, src1, src2, src2, src3, src3, 3,
+               src0, src1, src2, src3);
+    SLDI_B4_SB(src4, src4, src5, src5, src6, src6, src7, src7, 3,
+               src4, src5, src6, src7);
     PCKEV_D2_SB(src1, src0, src3, src2, src0, src1);
     PCKEV_D2_SB(src5, src4, src7, src6, src4, src5);
     SRARI_H4_SH(res0, res1, res2, res3, 5);
@@ -1007,8 +1007,8 @@ void ff_put_h264_qpel4_mc10_msa(uint8_t *dst, const uint8_t *src,
     SRARI_H2_SH(res0, res1, 5);
     SAT_SH2_SH(res0, res1, 7);
     res = __msa_pckev_b((v16i8) res1, (v16i8) res0);
-    SLDI_B2_SB(src0, src1, src0, src1, src0, src1, 2);
-    SLDI_B2_SB(src2, src3, src2, src3, src2, src3, 2);
+    SLDI_B4_SB(src0, src0, src1, src1, src2, src2, src3, src3, 2,
+               src0, src1, src2, src3);
     src0 = (v16i8) __msa_insve_w((v4i32) src0, 1, (v4i32) src1);
     src1 = (v16i8) __msa_insve_w((v4i32) src2, 1, (v4i32) src3);
     src0 = (v16i8) __msa_insve_d((v2i64) src0, 1, (v2i64) src1);
@@ -1038,8 +1038,8 @@ void ff_put_h264_qpel4_mc30_msa(uint8_t *dst, const uint8_t *src,
     SRARI_H2_SH(res0, res1, 5);
     SAT_SH2_SH(res0, res1, 7);
     res = __msa_pckev_b((v16i8) res1, (v16i8) res0);
-    SLDI_B2_SB(src0, src1, src0, src1, src0, src1, 3);
-    SLDI_B2_SB(src2, src3, src2, src3, src2, src3, 3);
+    SLDI_B4_SB(src0, src0, src1, src1, src2, src2, src3, src3, 3,
+               src0, src1, src2, src3);
     src0 = (v16i8) __msa_insve_w((v4i32) src0, 1, (v4i32) src1);
     src1 = (v16i8) __msa_insve_w((v4i32) src2, 1, (v4i32) src3);
     src0 = (v16i8) __msa_insve_d((v2i64) src0, 1, (v2i64) src1);
@@ -3194,8 +3194,8 @@ void ff_avg_h264_qpel16_mc10_msa(uint8_t *dst, const uint8_t *src,
                      minus5b, res4, res5, res6, res7);
         DPADD_SB4_SH(vec2, vec5, vec8, vec11, plus20b, plus20b, plus20b,
                      plus20b, res4, res5, res6, res7);
-        SLDI_B2_SB(src1, src3, src0, src2, src0, src2, 2);
-        SLDI_B2_SB(src5, src7, src4, src6, src4, src6, 2);
+        SLDI_B4_SB(src1, src0, src3, src2, src5, src4, src7, src6, 2,
+                   src0, src2, src4, src6);
         SRARI_H4_SH(res0, res1, res2, res3, 5);
         SRARI_H4_SH(res4, res5, res6, res7, 5);
         SAT_SH4_SH(res0, res1, res2, res3, 7);
@@ -3266,8 +3266,8 @@ void ff_avg_h264_qpel16_mc30_msa(uint8_t *dst, const uint8_t *src,
                      minus5b, res4, res5, res6, res7);
         DPADD_SB4_SH(vec2, vec5, vec8, vec11, plus20b, plus20b, plus20b,
                      plus20b, res4, res5, res6, res7);
-        SLDI_B2_SB(src1, src3, src0, src2, src0, src2, 3);
-        SLDI_B2_SB(src5, src7, src4, src6, src4, src6, 3);
+        SLDI_B4_SB(src1, src0, src3, src2, src5, src4, src7, src6, 3,
+                   src0, src2, src4, src6);
         SRARI_H4_SH(res0, res1, res2, res3, 5);
         SRARI_H4_SH(res4, res5, res6, res7, 5);
         SAT_SH4_SH(res0, res1, res2, res3, 7);
@@ -3323,10 +3323,10 @@ void ff_avg_h264_qpel8_mc10_msa(uint8_t *dst, const uint8_t *src,
     VSHF_B2_SB(src6, src6, src7, src7, mask2, mask2, vec10, vec11);
     DPADD_SB4_SH(vec8, vec9, vec10, vec11, plus20b, plus20b, plus20b, plus20b,
                  res4, res5, res6, res7);
-    SLDI_B2_SB(src0, src1, src0, src1, src0, src1, 2);
-    SLDI_B2_SB(src2, src3, src2, src3, src2, src3, 2);
-    SLDI_B2_SB(src4, src5, src4, src5, src4, src5, 2);
-    SLDI_B2_SB(src6, src7, src6, src7, src6, src7, 2);
+    SLDI_B4_SB(src0, src0, src1, src1, src2, src2, src3, src3, 2,
+               src0, src1, src2, src3);
+    SLDI_B4_SB(src4, src4, src5, src5, src6, src6, src7, src7, 2,
+               src4, src5, src6, src7);
     PCKEV_D2_SB(src1, src0, src3, src2, src0, src1);
     PCKEV_D2_SB(src5, src4, src7, src6, src4, src5);
     SRARI_H4_SH(res0, res1, res2, res3, 5);
@@ -3388,10 +3388,10 @@ void ff_avg_h264_qpel8_mc30_msa(uint8_t *dst, const uint8_t *src,
     VSHF_B2_SB(src6, src6, src7, src7, mask2, mask2, vec10, vec11);
     DPADD_SB4_SH(vec8, vec9, vec10, vec11, plus20b, plus20b, plus20b, plus20b,
                  res4, res5, res6, res7);
-    SLDI_B2_SB(src0, src1, src0, src1, src0, src1, 3);
-    SLDI_B2_SB(src2, src3, src2, src3, src2, src3, 3);
-    SLDI_B2_SB(src4, src5, src4, src5, src4, src5, 3);
-    SLDI_B2_SB(src6, src7, src6, src7, src6, src7, 3);
+    SLDI_B4_SB(src0, src0, src1, src1, src2, src2, src3, src3, 3,
+               src0, src1, src2, src3);
+    SLDI_B4_SB(src4, src4, src5, src5, src6, src6, src7, src7, 3,
+               src4, src5, src6, src7);
     PCKEV_D2_SB(src1, src0, src3, src2, src0, src1);
     PCKEV_D2_SB(src5, src4, src7, src6, src4, src5);
     SRARI_H4_SH(res0, res1, res2, res3, 5);
@@ -3439,8 +3439,8 @@ void ff_avg_h264_qpel4_mc10_msa(uint8_t *dst, const uint8_t *src,
     SRARI_H2_SH(out0, out1, 5);
     SAT_SH2_SH(out0, out1, 7);
     res = __msa_pckev_b((v16i8) out1, (v16i8) out0);
-    SLDI_B2_SB(src0, src1, src0, src1, src0, src1, 2);
-    SLDI_B2_SB(src2, src3, src2, src3, src2, src3, 2);
+    SLDI_B4_SB(src0, src0, src1, src1, src2, src2, src3, src3, 2,
+               src0, src1, src2, src3);
     src0 = (v16i8) __msa_insve_w((v4i32) src0, 1, (v4i32) src1);
     src1 = (v16i8) __msa_insve_w((v4i32) src2, 1, (v4i32) src3);
     src0 = (v16i8) __msa_insve_d((v2i64) src0, 1, (v2i64) src1);
@@ -3475,8 +3475,8 @@ void ff_avg_h264_qpel4_mc30_msa(uint8_t *dst, const uint8_t *src,
     SRARI_H2_SH(out0, out1, 5);
     SAT_SH2_SH(out0, out1, 7);
     res = __msa_pckev_b((v16i8) out1, (v16i8) out0);
-    SLDI_B2_SB(src0, src1, src0, src1, src0, src1, 3);
-    SLDI_B2_SB(src2, src3, src2, src3, src2, src3, 3);
+    SLDI_B4_SB(src0, src0, src1, src1, src2, src2, src3, src3, 3,
+               src0, src1, src2, src3);
     src0 = (v16i8) __msa_insve_w((v4i32) src0, 1, (v4i32) src1);
     src1 = (v16i8) __msa_insve_w((v4i32) src2, 1, (v4i32) src3);
     src0 = (v16i8) __msa_insve_d((v2i64) src0, 1, (v2i64) src1);
diff --git a/libavcodec/mips/hevc_idct_msa.c b/libavcodec/mips/hevc_idct_msa.c
index b14aec95eb..5ab6acd1df 100644
--- a/libavcodec/mips/hevc_idct_msa.c
+++ b/libavcodec/mips/hevc_idct_msa.c
@@ -803,8 +803,9 @@ static void hevc_addblk_16x16_msa(int16_t *coeffs, uint8_t *dst, int32_t stride)
         LD_SH4((coeffs + 8), 16, in1, in3, in5, in7);
         coeffs += 64;
 
-        CLIP_SH4_0_255(dst_r0, dst_l0, dst_r1, dst_l1);
-        CLIP_SH4_0_255(dst_r2, dst_l2, dst_r3, dst_l3);
+        CLIP_SH8_0_255(dst_r0, dst_l0, dst_r1, dst_l1,
+                       dst_r2, dst_l2, dst_r3, dst_l3);
+
         PCKEV_B4_UB(dst_l0, dst_r0, dst_l1, dst_r1, dst_l2, dst_r2, dst_l3,
                     dst_r3, dst0, dst1, dst2, dst3);
         ST_UB4(dst0, dst1, dst2, dst3, dst, stride);
@@ -825,8 +826,8 @@ static void hevc_addblk_16x16_msa(int16_t *coeffs, uint8_t *dst, int32_t stride)
     dst_r3 += in6;
     dst_l3 += in7;
 
-    CLIP_SH4_0_255(dst_r0, dst_l0, dst_r1, dst_l1);
-    CLIP_SH4_0_255(dst_r2, dst_l2, dst_r3, dst_l3);
+    CLIP_SH8_0_255(dst_r0, dst_l0, dst_r1, dst_l1,
+                   dst_r2, dst_l2, dst_r3, dst_l3);
     PCKEV_B4_UB(dst_l0, dst_r0, dst_l1, dst_r1, dst_l2, dst_r2, dst_l3,
                 dst_r3, dst0, dst1, dst2, dst3);
     ST_UB4(dst0, dst1, dst2, dst3, dst, stride);
@@ -873,8 +874,8 @@ static void hevc_addblk_32x32_msa(int16_t *coeffs, uint8_t *dst, int32_t stride)
         LD_SH4((coeffs + 8), 16, in1, in3, in5, in7);
         coeffs += 64;
 
-        CLIP_SH4_0_255(dst_r0, dst_l0, dst_r1, dst_l1);
-        CLIP_SH4_0_255(dst_r2, dst_l2, dst_r3, dst_l3);
+        CLIP_SH8_0_255(dst_r0, dst_l0, dst_r1, dst_l1,
+                       dst_r2, dst_l2, dst_r3, dst_l3);
         PCKEV_B4_UB(dst_l0, dst_r0, dst_l1, dst_r1, dst_l2, dst_r2, dst_l3,
                     dst_r3, dst0, dst1, dst2, dst3);
         ST_UB2(dst0, dst1, dst, 16);
@@ -905,8 +906,8 @@ static void hevc_addblk_32x32_msa(int16_t *coeffs, uint8_t *dst, int32_t stride)
     LD_SH4(coeffs, 16, in0, in2, in4, in6);
     LD_SH4((coeffs + 8), 16, in1, in3, in5, in7);
 
-    CLIP_SH4_0_255(dst_r0, dst_l0, dst_r1, dst_l1);
-    CLIP_SH4_0_255(dst_r2, dst_l2, dst_r3, dst_l3);
+    CLIP_SH8_0_255(dst_r0, dst_l0, dst_r1, dst_l1,
+                   dst_r2, dst_l2, dst_r3, dst_l3);
     PCKEV_B4_UB(dst_l0, dst_r0, dst_l1, dst_r1, dst_l2, dst_r2, dst_l3,
                 dst_r3, dst0, dst1, dst2, dst3);
     ST_UB2(dst0, dst1, dst, 16);
@@ -928,8 +929,8 @@ static void hevc_addblk_32x32_msa(int16_t *coeffs, uint8_t *dst, int32_t stride)
     dst_r3 += in6;
     dst_l3 += in7;
 
-    CLIP_SH4_0_255(dst_r0, dst_l0, dst_r1, dst_l1);
-    CLIP_SH4_0_255(dst_r2, dst_l2, dst_r3, dst_l3);
+    CLIP_SH8_0_255(dst_r0, dst_l0, dst_r1, dst_l1,
+                   dst_r2, dst_l2, dst_r3, dst_l3);
     PCKEV_B4_UB(dst_l0, dst_r0, dst_l1, dst_r1, dst_l2, dst_r2, dst_l3,
                 dst_r3, dst0, dst1, dst2, dst3);
     ST_UB2(dst0, dst1, dst, 16);
diff --git a/libavcodec/mips/hevc_lpf_sao_msa.c b/libavcodec/mips/hevc_lpf_sao_msa.c
index ac21806404..26663dd89b 100644
--- a/libavcodec/mips/hevc_lpf_sao_msa.c
+++ b/libavcodec/mips/hevc_lpf_sao_msa.c
@@ -140,19 +140,19 @@ static void hevc_loopfilter_luma_hor_msa(uint8_t *src, int32_t stride,
             temp1 = ((p3_src + p2_src) << 1) + p2_src + temp0;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 3);
             temp2 = (v8i16) (temp1 - p2_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst0 = (v16u8) (temp2 + (v8i16) p2_src);
 
             temp1 = temp0 + p2_src;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 2);
             temp2 = (v8i16) (temp1 - p1_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst1 = (v16u8) (temp2 + (v8i16) p1_src);
 
             temp1 = (temp0 << 1) + p2_src + q1_src;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 3);
             temp2 = (v8i16) (temp1 - p0_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst2 = (v16u8) (temp2 + (v8i16) p0_src);
 
             dst0 = __msa_bmz_v(dst0, (v16u8) p2_src, (v16u8) p_is_pcm_vec);
@@ -165,19 +165,19 @@ static void hevc_loopfilter_luma_hor_msa(uint8_t *src, int32_t stride,
             temp1 = ((q3_src + q2_src) << 1) + q2_src + temp0;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 3);
             temp2 = (v8i16) (temp1 - q2_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst5 = (v16u8) (temp2 + (v8i16) q2_src);
 
             temp1 = temp0 + q2_src;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 2);
             temp2 = (v8i16) (temp1 - q1_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst4 = (v16u8) (temp2 + (v8i16) q1_src);
 
             temp1 = (temp0 << 1) + p1_src + q2_src;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 3);
             temp2 = (v8i16) (temp1 - q0_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst3 = (v16u8) (temp2 + (v8i16) q0_src);
 
             dst3 = __msa_bmz_v(dst3, (v16u8) q0_src, (v16u8) q_is_pcm_vec);
@@ -218,15 +218,15 @@ static void hevc_loopfilter_luma_hor_msa(uint8_t *src, int32_t stride,
             abs_delta0 = __msa_add_a_h(delta0, (v8i16) zero);
             abs_delta0 = (v8u16) abs_delta0 < temp1;
 
-            delta0 = CLIP_SH(delta0, tc_neg, tc_pos);
+            CLIP_SH(delta0, tc_neg, tc_pos);
 
-            temp0 = (v8u16) (delta0 + p0_src);
-            temp0 = (v8u16) CLIP_SH_0_255(temp0);
-            temp0 = (v8u16) __msa_bmz_v((v16u8) temp0, (v16u8) p0_src,
+            temp2 = (v8i16) (delta0 + p0_src);
+            CLIP_SH_0_255(temp2);
+            temp0 = (v8u16) __msa_bmz_v((v16u8) temp2, (v16u8) p0_src,
                                         (v16u8) p_is_pcm_vec);
 
             temp2 = (v8i16) (q0_src - delta0);
-            temp2 = CLIP_SH_0_255(temp2);
+            CLIP_SH_0_255(temp2);
             temp2 = (v8i16) __msa_bmz_v((v16u8) temp2, (v16u8) q0_src,
                                         (v16u8) q_is_pcm_vec);
 
@@ -252,9 +252,9 @@ static void hevc_loopfilter_luma_hor_msa(uint8_t *src, int32_t stride,
             delta1 -= (v8i16) p1_src;
             delta1 += delta0;
             delta1 >>= 1;
-            delta1 = CLIP_SH(delta1, tc_neg, tc_pos);
+            CLIP_SH(delta1, tc_neg, tc_pos);
             delta1 = (v8i16) p1_src + (v8i16) delta1;
-            delta1 = CLIP_SH_0_255(delta1);
+            CLIP_SH_0_255(delta1);
             delta1 = (v8i16) __msa_bmnz_v((v16u8) delta1, (v16u8) p1_src,
                                           (v16u8) p_is_pcm_vec);
 
@@ -262,9 +262,9 @@ static void hevc_loopfilter_luma_hor_msa(uint8_t *src, int32_t stride,
             delta2 = delta2 - (v8i16) q1_src;
             delta2 = delta2 - delta0;
             delta2 = delta2 >> 1;
-            delta2 = CLIP_SH(delta2, tc_neg, tc_pos);
+            CLIP_SH(delta2, tc_neg, tc_pos);
             delta2 = (v8i16) q1_src + (v8i16) delta2;
-            delta2 = CLIP_SH_0_255(delta2);
+            CLIP_SH_0_255(delta2);
             delta2 = (v8i16) __msa_bmnz_v((v16u8) delta2, (v16u8) q1_src,
                                           (v16u8) q_is_pcm_vec);
 
@@ -298,19 +298,19 @@ static void hevc_loopfilter_luma_hor_msa(uint8_t *src, int32_t stride,
             temp1 = ((p3_src + p2_src) << 1) + p2_src + temp0;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 3);
             temp2 = (v8i16) (temp1 - p2_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst0 = (v16u8) (temp2 + (v8i16) p2_src);
 
             temp1 = temp0 + p2_src;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 2);
             temp2 = (v8i16) (temp1 - p1_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst1 = (v16u8) (temp2 + (v8i16) p1_src);
 
             temp1 = (temp0 << 1) + p2_src + q1_src;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 3);
             temp2 = (v8i16) (temp1 - p0_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst2 = (v16u8) (temp2 + (v8i16) p0_src);
 
             dst0 = __msa_bmz_v(dst0, (v16u8) p2_src, (v16u8) p_is_pcm_vec);
@@ -323,19 +323,19 @@ static void hevc_loopfilter_luma_hor_msa(uint8_t *src, int32_t stride,
             temp1 = ((q3_src + q2_src) << 1) + q2_src + temp0;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 3);
             temp2 = (v8i16) (temp1 - q2_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst5 = (v16u8) (temp2 + (v8i16) q2_src);
 
             temp1 = temp0 + q2_src;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 2);
             temp2 = (v8i16) (temp1 - q1_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst4 = (v16u8) (temp2 + (v8i16) q1_src);
 
             temp1 = (temp0 << 1) + p1_src + q2_src;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 3);
             temp2 = (v8i16) (temp1 - q0_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst3 = (v16u8) (temp2 + (v8i16) q0_src);
 
             dst3 = __msa_bmz_v(dst3, (v16u8) q0_src, (v16u8) q_is_pcm_vec);
@@ -362,15 +362,15 @@ static void hevc_loopfilter_luma_hor_msa(uint8_t *src, int32_t stride,
             abs_delta0 = __msa_add_a_h(delta0, (v8i16) zero);
             abs_delta0 = (v8u16) abs_delta0 < temp1;
 
-            delta0 = CLIP_SH(delta0, tc_neg, tc_pos);
+            CLIP_SH(delta0, tc_neg, tc_pos);
 
-            temp0 = (v8u16) (delta0 + p0_src);
-            temp0 = (v8u16) CLIP_SH_0_255(temp0);
-            temp0 = (v8u16) __msa_bmz_v((v16u8) temp0, (v16u8) p0_src,
+            temp2 = (v8i16) (delta0 + p0_src);
+            CLIP_SH_0_255(temp2);
+            temp0 = (v8u16) __msa_bmz_v((v16u8) temp2, (v16u8) p0_src,
                                         (v16u8) p_is_pcm_vec);
 
             temp2 = (v8i16) (q0_src - delta0);
-            temp2 = CLIP_SH_0_255(temp2);
+            CLIP_SH_0_255(temp2);
             temp2 = (v8i16) __msa_bmz_v((v16u8) temp2, (v16u8) q0_src,
                                         (v16u8) q_is_pcm_vec);
 
@@ -394,9 +394,9 @@ static void hevc_loopfilter_luma_hor_msa(uint8_t *src, int32_t stride,
             delta1 -= (v8i16) p1_src;
             delta1 += delta0;
             delta1 >>= 1;
-            delta1 = CLIP_SH(delta1, tc_neg, tc_pos);
+            CLIP_SH(delta1, tc_neg, tc_pos);
             delta1 = (v8i16) p1_src + (v8i16) delta1;
-            delta1 = CLIP_SH_0_255(delta1);
+            CLIP_SH_0_255(delta1);
             delta1 = (v8i16) __msa_bmnz_v((v16u8) delta1, (v16u8) p1_src,
                                           (v16u8) p_is_pcm_vec);
 
@@ -404,9 +404,9 @@ static void hevc_loopfilter_luma_hor_msa(uint8_t *src, int32_t stride,
             delta2 = delta2 - (v8i16) q1_src;
             delta2 = delta2 - delta0;
             delta2 = delta2 >> 1;
-            delta2 = CLIP_SH(delta2, tc_neg, tc_pos);
+            CLIP_SH(delta2, tc_neg, tc_pos);
             delta2 = (v8i16) q1_src + (v8i16) delta2;
-            delta2 = CLIP_SH_0_255(delta2);
+            CLIP_SH_0_255(delta2);
             delta2 = (v8i16) __msa_bmnz_v((v16u8) delta2, (v16u8) q1_src,
                                           (v16u8) q_is_pcm_vec);
 
@@ -561,19 +561,19 @@ static void hevc_loopfilter_luma_ver_msa(uint8_t *src, int32_t stride,
             temp1 = ((p3_src + p2_src) << 1) + p2_src + temp0;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 3);
             temp2 = (v8i16) (temp1 - p2_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst0 = (v16u8) (temp2 + (v8i16) p2_src);
 
             temp1 = temp0 + p2_src;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 2);
             temp2 = (v8i16) (temp1 - p1_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst1 = (v16u8) (temp2 + (v8i16) p1_src);
 
             temp1 = (temp0 << 1) + p2_src + q1_src;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 3);
             temp2 = (v8i16) (temp1 - p0_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst2 = (v16u8) (temp2 + (v8i16) p0_src);
 
             dst0 = __msa_bmz_v(dst0, (v16u8) p2_src, (v16u8) p_is_pcm_vec);
@@ -585,19 +585,19 @@ static void hevc_loopfilter_luma_ver_msa(uint8_t *src, int32_t stride,
             temp1 = ((q3_src + q2_src) << 1) + q2_src + temp0;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 3);
             temp2 = (v8i16) (temp1 - q2_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst5 = (v16u8) (temp2 + (v8i16) q2_src);
 
             temp1 = temp0 + q2_src;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 2);
             temp2 = (v8i16) (temp1 - q1_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst4 = (v16u8) (temp2 + (v8i16) q1_src);
 
             temp1 = (temp0 << 1) + p1_src + q2_src;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 3);
             temp2 = (v8i16) (temp1 - q0_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst3 = (v16u8) (temp2 + (v8i16) q0_src);
 
             dst3 = __msa_bmz_v(dst3, (v16u8) q0_src, (v16u8) q_is_pcm_vec);
@@ -620,14 +620,14 @@ static void hevc_loopfilter_luma_ver_msa(uint8_t *src, int32_t stride,
             abs_delta0 = __msa_add_a_h(delta0, (v8i16) zero);
             abs_delta0 = (v8u16) abs_delta0 < temp1;
 
-            delta0 = CLIP_SH(delta0, tc_neg, tc_pos);
-            temp0 = (v8u16) (delta0 + p0_src);
-            temp0 = (v8u16) CLIP_SH_0_255(temp0);
-            temp0 = (v8u16) __msa_bmz_v((v16u8) temp0, (v16u8) p0_src,
+            CLIP_SH(delta0, tc_neg, tc_pos);
+            temp2 = (v8i16) (delta0 + p0_src);
+            CLIP_SH_0_255(temp2);
+            temp0 = (v8u16) __msa_bmz_v((v16u8) temp2, (v16u8) p0_src,
                                         (v16u8) p_is_pcm_vec);
 
             temp2 = (v8i16) (q0_src - delta0);
-            temp2 = CLIP_SH_0_255(temp2);
+            CLIP_SH_0_255(temp2);
             temp2 = (v8i16) __msa_bmz_v((v16u8) temp2, (v16u8) q0_src,
                                         (v16u8) q_is_pcm_vec);
 
@@ -649,9 +649,9 @@ static void hevc_loopfilter_luma_ver_msa(uint8_t *src, int32_t stride,
             delta1 -= (v8i16) p1_src;
             delta1 += delta0;
             delta1 >>= 1;
-            delta1 = CLIP_SH(delta1, tc_neg, tc_pos);
+            CLIP_SH(delta1, tc_neg, tc_pos);
             delta1 = (v8i16) p1_src + (v8i16) delta1;
-            delta1 = CLIP_SH_0_255(delta1);
+            CLIP_SH_0_255(delta1);
             delta1 = (v8i16) __msa_bmnz_v((v16u8) delta1, (v16u8) p1_src,
                                           (v16u8) p_is_pcm_vec);
 
@@ -659,9 +659,9 @@ static void hevc_loopfilter_luma_ver_msa(uint8_t *src, int32_t stride,
             delta2 = delta2 - (v8i16) q1_src;
             delta2 = delta2 - delta0;
             delta2 = delta2 >> 1;
-            delta2 = CLIP_SH(delta2, tc_neg, tc_pos);
+            CLIP_SH(delta2, tc_neg, tc_pos);
             delta2 = (v8i16) q1_src + (v8i16) delta2;
-            delta2 = CLIP_SH_0_255(delta2);
+            CLIP_SH_0_255(delta2);
             delta2 = (v8i16) __msa_bmnz_v((v16u8) delta2, (v16u8) q1_src,
                                           (v16u8) q_is_pcm_vec);
 
@@ -726,19 +726,19 @@ static void hevc_loopfilter_luma_ver_msa(uint8_t *src, int32_t stride,
             temp1 = ((p3_src + p2_src) << 1) + p2_src + temp0;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 3);
             temp2 = (v8i16) (temp1 - p2_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst0 = (v16u8) (temp2 + (v8i16) p2_src);
 
             temp1 = temp0 + p2_src;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 2);
             temp2 = (v8i16) (temp1 - p1_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst1 = (v16u8) (temp2 + (v8i16) p1_src);
 
             temp1 = (temp0 << 1) + p2_src + q1_src;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 3);
             temp2 = (v8i16) (temp1 - p0_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst2 = (v16u8) (temp2 + (v8i16) p0_src);
 
             dst0 = __msa_bmz_v(dst0, (v16u8) p2_src, (v16u8) p_is_pcm_vec);
@@ -750,19 +750,19 @@ static void hevc_loopfilter_luma_ver_msa(uint8_t *src, int32_t stride,
             temp1 = ((q3_src + q2_src) << 1) + q2_src + temp0;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 3);
             temp2 = (v8i16) (temp1 - q2_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst5 = (v16u8) (temp2 + (v8i16) q2_src);
 
             temp1 = temp0 + q2_src;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 2);
             temp2 = (v8i16) (temp1 - q1_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst4 = (v16u8) (temp2 + (v8i16) q1_src);
 
             temp1 = (temp0 << 1) + p1_src + q2_src;
             temp1 = (v8u16) __msa_srari_h((v8i16) temp1, 3);
             temp2 = (v8i16) (temp1 - q0_src);
-            temp2 = CLIP_SH(temp2, tc_neg, tc_pos);
+            CLIP_SH(temp2, tc_neg, tc_pos);
             dst3 = (v16u8) (temp2 + (v8i16) q0_src);
 
             dst3 = __msa_bmz_v(dst3, (v16u8) q0_src, (v16u8) q_is_pcm_vec);
@@ -785,15 +785,15 @@ static void hevc_loopfilter_luma_ver_msa(uint8_t *src, int32_t stride,
             abs_delta0 = __msa_add_a_h(delta0, (v8i16) zero);
             abs_delta0 = (v8u16) abs_delta0 < temp1;
 
-            delta0 = CLIP_SH(delta0, tc_neg, tc_pos);
+            CLIP_SH(delta0, tc_neg, tc_pos);
 
-            temp0 = (v8u16) (delta0 + p0_src);
-            temp0 = (v8u16) CLIP_SH_0_255(temp0);
-            temp0 = (v8u16) __msa_bmz_v((v16u8) temp0, (v16u8) p0_src,
+            temp2 = (v8i16) (delta0 + p0_src);
+            CLIP_SH_0_255(temp2);
+            temp0 = (v8u16) __msa_bmz_v((v16u8) temp2, (v16u8) p0_src,
                                         (v16u8) p_is_pcm_vec);
 
             temp2 = (v8i16) (q0_src - delta0);
-            temp2 = CLIP_SH_0_255(temp2);
+            CLIP_SH_0_255(temp2);
             temp2 = (v8i16) __msa_bmz_v((v16u8) temp2, (v16u8) q0_src,
                                         (v16u8) q_is_pcm_vec);
 
@@ -815,9 +815,9 @@ static void hevc_loopfilter_luma_ver_msa(uint8_t *src, int32_t stride,
             delta1 -= (v8i16) p1_src;
             delta1 += delta0;
             delta1 >>= 1;
-            delta1 = CLIP_SH(delta1, tc_neg, tc_pos);
+            CLIP_SH(delta1, tc_neg, tc_pos);
             delta1 = (v8i16) p1_src + (v8i16) delta1;
-            delta1 = CLIP_SH_0_255(delta1);
+            CLIP_SH_0_255(delta1);
             delta1 = (v8i16) __msa_bmnz_v((v16u8) delta1, (v16u8) p1_src,
                                           (v16u8) p_is_pcm_vec);
 
@@ -825,9 +825,9 @@ static void hevc_loopfilter_luma_ver_msa(uint8_t *src, int32_t stride,
             delta2 = delta2 - (v8i16) q1_src;
             delta2 = delta2 - delta0;
             delta2 = delta2 >> 1;
-            delta2 = CLIP_SH(delta2, tc_neg, tc_pos);
+            CLIP_SH(delta2, tc_neg, tc_pos);
             delta2 = (v8i16) q1_src + (v8i16) delta2;
-            delta2 = CLIP_SH_0_255(delta2);
+            CLIP_SH_0_255(delta2);
             delta2 = (v8i16) __msa_bmnz_v((v16u8) delta2, (v16u8) q1_src,
                                           (v16u8) q_is_pcm_vec);
             delta1 = (v8i16) __msa_bmz_v((v16u8) delta1, (v16u8) p1_src,
@@ -955,15 +955,15 @@ static void hevc_loopfilter_chroma_hor_msa(uint8_t *src, int32_t stride,
         temp0 <<= 2;
         temp0 += temp1;
         delta = __msa_srari_h((v8i16) temp0, 3);
-        delta = CLIP_SH(delta, tc_neg, tc_pos);
+        CLIP_SH(delta, tc_neg, tc_pos);
 
         temp0 = (v8i16) ((v8i16) p0 + delta);
-        temp0 = CLIP_SH_0_255(temp0);
+        CLIP_SH_0_255(temp0);
         temp0 = (v8i16) __msa_bmz_v((v16u8) temp0, (v16u8) p0,
                                     (v16u8) p_is_pcm_vec);
 
         temp1 = (v8i16) ((v8i16) q0 - delta);
-        temp1 = CLIP_SH_0_255(temp1);
+        CLIP_SH_0_255(temp1);
         temp1 = (v8i16) __msa_bmz_v((v16u8) temp1, (v16u8) q0,
                                     (v16u8) q_is_pcm_vec);
 
@@ -1014,15 +1014,15 @@ static void hevc_loopfilter_chroma_ver_msa(uint8_t *src, int32_t stride,
         temp0 <<= 2;
         temp0 += temp1;
         delta = __msa_srari_h((v8i16) temp0, 3);
-        delta = CLIP_SH(delta, tc_neg, tc_pos);
+        CLIP_SH(delta, tc_neg, tc_pos);
 
         temp0 = (v8i16) ((v8i16) p0 + delta);
-        temp0 = CLIP_SH_0_255(temp0);
+        CLIP_SH_0_255(temp0);
         temp0 = (v8i16) __msa_bmz_v((v16u8) temp0, (v16u8) p0,
                                     (v16u8) p_is_pcm_vec);
 
         temp1 = (v8i16) ((v8i16) q0 - delta);
-        temp1 = CLIP_SH_0_255(temp1);
+        CLIP_SH_0_255(temp1);
         temp1 = (v8i16) __msa_bmz_v((v16u8) temp1, (v16u8) q0,
                                     (v16u8) q_is_pcm_vec);
 
@@ -1357,6 +1357,7 @@ static void hevc_sao_edge_filter_0degree_8width_msa(uint8_t *dst,
     v16u8 cmp_minus10, diff_minus10, diff_minus11;
     v16u8 src0, src1, dst0, src_minus10, src_minus11, src_plus10, src_plus11;
     v16i8 offset, sao_offset = LD_SB(sao_offset_val);
+    v16i8 zeros = { 0 };
 
     sao_offset = __msa_pckev_b(sao_offset, sao_offset);
     src -= 1;
@@ -1367,8 +1368,8 @@ static void hevc_sao_edge_filter_0degree_8width_msa(uint8_t *dst,
     for (height -= 2; height; height -= 2) {
         src += (src_stride << 1);
 
-        SLDI_B2_0_UB(src_minus10, src_minus11, src0, src1, 1);
-        SLDI_B2_0_UB(src_minus10, src_minus11, src_plus10, src_plus11, 2);
+        SLDI_B2_UB(zeros, src_minus10, zeros, src_minus11, 1, src0, src1);
+        SLDI_B2_UB(zeros, src_minus10, zeros, src_minus11, 2, src_plus10, src_plus11);
 
         PCKEV_D2_UB(src_minus11, src_minus10, src_plus11, src_plus10,
                     src_minus10, src_plus10);
@@ -1404,8 +1405,8 @@ static void hevc_sao_edge_filter_0degree_8width_msa(uint8_t *dst,
         dst += dst_stride;
     }
 
-    SLDI_B2_0_UB(src_minus10, src_minus11, src0, src1, 1);
-    SLDI_B2_0_UB(src_minus10, src_minus11, src_plus10, src_plus11, 2);
+    SLDI_B2_UB(zeros, src_minus10, zeros, src_minus11, 1, src0, src1);
+    SLDI_B2_UB(zeros, src_minus10, zeros, src_minus11, 2, src_plus10, src_plus11);
 
     PCKEV_D2_UB(src_minus11, src_minus10, src_plus11, src_plus10, src_minus10,
                 src_plus10);
@@ -1473,14 +1474,12 @@ static void hevc_sao_edge_filter_0degree_16multiple_msa(uint8_t *dst,
             dst_ptr = dst + v_cnt;
             LD_UB4(src_minus1, src_stride, src10, src11, src12, src13);
 
-            SLDI_B2_SB(src10, src11, src_minus10, src_minus11, src_zero0,
-                       src_zero1, 1);
-            SLDI_B2_SB(src12, src13, src_minus12, src_minus13, src_zero2,
-                       src_zero3, 1);
-            SLDI_B2_SB(src10, src11, src_minus10, src_minus11, src_plus10,
-                       src_plus11, 2);
-            SLDI_B2_SB(src12, src13, src_minus12, src_minus13, src_plus12,
-                       src_plus13, 2);
+            SLDI_B4_SB(src10, src_minus10, src11, src_minus11,
+                       src12, src_minus12, src13, src_minus13, 1,
+                       src_zero0, src_zero1, src_zero2, src_zero3);
+            SLDI_B4_SB(src10, src_minus10, src11, src_minus11,
+                       src12, src_minus12, src13, src_minus13, 2,
+                       src_plus10, src_plus11, src_plus12, src_plus13);
 
             cmp_minus10 = ((v16u8) src_zero0 == src_minus10);
             cmp_plus10 = ((v16u8) src_zero0 == (v16u8) src_plus10);
@@ -1880,6 +1879,7 @@ static void hevc_sao_edge_filter_45degree_4width_msa(uint8_t *dst,
     v16u8 src_minus11, src10, src11;
     v16i8 src_plus0, src_zero0, src_plus1, src_zero1, dst0;
     v8i16 offset_mask0, offset_mask1;
+    v16i8 zeros = { 0 };
 
     sao_offset = __msa_pckev_b(sao_offset, sao_offset);
 
@@ -1892,8 +1892,8 @@ static void hevc_sao_edge_filter_45degree_4width_msa(uint8_t *dst,
     for (height -= 2; height; height -= 2) {
         src_orig += (src_stride << 1);
 
-        SLDI_B2_0_SB(src_minus11, src10, src_zero0, src_zero1, 1);
-        SLDI_B2_0_SB(src10, src11, src_plus0, src_plus1, 2);
+        SLDI_B2_SB(zeros, src_minus11, zeros, src10, 1, src_zero0, src_zero1);
+        SLDI_B2_SB(zeros, src10, zeros, src11, 2, src_plus0, src_plus1);
 
         ILVR_B2_UB(src_plus0, src_minus10, src_plus1, src_minus11, src_minus10,
                    src_minus11);
@@ -1938,8 +1938,8 @@ static void hevc_sao_edge_filter_45degree_4width_msa(uint8_t *dst,
         dst += dst_stride;
     }
 
-    SLDI_B2_0_SB(src_minus11, src10, src_zero0, src_zero1, 1);
-    SLDI_B2_0_SB(src10, src11, src_plus0, src_plus1, 2);
+    SLDI_B2_SB(zeros, src_minus11, zeros, src10, 1, src_zero0, src_zero1);
+    SLDI_B2_SB(zeros, src10, zeros, src11, 2, src_plus0, src_plus1);
 
     ILVR_B2_UB(src_plus0, src_minus10, src_plus1, src_minus11, src_minus10,
                src_minus11);
@@ -1992,6 +1992,7 @@ static void hevc_sao_edge_filter_45degree_8width_msa(uint8_t *dst,
     v16u8 src_minus10, src10, src_minus11, src11;
     v16i8 src_zero0, src_plus10, src_zero1, src_plus11, dst0;
     v8i16 offset_mask0, offset_mask1;
+    v16i8 zeros = { 0 };
 
     sao_offset = __msa_pckev_b(sao_offset, sao_offset);
     src_orig = src - 1;
@@ -2003,8 +2004,8 @@ static void hevc_sao_edge_filter_45degree_8width_msa(uint8_t *dst,
     for (height -= 2; height; height -= 2) {
         src_orig += (src_stride << 1);
 
-        SLDI_B2_0_SB(src_minus11, src10, src_zero0, src_zero1, 1);
-        SLDI_B2_0_SB(src10, src11, src_plus10, src_plus11, 2);
+        SLDI_B2_SB(zeros, src_minus11, zeros, src10, 1, src_zero0, src_zero1);
+        SLDI_B2_SB(zeros, src10, zeros, src11, 2, src_plus10, src_plus11);
 
         ILVR_B2_UB(src_plus10, src_minus10, src_plus11, src_minus11,
                    src_minus10, src_minus11);
@@ -2048,8 +2049,8 @@ static void hevc_sao_edge_filter_45degree_8width_msa(uint8_t *dst,
         dst += dst_stride;
     }
 
-    SLDI_B2_0_SB(src_minus11, src10, src_zero0, src_zero1, 1);
-    SLDI_B2_0_SB(src10, src11, src_plus10, src_plus11, 2);
+    SLDI_B2_SB(zeros, src_minus11, zeros, src10, 1, src_zero0, src_zero1);
+    SLDI_B2_SB(zeros, src10, zeros, src11, 2, src_plus10, src_plus11);
     ILVR_B2_UB(src_plus10, src_minus10, src_plus11, src_minus11, src_minus10,
                src_minus11);
     ILVR_B2_SB(src_zero0, src_zero0, src_zero1, src_zero1, src_zero0,
@@ -2130,12 +2131,11 @@ static void hevc_sao_edge_filter_45degree_16multiple_msa(uint8_t *dst,
             src_plus13 = LD_UB(src + 1 + v_cnt + (src_stride << 2));
             src_orig += 16;
 
-            SLDI_B2_SB(src10, src11, src_minus11, src_minus12, src_zero0,
-                       src_zero1, 1);
-            SLDI_B2_SB(src12, src13, src_minus13, src_minus14, src_zero2,
-                       src_zero3, 1);
-            SLDI_B2_SB(src11, src12, src_minus12, src_minus13, src_plus10,
-                       src_plus11, 2);
+            SLDI_B4_SB(src10, src_minus11, src11, src_minus12,
+                       src12, src_minus13, src13, src_minus14, 1,
+                       src_zero0, src_zero1, src_zero2, src_zero3);
+            SLDI_B2_SB(src11, src_minus12, src12, src_minus13, 2, src_plus10,
+                       src_plus11);
 
             src_plus12 = __msa_sldi_b((v16i8) src13, (v16i8) src_minus14, 2);
 
@@ -2228,6 +2228,7 @@ static void hevc_sao_edge_filter_135degree_4width_msa(uint8_t *dst,
     v16u8 cmp_minus10, diff_minus10, cmp_minus11, diff_minus11;
     v16u8 src_minus10, src10, src_minus11, src11;
     v8i16 offset_mask0, offset_mask1;
+    v16i8 zeros = { 0 };
 
     sao_offset = __msa_pckev_b(sao_offset, sao_offset);
     src_orig = src - 1;
@@ -2239,8 +2240,8 @@ static void hevc_sao_edge_filter_135degree_4width_msa(uint8_t *dst,
     for (height -= 2; height; height -= 2) {
         src_orig += (src_stride << 1);
 
-        SLDI_B2_0_SB(src_minus11, src10, src_zero0, src_zero1, 1);
-        SLDI_B2_0_UB(src_minus10, src_minus11, src_minus10, src_minus11, 2);
+        SLDI_B2_SB(zeros, src_minus11, zeros, src10, 1, src_zero0, src_zero1);
+        SLDI_B2_UB(zeros, src_minus10, zeros, src_minus11, 2, src_minus10, src_minus11);
 
         ILVR_B2_UB(src10, src_minus10, src11, src_minus11, src_minus10,
                    src_minus11);
@@ -2286,8 +2287,8 @@ static void hevc_sao_edge_filter_135degree_4width_msa(uint8_t *dst,
         dst += dst_stride;
     }
 
-    SLDI_B2_0_SB(src_minus11, src10, src_zero0, src_zero1, 1);
-    SLDI_B2_0_UB(src_minus10, src_minus11, src_minus10, src_minus11, 2);
+    SLDI_B2_SB(zeros, src_minus11, zeros, src10, 1, src_zero0, src_zero1);
+    SLDI_B2_UB(zeros, src_minus10, zeros, src_minus11, 2, src_minus10, src_minus11);
 
     ILVR_B2_UB(src10, src_minus10, src11, src_minus11, src_minus10,
                src_minus11);
@@ -2342,6 +2343,7 @@ static void hevc_sao_edge_filter_135degree_8width_msa(uint8_t *dst,
     v16u8 src_minus10, src10, src_minus11, src11;
     v16i8 src_zero0, src_zero1, dst0;
     v8i16 offset_mask0, offset_mask1;
+    v16i8 zeros = { 0 };
 
     sao_offset = __msa_pckev_b(sao_offset, sao_offset);
     src_orig = src - 1;
@@ -2353,8 +2355,8 @@ static void hevc_sao_edge_filter_135degree_8width_msa(uint8_t *dst,
     for (height -= 2; height; height -= 2) {
         src_orig += (src_stride << 1);
 
-        SLDI_B2_0_SB(src_minus11, src10, src_zero0, src_zero1, 1);
-        SLDI_B2_0_UB(src_minus10, src_minus11, src_minus10, src_minus11, 2);
+        SLDI_B2_SB(zeros, src_minus11, zeros, src10, 1, src_zero0, src_zero1);
+        SLDI_B2_UB(zeros, src_minus10, zeros, src_minus11, 2, src_minus10, src_minus11);
         ILVR_B2_UB(src10, src_minus10, src11, src_minus11, src_minus10,
                    src_minus11);
         ILVR_B2_SB(src_zero0, src_zero0, src_zero1, src_zero1, src_zero0,
@@ -2398,8 +2400,8 @@ static void hevc_sao_edge_filter_135degree_8width_msa(uint8_t *dst,
         dst += dst_stride;
     }
 
-    SLDI_B2_0_SB(src_minus11, src10, src_zero0, src_zero1, 1);
-    SLDI_B2_0_UB(src_minus10, src_minus11, src_minus10, src_minus11, 2);
+    SLDI_B2_SB(zeros, src_minus11, zeros, src10, 1, src_zero0, src_zero1);
+    SLDI_B2_UB(zeros, src_minus10, zeros, src_minus11, 2, src_minus10, src_minus11);
     ILVR_B2_UB(src10, src_minus10, src11, src_minus11, src_minus10,
                src_minus11);
     ILVR_B2_SB(src_zero0, src_zero0, src_zero1, src_zero1, src_zero0,
diff --git a/libavcodec/mips/hevc_macros_msa.h b/libavcodec/mips/hevc_macros_msa.h
index ea53812b64..02c96b752b 100644
--- a/libavcodec/mips/hevc_macros_msa.h
+++ b/libavcodec/mips/hevc_macros_msa.h
@@ -52,6 +52,15 @@
     out_m;                                                       \
 } )
 
+#define HEVC_FILT_4TAP_SW(in0, in1, filt0, filt1)                \
+( {                                                              \
+    v4i32 out_m;                                                 \
+                                                                 \
+    out_m = __msa_dotp_s_w((v8i16) in0, (v8i16) filt0);          \
+    out_m = __msa_dpadd_s_w(out_m, (v8i16) in1, (v8i16) filt1);  \
+    out_m;                                                       \
+} )
+
 #define HEVC_FILT_4TAP(in0, in1, filt0, filt1)           \
 ( {                                                      \
     v4i32 out_m;                                         \
diff --git a/libavcodec/mips/hevc_mc_bi_msa.c b/libavcodec/mips/hevc_mc_bi_msa.c
index 34613c84b8..cf932ec811 100644
--- a/libavcodec/mips/hevc_mc_bi_msa.c
+++ b/libavcodec/mips/hevc_mc_bi_msa.c
@@ -48,7 +48,7 @@ static const uint8_t ff_hevc_mask_arr[16 * 2] __attribute__((aligned(0x40))) = {
 {                                                                  \
     ADDS_SH2_SH(vec0, in0, vec1, in1, out0, out1);                 \
     SRARI_H2_SH(out0, out1, rnd_val);                              \
-    CLIP_SH2_0_255_MAX_SATU(out0, out1);                           \
+    CLIP_SH2_0_255(out0, out1);                                    \
 }
 
 #define HEVC_BI_RND_CLIP4_MAX_SATU(in0, in1, in2, in3, vec0, vec1, vec2,    \
@@ -83,7 +83,7 @@ static void hevc_bi_copy_4w_msa(uint8_t *src0_ptr,
         dst0 <<= 6;
         dst0 += in0;
         dst0 = __msa_srari_h(dst0, 7);
-        dst0 = CLIP_SH_0_255_MAX_SATU(dst0);
+        CLIP_SH_0_255(dst0);
 
         dst0 = (v8i16) __msa_pckev_b((v16i8) dst0, (v16i8) dst0);
         ST_W2(dst0, 0, 1, dst, dst_stride);
@@ -136,6 +136,7 @@ static void hevc_bi_copy_6w_msa(uint8_t *src0_ptr,
 {
     uint32_t loop_cnt;
     uint64_t tp0, tp1, tp2, tp3;
+    int32_t res = height & 0x07;
     v16u8 out0, out1, out2, out3;
     v16i8 zero = { 0 };
     v16i8 src0 = { 0 }, src1 = { 0 }, src2 = { 0 }, src3 = { 0 };
@@ -176,6 +177,45 @@ static void hevc_bi_copy_6w_msa(uint8_t *src0_ptr,
         ST_H2(out3, 2, 6, dst + 2 * dst_stride + 4, dst_stride);
         dst += (4 * dst_stride);
     }
+    if (res) {
+        LD4(src0_ptr, src_stride, tp0, tp1, tp2, tp3);
+        src0_ptr += (4 * src_stride);
+        INSERT_D2_SB(tp0, tp1, src0);
+        INSERT_D2_SB(tp2, tp3, src1);
+        LD4(src0_ptr, src_stride, tp0, tp1, tp2, tp3);
+        INSERT_D2_SB(tp0, tp1, src2);
+        INSERT_D2_SB(tp2, tp3, src3);
+        LD_SH8(src1_ptr, src2_stride, in0, in1, in2, in3, in4, in5, in6, in7);
+        ILVRL_B2_SH(zero, src0, dst0, dst1);
+        ILVRL_B2_SH(zero, src1, dst2, dst3);
+        ILVRL_B2_SH(zero, src2, dst4, dst5);
+        ILVRL_B2_SH(zero, src3, dst6, dst7);
+        SLLI_4V(dst0, dst1, dst2, dst3, 6);
+        SLLI_4V(dst4, dst5, dst6, dst7, 6);
+        HEVC_BI_RND_CLIP4_MAX_SATU(in0, in1, in2, in3, dst0, dst1, dst2, dst3,
+                                   7, dst0, dst1, dst2, dst3);
+        HEVC_BI_RND_CLIP4_MAX_SATU(in4, in5, in6, in7, dst4, dst5, dst6, dst7,
+                                   7, dst4, dst5, dst6, dst7);
+        PCKEV_B2_UB(dst1, dst0, dst3, dst2, out0, out1);
+        PCKEV_B2_UB(dst5, dst4, dst7, dst6, out2, out3);
+        if (res == 2) {
+            ST_W2(out0, 0, 2, dst, dst_stride);
+            ST_H2(out0, 2, 6, dst + 4, dst_stride);
+        } else if (res == 4) {
+            ST_W2(out0, 0, 2, dst, dst_stride);
+            ST_H2(out0, 2, 6, dst + 4, dst_stride);
+            ST_W2(out1, 0, 2, dst + 2 * dst_stride, dst_stride);
+            ST_H2(out1, 2, 6, dst + 2 * dst_stride + 4, dst_stride);
+        } else {
+            ST_W2(out0, 0, 2, dst, dst_stride);
+            ST_H2(out0, 2, 6, dst + 4, dst_stride);
+            ST_W2(out1, 0, 2, dst + 2 * dst_stride, dst_stride);
+            ST_H2(out1, 2, 6, dst + 2 * dst_stride + 4, dst_stride);
+            dst += (4 * dst_stride);
+            ST_W2(out2, 0, 2, dst, dst_stride);
+            ST_H2(out2, 2, 6, dst + 4, dst_stride);
+        }
+    }
 }
 
 static void hevc_bi_copy_8w_msa(uint8_t *src0_ptr,
@@ -536,6 +576,7 @@ static void hevc_hz_bi_8t_4w_msa(uint8_t *src0_ptr,
                                  int32_t height)
 {
     uint32_t loop_cnt;
+    int32_t res = height & 0x07;
     v8i16 filt0, filt1, filt2, filt3;
     v16i8 src0, src1, src2, src3, src4, src5, src6, src7;
     v16i8 mask1, mask2, mask3;
@@ -597,6 +638,50 @@ static void hevc_hz_bi_8t_4w_msa(uint8_t *src0_ptr,
         ST_W8(dst0, dst1, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
         dst += (8 * dst_stride);
     }
+    if (res) {
+        LD_SB8(src0_ptr, src_stride, src0, src1, src2, src3,
+               src4, src5, src6, src7);
+        LD_SH8(src1_ptr, src2_stride, in0, in1, in2, in3, in4, in5, in6, in7);
+
+        ILVR_D2_SH(in1, in0, in3, in2, in0, in1);
+        ILVR_D2_SH(in5, in4, in7, in6, in2, in3);
+        XORI_B8_128_SB(src0, src1, src2, src3, src4, src5, src6, src7);
+
+        dst0 = const_vec;
+        dst1 = const_vec;
+        dst2 = const_vec;
+        dst3 = const_vec;
+        VSHF_B2_SB(src0, src1, src2, src3, mask0, mask0, vec0, vec1);
+        VSHF_B2_SB(src4, src5, src6, src7, mask0, mask0, vec2, vec3);
+        DPADD_SB4_SH(vec0, vec1, vec2, vec3, filt0, filt0, filt0, filt0, dst0,
+                     dst1, dst2, dst3);
+        VSHF_B2_SB(src0, src1, src2, src3, mask1, mask1, vec0, vec1);
+        VSHF_B2_SB(src4, src5, src6, src7, mask1, mask1, vec2, vec3);
+        DPADD_SB4_SH(vec0, vec1, vec2, vec3, filt1, filt1, filt1, filt1, dst0,
+                     dst1, dst2, dst3);
+        VSHF_B2_SB(src0, src1, src2, src3, mask2, mask2, vec0, vec1);
+        VSHF_B2_SB(src4, src5, src6, src7, mask2, mask2, vec2, vec3);
+        DPADD_SB4_SH(vec0, vec1, vec2, vec3, filt2, filt2, filt2, filt2, dst0,
+                     dst1, dst2, dst3);
+        VSHF_B2_SB(src0, src1, src2, src3, mask3, mask3, vec0, vec1);
+        VSHF_B2_SB(src4, src5, src6, src7, mask3, mask3, vec2, vec3);
+        DPADD_SB4_SH(vec0, vec1, vec2, vec3, filt3, filt3, filt3, filt3, dst0,
+                     dst1, dst2, dst3);
+
+        HEVC_BI_RND_CLIP4(in0, in1, in2, in3,
+                          dst0, dst1, dst2, dst3, 7, dst0, dst1, dst2, dst3);
+
+        PCKEV_B2_SH(dst1, dst0, dst3, dst2, dst0, dst1);
+        if (res == 2) {
+            ST_W2(dst0, 0, 1, dst, dst_stride);
+        } else if (res == 4) {
+            ST_W4(dst0, 0, 1, 2, 3, dst, dst_stride);
+        } else {
+            ST_W4(dst0, 0, 1, 2, 3, dst, dst_stride);
+            dst += (4 * dst_stride);
+            ST_W2(dst1, 0, 1, dst, dst_stride);
+        }
+    }
 }
 
 static void hevc_hz_bi_8t_8w_msa(uint8_t *src0_ptr,
@@ -739,7 +824,7 @@ static void hevc_hz_bi_8t_12w_msa(uint8_t *src0_ptr,
         HEVC_BI_RND_CLIP2(in0, in1, dst0, dst1, 7, dst0, dst1);
         dst2 = __msa_adds_s_h(in2, dst2);
         dst2 = __msa_srari_h(dst2, 7);
-        dst2 = CLIP_SH_0_255(dst2);
+        CLIP_SH_0_255(dst2);
         PCKEV_B2_SH(dst1, dst0, dst2, dst2, dst0, dst1);
 
         tmp2 = __msa_copy_s_d((v2i64) dst0, 0);
@@ -888,7 +973,7 @@ static void hevc_hz_bi_8t_24w_msa(uint8_t *src0_ptr,
         HEVC_BI_RND_CLIP2(in0, in1, dst0, dst1, 7, dst0, dst1);
         dst2 = __msa_adds_s_h(dst2, in2);
         dst2 = __msa_srari_h(dst2, 7);
-        dst2 = CLIP_SH_0_255(dst2);
+        CLIP_SH_0_255(dst2);
 
         PCKEV_B2_SB(dst1, dst0, dst2, dst2, tmp0, tmp1);
         dst_val0 = __msa_copy_u_d((v2i64) tmp1, 0);
@@ -1182,6 +1267,7 @@ static void hevc_vt_bi_8t_4w_msa(uint8_t *src0_ptr,
                                  int32_t height)
 {
     int32_t loop_cnt;
+    int32_t res = height & 0x07;
     v16i8 src0, src1, src2, src3, src4, src5;
     v16i8 src6, src7, src8, src9, src10;
     v8i16 in0, in1, in2, in3, in4, in5, in6, in7;
@@ -1256,6 +1342,50 @@ static void hevc_vt_bi_8t_4w_msa(uint8_t *src0_ptr,
         src6554 = src14131312;
         src6 = src14;
     }
+    if (res) {
+        LD_SB8(src0_ptr, src_stride,
+               src7, src8, src9, src10, src11, src12, src13, src14);
+        LD_SH8(src1_ptr, src2_stride, in0, in1, in2, in3, in4, in5, in6, in7);
+
+        ILVR_D2_SH(in1, in0, in3, in2, in0, in1);
+        ILVR_D2_SH(in5, in4, in7, in6, in2, in3);
+        ILVR_B4_SB(src7, src6, src8, src7, src9, src8, src10, src9,
+                   src76_r, src87_r, src98_r, src109_r);
+        ILVR_B4_SB(src11, src10, src12, src11, src13, src12, src14, src13,
+                   src1110_r, src1211_r, src1312_r, src1413_r);
+        ILVR_D4_SB(src87_r, src76_r, src109_r, src98_r, src1211_r, src1110_r,
+                   src1413_r, src1312_r,
+                   src8776, src10998, src12111110, src14131312);
+        XORI_B4_128_SB(src8776, src10998, src12111110, src14131312);
+
+        dst10 = const_vec;
+        DPADD_SB4_SH(src2110, src4332, src6554, src8776,
+                     filt0, filt1, filt2, filt3, dst10, dst10, dst10, dst10);
+        dst32 = const_vec;
+        DPADD_SB4_SH(src4332, src6554, src8776, src10998,
+                     filt0, filt1, filt2, filt3, dst32, dst32, dst32, dst32);
+        dst54 = const_vec;
+        DPADD_SB4_SH(src6554, src8776, src10998, src12111110,
+                     filt0, filt1, filt2, filt3, dst54, dst54, dst54, dst54);
+        dst76 = const_vec;
+        DPADD_SB4_SH(src8776, src10998, src12111110, src14131312,
+                     filt0, filt1, filt2, filt3, dst76, dst76, dst76, dst76);
+
+        HEVC_BI_RND_CLIP4(in0, in1, in2, in3,
+                          dst10, dst32, dst54, dst76, 7,
+                          dst10, dst32, dst54, dst76);
+
+        PCKEV_B2_SH(dst32, dst10, dst76, dst54, dst10, dst54);
+        if (res == 2) {
+            ST_W2(dst10, 0, 1, dst, dst_stride);
+        } else if (res == 4) {
+            ST_W4(dst10, 0, 1, 2, 3, dst, dst_stride);
+        } else {
+            ST_W4(dst10, 0, 1, 2, 3, dst, dst_stride);
+            dst += 4 * dst_stride;
+            ST_W2(dst54, 0, 1, dst, dst_stride);
+        }
+    }
 }
 
 static void hevc_vt_bi_8t_8w_msa(uint8_t *src0_ptr,
@@ -1726,7 +1856,7 @@ static void hevc_hv_bi_8t_4w_msa(uint8_t *src0_ptr,
         ADDS_SH2_SH(out0, in0, out1, in1, out0, out1);
         ADDS_SH2_SH(out0, const_vec, out1, const_vec, out0, out1);
         SRARI_H2_SH(out0, out1, 7);
-        CLIP_SH2_0_255_MAX_SATU(out0, out1);
+        CLIP_SH2_0_255(out0, out1);
         out = (v16u8) __msa_pckev_b((v16i8) out1, (v16i8) out0);
         ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
         dst += (4 * dst_stride);
@@ -1854,7 +1984,7 @@ static void hevc_hv_bi_8t_8multx1mult_msa(uint8_t *src0_ptr,
             tmp = __msa_pckev_h((v8i16) dst0_l, (v8i16) dst0_r);
             ADDS_SH2_SH(tmp, in0, tmp, const_vec, tmp, tmp);
             tmp = __msa_srari_h(tmp, 7);
-            tmp = CLIP_SH_0_255_MAX_SATU(tmp);
+            CLIP_SH_0_255(tmp);
             out = (v16u8) __msa_pckev_b((v16i8) tmp, (v16i8) tmp);
             ST_D1(out, 0, dst_tmp);
             dst_tmp += dst_stride;
@@ -2000,7 +2130,7 @@ static void hevc_hv_bi_8t_12w_msa(uint8_t *src0_ptr,
         tmp = __msa_pckev_h((v8i16) dst0_l, (v8i16) dst0_r);
         ADDS_SH2_SH(tmp, in0, tmp, const_vec, tmp, tmp);
         tmp = __msa_srari_h(tmp, 7);
-        tmp = CLIP_SH_0_255_MAX_SATU(tmp);
+        CLIP_SH_0_255(tmp);
         out = (v16u8) __msa_pckev_b((v16i8) tmp, (v16i8) tmp);
         ST_D1(out, 0, dst_tmp);
         dst_tmp += dst_stride;
@@ -2088,7 +2218,7 @@ static void hevc_hv_bi_8t_12w_msa(uint8_t *src0_ptr,
         ADDS_SH2_SH(out0, in0, out1, in1, out0, out1);
         ADDS_SH2_SH(out0, const_vec, out1, const_vec, out0, out1);
         SRARI_H2_SH(out0, out1, 7);
-        CLIP_SH2_0_255_MAX_SATU(out0, out1);
+        CLIP_SH2_0_255(out0, out1);
         out = (v16u8) __msa_pckev_b((v16i8) out1, (v16i8) out0);
         ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
         dst += (4 * dst_stride);
@@ -2215,7 +2345,7 @@ static void hevc_hz_bi_4t_4x2_msa(uint8_t *src0_ptr,
 
     tmp0 = __msa_adds_s_h(tmp0, in0);
     tmp0 = __msa_srari_h(tmp0, 7);
-    tmp0 = CLIP_SH_0_255(tmp0);
+    CLIP_SH_0_255(tmp0);
     dst0 = __msa_pckev_b((v16i8) tmp0, (v16i8) tmp0);
 
     ST_W2(dst0, 0, 1, dst, dst_stride);
@@ -2362,6 +2492,7 @@ static void hevc_hz_bi_4t_6w_msa(uint8_t *src0_ptr,
                                  int32_t height)
 {
     uint32_t loop_cnt;
+    int32_t res = height & 0x03;
     v8i16 filt0, filt1;
     v16i8 src0, src1, src2, src3;
     v8i16 in0, in1, in2, in3;
@@ -2411,6 +2542,24 @@ static void hevc_hz_bi_4t_6w_msa(uint8_t *src0_ptr,
         ST_H2(dst1, 2, 6, dst + 2 * dst_stride + 4, dst_stride);
         dst += (4 * dst_stride);
     }
+    if (res) {
+        LD_SB2(src0_ptr, src_stride, src0, src1);
+        LD_SH2(src1_ptr, src2_stride, in0, in1);
+        XORI_B2_128_SB(src0, src1);
+
+        dst0 = const_vec;
+        dst1 = const_vec;
+        VSHF_B2_SB(src0, src0, src1, src1, mask0, mask0, vec0, vec1);
+        DPADD_SB2_SH(vec0, vec1, filt0, filt0, dst0, dst1);
+        VSHF_B2_SB(src0, src0, src1, src1, mask1, mask1, vec0, vec1);
+        DPADD_SB2_SH(vec0, vec1, filt1, filt1, dst0, dst1);
+
+        HEVC_BI_RND_CLIP2(in0, in1, dst0, dst1, 7, dst0, dst1);
+
+        dst0 = (v8i16) __msa_pckev_b((v16i8) dst1, (v16i8) dst0);
+        ST_W2(dst0, 0, 2, dst, dst_stride);
+        ST_H2(dst0, 2, 6, dst + 4, dst_stride);
+    }
 }
 
 static void hevc_hz_bi_4t_8x2_msa(uint8_t *src0_ptr,
@@ -2943,7 +3092,7 @@ static void hevc_vt_bi_4t_4x2_msa(uint8_t *src0_ptr,
     DPADD_SB2_SH(src2110, src4332, filt0, filt1, dst10, dst10);
     dst10 = __msa_adds_s_h(dst10, in0);
     dst10 = __msa_srari_h(dst10, 7);
-    dst10 = CLIP_SH_0_255(dst10);
+    CLIP_SH_0_255(dst10);
 
     dst10 = (v8i16) __msa_pckev_b((v16i8) dst10, (v16i8) dst10);
     ST_W2(dst10, 0, 1, dst, dst_stride);
@@ -3843,7 +3992,7 @@ static void hevc_hv_bi_4t_4x2_msa(uint8_t *src0_ptr,
     tmp = __msa_pckev_h((v8i16) dst1, (v8i16) dst0);
     tmp = __msa_adds_s_h(tmp, in0);
     tmp = __msa_srari_h(tmp, 7);
-    tmp = CLIP_SH_0_255_MAX_SATU(tmp);
+    CLIP_SH_0_255(tmp);
     out = (v16u8) __msa_pckev_b((v16i8) tmp, (v16i8) tmp);
     ST_W2(out, 0, 1, dst, dst_stride);
 }
@@ -3919,7 +4068,7 @@ static void hevc_hv_bi_4t_4x4_msa(uint8_t *src0_ptr,
     PCKEV_H2_SH(dst1, dst0, dst3, dst2, tmp0, tmp1);
     ADDS_SH2_SH(tmp0, in0, tmp1, in1, tmp0, tmp1);
     SRARI_H2_SH(tmp0, tmp1, 7);
-    CLIP_SH2_0_255_MAX_SATU(tmp0, tmp1);
+    CLIP_SH2_0_255(tmp0, tmp1);
     out = (v16u8) __msa_pckev_b((v16i8) tmp1, (v16i8) tmp0);
     ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
 }
@@ -4032,7 +4181,7 @@ static void hevc_hv_bi_4t_4multx8mult_msa(uint8_t *src0_ptr,
         ADDS_SH4_SH(in0, tmp0, in1, tmp1, in2, tmp2, in3, tmp3, tmp0, tmp1,
                     tmp2, tmp3);
         SRARI_H4_SH(tmp0, tmp1, tmp2, tmp3, 7);
-        CLIP_SH4_0_255_MAX_SATU(tmp0, tmp1, tmp2, tmp3);
+        CLIP_SH4_0_255(tmp0, tmp1, tmp2, tmp3);
         PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out0, out1);
         ST_W8(out0, out1, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
         dst += (8 * dst_stride);
@@ -4200,7 +4349,7 @@ static void hevc_hv_bi_4t_6w_msa(uint8_t *src0_ptr,
     ADDS_SH4_SH(in0, tmp0, in1, tmp1, in2, tmp2, in3, tmp3, tmp0, tmp1, tmp2,
                 tmp3);
     SRARI_H4_SH(tmp0, tmp1, tmp2, tmp3, 7);
-    CLIP_SH4_0_255_MAX_SATU(tmp0, tmp1, tmp2, tmp3);
+    CLIP_SH4_0_255(tmp0, tmp1, tmp2, tmp3);
     PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out0, out1);
     ST_W8(out0, out1, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
 
@@ -4212,7 +4361,7 @@ static void hevc_hv_bi_4t_6w_msa(uint8_t *src0_ptr,
     ADDS_SH2_SH(in4, const_vec, in5, const_vec, in4, in5);
     ADDS_SH2_SH(in4, tmp4, in5, tmp5, tmp4, tmp5);
     SRARI_H2_SH(tmp4, tmp5, 7);
-    CLIP_SH2_0_255_MAX_SATU(tmp4, tmp5);
+    CLIP_SH2_0_255(tmp4, tmp5);
     out2 = (v16u8) __msa_pckev_b((v16i8) tmp5, (v16i8) tmp4);
     ST_H8(out2, 0, 1, 2, 3, 4, 5, 6, 7, dst + 4, dst_stride);
 }
@@ -4286,7 +4435,7 @@ static void hevc_hv_bi_4t_8x2_msa(uint8_t *src0_ptr,
     PCKEV_H2_SH(dst0_l, dst0_r, dst1_l, dst1_r, tmp0, tmp1);
     ADDS_SH2_SH(in0, tmp0, in1, tmp1, tmp0, tmp1);
     SRARI_H2_SH(tmp0, tmp1, 7);
-    CLIP_SH2_0_255_MAX_SATU(tmp0, tmp1);
+    CLIP_SH2_0_255(tmp0, tmp1);
     out = (v16u8) __msa_pckev_b((v16i8) tmp1, (v16i8) tmp0);
     ST_D2(out, 0, 1, dst, dst_stride);
 }
@@ -4380,7 +4529,7 @@ static void hevc_hv_bi_4t_8multx4_msa(uint8_t *src0_ptr,
         ADDS_SH4_SH(in0, tmp0, in1, tmp1, in2, tmp2, in3, tmp3,
                     tmp0, tmp1, tmp2, tmp3);
         SRARI_H4_SH(tmp0, tmp1, tmp2, tmp3, 7);
-        CLIP_SH4_0_255_MAX_SATU(tmp0, tmp1, tmp2, tmp3);
+        CLIP_SH4_0_255(tmp0, tmp1, tmp2, tmp3);
         PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out0, out1);
         ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
         dst += 8;
@@ -4495,8 +4644,8 @@ static void hevc_hv_bi_4t_8x6_msa(uint8_t *src0_ptr,
     ADDS_SH2_SH(in4, tmp4, in5, tmp5, tmp4, tmp5);
     SRARI_H4_SH(tmp0, tmp1, tmp2, tmp3, 7);
     SRARI_H2_SH(tmp4, tmp5, 7);
-    CLIP_SH4_0_255_MAX_SATU(tmp0, tmp1, tmp2, tmp3);
-    CLIP_SH2_0_255_MAX_SATU(tmp4, tmp5);
+    CLIP_SH4_0_255(tmp0, tmp1, tmp2, tmp3);
+    CLIP_SH2_0_255(tmp4, tmp5);
     PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out0, out1);
     out2 = (v16u8) __msa_pckev_b((v16i8) tmp5, (v16i8) tmp4);
     ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
@@ -4610,7 +4759,7 @@ static void hevc_hv_bi_4t_8multx4mult_msa(uint8_t *src0_ptr,
             ADDS_SH4_SH(in0, tmp0, in1, tmp1, in2, tmp2, in3, tmp3,
                         tmp0, tmp1, tmp2, tmp3);
             SRARI_H4_SH(tmp0, tmp1, tmp2, tmp3, 7);
-            CLIP_SH4_0_255_MAX_SATU(tmp0, tmp1, tmp2, tmp3);
+            CLIP_SH4_0_255(tmp0, tmp1, tmp2, tmp3);
             PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out0, out1);
             ST_D4(out0, out1, 0, 1, 0, 1, dst_tmp, dst_stride);
             dst_tmp += (4 * dst_stride);
@@ -4760,7 +4909,7 @@ static void hevc_hv_bi_4t_12w_msa(uint8_t *src0_ptr,
         ADDS_SH4_SH(in0, tmp0, in1, tmp1, in2, tmp2, in3, tmp3,
                     tmp0, tmp1, tmp2, tmp3);
         SRARI_H4_SH(tmp0, tmp1, tmp2, tmp3, 7);
-        CLIP_SH4_0_255_MAX_SATU(tmp0, tmp1, tmp2, tmp3);
+        CLIP_SH4_0_255(tmp0, tmp1, tmp2, tmp3);
         PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out0, out1);
         ST_D4(out0, out1, 0, 1, 0, 1, dst_tmp, dst_stride);
         dst_tmp += (4 * dst_stride);
@@ -4846,7 +4995,7 @@ static void hevc_hv_bi_4t_12w_msa(uint8_t *src0_ptr,
         ADDS_SH4_SH(in0, tmp0, in1, tmp1, in2, tmp2, in3, tmp3,
                     tmp0, tmp1, tmp2, tmp3);
         SRARI_H4_SH(tmp0, tmp1, tmp2, tmp3, 7);
-        CLIP_SH4_0_255_MAX_SATU(tmp0, tmp1, tmp2, tmp3);
+        CLIP_SH4_0_255(tmp0, tmp1, tmp2, tmp3);
         PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out0, out1);
         ST_W8(out0, out1, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
         dst += (8 * dst_stride);
diff --git a/libavcodec/mips/hevc_mc_biw_msa.c b/libavcodec/mips/hevc_mc_biw_msa.c
index 68f122ea48..7f10214b67 100644
--- a/libavcodec/mips/hevc_mc_biw_msa.c
+++ b/libavcodec/mips/hevc_mc_biw_msa.c
@@ -42,8 +42,8 @@ static const uint8_t ff_hevc_mask_arr[16 * 2] __attribute__((aligned(0x40))) = {
     out1_l = __msa_dpadd_s_w(offset, (v8i16) out1_l, (v8i16) wgt);  \
                                                                     \
     SRAR_W4_SW(out0_r, out1_r, out0_l, out1_l, rnd);                \
+    CLIP_SW4_0_255(out0_l, out0_r, out1_l, out1_r);                 \
     PCKEV_H2_SH(out0_l, out0_r, out1_l, out1_r, out0, out1);        \
-    CLIP_SH2_0_255(out0, out1);                                     \
 }
 
 #define HEVC_BIW_RND_CLIP4(in0, in1, in2, in3, vec0, vec1, vec2, vec3,       \
@@ -65,8 +65,8 @@ static const uint8_t ff_hevc_mask_arr[16 * 2] __attribute__((aligned(0x40))) = {
     out0_l = __msa_dpadd_s_w(offset, (v8i16) out0_l, (v8i16) wgt);   \
     out1_l = __msa_dpadd_s_w(offset, (v8i16) out1_l, (v8i16) wgt);   \
     SRAR_W4_SW(out0_r, out1_r, out0_l, out1_l, rnd);                 \
+    CLIP_SW4_0_255(out0_r, out1_r, out0_l, out1_l);                  \
     PCKEV_H2_SH(out0_l, out0_r, out1_l, out1_r, out0, out1);         \
-    CLIP_SH2_0_255_MAX_SATU(out0, out1);                             \
 }
 
 #define HEVC_BIW_RND_CLIP4_MAX_SATU(in0, in1, in2, in3, vec0, vec1, vec2,  \
@@ -123,8 +123,8 @@ static void hevc_biwgt_copy_4w_msa(uint8_t *src0_ptr,
         dst0_r = __msa_dpadd_s_w(offset_vec, (v8i16) dst0_r, weight_vec);
         dst0_l = __msa_dpadd_s_w(offset_vec, (v8i16) dst0_l, weight_vec);
         SRAR_W2_SW(dst0_r, dst0_l, rnd_vec);
+        CLIP_SW2_0_255(dst0_r, dst0_l);
         dst0 = (v8i16) __msa_pckev_h((v8i16) dst0_l, (v8i16) dst0_r);
-        dst0 = CLIP_SH_0_255_MAX_SATU(dst0);
         out0 = (v16u8) __msa_pckev_b((v16i8) dst0, (v16i8) dst0);
         ST_W2(out0, 0, 1, dst, dst_stride);
     } else if (4 == height) {
@@ -182,6 +182,7 @@ static void hevc_biwgt_copy_6w_msa(uint8_t *src0_ptr,
                                    int32_t rnd_val)
 {
     uint32_t loop_cnt;
+    int32_t res = height & 0x03;
     int32_t offset, weight;
     uint64_t tp0, tp1, tp2, tp3;
     v16u8 out0, out1;
@@ -220,6 +221,27 @@ static void hevc_biwgt_copy_6w_msa(uint8_t *src0_ptr,
         ST_H2(out1, 2, 6, dst + 2 * dst_stride + 4, dst_stride);
         dst += (4 * dst_stride);
     }
+    if (res) {
+        LD4(src0_ptr, src_stride, tp0, tp1, tp2, tp3);
+        src0_ptr += (4 * src_stride);
+        INSERT_D2_SB(tp0, tp1, src0);
+        INSERT_D2_SB(tp2, tp3, src1);
+        LD_SH4(src1_ptr, src2_stride, in0, in1, in2, in3);
+        src1_ptr += (4 * src2_stride);
+        ILVRL_B2_SH(zero, src0, dst0, dst1);
+        ILVRL_B2_SH(zero, src1, dst2, dst3);
+        SLLI_4V(dst0, dst1, dst2, dst3, 6);
+        HEVC_BIW_RND_CLIP4_MAX_SATU(dst0, dst1, dst2, dst3,
+                                    in0, in1, in2, in3,
+                                    weight_vec, rnd_vec, offset_vec,
+                                    dst0, dst1, dst2, dst3);
+
+        PCKEV_B2_UB(dst1, dst0, dst3, dst2, out0, out1);
+        ST_W2(out0, 0, 2, dst, dst_stride);
+        ST_H2(out0, 2, 6, dst + 4, dst_stride);
+        ST_W2(out1, 0, 2, dst + 2 * dst_stride, dst_stride);
+        ST_H2(out1, 2, 6, dst + 2 * dst_stride + 4, dst_stride);
+    }
 }
 
 static void hevc_biwgt_copy_8w_msa(uint8_t *src0_ptr,
@@ -340,7 +362,7 @@ static void hevc_biwgt_copy_12w_msa(uint8_t *src0_ptr,
     weight_vec = __msa_fill_w(weight);
     rnd_vec = __msa_fill_w(rnd_val + 1);
 
-    for (loop_cnt = (16 >> 2); loop_cnt--;) {
+    for (loop_cnt = (height >> 2); loop_cnt--;) {
         LD_SB4(src0_ptr, src_stride, src0, src1, src2, src3);
         src0_ptr += (4 * src_stride);
         LD_SH4(src1_ptr, src2_stride, in0, in1, in2, in3);
@@ -1069,8 +1091,8 @@ static void hevc_hz_biwgt_8t_24w_msa(uint8_t *src0_ptr,
         dst2_l = __msa_dpadd_s_w(offset_vec, (v8i16) dst2_l,
                                  (v8i16) weight_vec);
         SRAR_W2_SW(dst2_r, dst2_l, rnd_vec);
-        dst2_r = (v4i32) __msa_pckev_h((v8i16) dst2_l, (v8i16) dst2_r);
-        out2 = CLIP_SH_0_255(dst2_r);
+        CLIP_SW2_0_255(dst2_r, dst2_l);
+        out2 = __msa_pckev_h((v8i16) dst2_l, (v8i16) dst2_r);
 
         LD_SB2(src0_ptr, 16, src0, src1);
         src0_ptr += src_stride;
@@ -1100,8 +1122,8 @@ static void hevc_hz_biwgt_8t_24w_msa(uint8_t *src0_ptr,
     dst2_r = __msa_dpadd_s_w(offset_vec, (v8i16) dst2_r, (v8i16) weight_vec);
     dst2_l = __msa_dpadd_s_w(offset_vec, (v8i16) dst2_l, (v8i16) weight_vec);
     SRAR_W2_SW(dst2_r, dst2_l, rnd_vec);
-    dst2_r = (v4i32) __msa_pckev_h((v8i16) dst2_l, (v8i16) dst2_r);
-    out2 = CLIP_SH_0_255(dst2_r);
+    CLIP_SW2_0_255(dst2_r, dst2_l);
+    out2 = __msa_pckev_h((v8i16) dst2_l, (v8i16) dst2_r);
     PCKEV_B2_SH(out1, out0, out2, out2, out0, out2);
     dst_val0 = __msa_copy_u_d((v2i64) out2, 0);
     ST_SH(out0, dst);
@@ -1413,6 +1435,7 @@ static void hevc_vt_biwgt_8t_4w_msa(uint8_t *src0_ptr,
                                     int32_t rnd_val)
 {
     uint32_t loop_cnt;
+    int32_t res = height & 0x07;
     int32_t offset, weight;
     v16i8 src0, src1, src2, src3, src4, src5, src6, src7, src8, src9, src10;
     v16i8 src11, src12, src13, src14;
@@ -1489,6 +1512,46 @@ static void hevc_vt_biwgt_8t_4w_msa(uint8_t *src0_ptr,
         ST_W8(out0, out1, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
         dst += (8 * dst_stride);
 
+        src2110 = src10998;
+        src4332 = src12111110;
+        src6554 = src14131312;
+        src6 = src14;
+    }
+    if (res) {
+       LD_SB8(src0_ptr, src_stride,
+               src7, src8, src9, src10, src11, src12, src13, src14);
+        src0_ptr += (8 * src_stride);
+        LD_SH8(src1_ptr, src2_stride, in0, in1, in2, in3, in4, in5, in6, in7);
+        src1_ptr += (8 * src2_stride);
+
+        ILVR_D2_SH(in1, in0, in3, in2, in0, in1);
+        ILVR_D2_SH(in5, in4, in7, in6, in2, in3);
+        ILVR_B4_SB(src7, src6, src8, src7, src9, src8, src10, src9,
+                   src76_r, src87_r, src98_r, src109_r);
+        ILVR_B4_SB(src11, src10, src12, src11, src13, src12, src14, src13,
+                   src1110_r, src1211_r, src1312_r, src1413_r);
+        ILVR_D4_SB(src87_r, src76_r, src109_r, src98_r, src1211_r, src1110_r,
+                   src1413_r, src1312_r,
+                   src8776, src10998, src12111110, src14131312);
+        XORI_B4_128_SB(src8776, src10998, src12111110, src14131312);
+
+        DOTP_SB4_SH(src2110, src4332, src6554, src8776, filt0, filt0, filt0,
+                    filt0, dst10, dst32, dst54, dst76);
+        DPADD_SB4_SH(src4332, src6554, src8776, src10998, filt1, filt1, filt1,
+                     filt1, dst10, dst32, dst54, dst76);
+        DPADD_SB4_SH(src6554, src8776, src10998, src12111110, filt2, filt2,
+                     filt2, filt2, dst10, dst32, dst54, dst76);
+        DPADD_SB4_SH(src8776, src10998, src12111110, src14131312, filt3, filt3,
+                     filt3, filt3, dst10, dst32, dst54, dst76);
+
+        HEVC_BIW_RND_CLIP4(dst10, dst32, dst54, dst76,
+                           in0, in1, in2, in3,
+                           weight_vec, rnd_vec, offset_vec,
+                           out0, out1, out2, out3);
+
+        PCKEV_B2_SH(out1, out0, out3, out2, out0, out1);
+        ST_W8(out0, out1, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
+
         src2110 = src10998;
         src4332 = src12111110;
         src6554 = src14131312;
@@ -1674,8 +1737,8 @@ static void hevc_vt_biwgt_8t_12w_msa(uint8_t *src0_ptr,
         dst2_l = __msa_dpadd_s_w(offset_vec, (v8i16) dst2_l,
                                  (v8i16) weight_vec);
         SRAR_W2_SW(dst2_r, dst2_l, rnd_vec);
-        dst2_r = (v4i32) __msa_pckev_h((v8i16) dst2_l, (v8i16) dst2_r);
-        out2 = CLIP_SH_0_255(dst2_r);
+        CLIP_SW2_0_255(dst2_r, dst2_l);
+        out2 = __msa_pckev_h((v8i16) dst2_l, (v8i16) dst2_r);
         PCKEV_B2_SH(out1, out0, out2, out2, out0, out2);
         ST_D2(out0, 0, 1, dst, dst_stride);
         ST_W2(out2, 0, 1, dst + 8, dst_stride);
@@ -2048,7 +2111,7 @@ static void hevc_hv_biwgt_8t_4w_msa(uint8_t *src0_ptr,
         dst2 = __msa_dpadd_s_w(offset_vec, tmp2, weight_vec);
         dst3 = __msa_dpadd_s_w(offset_vec, tmp3, weight_vec);
         SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
-        CLIP_SW4_0_255_MAX_SATU(dst0, dst1, dst2, dst3);
+        CLIP_SW4_0_255(dst0, dst1, dst2, dst3);
         PCKEV_H2_SH(dst1, dst0, dst3, dst2, tmp0, tmp1);
         out = (v16u8) __msa_pckev_b((v16i8) tmp1, (v16i8) tmp0);
         ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
@@ -2226,7 +2289,7 @@ static void hevc_hv_biwgt_8t_8multx2mult_msa(uint8_t *src0_ptr,
             dst1_r = __msa_dpadd_s_w(offset_vec, tmp2, weight_vec);
             dst1_l = __msa_dpadd_s_w(offset_vec, tmp3, weight_vec);
             SRAR_W4_SW(dst0_l, dst0_r, dst1_l, dst1_r, rnd_vec);
-            CLIP_SW4_0_255_MAX_SATU(dst0_l, dst0_r, dst1_l, dst1_r);
+            CLIP_SW4_0_255(dst0_l, dst0_r, dst1_l, dst1_r);
             PCKEV_H2_SH(dst0_l, dst0_r, dst1_l, dst1_r, tmp0, tmp1);
             out = (v16u8) __msa_pckev_b((v16i8) tmp1, (v16i8) tmp0);
             ST_D2(out, 0, 1, dst_tmp, dst_stride);
@@ -2412,7 +2475,7 @@ static void hevc_hv_biwgt_8t_12w_msa(uint8_t *src0_ptr,
         dst2 = __msa_dpadd_s_w(offset_vec, tmp2, weight_vec);
         dst3 = __msa_dpadd_s_w(offset_vec, tmp3, weight_vec);
         SRAR_W4_SW(dst1, dst0, dst3, dst2, rnd_vec);
-        CLIP_SW4_0_255_MAX_SATU(dst1, dst0, dst3, dst2);
+        CLIP_SW4_0_255(dst1, dst0, dst3, dst2);
         PCKEV_H2_SH(dst1, dst0, dst3, dst2, tmp0, tmp1);
         out = (v16u8) __msa_pckev_b((v16i8) tmp1, (v16i8) tmp0);
         ST_D2(out, 0, 1, dst_tmp, dst_stride);
@@ -2503,7 +2566,7 @@ static void hevc_hv_biwgt_8t_12w_msa(uint8_t *src0_ptr,
         dst2 = __msa_dpadd_s_w(offset_vec, tmp2, weight_vec);
         dst3 = __msa_dpadd_s_w(offset_vec, tmp3, weight_vec);
         SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
-        CLIP_SW4_0_255_MAX_SATU(dst0, dst1, dst2, dst3);
+        CLIP_SW4_0_255(dst0, dst1, dst2, dst3);
         PCKEV_H2_SH(dst1, dst0, dst3, dst2, tmp0, tmp1);
         out = (v16u8) __msa_pckev_b((v16i8) tmp1, (v16i8) tmp0);
         ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
@@ -2683,8 +2746,8 @@ static void hevc_hz_biwgt_4t_4x2_msa(uint8_t *src0_ptr,
     dst0_r = __msa_dpadd_s_w(offset_vec, (v8i16) dst0_r, (v8i16) weight_vec);
     dst0_l = __msa_dpadd_s_w(offset_vec, (v8i16) dst0_l, (v8i16) weight_vec);
     SRAR_W2_SW(dst0_r, dst0_l, rnd_vec);
-    dst0_r = (v4i32) __msa_pckev_h((v8i16) dst0_l, (v8i16) dst0_r);
-    out0 = CLIP_SH_0_255(dst0_r);
+    out0 = __msa_pckev_h((v8i16) dst0_l, (v8i16) dst0_r);
+    CLIP_SH_0_255(out0);
     out0 = (v8i16) __msa_pckev_b((v16i8) out0, (v16i8) out0);
     ST_W2(out0, 0, 1, dst, dst_stride);
 }
@@ -3554,8 +3617,8 @@ static void hevc_vt_biwgt_4t_4x2_msa(uint8_t *src0_ptr,
     dst10_r = __msa_dpadd_s_w(offset_vec, (v8i16) dst10_r, (v8i16) weight_vec);
     dst10_l = __msa_dpadd_s_w(offset_vec, (v8i16) dst10_l, (v8i16) weight_vec);
     SRAR_W2_SW(dst10_r, dst10_l, rnd_vec);
-    dst10_r = (v4i32) __msa_pckev_h((v8i16) dst10_l, (v8i16) dst10_r);
-    out = CLIP_SH_0_255(dst10_r);
+    CLIP_SW2_0_255(dst10_r, dst10_l);
+    out = __msa_pckev_h((v8i16) dst10_l, (v8i16) dst10_r);
     out = (v8i16) __msa_pckev_b((v16i8) out, (v16i8) out);
     ST_W2(out, 0, 1, dst, dst_stride);
 }
@@ -3759,6 +3822,7 @@ static void hevc_vt_biwgt_4t_6w_msa(uint8_t *src0_ptr,
                                     int32_t rnd_val)
 {
     uint32_t loop_cnt;
+    int32_t res = height & 0x03;
     int32_t offset, weight, constant;
     v16i8 src0, src1, src2, src3, src4;
     v8i16 in0, in1, in2, in3;
@@ -3819,6 +3883,35 @@ static void hevc_vt_biwgt_4t_6w_msa(uint8_t *src0_ptr,
         ST_H2(tmp1, 2, 6, dst + 2 * dst_stride + 4, dst_stride);
         dst += (4 * dst_stride);
     }
+    if (res) {
+        LD_SB2(src0_ptr, src_stride, src3, src4);
+        src0_ptr += (2 * src_stride);
+        LD_SH4(src1_ptr, src2_stride, in0, in1, in2, in3);
+        src1_ptr += (4 * src2_stride);
+        XORI_B2_128_SB(src3, src4);
+        ILVR_B2_SB(src3, src2, src4, src3, src32_r, src43_r);
+
+        tmp0 = HEVC_FILT_4TAP_SH(src10_r, src32_r, filt0, filt1);
+        tmp1 = HEVC_FILT_4TAP_SH(src21_r, src43_r, filt0, filt1);
+
+        LD_SB2(src0_ptr, src_stride, src1, src2);
+        src0_ptr += (2 * src_stride);
+        XORI_B2_128_SB(src1, src2);
+        ILVR_B2_SB(src1, src4, src2, src1, src10_r, src21_r);
+
+        tmp2 = HEVC_FILT_4TAP_SH(src32_r, src10_r, filt0, filt1);
+        tmp3 = HEVC_FILT_4TAP_SH(src43_r, src21_r, filt0, filt1);
+        HEVC_BIW_RND_CLIP4(tmp0, tmp1, tmp2, tmp3,
+                           in0, in1, in2, in3,
+                           weight_vec, rnd_vec, offset_vec,
+                           tmp0, tmp1, tmp2, tmp3);
+
+        PCKEV_B2_SH(tmp1, tmp0, tmp3, tmp2, tmp0, tmp1);
+        ST_W2(tmp0, 0, 2, dst, dst_stride);
+        ST_H2(tmp0, 2, 6, dst + 4, dst_stride);
+        ST_W2(tmp1, 0, 2, dst + 2 * dst_stride, dst_stride);
+        ST_H2(tmp1, 2, 6, dst + 2 * dst_stride + 4, dst_stride);
+    }
 }
 
 static void hevc_vt_biwgt_4t_8x2_msa(uint8_t *src0_ptr,
@@ -4574,8 +4667,8 @@ static void hevc_hv_biwgt_4t_4x2_msa(uint8_t *src0_ptr,
     dst0 = __msa_dpadd_s_w(offset_vec, tmp0, weight_vec);
     dst1 = __msa_dpadd_s_w(offset_vec, tmp1, weight_vec);
     SRAR_W2_SW(dst0, dst1, rnd_vec);
+    CLIP_SW2_0_255(dst0, dst1);
     tmp = __msa_pckev_h((v8i16) dst1, (v8i16) dst0);
-    tmp = CLIP_SH_0_255_MAX_SATU(tmp);
     out = (v16u8) __msa_pckev_b((v16i8) tmp, (v16i8) tmp);
     ST_W2(out, 0, 1, dst, dst_stride);
 }
@@ -4671,8 +4764,8 @@ static void hevc_hv_biwgt_4t_4x4_msa(uint8_t *src0_ptr,
     dst2 = __msa_dpadd_s_w(offset_vec, tmp2, weight_vec);
     dst3 = __msa_dpadd_s_w(offset_vec, tmp3, weight_vec);
     SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
+    CLIP_SW4_0_255(dst0, dst1, dst2, dst3);
     PCKEV_H2_SH(dst1, dst0, dst3, dst2, tmp0, tmp1);
-    CLIP_SH2_0_255_MAX_SATU(tmp0, tmp1);
     out = (v16u8) __msa_pckev_b((v16i8) tmp1, (v16i8) tmp0);
     ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
 }
@@ -4808,9 +4901,10 @@ static void hevc_hv_biwgt_4t_4multx8mult_msa(uint8_t *src0_ptr,
         dst7 = __msa_dpadd_s_w(offset_vec, tmp7, weight_vec);
         SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
         SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
+        CLIP_SW4_0_255(dst0, dst1, dst2, dst3);
+        CLIP_SW4_0_255(dst4, dst5, dst6, dst7);
         PCKEV_H4_SH(dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6, tmp0, tmp1,
                     tmp2, tmp3);
-        CLIP_SH4_0_255_MAX_SATU(tmp0, tmp1, tmp2, tmp3);
         PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out0, out1);
         ST_W8(out0, out1, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
         dst += (8 * dst_stride);
@@ -5006,9 +5100,10 @@ static void hevc_hv_biwgt_4t_6w_msa(uint8_t *src0_ptr,
     dst7 = __msa_dpadd_s_w(offset_vec, tmp7, weight_vec);
     SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
     SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
+    CLIP_SW4_0_255(dst0, dst1, dst2, dst3);
+    CLIP_SW4_0_255(dst4, dst5, dst6, dst7);
     PCKEV_H4_SH(dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6, tmp0, tmp1,
                 tmp2, tmp3);
-    CLIP_SH4_0_255_MAX_SATU(tmp0, tmp1, tmp2, tmp3);
     PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out0, out1);
     ST_W8(out0, out1, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
 
@@ -5028,9 +5123,9 @@ static void hevc_hv_biwgt_4t_6w_msa(uint8_t *src0_ptr,
     dst2 = __msa_dpadd_s_w(offset_vec, tmp2, weight_vec);
     dst3 = __msa_dpadd_s_w(offset_vec, tmp3, weight_vec);
     SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
+    CLIP_SW4_0_255(dst0, dst1, dst2, dst3);
     PCKEV_H2_SH(dst1, dst0, dst3, dst2, tmp4, tmp5);
 
-    CLIP_SH2_0_255_MAX_SATU(tmp4, tmp5);
     out2 = (v16u8) __msa_pckev_b((v16i8) tmp5, (v16i8) tmp4);
     ST_H8(out2, 0, 1, 2, 3, 4, 5, 6, 7, dst + 4, dst_stride);
 }
@@ -5125,8 +5220,8 @@ static void hevc_hv_biwgt_4t_8x2_msa(uint8_t *src0_ptr,
     dst1_r = __msa_dpadd_s_w(offset_vec, tmp2, weight_vec);
     dst1_l = __msa_dpadd_s_w(offset_vec, tmp3, weight_vec);
     SRAR_W4_SW(dst0_r, dst0_l, dst1_r, dst1_l, rnd_vec);
+    CLIP_SW4_0_255(dst0_r, dst0_l, dst1_r, dst1_l);
     PCKEV_H2_SH(dst0_l, dst0_r, dst1_l, dst1_r, tmp0, tmp1);
-    CLIP_SH2_0_255_MAX_SATU(tmp0, tmp1);
     out = (v16u8) __msa_pckev_b((v16i8) tmp1, (v16i8) tmp0);
     ST_D2(out, 0, 1, dst, dst_stride);
 }
@@ -5246,9 +5341,10 @@ static void hevc_hv_biwgt_4t_8multx4_msa(uint8_t *src0_ptr,
         dst7 = __msa_dpadd_s_w(offset_vec, tmp7, weight_vec);
         SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
         SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
+        CLIP_SW4_0_255(dst0, dst1, dst2, dst3);
+        CLIP_SW4_0_255(dst4, dst5, dst6, dst7);
         PCKEV_H4_SH(dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
                     tmp0, tmp1, tmp2, tmp3);
-        CLIP_SH4_0_255_MAX_SATU(tmp0, tmp1, tmp2, tmp3);
         PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out0, out1);
         ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
         dst += 8;
@@ -5385,9 +5481,10 @@ static void hevc_hv_biwgt_4t_8x6_msa(uint8_t *src0_ptr,
     dst7 = __msa_dpadd_s_w(offset_vec, tmp7, weight_vec);
     SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
     SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
+    CLIP_SW4_0_255(dst0, dst1, dst2, dst3);
+    CLIP_SW4_0_255(dst4, dst5, dst6, dst7);
     PCKEV_H4_SH(dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
                 tmp0, tmp1, tmp2, tmp3);
-    CLIP_SH4_0_255_MAX_SATU(tmp0, tmp1, tmp2, tmp3);
     PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out0, out1);
 
     PCKEV_H2_SW(dst4_l, dst4_r, dst5_l, dst5_r, dst0, dst1);
@@ -5398,8 +5495,8 @@ static void hevc_hv_biwgt_4t_8x6_msa(uint8_t *src0_ptr,
     dst2 = __msa_dpadd_s_w(offset_vec, tmp2, weight_vec);
     dst3 = __msa_dpadd_s_w(offset_vec, tmp3, weight_vec);
     SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
+    CLIP_SW4_0_255(dst0, dst1, dst2, dst3);
     PCKEV_H2_SH(dst1, dst0, dst3, dst2, tmp4, tmp5);
-    CLIP_SH2_0_255_MAX_SATU(tmp4, tmp5);
     out2 = (v16u8) __msa_pckev_b((v16i8) tmp5, (v16i8) tmp4);
     ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
     ST_D2(out2, 0, 1, dst + 4 * dst_stride, dst_stride);
@@ -5535,9 +5632,10 @@ static void hevc_hv_biwgt_4t_8multx4mult_msa(uint8_t *src0_ptr,
             dst7 = __msa_dpadd_s_w(offset_vec, tmp7, weight_vec);
             SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
             SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
+            CLIP_SW4_0_255(dst0, dst1, dst2, dst3);
+            CLIP_SW4_0_255(dst4, dst5, dst6, dst7);
             PCKEV_H4_SH(dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
                         tmp0, tmp1, tmp2, tmp3);
-            CLIP_SH4_0_255_MAX_SATU(tmp0, tmp1, tmp2, tmp3);
             PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out0, out1);
             ST_D4(out0, out1, 0, 1, 0, 1, dst_tmp, dst_stride);
             dst_tmp += (4 * dst_stride);
@@ -5722,9 +5820,10 @@ static void hevc_hv_biwgt_4t_12w_msa(uint8_t *src0_ptr,
         dst7 = __msa_dpadd_s_w(offset_vec, tmp7, weight_vec);
         SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
         SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
+        CLIP_SW4_0_255(dst0, dst1, dst2, dst3);
+        CLIP_SW4_0_255(dst4, dst5, dst6, dst7);
         PCKEV_H4_SH(dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
                     tmp0, tmp1, tmp2, tmp3);
-        CLIP_SH4_0_255_MAX_SATU(tmp0, tmp1, tmp2, tmp3);
         PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out0, out1);
         ST_D4(out0, out1, 0, 1, 0, 1, dst_tmp, dst_stride);
         dst_tmp += (4 * dst_stride);
@@ -5818,9 +5917,10 @@ static void hevc_hv_biwgt_4t_12w_msa(uint8_t *src0_ptr,
         dst7 = __msa_dpadd_s_w(offset_vec, tmp7, weight_vec);
         SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
         SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
+        CLIP_SW4_0_255(dst0, dst1, dst2, dst3);
+        CLIP_SW4_0_255(dst4, dst5, dst6, dst7);
         PCKEV_H4_SH(dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
                     tmp0, tmp1, tmp2, tmp3);
-        CLIP_SH4_0_255_MAX_SATU(tmp0, tmp1, tmp2, tmp3);
         PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out0, out1);
         ST_W8(out0, out1, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
         dst += (8 * dst_stride);
diff --git a/libavcodec/mips/hevc_mc_uni_msa.c b/libavcodec/mips/hevc_mc_uni_msa.c
index 36e65527af..fbdaffc716 100644
--- a/libavcodec/mips/hevc_mc_uni_msa.c
+++ b/libavcodec/mips/hevc_mc_uni_msa.c
@@ -908,6 +908,7 @@ static void common_vt_8t_4w_msa(uint8_t *src, int32_t src_stride,
                                 const int8_t *filter, int32_t height)
 {
     uint32_t loop_cnt;
+    uint32_t res = (height & 0x07) >> 1;
     v16u8 out0, out1;
     v16i8 src0, src1, src2, src3, src4, src5, src6, src7, src8, src9, src10;
     v16i8 src11, src12, src13, src14;
@@ -970,6 +971,27 @@ static void common_vt_8t_4w_msa(uint8_t *src, int32_t src_stride,
         src6554 = src14131312;
         src6 = src14;
     }
+    for (; res--; ) {
+        LD_SB2(src, src_stride, src7, src8);
+        src += 2 * src_stride;
+        ILVR_B2_SB(src7, src6, src8, src7, src76_r, src87_r);
+        src8776 = (v16i8)__msa_ilvr_d((v2i64) src87_r, (v2i64) src76_r);
+        src8776 = (v16i8)__msa_xori_b(src8776, 128);
+        out10 = (v8i16)__msa_dotp_s_h((v16i8) src2110, (v16i8) filt0);
+        out10 = (v8i16)__msa_dpadd_s_h((v8i16) out10, src4332, filt1);
+        out10 = (v8i16)__msa_dpadd_s_h((v8i16) out10, src6554, filt2);
+        out10 = (v8i16)__msa_dpadd_s_h((v8i16) out10, src8776, filt3);
+        out10 = (v8i16)__msa_srari_h((v8i16) out10, 6);
+        out10 = (v8i16)__msa_sat_s_h((v8i16) out10, 7);
+        out0  = (v16u8)__msa_pckev_b((v16i8) out10, (v16i8) out10);
+        out0  = (v16u8)__msa_xori_b((v16u8) out0, 128);
+        ST_W2(out0, 0, 1, dst, dst_stride);
+        dst += 2 * dst_stride;
+        src2110 = src4332;
+        src4332 = src6554;
+        src6554 = src8776;
+        src6 = src8;
+    }
 }
 
 static void common_vt_8t_8w_msa(uint8_t *src, int32_t src_stride,
@@ -1341,6 +1363,7 @@ static void hevc_hv_uni_8t_4w_msa(uint8_t *src,
                                   int32_t height)
 {
     uint32_t loop_cnt;
+    uint32_t res = height & 0x07;
     v16u8 out0, out1;
     v16i8 src0, src1, src2, src3, src4, src5, src6, src7, src8;
     v16i8 src9, src10, src11, src12, src13, src14;
@@ -1464,6 +1487,71 @@ static void hevc_hv_uni_8t_4w_msa(uint8_t *src,
         dst65_r = dst1413_r;
         dst66 = (v8i16) __msa_splati_d((v2i64) dst1410, 1);
     }
+    if (res) {
+        LD_SB8(src, src_stride, src7, src8, src9, src10, src11, src12, src13,
+               src14);
+        XORI_B8_128_SB(src7, src8, src9, src10, src11, src12, src13, src14);
+
+        VSHF_B4_SB(src7, src11, mask0, mask1, mask2, mask3,
+                   vec0, vec1, vec2, vec3);
+        VSHF_B4_SB(src8, src12, mask0, mask1, mask2, mask3,
+                   vec4, vec5, vec6, vec7);
+        VSHF_B4_SB(src9, src13, mask0, mask1, mask2, mask3,
+                   vec8, vec9, vec10, vec11);
+        VSHF_B4_SB(src10, src14, mask0, mask1, mask2, mask3,
+                   vec12, vec13, vec14, vec15);
+
+        dst117 = HEVC_FILT_8TAP_SH(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
+                                   filt3);
+        dst128 = HEVC_FILT_8TAP_SH(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
+                                   filt3);
+        dst139 = HEVC_FILT_8TAP_SH(vec8, vec9, vec10, vec11, filt0, filt1,
+                                   filt2, filt3);
+        dst1410 = HEVC_FILT_8TAP_SH(vec12, vec13, vec14, vec15, filt0, filt1,
+                                   filt2, filt3);
+
+        dst76_r = __msa_ilvr_h(dst117, dst66);
+        ILVRL_H2_SH(dst128, dst117, dst87_r, dst1211_r);
+        ILVRL_H2_SH(dst139, dst128, dst98_r, dst1312_r);
+        ILVRL_H2_SH(dst1410, dst139, dst109_r, dst1413_r);
+        dst117 = (v8i16) __msa_splati_d((v2i64) dst117, 1);
+        dst1110_r = __msa_ilvr_h(dst117, dst1410);
+        dst0_r = HEVC_FILT_8TAP(dst10_r, dst32_r, dst54_r, dst76_r, filt_h0,
+                                filt_h1, filt_h2, filt_h3);
+        dst1_r = HEVC_FILT_8TAP(dst21_r, dst43_r, dst65_r, dst87_r, filt_h0,
+                                filt_h1, filt_h2, filt_h3);
+        dst2_r = HEVC_FILT_8TAP(dst32_r, dst54_r, dst76_r, dst98_r, filt_h0,
+                                filt_h1, filt_h2, filt_h3);
+        dst3_r = HEVC_FILT_8TAP(dst43_r, dst65_r, dst87_r, dst109_r, filt_h0,
+                                filt_h1, filt_h2, filt_h3);
+        dst4_r = HEVC_FILT_8TAP(dst54_r, dst76_r, dst98_r, dst1110_r, filt_h0,
+                                filt_h1, filt_h2, filt_h3);
+        dst5_r = HEVC_FILT_8TAP(dst65_r, dst87_r, dst109_r, dst1211_r, filt_h0,
+                                filt_h1, filt_h2, filt_h3);
+        dst6_r = HEVC_FILT_8TAP(dst76_r, dst98_r, dst1110_r, dst1312_r, filt_h0,
+                                filt_h1, filt_h2, filt_h3);
+        dst7_r = HEVC_FILT_8TAP(dst87_r, dst109_r, dst1211_r, dst1413_r,
+                                filt_h0, filt_h1, filt_h2, filt_h3);
+
+        SRA_4V(dst0_r, dst1_r, dst2_r, dst3_r, 6);
+        SRA_4V(dst4_r, dst5_r, dst6_r, dst7_r, 6);
+        SRARI_W4_SW(dst0_r, dst1_r, dst2_r, dst3_r, 6);
+        SRARI_W4_SW(dst4_r, dst5_r, dst6_r, dst7_r, 6);
+        SAT_SW4_SW(dst0_r, dst1_r, dst2_r, dst3_r, 7);
+        SAT_SW4_SW(dst4_r, dst5_r, dst6_r, dst7_r, 7);
+        PCKEV_H2_SW(dst1_r, dst0_r, dst3_r, dst2_r, dst0_r, dst1_r);
+        PCKEV_H2_SW(dst5_r, dst4_r, dst7_r, dst6_r, dst4_r, dst5_r);
+        out0 = PCKEV_XORI128_UB(dst0_r, dst1_r);
+        out1 = PCKEV_XORI128_UB(dst4_r, dst5_r);
+        if (res == 2) {
+            ST_W2(out0, 0, 1, dst, dst_stride);
+        } else if(res == 4) {
+            ST_W4(out0, 0, 1, 2, 3, dst, dst_stride);
+        } else {
+            ST_W4(out0, 0, 1, 2, 3, dst, dst_stride);
+            ST_W2(out1, 0, 1, dst + 4 * dst_stride, dst_stride);
+        }
+    }
 }
 
 static void hevc_hv_uni_8t_8multx2mult_msa(uint8_t *src,
diff --git a/libavcodec/mips/hevc_mc_uniw_msa.c b/libavcodec/mips/hevc_mc_uniw_msa.c
index cad1240b40..9772dc32b5 100644
--- a/libavcodec/mips/hevc_mc_uniw_msa.c
+++ b/libavcodec/mips/hevc_mc_uniw_msa.c
@@ -33,15 +33,17 @@ static const uint8_t ff_hevc_mask_arr[16 * 2] __attribute__((aligned(0x40))) = {
                                        out0_h, out1_h)                        \
 {                                                                             \
     v4i32 in0_r_m, in0_l_m, in1_r_m, in1_l_m;                                 \
+    v8i16 zero = { 0 };                                                       \
                                                                               \
-    ILVRL_H2_SW(in0_h, in0_h, in0_r_m, in0_l_m);                              \
-    ILVRL_H2_SW(in1_h, in1_h, in1_r_m, in1_l_m);                              \
-    DOTP_SH4_SW(in0_r_m, in1_r_m, in0_l_m, in1_l_m, wgt_w, wgt_w, wgt_w,      \
-                wgt_w, in0_r_m, in1_r_m, in0_l_m, in1_l_m);                   \
-    SRAR_W4_SW(in0_r_m, in1_r_m, in0_l_m, in1_l_m, rnd_w);                    \
+    ILVRL_H2_SW(zero, in0_h, in0_r_m, in0_l_m);                               \
+    ILVRL_H2_SW(zero, in1_h, in1_r_m, in1_l_m);                               \
+    MUL4(in0_r_m, wgt_w, in0_l_m, wgt_w, in1_r_m, wgt_w, in1_l_m, wgt_w,      \
+         in0_r_m, in0_l_m, in1_r_m, in1_l_m);                                 \
+    SRAR_W4_SW(in0_r_m, in0_l_m, in1_r_m, in1_l_m, rnd_w);                    \
+    ADD4(in0_r_m, offset_h, in0_l_m, offset_h, in1_r_m, offset_h, in1_l_m,    \
+         offset_h, in0_r_m, in0_l_m, in1_r_m, in1_l_m);                       \
+    CLIP_SW4_0_255(in0_r_m, in0_l_m, in1_r_m, in1_l_m);                       \
     PCKEV_H2_SH(in0_l_m, in0_r_m, in1_l_m, in1_r_m, out0_h, out1_h);          \
-    ADDS_SH2_SH(out0_h, offset_h, out1_h, offset_h, out0_h, out1_h);          \
-    CLIP_SH2_0_255_MAX_SATU(out0_h, out1_h);                                  \
 }
 
 #define HEVC_UNIW_RND_CLIP4_MAX_SATU_H(in0_h, in1_h, in2_h, in3_h, wgt_w,  \
@@ -54,6 +56,21 @@ static const uint8_t ff_hevc_mask_arr[16 * 2] __attribute__((aligned(0x40))) = {
                                    out2_h, out3_h);                        \
 }
 
+#define HEVC_FILT_8TAP_4W_SH(in0, in1, in2, in3, filt0, filt1,   \
+                             filt2, filt3, dst0, dst1)           \
+{                                                                \
+    v8i16 tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;        \
+    ILVRL_B2_SH(zero, in0, tmp0, tmp4);                          \
+    ILVRL_B2_SH(zero, in1, tmp1, tmp5);                          \
+    ILVRL_B2_SH(zero, in2, tmp2, tmp6);                          \
+    ILVRL_B2_SH(zero, in3, tmp3, tmp7);                          \
+    dst0 = __msa_dotp_s_w((v8i16) tmp0, (v8i16) filt0);          \
+    dst1 = __msa_dotp_s_w((v8i16) tmp4, (v8i16) filt0);          \
+    DPADD_SH2_SW(tmp1, tmp5, filt1, filt1, dst0, dst1);          \
+    DPADD_SH2_SW(tmp2, tmp6, filt2, filt2, dst0, dst1);          \
+    DPADD_SH2_SW(tmp3, tmp7, filt3, filt3, dst0, dst1);          \
+}
+
 static void hevc_uniwgt_copy_4w_msa(uint8_t *src,
                                     int32_t src_stride,
                                     uint8_t *dst,
@@ -67,12 +84,11 @@ static void hevc_uniwgt_copy_4w_msa(uint8_t *src,
     v16i8 zero = { 0 };
     v16u8 out0, out1;
     v16i8 src0 = { 0 }, src1 = { 0 };
-    v8i16 dst0, dst1, dst2, dst3, offset_vec;
-    v4i32 weight_vec, rnd_vec;
+    v8i16 dst0, dst1, dst2, dst3;
+    v4i32 weight_vec, rnd_vec, offset_vec;
 
-    weight = weight & 0x0000FFFF;
     weight_vec = __msa_fill_w(weight);
-    offset_vec = __msa_fill_h(offset);
+    offset_vec = __msa_fill_w(offset);
     rnd_vec = __msa_fill_w(rnd_val);
 
     if (2 == height) {
@@ -83,12 +99,13 @@ static void hevc_uniwgt_copy_4w_msa(uint8_t *src,
         dst0 = (v8i16) __msa_ilvr_b(zero, src0);
         dst0 <<= 6;
 
-        ILVRL_H2_SW(dst0, dst0, dst0_r, dst0_l);
+        ILVRL_H2_SW(zero, dst0, dst0_r, dst0_l);
         DOTP_SH2_SW(dst0_r, dst0_l, weight_vec, weight_vec, dst0_r, dst0_l);
         SRAR_W2_SW(dst0_r, dst0_l, rnd_vec);
+        dst0_r += offset_vec;
+        dst0_l += offset_vec;
+        CLIP_SW2_0_255(dst0_r, dst0_l);
         dst0 = __msa_pckev_h((v8i16) dst0_l, (v8i16) dst0_r);
-        dst0 += offset_vec;
-        dst0 = CLIP_SH_0_255_MAX_SATU(dst0);
         out0 = (v16u8) __msa_pckev_b((v16i8) dst0, (v16i8) dst0);
         ST_W2(out0, 0, 1, dst, dst_stride);
     } else if (4 == height) {
@@ -131,16 +148,16 @@ static void hevc_uniwgt_copy_6w_msa(uint8_t *src,
                                     int32_t rnd_val)
 {
     uint32_t loop_cnt;
+    int32_t res = height & 0x07;
     uint64_t tp0, tp1, tp2, tp3;
     v16i8 zero = { 0 };
     v16u8 out0, out1, out2, out3;
     v16i8 src0, src1, src2, src3;
-    v8i16 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7, offset_vec;
-    v4i32 weight_vec, rnd_vec;
+    v8i16 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    v4i32 weight_vec, rnd_vec, offset_vec;
 
-    weight = weight & 0x0000FFFF;
     weight_vec = __msa_fill_w(weight);
-    offset_vec = __msa_fill_h(offset);
+    offset_vec = __msa_fill_w(offset);
     rnd_vec = __msa_fill_w(rnd_val);
 
     for (loop_cnt = (height >> 3); loop_cnt--;) {
@@ -181,6 +198,51 @@ static void hevc_uniwgt_copy_6w_msa(uint8_t *src,
         ST_H2(out3, 2, 6, dst + 2 * dst_stride + 4, dst_stride);
         dst += (4 * dst_stride);
     }
+    if (res) {
+        LD4(src, src_stride, tp0, tp1, tp2, tp3);
+        src += (4 * src_stride);
+        INSERT_D2_SB(tp0, tp1, src0);
+        INSERT_D2_SB(tp2, tp3, src1);
+        LD4(src, src_stride, tp0, tp1, tp2, tp3);
+        src += (4 * src_stride);
+        INSERT_D2_SB(tp0, tp1, src2);
+        INSERT_D2_SB(tp2, tp3, src3);
+
+        ILVRL_B2_SH(zero, src0, dst0, dst1);
+        ILVRL_B2_SH(zero, src1, dst2, dst3);
+        ILVRL_B2_SH(zero, src2, dst4, dst5);
+        ILVRL_B2_SH(zero, src3, dst6, dst7);
+
+        SLLI_4V(dst0, dst1, dst2, dst3, 6);
+        SLLI_4V(dst4, dst5, dst6, dst7, 6);
+
+        HEVC_UNIW_RND_CLIP4_MAX_SATU_H(dst0, dst1, dst2, dst3, weight_vec,
+                                       offset_vec, rnd_vec, dst0, dst1, dst2,
+                                       dst3);
+        HEVC_UNIW_RND_CLIP4_MAX_SATU_H(dst4, dst5, dst6, dst7, weight_vec,
+                                       offset_vec, rnd_vec, dst4, dst5, dst6,
+                                       dst7);
+        PCKEV_B2_UB(dst1, dst0, dst3, dst2, out0, out1);
+        PCKEV_B2_UB(dst5, dst4, dst7, dst6, out2, out3);
+
+        if (res == 2) {
+            ST_W2(out0, 0, 2, dst, dst_stride);
+            ST_H2(out0, 2, 6, dst + 4, dst_stride);
+        } else if (res == 4) {
+            ST_W2(out0, 0, 2, dst, dst_stride);
+            ST_H2(out0, 2, 6, dst + 4, dst_stride);
+            ST_W2(out1, 0, 2, dst + 2 * dst_stride, dst_stride);
+            ST_H2(out1, 2, 6, dst + 2 * dst_stride + 4, dst_stride);
+        } else {
+            ST_W2(out0, 0, 2, dst, dst_stride);
+            ST_H2(out0, 2, 6, dst + 4, dst_stride);
+            ST_W2(out1, 0, 2, dst + 2 * dst_stride, dst_stride);
+            ST_H2(out1, 2, 6, dst + 2 * dst_stride + 4, dst_stride);
+            dst += (4 * dst_stride);
+            ST_W2(out2, 0, 2, dst, dst_stride);
+            ST_H2(out2, 2, 6, dst + 4, dst_stride);
+        }
+    }
 }
 
 static void hevc_uniwgt_copy_8w_msa(uint8_t *src,
@@ -197,12 +259,11 @@ static void hevc_uniwgt_copy_8w_msa(uint8_t *src,
     v16i8 src0 = { 0 }, src1 = { 0 }, src2 = { 0 }, src3 = { 0 };
     v16i8 zero = { 0 };
     v16u8 out0, out1, out2, out3;
-    v8i16 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7, offset_vec;
-    v4i32 weight_vec, rnd_vec;
+    v8i16 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    v4i32 weight_vec, rnd_vec, offset_vec;
 
-    weight = weight & 0x0000FFFF;
     weight_vec = __msa_fill_w(weight);
-    offset_vec = __msa_fill_h(offset);
+    offset_vec = __msa_fill_w(offset);
     rnd_vec = __msa_fill_w(rnd_val);
 
     if (2 == height) {
@@ -291,13 +352,11 @@ static void hevc_uniwgt_copy_12w_msa(uint8_t *src,
     v16u8 out0, out1, out2;
     v16i8 src0, src1, src2, src3;
     v8i16 dst0, dst1, dst2, dst3, dst4, dst5;
-    v8i16 offset_vec;
     v16i8 zero = { 0 };
-    v4i32 weight_vec, rnd_vec;
+    v4i32 weight_vec, rnd_vec, offset_vec;
 
-    weight = weight & 0x0000FFFF;
     weight_vec = __msa_fill_w(weight);
-    offset_vec = __msa_fill_h(offset);
+    offset_vec = __msa_fill_w(offset);
     rnd_vec = __msa_fill_w(rnd_val);
 
     for (loop_cnt = 4; loop_cnt--;) {
@@ -336,12 +395,11 @@ static void hevc_uniwgt_copy_16w_msa(uint8_t *src,
     v16u8 out0, out1, out2, out3;
     v16i8 src0, src1, src2, src3;
     v16i8 zero = { 0 };
-    v8i16 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7, offset_vec;
-    v4i32 weight_vec, rnd_vec;
+    v8i16 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    v4i32 weight_vec, rnd_vec, offset_vec;
 
-    weight = weight & 0x0000FFFF;
     weight_vec = __msa_fill_w(weight);
-    offset_vec = __msa_fill_h(offset);
+    offset_vec = __msa_fill_w(offset);
     rnd_vec = __msa_fill_w(rnd_val);
 
     for (loop_cnt = height >> 2; loop_cnt--;) {
@@ -379,13 +437,12 @@ static void hevc_uniwgt_copy_24w_msa(uint8_t *src,
     v16u8 out0, out1, out2, out3, out4, out5;
     v16i8 src0, src1, src2, src3, src4, src5, src6, src7;
     v16i8 zero = { 0 };
-    v8i16 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7, offset_vec;
+    v8i16 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
     v8i16 dst8, dst9, dst10, dst11;
-    v4i32 weight_vec, rnd_vec;
+    v4i32 weight_vec, rnd_vec, offset_vec;
 
-    weight = weight & 0x0000FFFF;
     weight_vec = __msa_fill_w(weight);
-    offset_vec = __msa_fill_h(offset);
+    offset_vec = __msa_fill_w(offset);
     rnd_vec = __msa_fill_w(rnd_val);
 
     for (loop_cnt = (height >> 2); loop_cnt--;) {
@@ -432,12 +489,11 @@ static void hevc_uniwgt_copy_32w_msa(uint8_t *src,
     v16u8 out0, out1, out2, out3;
     v16i8 src0, src1, src2, src3;
     v16i8 zero = { 0 };
-    v8i16 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7, offset_vec;
-    v4i32 weight_vec, rnd_vec;
+    v8i16 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    v4i32 weight_vec, rnd_vec, offset_vec;
 
-    weight = weight & 0x0000FFFF;
     weight_vec = __msa_fill_w(weight);
-    offset_vec = __msa_fill_h(offset);
+    offset_vec = __msa_fill_w(offset);
     rnd_vec = __msa_fill_w(rnd_val);
 
     for (loop_cnt = (height >> 1); loop_cnt--;) {
@@ -478,13 +534,12 @@ static void hevc_uniwgt_copy_48w_msa(uint8_t *src,
     v16u8 out0, out1, out2, out3, out4, out5;
     v16i8 src0, src1, src2, src3, src4, src5;
     v16i8 zero = { 0 };
-    v8i16 dst0, dst1, dst2, dst3, dst4, dst5, offset_vec;
+    v8i16 dst0, dst1, dst2, dst3, dst4, dst5;
     v8i16 dst6, dst7, dst8, dst9, dst10, dst11;
-    v4i32 weight_vec, rnd_vec;
+    v4i32 weight_vec, rnd_vec, offset_vec;
 
-    weight = weight & 0x0000FFFF;
     weight_vec = __msa_fill_w(weight);
-    offset_vec = __msa_fill_h(offset);
+    offset_vec = __msa_fill_w(offset);
     rnd_vec = __msa_fill_w(rnd_val);
 
     for (loop_cnt = (height >> 1); loop_cnt--;) {
@@ -535,13 +590,12 @@ static void hevc_uniwgt_copy_64w_msa(uint8_t *src,
     v16u8 out0, out1, out2, out3, out4, out5, out6, out7;
     v16i8 src0, src1, src2, src3, src4, src5, src6, src7;
     v16i8 zero = { 0 };
-    v8i16 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7, offset_vec;
+    v8i16 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
     v8i16 dst8, dst9, dst10, dst11, dst12, dst13, dst14, dst15;
-    v4i32 weight_vec, rnd_vec;
+    v4i32 weight_vec, rnd_vec, offset_vec;
 
-    weight = weight & 0x0000FFFF;
     weight_vec = __msa_fill_w(weight);
-    offset_vec = __msa_fill_h(offset);
+    offset_vec = __msa_fill_w(offset);
     rnd_vec = __msa_fill_w(rnd_val);
 
     for (loop_cnt = (height >> 1); loop_cnt--;) {
@@ -596,33 +650,26 @@ static void hevc_hz_uniwgt_8t_4w_msa(uint8_t *src,
                                      int32_t rnd_val)
 {
     uint32_t loop_cnt;
+    uint32_t res = height & 0x07;
     v16u8 out0, out1;
-    v8i16 filt0, filt1, filt2, filt3;
     v16i8 src0, src1, src2, src3, src4, src5, src6, src7;
-    v16i8 vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7, vec8, vec9, vec10;
-    v16i8 mask0, mask1, mask2, mask3, vec11, vec12, vec13, vec14, vec15;
-    v8i16 filter_vec, dst01, dst23, dst45, dst67;
-    v8i16 dst0, dst1, dst2, dst3, weight_vec_h, offset_vec, denom_vec;
-    v4i32 weight_vec, rnd_vec;
+    v8i16 vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
+    v8i16 vec8, vec9, vec10, vec11, vec12, vec13, vec14, vec15;
+    v16i8 mask0, mask1, mask2, mask3;
+    v8i16 filter_vec, filt0, filt1, filt2, filt3;
+    v4i32 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    v4i32 weight_vec, rnd_vec, offset_vec;
+    v8i16 zero = { 0 };
 
     src -= 3;
-    weight = weight & 0x0000FFFF;
 
     weight_vec = __msa_fill_w(weight);
     rnd_vec = __msa_fill_w(rnd_val);
-
-    weight *= 128;
-    rnd_val -= 6;
-
-    weight_vec_h = __msa_fill_h(weight);
-    offset_vec = __msa_fill_h(offset);
-    denom_vec = __msa_fill_h(rnd_val);
-
-    weight_vec_h = __msa_srar_h(weight_vec_h, denom_vec);
-    offset_vec = __msa_adds_s_h(offset_vec, weight_vec_h);
+    offset_vec = __msa_fill_w(offset);
 
     filter_vec = LD_SH(filter);
-    SPLATI_H4_SH(filter_vec, 0, 1, 2, 3, filt0, filt1, filt2, filt3);
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W4_SH(filter_vec, filt0, filt1, filt2, filt3);
 
     mask0 = LD_SB(&ff_hevc_mask_arr[16]);
     mask1 = mask0 + 2;
@@ -632,8 +679,6 @@ static void hevc_hz_uniwgt_8t_4w_msa(uint8_t *src,
     for (loop_cnt = (height >> 3); loop_cnt--;) {
         LD_SB8(src, src_stride, src0, src1, src2, src3, src4, src5, src6, src7);
         src += (8 * src_stride);
-        XORI_B8_128_SB(src0, src1, src2, src3, src4, src5, src6, src7);
-
         VSHF_B4_SB(src0, src1, mask0, mask1, mask2, mask3,
                    vec0, vec1, vec2, vec3);
         VSHF_B4_SB(src2, src3, mask0, mask1, mask2, mask3,
@@ -642,23 +687,77 @@ static void hevc_hz_uniwgt_8t_4w_msa(uint8_t *src,
                    vec8, vec9, vec10, vec11);
         VSHF_B4_SB(src6, src7, mask0, mask1, mask2, mask3,
                    vec12, vec13, vec14, vec15);
-        dst01 = HEVC_FILT_8TAP_SH(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
-                                  filt3);
-        dst23 = HEVC_FILT_8TAP_SH(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
-                                  filt3);
-        dst45 = HEVC_FILT_8TAP_SH(vec8, vec9, vec10, vec11, filt0, filt1, filt2,
-                                  filt3);
-        dst67 = HEVC_FILT_8TAP_SH(vec12, vec13, vec14, vec15, filt0, filt1,
-                                  filt2, filt3);
-
-        HEVC_UNIW_RND_CLIP4_MAX_SATU_H(dst01, dst23, dst45, dst67, weight_vec,
-                                       offset_vec, rnd_vec, dst0, dst1, dst2,
-                                       dst3);
+        HEVC_FILT_8TAP_4W_SH(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
+                             filt3, dst0, dst1);
+        HEVC_FILT_8TAP_4W_SH(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
+                             filt3, dst2, dst3);
+        HEVC_FILT_8TAP_4W_SH(vec8, vec9, vec10, vec11, filt0, filt1, filt2,
+                             filt3, dst4, dst5);
+        HEVC_FILT_8TAP_4W_SH(vec12, vec13, vec14, vec15, filt0, filt1, filt2,
+                             filt3, dst6, dst7);
+
+        MUL4(dst0, weight_vec, dst1, weight_vec, dst2, weight_vec, dst3,
+             weight_vec, dst0, dst1, dst2, dst3)
+        MUL4(dst4, weight_vec, dst5, weight_vec, dst6, weight_vec, dst7,
+             weight_vec, dst4, dst5, dst6, dst7);
+        SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
+        SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
+        ADD4(dst0, offset_vec, dst1, offset_vec, dst2, offset_vec, dst3,
+             offset_vec, dst0, dst1, dst2, dst3);
+        ADD4(dst4, offset_vec, dst5, offset_vec, dst6, offset_vec, dst7,
+             offset_vec, dst4, dst5, dst6, dst7);
+        CLIP_SW8_0_255(dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7);
+        PCKEV_H4_SH(dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
+                    vec0, vec1, vec2, vec3);
+        PCKEV_B2_UB(vec1, vec0, vec3, vec2, out0, out1);
 
-        PCKEV_B2_UB(dst1, dst0, dst3, dst2, out0, out1);
         ST_W8(out0, out1, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
         dst += (8 * dst_stride);
     }
+    if (res) {
+        LD_SB8(src, src_stride, src0, src1, src2, src3, src4, src5, src6, src7);
+        VSHF_B4_SB(src0, src1, mask0, mask1, mask2, mask3,
+                   vec0, vec1, vec2, vec3);
+        VSHF_B4_SB(src2, src3, mask0, mask1, mask2, mask3,
+                   vec4, vec5, vec6, vec7);
+        VSHF_B4_SB(src4, src5, mask0, mask1, mask2, mask3,
+                   vec8, vec9, vec10, vec11);
+        VSHF_B4_SB(src6, src7, mask0, mask1, mask2, mask3,
+                   vec12, vec13, vec14, vec15);
+
+        HEVC_FILT_8TAP_4W_SH(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
+                             filt3, dst0, dst1);
+        HEVC_FILT_8TAP_4W_SH(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
+                             filt3, dst2, dst3);
+        HEVC_FILT_8TAP_4W_SH(vec8, vec9, vec10, vec11, filt0, filt1, filt2,
+                             filt3, dst4, dst5);
+        HEVC_FILT_8TAP_4W_SH(vec12, vec13, vec14, vec15, filt0, filt1, filt2,
+                             filt3, dst6, dst7);
+
+        MUL4(dst0, weight_vec, dst1, weight_vec, dst2, weight_vec, dst3,
+             weight_vec, dst0, dst1, dst2, dst3)
+        MUL4(dst4, weight_vec, dst5, weight_vec, dst6, weight_vec, dst7,
+             weight_vec, dst4, dst5, dst6, dst7);
+        SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
+        SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
+        ADD4(dst0, offset_vec, dst1, offset_vec, dst2, offset_vec, dst3,
+             offset_vec, dst0, dst1, dst2, dst3);
+        ADD4(dst4, offset_vec, dst5, offset_vec, dst6, offset_vec, dst7,
+             offset_vec, dst4, dst5, dst6, dst7);
+        CLIP_SW8_0_255(dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7);
+        PCKEV_H4_SH(dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
+                    vec0, vec1, vec2, vec3);
+        PCKEV_B2_UB(vec1, vec0, vec3, vec2, out0, out1);
+
+        if (res == 2) {
+            ST_W2(out0, 0, 1, dst, dst_stride);
+        } else if (res == 4) {
+            ST_W4(out0, 0, 1, 2, 3, dst, dst_stride);
+        } else {
+            ST_W4(out0, 0, 1, 2, 3, dst, dst_stride);
+            ST_W2(out1, 0, 1, dst + 4 * dst_stride, dst_stride);
+        }
+    }
 }
 
 static void hevc_hz_uniwgt_8t_8w_msa(uint8_t *src,
@@ -672,35 +771,27 @@ static void hevc_hz_uniwgt_8t_8w_msa(uint8_t *src,
                                      int32_t rnd_val)
 {
     uint32_t loop_cnt;
+    uint32_t res = height & 0x03;
     v16u8 out0, out1;
     v16i8 src0, src1, src2, src3;
     v8i16 filt0, filt1, filt2, filt3;
     v16i8 mask0, mask1, mask2, mask3;
     v8i16 filter_vec;
-    v16i8 vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
-    v16i8 vec8, vec9, vec10, vec11, vec12, vec13, vec14, vec15;
-    v8i16 dst0, dst1, dst2, dst3;
-    v8i16 weight_vec_h, offset_vec, denom_vec;
-    v4i32 weight_vec, rnd_vec;
+    v8i16 vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
+    v8i16 vec8, vec9, vec10, vec11, vec12, vec13, vec14, vec15;
+    v4i32 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    v4i32 weight_vec, rnd_vec, offset_vec;
+    v8i16 zero = { 0 };
 
     src -= 3;
-    weight = weight & 0x0000FFFF;
 
     weight_vec = __msa_fill_w(weight);
     rnd_vec = __msa_fill_w(rnd_val);
-
-    weight *= 128;
-    rnd_val -= 6;
-
-    weight_vec_h = __msa_fill_h(weight);
-    offset_vec = __msa_fill_h(offset);
-    denom_vec = __msa_fill_h(rnd_val);
-
-    weight_vec_h = __msa_srar_h(weight_vec_h, denom_vec);
-    offset_vec = __msa_adds_s_h(offset_vec, weight_vec_h);
+    offset_vec = __msa_fill_w(offset);
 
     filter_vec = LD_SH(filter);
-    SPLATI_H4_SH(filter_vec, 0, 1, 2, 3, filt0, filt1, filt2, filt3);
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W4_SH(filter_vec, filt0, filt1, filt2, filt3);
 
     mask0 = LD_SB(&ff_hevc_mask_arr[0]);
     mask1 = mask0 + 2;
@@ -710,7 +801,6 @@ static void hevc_hz_uniwgt_8t_8w_msa(uint8_t *src,
     for (loop_cnt = (height >> 2); loop_cnt--;) {
         LD_SB4(src, src_stride, src0, src1, src2, src3);
         src += (4 * src_stride);
-        XORI_B4_128_SB(src0, src1, src2, src3);
 
         VSHF_B4_SB(src0, src0, mask0, mask1, mask2, mask3,
                    vec0, vec1, vec2, vec3);
@@ -720,23 +810,55 @@ static void hevc_hz_uniwgt_8t_8w_msa(uint8_t *src,
                    vec8, vec9, vec10, vec11);
         VSHF_B4_SB(src3, src3, mask0, mask1, mask2, mask3,
                    vec12, vec13, vec14, vec15);
-        dst0 = HEVC_FILT_8TAP_SH(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
-                                 filt3);
-        dst1 = HEVC_FILT_8TAP_SH(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
-                                 filt3);
-        dst2 = HEVC_FILT_8TAP_SH(vec8, vec9, vec10, vec11, filt0, filt1, filt2,
-                                 filt3);
-        dst3 = HEVC_FILT_8TAP_SH(vec12, vec13, vec14, vec15, filt0, filt1,
-                                 filt2, filt3);
-
-        HEVC_UNIW_RND_CLIP4_MAX_SATU_H(dst0, dst1, dst2, dst3, weight_vec,
-                                       offset_vec, rnd_vec, dst0, dst1, dst2,
-                                       dst3);
+        HEVC_FILT_8TAP_4W_SH(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
+                             filt3, dst0, dst1);
+        HEVC_FILT_8TAP_4W_SH(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
+                             filt3, dst2, dst3);
+        HEVC_FILT_8TAP_4W_SH(vec8, vec9, vec10, vec11, filt0, filt1, filt2,
+                             filt3, dst4, dst5);
+        HEVC_FILT_8TAP_4W_SH(vec12, vec13, vec14, vec15, filt0, filt1, filt2,
+                             filt3, dst6, dst7);
+
+        MUL4(dst0, weight_vec, dst1, weight_vec, dst2, weight_vec, dst3,
+             weight_vec, dst0, dst1, dst2, dst3)
+        MUL4(dst4, weight_vec, dst5, weight_vec, dst6, weight_vec, dst7,
+             weight_vec, dst4, dst5, dst6, dst7);
+        SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
+        SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
+        ADD4(dst0, offset_vec, dst1, offset_vec, dst2, offset_vec, dst3,
+             offset_vec, dst0, dst1, dst2, dst3);
+        ADD4(dst4, offset_vec, dst5, offset_vec, dst6, offset_vec, dst7,
+             offset_vec, dst4, dst5, dst6, dst7);
+        CLIP_SW8_0_255(dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7);
+        PCKEV_H4_SH(dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
+                    vec0, vec1, vec2, vec3);
+        PCKEV_B2_UB(vec1, vec0, vec3, vec2, out0, out1);
 
-        PCKEV_B2_UB(dst1, dst0, dst3, dst2, out0, out1);
         ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
         dst += (4 * dst_stride);
     }
+    if (res) {
+        LD_SB2(src, src_stride, src0, src1);
+
+        VSHF_B4_SB(src0, src0, mask0, mask1, mask2, mask3,
+                   vec0, vec1, vec2, vec3);
+        VSHF_B4_SB(src1, src1, mask0, mask1, mask2, mask3,
+                   vec4, vec5, vec6, vec7);
+        HEVC_FILT_8TAP_4W_SH(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
+                             filt3, dst0, dst1);
+        HEVC_FILT_8TAP_4W_SH(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
+                             filt3, dst2, dst3);
+
+        MUL4(dst0, weight_vec, dst1, weight_vec, dst2, weight_vec, dst3,
+             weight_vec, dst0, dst1, dst2, dst3)
+        SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
+        ADD4(dst0, offset_vec, dst1, offset_vec, dst2, offset_vec, dst3,
+             offset_vec, dst0, dst1, dst2, dst3);
+        CLIP_SW4_0_255(dst0, dst1, dst2, dst3);
+        PCKEV_H2_SH(dst1, dst0, dst3, dst2, vec0, vec1);
+        out0 = __msa_pckev_b((v16i8) vec1, (v16i8) vec0);
+        ST_D2(out0, 0, 1, dst, dst_stride);
+    }
 }
 
 static void hevc_hz_uniwgt_8t_12w_msa(uint8_t *src,
@@ -750,35 +872,26 @@ static void hevc_hz_uniwgt_8t_12w_msa(uint8_t *src,
                                       int32_t rnd_val)
 {
     uint32_t loop_cnt;
-    v16u8 out0, out1, out2;
+    v16u8 out0, out1;
     v8i16 filt0, filt1, filt2, filt3;
-    v16i8 src0, src1, src2, src3, src4, src5, src6, src7;
+    v16i8 src0, src1, src2, src3;
     v16i8 mask0, mask1, mask2, mask3, mask4, mask5, mask6, mask7;
     v16i8 vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
-    v16i8 vec8, vec9, vec10, vec11, vec12, vec13, vec14, vec15;
     v8i16 filter_vec;
-    v8i16 dst01, dst23, dst0, dst1, dst2, dst3, dst4, dst5;
-    v8i16 weight_vec_h, offset_vec, denom_vec;
-    v4i32 weight_vec, rnd_vec;
+    v4i32 dst0, dst1, dst2, dst3;
+    v4i32 dst00, dst01;
+    v4i32 weight_vec, rnd_vec, offset_vec;
+    v8i16 zero = { 0 };
 
     src -= 3;
-    weight = weight & 0x0000FFFF;
 
     weight_vec = __msa_fill_w(weight);
     rnd_vec = __msa_fill_w(rnd_val);
-
-    weight *= 128;
-    rnd_val -= 6;
-
-    weight_vec_h = __msa_fill_h(weight);
-    offset_vec = __msa_fill_h(offset);
-    denom_vec = __msa_fill_h(rnd_val);
-
-    weight_vec_h = __msa_srar_h(weight_vec_h, denom_vec);
-    offset_vec = __msa_adds_s_h(offset_vec, weight_vec_h);
+    offset_vec = __msa_fill_w(offset);
 
     filter_vec = LD_SH(filter);
-    SPLATI_H4_SH(filter_vec, 0, 1, 2, 3, filt0, filt1, filt2, filt3);
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W4_SH(filter_vec, filt0, filt1, filt2, filt3);
 
     mask0 = LD_SB(&ff_hevc_mask_arr[0]);
     mask1 = mask0 + 2;
@@ -789,47 +902,41 @@ static void hevc_hz_uniwgt_8t_12w_msa(uint8_t *src,
     mask6 = mask4 + 4;
     mask7 = mask4 + 6;
 
-    for (loop_cnt = (height >> 2); loop_cnt--;) {
-        LD_SB4(src, src_stride, src0, src1, src2, src3);
-        LD_SB4(src + 8, src_stride, src4, src5, src6, src7);
-        src += (4 * src_stride);
-        XORI_B8_128_SB(src0, src1, src2, src3, src4, src5, src6, src7);
+    for (loop_cnt = (height >> 1); loop_cnt--;) {
+        LD_SB2(src, src_stride, src0, src1);
+        LD_SB2(src + 8, src_stride, src2, src3);
+        src += (2 * src_stride);
 
         VSHF_B4_SB(src0, src0, mask0, mask1, mask2, mask3,
                    vec0, vec1, vec2, vec3);
         VSHF_B4_SB(src1, src1, mask0, mask1, mask2, mask3,
                    vec4, vec5, vec6, vec7);
-        VSHF_B4_SB(src2, src2, mask0, mask1, mask2, mask3,
-                   vec8, vec9, vec10, vec11);
-        VSHF_B4_SB(src3, src3, mask0, mask1, mask2, mask3,
-                   vec12, vec13, vec14, vec15);
-        dst0 = HEVC_FILT_8TAP_SH(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
-                                 filt3);
-        dst1 = HEVC_FILT_8TAP_SH(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
-                                 filt3);
-        dst2 = HEVC_FILT_8TAP_SH(vec8, vec9, vec10, vec11, filt0, filt1, filt2,
-                                 filt3);
-        dst3 = HEVC_FILT_8TAP_SH(vec12, vec13, vec14, vec15, filt0, filt1,
-                                 filt2, filt3);
-        VSHF_B4_SB(src4, src5, mask4, mask5, mask6, mask7,
+        HEVC_FILT_8TAP_4W_SH(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
+                             filt3, dst0, dst1);
+        HEVC_FILT_8TAP_4W_SH(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
+                             filt3, dst2, dst3);
+        VSHF_B4_SB(src2, src3, mask4, mask5, mask6, mask7,
                    vec0, vec1, vec2, vec3);
-        VSHF_B4_SB(src6, src7, mask4, mask5, mask6, mask7,
-                   vec4, vec5, vec6, vec7);
-        dst01 = HEVC_FILT_8TAP_SH(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
-                                  filt3);
-        dst23 = HEVC_FILT_8TAP_SH(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
-                                  filt3);
+        HEVC_FILT_8TAP_4W_SH(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
+                             filt3, dst00, dst01);
 
-        HEVC_UNIW_RND_CLIP4_MAX_SATU_H(dst0, dst1, dst2, dst3, weight_vec,
-                                       offset_vec, rnd_vec, dst0, dst1, dst2,
-                                       dst3);
-        HEVC_UNIW_RND_CLIP2_MAX_SATU_H(dst01, dst23, weight_vec, offset_vec,
-                                       rnd_vec, dst4, dst5);
+        MUL4(dst0, weight_vec, dst1, weight_vec, dst2, weight_vec, dst3,
+             weight_vec, dst0, dst1, dst2, dst3)
+        MUL2(dst00, weight_vec, dst01, weight_vec, dst00, dst01);
+        SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
+        SRAR_W2_SW(dst00, dst01, rnd_vec);
+        ADD4(dst0, offset_vec, dst1, offset_vec, dst2, offset_vec, dst3,
+             offset_vec, dst0, dst1, dst2, dst3);
+        ADD2(dst00, offset_vec, dst01, offset_vec, dst00, dst01);
+        CLIP_SW4_0_255(dst0, dst1, dst2, dst3);
+        CLIP_SW2_0_255(dst00, dst01);
+        PCKEV_H2_SH(dst1, dst0, dst3, dst2, vec0, vec1);
+        vec2 = __msa_pckev_h((v8i16) dst01, (v8i16) dst00);
+        PCKEV_B2_UB(vec1, vec0, zero, vec2, out0, out1);
 
-        PCKEV_B3_UB(dst1, dst0, dst3, dst2, dst5, dst4, out0, out1, out2);
-        ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
-        ST_W4(out2, 0, 1, 2, 3, dst + 8, dst_stride);
-        dst += (4 * dst_stride);
+        ST_D2(out0, 0, 1, dst, dst_stride);
+        ST_W2(out1, 0, 1, dst + 8, dst_stride);
+        dst += (2 * dst_stride);
     }
 }
 
@@ -851,27 +958,19 @@ static void hevc_hz_uniwgt_8t_16w_msa(uint8_t *src,
     v8i16 filter_vec;
     v16i8 vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
     v16i8 vec8, vec9, vec10, vec11, vec12, vec13, vec14, vec15;
-    v8i16 dst0, dst1, dst2, dst3;
-    v8i16 weight_vec_h, offset_vec, denom_vec;
-    v4i32 weight_vec, rnd_vec;
+    v4i32 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    v4i32 weight_vec, rnd_vec, offset_vec;
+    v8i16 zero = { 0 };
 
     src -= 3;
 
     weight_vec = __msa_fill_w(weight);
     rnd_vec = __msa_fill_w(rnd_val);
-
-    weight *= 128;
-    rnd_val -= 6;
-
-    weight_vec_h = __msa_fill_h(weight);
-    offset_vec = __msa_fill_h(offset);
-    denom_vec = __msa_fill_h(rnd_val);
-
-    weight_vec_h = __msa_srar_h(weight_vec_h, denom_vec);
-    offset_vec = __msa_adds_s_h(offset_vec, weight_vec_h);
+    offset_vec = __msa_fill_w(offset);
 
     filter_vec = LD_SH(filter);
-    SPLATI_H4_SH(filter_vec, 0, 1, 2, 3, filt0, filt1, filt2, filt3);
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W4_SH(filter_vec, filt0, filt1, filt2, filt3);
 
     mask0 = LD_SB(&ff_hevc_mask_arr[0]);
     mask1 = mask0 + 2;
@@ -882,7 +981,6 @@ static void hevc_hz_uniwgt_8t_16w_msa(uint8_t *src,
         LD_SB2(src, src_stride, src0, src2);
         LD_SB2(src + 8, src_stride, src1, src3);
         src += (2 * src_stride);
-        XORI_B4_128_SB(src0, src1, src2, src3);
 
         VSHF_B4_SB(src0, src0, mask0, mask1, mask2, mask3,
                    vec0, vec1, vec2, vec3);
@@ -892,20 +990,30 @@ static void hevc_hz_uniwgt_8t_16w_msa(uint8_t *src,
                    vec8, vec9, vec10, vec11);
         VSHF_B4_SB(src3, src3, mask0, mask1, mask2, mask3,
                    vec12, vec13, vec14, vec15);
-        dst0 = HEVC_FILT_8TAP_SH(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
-                                 filt3);
-        dst1 = HEVC_FILT_8TAP_SH(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
-                                 filt3);
-        dst2 = HEVC_FILT_8TAP_SH(vec8, vec9, vec10, vec11, filt0, filt1, filt2,
-                                 filt3);
-        dst3 = HEVC_FILT_8TAP_SH(vec12, vec13, vec14, vec15, filt0, filt1,
-                                 filt2, filt3);
-
-        HEVC_UNIW_RND_CLIP4_MAX_SATU_H(dst0, dst1, dst2, dst3, weight_vec,
-                                       offset_vec, rnd_vec, dst0, dst1, dst2,
-                                       dst3);
+        HEVC_FILT_8TAP_4W_SH(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
+                             filt3, dst0, dst1);
+        HEVC_FILT_8TAP_4W_SH(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
+                             filt3, dst2, dst3);
+        HEVC_FILT_8TAP_4W_SH(vec8, vec9, vec10, vec11, filt0, filt1, filt2,
+                             filt3, dst4, dst5);
+        HEVC_FILT_8TAP_4W_SH(vec12, vec13, vec14, vec15, filt0, filt1, filt2,
+                             filt3, dst6, dst7);
+
+        MUL4(dst0, weight_vec, dst1, weight_vec, dst2, weight_vec, dst3,
+             weight_vec, dst0, dst1, dst2, dst3)
+        MUL4(dst4, weight_vec, dst5, weight_vec, dst6, weight_vec, dst7,
+             weight_vec, dst4, dst5, dst6, dst7);
+        SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
+        SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
+        ADD4(dst0, offset_vec, dst1, offset_vec, dst2, offset_vec, dst3,
+             offset_vec, dst0, dst1, dst2, dst3);
+        ADD4(dst4, offset_vec, dst5, offset_vec, dst6, offset_vec, dst7,
+             offset_vec, dst4, dst5, dst6, dst7);
+        CLIP_SW8_0_255(dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7);
+        PCKEV_H4_SH(dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
+                    vec0, vec1, vec2, vec3);
+        PCKEV_B2_UB(vec1, vec0, vec3, vec2, out0, out1);
 
-        PCKEV_B2_UB(dst1, dst0, dst3, dst2, out0, out1);
         ST_UB2(out0, out1, dst, dst_stride);
         dst += (2 * dst_stride);
     }
@@ -928,27 +1036,21 @@ static void hevc_hz_uniwgt_8t_24w_msa(uint8_t *src,
     v16i8 mask0, mask1, mask2, mask3, mask4, mask5, mask6, mask7;
     v16i8 vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
     v16i8 vec8, vec9, vec10, vec11, vec12, vec13, vec14, vec15;
-    v8i16 dst0, dst1, dst2, dst3, dst4, dst5;
-    v8i16 filter_vec, weight_vec_h, offset_vec, denom_vec;
-    v4i32 weight_vec, rnd_vec;
+    v4i32 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    v4i32 dst8, dst9, dst10, dst11;
+    v8i16 filter_vec;
+    v4i32 weight_vec, rnd_vec, offset_vec;
+    v8i16 zero = { 0 };
 
     src -= 3;
 
     weight_vec = __msa_fill_w(weight);
     rnd_vec = __msa_fill_w(rnd_val);
-
-    weight *= 128;
-    rnd_val -= 6;
-
-    weight_vec_h = __msa_fill_h(weight);
-    offset_vec = __msa_fill_h(offset);
-    denom_vec = __msa_fill_h(rnd_val);
-
-    weight_vec_h = __msa_srar_h(weight_vec_h, denom_vec);
-    offset_vec = __msa_adds_s_h(offset_vec, weight_vec_h);
+    offset_vec = __msa_fill_w(offset);
 
     filter_vec = LD_SH(filter);
-    SPLATI_H4_SH(filter_vec, 0, 1, 2, 3, filt0, filt1, filt2, filt3);
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W4_SH(filter_vec, filt0, filt1, filt2, filt3);
 
     mask0 = LD_SB(&ff_hevc_mask_arr[0]);
     mask1 = mask0 + 2;
@@ -964,7 +1066,6 @@ static void hevc_hz_uniwgt_8t_24w_msa(uint8_t *src,
         src += src_stride;
         LD_SB2(src, 16, src2, src3);
         src += src_stride;
-        XORI_B4_128_SB(src0, src1, src2, src3);
         VSHF_B4_SB(src0, src0, mask0, mask1, mask2, mask3,
                    vec0, vec1, vec2, vec3);
         VSHF_B4_SB(src0, src1, mask4, mask5, mask6, mask7,
@@ -973,31 +1074,46 @@ static void hevc_hz_uniwgt_8t_24w_msa(uint8_t *src,
                    vec8, vec9, vec10, vec11);
         VSHF_B4_SB(src2, src2, mask0, mask1, mask2, mask3,
                    vec12, vec13, vec14, vec15);
-        dst0 = HEVC_FILT_8TAP_SH(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
-                                 filt3);
-        dst1 = HEVC_FILT_8TAP_SH(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
-                                 filt3);
-        dst2 = HEVC_FILT_8TAP_SH(vec8, vec9, vec10, vec11, filt0, filt1, filt2,
-                                 filt3);
-        dst3 = HEVC_FILT_8TAP_SH(vec12, vec13, vec14, vec15, filt0, filt1,
-                                 filt2, filt3);
+        HEVC_FILT_8TAP_4W_SH(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
+                             filt3, dst0, dst1);
+        HEVC_FILT_8TAP_4W_SH(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
+                             filt3, dst2, dst3);
+        HEVC_FILT_8TAP_4W_SH(vec8, vec9, vec10, vec11, filt0, filt1, filt2,
+                             filt3, dst4, dst5);
+        HEVC_FILT_8TAP_4W_SH(vec12, vec13, vec14, vec15, filt0, filt1, filt2,
+                             filt3, dst6, dst7);
 
         VSHF_B4_SB(src2, src3, mask4, mask5, mask6, mask7,
                    vec0, vec1, vec2, vec3);
         VSHF_B4_SB(src3, src3, mask0, mask1, mask2, mask3,
                    vec4, vec5, vec6, vec7);
-        dst4 = HEVC_FILT_8TAP_SH(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
-                                 filt3);
-        dst5 = HEVC_FILT_8TAP_SH(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
-                                 filt3);
-
-        HEVC_UNIW_RND_CLIP4_MAX_SATU_H(dst0, dst1, dst2, dst3, weight_vec,
-                                       offset_vec, rnd_vec, dst0, dst1, dst2,
-                                       dst3);
-        HEVC_UNIW_RND_CLIP2_MAX_SATU_H(dst4, dst5, weight_vec, offset_vec,
-                                       rnd_vec, dst4, dst5);
-
-        PCKEV_B3_UB(dst1, dst0, dst4, dst3, dst5, dst2, out0, out1, out2);
+        HEVC_FILT_8TAP_4W_SH(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
+                             filt3, dst8, dst9);
+        HEVC_FILT_8TAP_4W_SH(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
+                             filt3, dst10, dst11);
+
+        MUL4(dst0, weight_vec, dst1, weight_vec, dst2, weight_vec, dst3,
+             weight_vec, dst0, dst1, dst2, dst3)
+        MUL4(dst4, weight_vec, dst5, weight_vec, dst6, weight_vec, dst7,
+             weight_vec, dst4, dst5, dst6, dst7);
+        MUL4(dst8, weight_vec, dst9, weight_vec, dst10, weight_vec, dst11,
+             weight_vec, dst8, dst9, dst10, dst11)
+        SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
+        SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
+        SRAR_W4_SW(dst8, dst9, dst10, dst11, rnd_vec);
+        ADD4(dst0, offset_vec, dst1, offset_vec, dst2, offset_vec, dst3,
+             offset_vec, dst0, dst1, dst2, dst3);
+        ADD4(dst4, offset_vec, dst5, offset_vec, dst6, offset_vec, dst7,
+             offset_vec, dst4, dst5, dst6, dst7);
+        ADD4(dst8, offset_vec, dst9, offset_vec, dst10, offset_vec, dst11,
+             offset_vec, dst8, dst9, dst10, dst11);
+        CLIP_SW8_0_255(dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7);
+        CLIP_SW4_0_255(dst8, dst9, dst10, dst11);
+        PCKEV_H4_SH(dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
+                    vec0, vec1, vec2, vec3);
+        PCKEV_H2_SH(dst9, dst8, dst11, dst10, vec4, vec5);
+
+        PCKEV_B3_UB(vec1, vec0, vec4, vec3, vec5, vec2, out0, out1, out2);
         ST_UB2(out0, out1, dst, dst_stride);
         ST_D2(out2, 0, 1, dst + 16, dst_stride);
         dst += (2 * dst_stride);
@@ -1022,27 +1138,20 @@ static void hevc_hz_uniwgt_8t_32w_msa(uint8_t *src,
     v16i8 vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
     v16i8 vec8, vec9, vec10, vec11, vec12, vec13, vec14, vec15;
     v8i16 filter_vec;
-    v8i16 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
-    v8i16 weight_vec_h, offset_vec, denom_vec;
-    v4i32 weight_vec, rnd_vec;
+    v4i32 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    v4i32 dst10, dst11, dst12, dst13, dst14, dst15, dst16, dst17;
+    v4i32 weight_vec, rnd_vec, offset_vec;
+    v8i16 zero = { 0 };
 
     src -= 3;
 
     weight_vec = __msa_fill_w(weight);
     rnd_vec = __msa_fill_w(rnd_val);
-
-    weight *= 128;
-    rnd_val -= 6;
-
-    weight_vec_h = __msa_fill_h(weight);
-    offset_vec = __msa_fill_h(offset);
-    denom_vec = __msa_fill_h(rnd_val);
-
-    weight_vec_h = __msa_srar_h(weight_vec_h, denom_vec);
-    offset_vec = __msa_adds_s_h(offset_vec, weight_vec_h);
+    offset_vec = __msa_fill_w(offset);
 
     filter_vec = LD_SH(filter);
-    SPLATI_H4_SH(filter_vec, 0, 1, 2, 3, filt0, filt1, filt2, filt3);
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W4_SH(filter_vec, filt0, filt1, filt2, filt3);
 
     mask0 = LD_SB(&ff_hevc_mask_arr[0]);
     mask1 = mask0 + 2;
@@ -1054,7 +1163,6 @@ static void hevc_hz_uniwgt_8t_32w_msa(uint8_t *src,
         src += src_stride;
         LD_SB4(src, 8, src4, src5, src6, src7);
         src += src_stride;
-        XORI_B8_128_SB(src0, src1, src2, src3, src4, src5, src6, src7);
 
         VSHF_B4_SB(src0, src0, mask0, mask1, mask2, mask3,
                    vec0, vec1, vec2, vec3);
@@ -1064,14 +1172,14 @@ static void hevc_hz_uniwgt_8t_32w_msa(uint8_t *src,
                    vec8, vec9, vec10, vec11);
         VSHF_B4_SB(src3, src3, mask0, mask1, mask2, mask3,
                    vec12, vec13, vec14, vec15);
-        dst0 = HEVC_FILT_8TAP_SH(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
-                                 filt3);
-        dst1 = HEVC_FILT_8TAP_SH(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
-                                 filt3);
-        dst2 = HEVC_FILT_8TAP_SH(vec8, vec9, vec10, vec11, filt0, filt1, filt2,
-                                 filt3);
-        dst3 = HEVC_FILT_8TAP_SH(vec12, vec13, vec14, vec15, filt0, filt1,
-                                 filt2, filt3);
+        HEVC_FILT_8TAP_4W_SH(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
+                             filt3, dst0, dst1);
+        HEVC_FILT_8TAP_4W_SH(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
+                             filt3, dst2, dst3);
+        HEVC_FILT_8TAP_4W_SH(vec8, vec9, vec10, vec11, filt0, filt1, filt2,
+                             filt3, dst4, dst5);
+        HEVC_FILT_8TAP_4W_SH(vec12, vec13, vec14, vec15, filt0, filt1, filt2,
+                             filt3, dst6, dst7);
 
         VSHF_B4_SB(src4, src4, mask0, mask1, mask2, mask3,
                    vec0, vec1, vec2, vec3);
@@ -1081,24 +1189,44 @@ static void hevc_hz_uniwgt_8t_32w_msa(uint8_t *src,
                    vec8, vec9, vec10, vec11);
         VSHF_B4_SB(src7, src7, mask0, mask1, mask2, mask3,
                    vec12, vec13, vec14, vec15);
-        dst4 = HEVC_FILT_8TAP_SH(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
-                                 filt3);
-        dst5 = HEVC_FILT_8TAP_SH(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
-                                 filt3);
-        dst6 = HEVC_FILT_8TAP_SH(vec8, vec9, vec10, vec11, filt0, filt1, filt2,
-                                 filt3);
-        dst7 = HEVC_FILT_8TAP_SH(vec12, vec13, vec14, vec15, filt0, filt1,
-                                 filt2, filt3);
-
-        HEVC_UNIW_RND_CLIP4_MAX_SATU_H(dst0, dst1, dst2, dst3, weight_vec,
-                                       offset_vec, rnd_vec, dst0, dst1, dst2,
-                                       dst3);
-        HEVC_UNIW_RND_CLIP4_MAX_SATU_H(dst4, dst5, dst6, dst7, weight_vec,
-                                       offset_vec, rnd_vec, dst4, dst5, dst6,
-                                       dst7);
-
-        PCKEV_B2_UB(dst1, dst0, dst3, dst2, out0, out1);
-        PCKEV_B2_UB(dst5, dst4, dst7, dst6, out2, out3);
+        HEVC_FILT_8TAP_4W_SH(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
+                             filt3, dst10, dst11);
+        HEVC_FILT_8TAP_4W_SH(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
+                             filt3, dst12, dst13);
+        HEVC_FILT_8TAP_4W_SH(vec8, vec9, vec10, vec11, filt0, filt1, filt2,
+                             filt3, dst14, dst15);
+        HEVC_FILT_8TAP_4W_SH(vec12, vec13, vec14, vec15, filt0, filt1, filt2,
+                             filt3, dst16, dst17);
+
+        MUL4(dst0, weight_vec, dst1, weight_vec, dst2, weight_vec, dst3,
+             weight_vec, dst0, dst1, dst2, dst3)
+        MUL4(dst4, weight_vec, dst5, weight_vec, dst6, weight_vec, dst7,
+             weight_vec, dst4, dst5, dst6, dst7);
+        MUL4(dst10, weight_vec, dst11, weight_vec, dst12, weight_vec, dst13,
+             weight_vec, dst10, dst11, dst12, dst13)
+        MUL4(dst14, weight_vec, dst15, weight_vec, dst16, weight_vec, dst17,
+             weight_vec, dst14, dst15, dst16, dst17);
+        SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
+        SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
+        SRAR_W4_SW(dst10, dst11, dst12, dst13, rnd_vec);
+        SRAR_W4_SW(dst14, dst15, dst16, dst17, rnd_vec);
+        ADD4(dst0, offset_vec, dst1, offset_vec, dst2, offset_vec, dst3,
+             offset_vec, dst0, dst1, dst2, dst3);
+        ADD4(dst4, offset_vec, dst5, offset_vec, dst6, offset_vec, dst7,
+             offset_vec, dst4, dst5, dst6, dst7);
+        ADD4(dst10, offset_vec, dst11, offset_vec, dst12, offset_vec, dst13,
+             offset_vec, dst10, dst11, dst12, dst13);
+        ADD4(dst14, offset_vec, dst15, offset_vec, dst16, offset_vec, dst17,
+             offset_vec, dst14, dst15, dst16, dst17);
+        CLIP_SW8_0_255(dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7);
+        CLIP_SW8_0_255(dst10, dst11, dst12, dst13, dst14, dst15, dst16, dst17);
+        PCKEV_H4_SH(dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
+                    vec0, vec1, vec2, vec3);
+        PCKEV_H4_SH(dst11, dst10, dst13, dst12, dst15, dst14, dst17, dst16,
+                    vec4, vec5, vec6, vec7);
+
+        PCKEV_B2_UB(vec1, vec0, vec3, vec2, out0, out1);
+        PCKEV_B2_UB(vec5, vec4, vec7, vec6, out2, out3);
         ST_UB2(out0, out1, dst, 16);
         dst += dst_stride;
         ST_UB2(out2, out3, dst, 16);
@@ -1123,28 +1251,21 @@ static void hevc_hz_uniwgt_8t_48w_msa(uint8_t *src,
     v16i8 mask0, mask1, mask2, mask3, mask4, mask5, mask6, mask7;
     v16i8 vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
     v16i8 vec8, vec9, vec10, vec11, vec12, vec13, vec14, vec15;
-    v8i16 dst0, dst1, dst2, dst3, dst4, dst5;
-    v8i16 filter_vec, weight_vec_h, offset_vec, denom_vec;
-    v4i32 weight_vec, rnd_vec;
+    v4i32 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    v4i32 dst8, dst9, dst10, dst11;
+    v8i16 filter_vec;
+    v4i32 weight_vec, rnd_vec, offset_vec;
+    v8i16 zero = { 0 };
 
     src -= 3;
 
-    weight = weight & 0x0000FFFF;
     weight_vec = __msa_fill_w(weight);
     rnd_vec = __msa_fill_w(rnd_val);
-
-    weight *= 128;
-    rnd_val -= 6;
-
-    weight_vec_h = __msa_fill_h(weight);
-    offset_vec = __msa_fill_h(offset);
-    denom_vec = __msa_fill_h(rnd_val);
-
-    weight_vec_h = __msa_srar_h(weight_vec_h, denom_vec);
-    offset_vec = __msa_adds_s_h(offset_vec, weight_vec_h);
+    offset_vec = __msa_fill_w(offset);
 
     filter_vec = LD_SH(filter);
-    SPLATI_H4_SH(filter_vec, 0, 1, 2, 3, filt0, filt1, filt2, filt3);
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W4_SH(filter_vec, filt0, filt1, filt2, filt3);
 
     mask0 = LD_SB(&ff_hevc_mask_arr[0]);
     mask1 = mask0 + 2;
@@ -1159,7 +1280,6 @@ static void hevc_hz_uniwgt_8t_48w_msa(uint8_t *src,
         LD_SB3(src, 16, src0, src1, src2);
         src3 = LD_SB(src + 40);
         src += src_stride;
-        XORI_B4_128_SB(src0, src1, src2, src3);
 
         VSHF_B4_SB(src0, src0, mask0, mask1, mask2, mask3,
                    vec0, vec1, vec2, vec3);
@@ -1169,31 +1289,46 @@ static void hevc_hz_uniwgt_8t_48w_msa(uint8_t *src,
                    vec8, vec9, vec10, vec11);
         VSHF_B4_SB(src1, src2, mask4, mask5, mask6, mask7,
                    vec12, vec13, vec14, vec15);
-        dst0 = HEVC_FILT_8TAP_SH(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
-                                 filt3);
-        dst1 = HEVC_FILT_8TAP_SH(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
-                                 filt3);
-        dst2 = HEVC_FILT_8TAP_SH(vec8, vec9, vec10, vec11, filt0, filt1, filt2,
-                                 filt3);
-        dst3 = HEVC_FILT_8TAP_SH(vec12, vec13, vec14, vec15, filt0, filt1,
-                                 filt2, filt3);
+        HEVC_FILT_8TAP_4W_SH(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
+                             filt3, dst0, dst1);
+        HEVC_FILT_8TAP_4W_SH(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
+                             filt3, dst2, dst3);
+        HEVC_FILT_8TAP_4W_SH(vec8, vec9, vec10, vec11, filt0, filt1, filt2,
+                             filt3, dst4, dst5);
+        HEVC_FILT_8TAP_4W_SH(vec12, vec13, vec14, vec15, filt0, filt1, filt2,
+                             filt3, dst6, dst7);
 
         VSHF_B4_SB(src2, src2, mask0, mask1, mask2, mask3,
                    vec0, vec1, vec2, vec3);
         VSHF_B4_SB(src3, src3, mask0, mask1, mask2, mask3,
                    vec4, vec5, vec6, vec7);
-        dst4 = HEVC_FILT_8TAP_SH(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
-                                 filt3);
-        dst5 = HEVC_FILT_8TAP_SH(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
-                                 filt3);
-
-        HEVC_UNIW_RND_CLIP4_MAX_SATU_H(dst0, dst1, dst2, dst3, weight_vec,
-                                       offset_vec, rnd_vec, dst0, dst1, dst2,
-                                       dst3);
-        HEVC_UNIW_RND_CLIP2_MAX_SATU_H(dst4, dst5, weight_vec, offset_vec,
-                                       rnd_vec, dst4, dst5);
-
-        PCKEV_B3_UB(dst1, dst0, dst3, dst2, dst5, dst4, out0, out1, out2);
+        HEVC_FILT_8TAP_4W_SH(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
+                             filt3, dst8, dst9);
+        HEVC_FILT_8TAP_4W_SH(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
+                             filt3, dst10, dst11);
+
+        MUL4(dst0, weight_vec, dst1, weight_vec, dst2, weight_vec, dst3,
+             weight_vec, dst0, dst1, dst2, dst3)
+        MUL4(dst4, weight_vec, dst5, weight_vec, dst6, weight_vec, dst7,
+             weight_vec, dst4, dst5, dst6, dst7);
+        MUL4(dst8, weight_vec, dst9, weight_vec, dst10, weight_vec, dst11,
+             weight_vec, dst8, dst9, dst10, dst11)
+        SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
+        SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
+        SRAR_W4_SW(dst8, dst9, dst10, dst11, rnd_vec);
+        ADD4(dst0, offset_vec, dst1, offset_vec, dst2, offset_vec, dst3,
+             offset_vec, dst0, dst1, dst2, dst3);
+        ADD4(dst4, offset_vec, dst5, offset_vec, dst6, offset_vec, dst7,
+             offset_vec, dst4, dst5, dst6, dst7);
+        ADD4(dst8, offset_vec, dst9, offset_vec, dst10, offset_vec, dst11,
+             offset_vec, dst8, dst9, dst10, dst11);
+        CLIP_SW8_0_255(dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7);
+        CLIP_SW4_0_255(dst8, dst9, dst10, dst11);
+        PCKEV_H4_SH(dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
+                    vec0, vec1, vec2, vec3);
+        PCKEV_H2_SH(dst9, dst8, dst11, dst10, vec4, vec5);
+        PCKEV_B2_UB(vec1, vec0, vec3, vec2, out0, out1);
+        out2 = __msa_pckev_b((v16i8) vec5, (v16i8) vec4);
         ST_UB2(out0, out1, dst, 16);
         ST_UB(out2, dst + 32);
         dst += dst_stride;
@@ -1219,27 +1354,20 @@ static void hevc_hz_uniwgt_8t_64w_msa(uint8_t *src,
     v16i8 mask0, mask1, mask2, mask3, mask4, mask5, mask6, mask7;
     v16i8 vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
     v16i8 vec8, vec9, vec10, vec11, vec12, vec13, vec14, vec15;
-    v8i16 dst0, dst1, dst2, dst3;
-    v8i16 filter_vec, weight_vec_h, offset_vec, denom_vec;
-    v4i32 weight_vec, rnd_vec;
+    v4i32 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    v8i16 filter_vec;
+    v4i32 weight_vec, rnd_vec, offset_vec;
+    v8i16 zero = { 0 };
 
     src -= 3;
 
     weight_vec = __msa_fill_w(weight);
     rnd_vec = __msa_fill_w(rnd_val);
-
-    weight *= 128;
-    rnd_val -= 6;
-
-    weight_vec_h = __msa_fill_h(weight);
-    offset_vec = __msa_fill_h(offset);
-    denom_vec = __msa_fill_h(rnd_val);
-
-    weight_vec_h = __msa_srar_h(weight_vec_h, denom_vec);
-    offset_vec = __msa_adds_s_h(offset_vec, weight_vec_h);
+    offset_vec = __msa_fill_w(offset);
 
     filter_vec = LD_SH(filter);
-    SPLATI_H4_SH(filter_vec, 0, 1, 2, 3, filt0, filt1, filt2, filt3);
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W4_SH(filter_vec, filt0, filt1, filt2, filt3);
 
     mask0 = LD_SB(&ff_hevc_mask_arr[0]);
     mask1 = mask0 + 2;
@@ -1258,7 +1386,6 @@ static void hevc_hz_uniwgt_8t_64w_msa(uint8_t *src,
             LD_SB2(src_tmp, 16, src0, src1);
             src2 = LD_SB(src_tmp + 24);
             src_tmp += 32;
-            XORI_B3_128_SB(src0, src1, src2);
 
             VSHF_B4_SB(src0, src0, mask0, mask1, mask2, mask3,
                        vec0, vec1, vec2, vec3);
@@ -1268,20 +1395,28 @@ static void hevc_hz_uniwgt_8t_64w_msa(uint8_t *src,
                        vec8, vec9, vec10, vec11);
             VSHF_B4_SB(src2, src2, mask0, mask1, mask2, mask3,
                        vec12, vec13, vec14, vec15);
-            dst0 = HEVC_FILT_8TAP_SH(vec0, vec1, vec2, vec3, filt0, filt1,
-                                     filt2, filt3);
-            dst1 = HEVC_FILT_8TAP_SH(vec4, vec5, vec6, vec7, filt0, filt1,
-                                     filt2, filt3);
-            dst2 = HEVC_FILT_8TAP_SH(vec8, vec9, vec10, vec11, filt0, filt1,
-                                     filt2, filt3);
-            dst3 = HEVC_FILT_8TAP_SH(vec12, vec13, vec14, vec15, filt0, filt1,
-                                     filt2, filt3);
-
-            HEVC_UNIW_RND_CLIP4_MAX_SATU_H(dst0, dst1, dst2, dst3, weight_vec,
-                                           offset_vec, rnd_vec, dst0, dst1,
-                                           dst2, dst3);
-
-            PCKEV_B2_UB(dst1, dst0, dst3, dst2, out0, out1);
+            HEVC_FILT_8TAP_4W_SH(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
+                                 filt3, dst0, dst1);
+            HEVC_FILT_8TAP_4W_SH(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
+                                 filt3, dst2, dst3);
+            HEVC_FILT_8TAP_4W_SH(vec8, vec9, vec10, vec11, filt0, filt1, filt2,
+                                 filt3, dst4, dst5);
+            HEVC_FILT_8TAP_4W_SH(vec12, vec13, vec14, vec15, filt0, filt1, filt2,
+                                 filt3, dst6, dst7);
+            MUL4(dst0, weight_vec, dst1, weight_vec, dst2, weight_vec, dst3,
+                 weight_vec, dst0, dst1, dst2, dst3)
+            MUL4(dst4, weight_vec, dst5, weight_vec, dst6, weight_vec, dst7,
+                 weight_vec, dst4, dst5, dst6, dst7);
+            SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
+            SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
+            ADD4(dst0, offset_vec, dst1, offset_vec, dst2, offset_vec, dst3,
+                 offset_vec, dst0, dst1, dst2, dst3);
+            ADD4(dst4, offset_vec, dst5, offset_vec, dst6, offset_vec, dst7,
+                 offset_vec, dst4, dst5, dst6, dst7);
+            CLIP_SW8_0_255(dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7);
+            PCKEV_H4_SH(dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
+                        vec0, vec1, vec2, vec3);
+            PCKEV_B2_UB(vec1, vec0, vec3, vec2, out0, out1);
             ST_UB2(out0, out1, dst_tmp, 16);
             dst_tmp += 32;
         }
@@ -1302,6 +1437,7 @@ static void hevc_vt_uniwgt_8t_4w_msa(uint8_t *src,
                                      int32_t rnd_val)
 {
     int32_t loop_cnt;
+    int32_t res = height & 0x07;
     v16u8 out0, out1;
     v16i8 src0, src1, src2, src3, src4, src5, src6, src7, src8;
     v16i8 src9, src10, src11, src12, src13, src14;
@@ -1310,29 +1446,23 @@ static void hevc_vt_uniwgt_8t_4w_msa(uint8_t *src,
     v16i8 src1110_r, src1211_r, src1312_r, src1413_r;
     v16i8 src2110, src4332, src6554, src8776, src10998;
     v16i8 src12111110, src14131312;
-    v8i16 filter_vec, dst01, dst23, dst45, dst67;
+    v8i16 filter_vec;
+    v4i32 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
     v8i16 filt0, filt1, filt2, filt3;
-    v8i16 dst0, dst1, dst2, dst3, weight_vec_h, offset_vec, denom_vec;
-    v4i32 weight_vec, rnd_vec;
+    v8i16 vec0, vec1, vec2, vec3;
+    v4i32 weight_vec, rnd_vec, offset_vec;
+    v8i16 zero = { 0 };
 
     src -= (3 * src_stride);
 
 
     weight_vec = __msa_fill_w(weight);
     rnd_vec = __msa_fill_w(rnd_val);
-
-    weight *= 128;
-    rnd_val -= 6;
-
-    weight_vec_h = __msa_fill_h(weight);
-    offset_vec = __msa_fill_h(offset);
-    denom_vec = __msa_fill_h(rnd_val);
-
-    weight_vec_h = __msa_srar_h(weight_vec_h, denom_vec);
-    offset_vec = __msa_adds_s_h(offset_vec, weight_vec_h);
+    offset_vec = __msa_fill_w(offset);
 
     filter_vec = LD_SH(filter);
-    SPLATI_H4_SH(filter_vec, 0, 1, 2, 3, filt0, filt1, filt2, filt3);
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W4_SH(filter_vec, filt0, filt1, filt2, filt3);
 
     LD_SB7(src, src_stride, src0, src1, src2, src3, src4, src5, src6);
     src += (7 * src_stride);
@@ -1345,7 +1475,6 @@ static void hevc_vt_uniwgt_8t_4w_msa(uint8_t *src,
     ILVR_D3_SB(src21_r, src10_r, src43_r,
                src32_r, src65_r, src54_r, src2110, src4332, src6554);
 
-    XORI_B3_128_SB(src2110, src4332, src6554);
 
     for (loop_cnt = (height >> 3); loop_cnt--;) {
         LD_SB8(src, src_stride,
@@ -1358,21 +1487,30 @@ static void hevc_vt_uniwgt_8t_4w_msa(uint8_t *src,
         ILVR_D4_SB(src87_r, src76_r, src109_r, src98_r, src1211_r, src1110_r,
                    src1413_r, src1312_r,
                    src8776, src10998, src12111110, src14131312);
-        XORI_B4_128_SB(src8776, src10998, src12111110, src14131312);
-        dst01 = HEVC_FILT_8TAP_SH(src2110, src4332, src6554, src8776, filt0,
-                                  filt1, filt2, filt3);
-        dst23 = HEVC_FILT_8TAP_SH(src4332, src6554, src8776, src10998, filt0,
-                                  filt1, filt2, filt3);
-        dst45 = HEVC_FILT_8TAP_SH(src6554, src8776, src10998, src12111110,
-                                  filt0, filt1, filt2, filt3);
-        dst67 = HEVC_FILT_8TAP_SH(src8776, src10998, src12111110, src14131312,
-                                  filt0, filt1, filt2, filt3);
-
-        HEVC_UNIW_RND_CLIP4_MAX_SATU_H(dst01, dst23, dst45, dst67, weight_vec,
-                                       offset_vec, rnd_vec, dst0, dst1, dst2,
-                                       dst3);
+        HEVC_FILT_8TAP_4W_SH(src2110, src4332, src6554, src8776, filt0,
+                             filt1, filt2, filt3, dst0, dst1);
+        HEVC_FILT_8TAP_4W_SH(src4332, src6554, src8776, src10998, filt0,
+                             filt1, filt2, filt3, dst2, dst3);
+        HEVC_FILT_8TAP_4W_SH(src6554, src8776, src10998, src12111110,
+                             filt0, filt1, filt2, filt3, dst4, dst5);
+        HEVC_FILT_8TAP_4W_SH(src8776, src10998, src12111110, src14131312,
+                             filt0, filt1, filt2, filt3, dst6, dst7);
+        MUL4(dst0, weight_vec, dst1, weight_vec, dst2, weight_vec, dst3,
+             weight_vec, dst0, dst1, dst2, dst3)
+        MUL4(dst4, weight_vec, dst5, weight_vec, dst6, weight_vec, dst7,
+             weight_vec, dst4, dst5, dst6, dst7);
+        SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
+        SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
+        ADD4(dst0, offset_vec, dst1, offset_vec, dst2, offset_vec, dst3,
+             offset_vec, dst0, dst1, dst2, dst3);
+        ADD4(dst4, offset_vec, dst5, offset_vec, dst6, offset_vec, dst7,
+             offset_vec, dst4, dst5, dst6, dst7);
+        CLIP_SW8_0_255(dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7);
+        PCKEV_H4_SH(dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
+                    vec0, vec1, vec2, vec3);
+        PCKEV_B2_UB(vec1, vec0, vec3, vec2, out0, out1);
+
 
-        PCKEV_B2_UB(dst1, dst0, dst3, dst2, out0, out1);
         ST_W8(out0, out1, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
         dst += (8 * dst_stride);
 
@@ -1381,6 +1519,48 @@ static void hevc_vt_uniwgt_8t_4w_msa(uint8_t *src,
         src6554 = src14131312;
         src6 = src14;
     }
+    if (res) {
+        LD_SB8(src, src_stride,
+               src7, src8, src9, src10, src11, src12, src13, src14);
+        ILVR_B4_SB(src7, src6, src8, src7, src9, src8, src10, src9,
+                   src76_r, src87_r, src98_r, src109_r);
+        ILVR_B4_SB(src11, src10, src12, src11, src13, src12, src14, src13,
+                   src1110_r, src1211_r, src1312_r, src1413_r);
+        ILVR_D4_SB(src87_r, src76_r, src109_r, src98_r, src1211_r, src1110_r,
+                   src1413_r, src1312_r,
+                   src8776, src10998, src12111110, src14131312);
+        HEVC_FILT_8TAP_4W_SH(src2110, src4332, src6554, src8776, filt0,
+                             filt1, filt2, filt3, dst0, dst1);
+        HEVC_FILT_8TAP_4W_SH(src4332, src6554, src8776, src10998, filt0,
+                             filt1, filt2, filt3, dst2, dst3);
+        HEVC_FILT_8TAP_4W_SH(src6554, src8776, src10998, src12111110,
+                             filt0, filt1, filt2, filt3, dst4, dst5);
+        HEVC_FILT_8TAP_4W_SH(src8776, src10998, src12111110, src14131312,
+                             filt0, filt1, filt2, filt3, dst6, dst7);
+        MUL4(dst0, weight_vec, dst1, weight_vec, dst2, weight_vec, dst3,
+             weight_vec, dst0, dst1, dst2, dst3)
+        MUL4(dst4, weight_vec, dst5, weight_vec, dst6, weight_vec, dst7,
+             weight_vec, dst4, dst5, dst6, dst7);
+        SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
+        SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
+        ADD4(dst0, offset_vec, dst1, offset_vec, dst2, offset_vec, dst3,
+             offset_vec, dst0, dst1, dst2, dst3);
+        ADD4(dst4, offset_vec, dst5, offset_vec, dst6, offset_vec, dst7,
+             offset_vec, dst4, dst5, dst6, dst7);
+        CLIP_SW8_0_255(dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7);
+        PCKEV_H4_SH(dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
+                    vec0, vec1, vec2, vec3);
+        PCKEV_B2_UB(vec1, vec0, vec3, vec2, out0, out1);
+
+        if (res == 2) {
+            ST_W2(out0, 0, 1, dst, dst_stride);
+        } else if (res == 4) {
+            ST_W4(out0, 0, 1, 2, 3, dst, dst_stride);
+        } else {
+            ST_W4(out0, 0, 1, 2, 3, dst, dst_stride);
+            ST_W2(out1, 0, 1, dst + 4 * dst_stride, dst_stride);
+        }
+    }
 }
 
 static void hevc_vt_uniwgt_8t_8w_msa(uint8_t *src,
@@ -1394,36 +1574,29 @@ static void hevc_vt_uniwgt_8t_8w_msa(uint8_t *src,
                                      int32_t rnd_val)
 {
     int32_t loop_cnt;
+    int32_t res = height & 0x03;
     v16u8 out0, out1;
     v16i8 src0, src1, src2, src3, src4, src5, src6, src7, src8, src9, src10;
     v16i8 src10_r, src32_r, src54_r, src76_r, src98_r;
     v16i8 src21_r, src43_r, src65_r, src87_r, src109_r;
     v8i16 filt0, filt1, filt2, filt3;
-    v8i16 filter_vec;
-    v8i16 dst0, dst1, dst2, dst3, weight_vec_h, offset_vec, denom_vec;
-    v4i32 weight_vec, rnd_vec;
+    v8i16 filter_vec, vec0, vec1, vec2, vec3;
+    v4i32 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    v4i32 weight_vec, rnd_vec, offset_vec;
+    v8i16 zero = { 0 };
 
     src -= (3 * src_stride);
 
     weight_vec = __msa_fill_w(weight);
     rnd_vec = __msa_fill_w(rnd_val);
-
-    weight *= 128;
-    rnd_val -= 6;
-
-    weight_vec_h = __msa_fill_h(weight);
-    offset_vec = __msa_fill_h(offset);
-    denom_vec = __msa_fill_h(rnd_val);
-
-    weight_vec_h = __msa_srar_h(weight_vec_h, denom_vec);
-    offset_vec = __msa_adds_s_h(offset_vec, weight_vec_h);
+    offset_vec = __msa_fill_w(offset);
 
     filter_vec = LD_SH(filter);
-    SPLATI_H4_SH(filter_vec, 0, 1, 2, 3, filt0, filt1, filt2, filt3);
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W4_SH(filter_vec, filt0, filt1, filt2, filt3);
 
     LD_SB7(src, src_stride, src0, src1, src2, src3, src4, src5, src6);
     src += (7 * src_stride);
-    XORI_B7_128_SB(src0, src1, src2, src3, src4, src5, src6);
 
     ILVR_B4_SB(src1, src0, src3, src2, src5, src4, src2, src1,
                src10_r, src32_r, src54_r, src21_r);
@@ -1432,23 +1605,30 @@ static void hevc_vt_uniwgt_8t_8w_msa(uint8_t *src,
     for (loop_cnt = (height >> 2); loop_cnt--;) {
         LD_SB4(src, src_stride, src7, src8, src9, src10);
         src += (4 * src_stride);
-        XORI_B4_128_SB(src7, src8, src9, src10);
         ILVR_B4_SB(src7, src6, src8, src7, src9, src8, src10, src9,
                    src76_r, src87_r, src98_r, src109_r);
-        dst0 = HEVC_FILT_8TAP_SH(src10_r, src32_r, src54_r, src76_r, filt0,
-                                 filt1, filt2, filt3);
-        dst1 = HEVC_FILT_8TAP_SH(src21_r, src43_r, src65_r, src87_r, filt0,
-                                 filt1, filt2, filt3);
-        dst2 = HEVC_FILT_8TAP_SH(src32_r, src54_r, src76_r, src98_r, filt0,
-                                 filt1, filt2, filt3);
-        dst3 = HEVC_FILT_8TAP_SH(src43_r, src65_r, src87_r, src109_r, filt0,
-                                 filt1, filt2, filt3);
-
-        HEVC_UNIW_RND_CLIP4_MAX_SATU_H(dst0, dst1, dst2, dst3, weight_vec,
-                                       offset_vec, rnd_vec, dst0, dst1, dst2,
-                                       dst3);
-
-        PCKEV_B2_UB(dst1, dst0, dst3, dst2, out0, out1);
+        HEVC_FILT_8TAP_4W_SH(src10_r, src32_r, src54_r, src76_r, filt0,
+                             filt1, filt2, filt3, dst0, dst1);
+        HEVC_FILT_8TAP_4W_SH(src21_r, src43_r, src65_r, src87_r, filt0,
+                             filt1, filt2, filt3, dst2, dst3);
+        HEVC_FILT_8TAP_4W_SH(src32_r, src54_r, src76_r, src98_r,
+                             filt0, filt1, filt2, filt3, dst4, dst5);
+        HEVC_FILT_8TAP_4W_SH(src43_r, src65_r, src87_r, src109_r,
+                             filt0, filt1, filt2, filt3, dst6, dst7);
+        MUL4(dst0, weight_vec, dst1, weight_vec, dst2, weight_vec, dst3,
+             weight_vec, dst0, dst1, dst2, dst3)
+        MUL4(dst4, weight_vec, dst5, weight_vec, dst6, weight_vec, dst7,
+             weight_vec, dst4, dst5, dst6, dst7);
+        SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
+        SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
+        ADD4(dst0, offset_vec, dst1, offset_vec, dst2, offset_vec, dst3,
+             offset_vec, dst0, dst1, dst2, dst3);
+        ADD4(dst4, offset_vec, dst5, offset_vec, dst6, offset_vec, dst7,
+             offset_vec, dst4, dst5, dst6, dst7);
+        CLIP_SW8_0_255(dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7);
+        PCKEV_H4_SH(dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
+                    vec0, vec1, vec2, vec3);
+        PCKEV_B2_UB(vec1, vec0, vec3, vec2, out0, out1);
         ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
         dst += (4 * dst_stride);
 
@@ -1460,6 +1640,23 @@ static void hevc_vt_uniwgt_8t_8w_msa(uint8_t *src,
         src65_r = src109_r;
         src6 = src10;
     }
+    if (res) {
+        LD_SB2(src, src_stride, src7, src8);
+        ILVR_B2_SB(src7, src6, src8, src7, src76_r, src87_r);
+        HEVC_FILT_8TAP_4W_SH(src10_r, src32_r, src54_r, src76_r, filt0,
+                             filt1, filt2, filt3, dst0, dst1);
+        HEVC_FILT_8TAP_4W_SH(src21_r, src43_r, src65_r, src87_r, filt0,
+                             filt1, filt2, filt3, dst2, dst3);
+        MUL4(dst0, weight_vec, dst1, weight_vec, dst2, weight_vec, dst3,
+             weight_vec, dst0, dst1, dst2, dst3)
+        SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
+        ADD4(dst0, offset_vec, dst1, offset_vec, dst2, offset_vec, dst3,
+             offset_vec, dst0, dst1, dst2, dst3);
+        CLIP_SW4_0_255(dst0, dst1, dst2, dst3);
+        PCKEV_H2_SH(dst1, dst0, dst3, dst2, vec0, vec1);
+        out0 = __msa_pckev_b((v16i8) vec1, (v16i8) vec0);
+        ST_D2(out0, 0, 1, dst, dst_stride);
+    }
 }
 
 static void hevc_vt_uniwgt_8t_12w_msa(uint8_t *src,
@@ -1481,32 +1678,24 @@ static void hevc_vt_uniwgt_8t_12w_msa(uint8_t *src,
     v16i8 src21_l, src43_l, src65_l, src87_l, src109_l;
     v16i8 src2110, src4332, src6554, src8776, src10998;
     v8i16 filt0, filt1, filt2, filt3;
-    v8i16 dst0, dst1, dst2, dst3, dst4, dst5;
-    v8i16 weight_vec_h, offset_vec, denom_vec, filter_vec;
-    v4i32 weight_vec, rnd_vec;
+    v4i32 dst0, dst1, dst2, dst3, dst4, dst5;
+    v4i32 dst6, dst7, dst8, dst9, dst10, dst11;
+    v8i16 filter_vec, vec0, vec1, vec2, vec3, vec4, vec5;
+    v4i32 weight_vec, rnd_vec, offset_vec;
+    v8i16 zero = { 0 };
 
     src -= (3 * src_stride);
 
-    weight = weight & 0x0000FFFF;
     weight_vec = __msa_fill_w(weight);
     rnd_vec = __msa_fill_w(rnd_val);
-
-    weight *= 128;
-    rnd_val -= 6;
-
-    weight_vec_h = __msa_fill_h(weight);
-    offset_vec = __msa_fill_h(offset);
-    denom_vec = __msa_fill_h(rnd_val);
-
-    weight_vec_h = __msa_srar_h(weight_vec_h, denom_vec);
-    offset_vec = __msa_adds_s_h(offset_vec, weight_vec_h);
+    offset_vec = __msa_fill_w(offset);
 
     filter_vec = LD_SH(filter);
-    SPLATI_H4_SH(filter_vec, 0, 1, 2, 3, filt0, filt1, filt2, filt3);
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W4_SH(filter_vec, filt0, filt1, filt2, filt3);
 
     LD_SB7(src, src_stride, src0, src1, src2, src3, src4, src5, src6);
     src += (7 * src_stride);
-    XORI_B7_128_SB(src0, src1, src2, src3, src4, src5, src6);
 
     ILVR_B4_SB(src1, src0, src3, src2, src5, src4, src2, src1,
                src10_r, src32_r, src54_r, src21_r);
@@ -1520,7 +1709,6 @@ static void hevc_vt_uniwgt_8t_12w_msa(uint8_t *src,
     for (loop_cnt = 4; loop_cnt--;) {
         LD_SB4(src, src_stride, src7, src8, src9, src10);
         src += (4 * src_stride);
-        XORI_B4_128_SB(src7, src8, src9, src10);
 
         ILVR_B4_SB(src7, src6, src8, src7, src9, src8, src10, src9,
                    src76_r, src87_r, src98_r, src109_r);
@@ -1528,26 +1716,40 @@ static void hevc_vt_uniwgt_8t_12w_msa(uint8_t *src,
                    src76_l, src87_l, src98_l, src109_l);
         ILVR_D2_SB(src87_l, src76_l, src109_l, src98_l, src8776, src10998);
 
-        dst0 = HEVC_FILT_8TAP_SH(src10_r, src32_r, src54_r, src76_r, filt0,
-                                 filt1, filt2, filt3);
-        dst1 = HEVC_FILT_8TAP_SH(src21_r, src43_r, src65_r, src87_r, filt0,
-                                 filt1, filt2, filt3);
-        dst2 = HEVC_FILT_8TAP_SH(src32_r, src54_r, src76_r, src98_r, filt0,
-                                 filt1, filt2, filt3);
-        dst3 = HEVC_FILT_8TAP_SH(src43_r, src65_r, src87_r, src109_r, filt0,
-                                 filt1, filt2, filt3);
-        dst4 = HEVC_FILT_8TAP_SH(src2110, src4332, src6554, src8776, filt0,
-                                 filt1, filt2, filt3);
-        dst5 = HEVC_FILT_8TAP_SH(src4332, src6554, src8776, src10998, filt0,
-                                 filt1, filt2, filt3);
-
-        HEVC_UNIW_RND_CLIP4_MAX_SATU_H(dst0, dst1, dst2, dst3, weight_vec,
-                                       offset_vec, rnd_vec, dst0, dst1, dst2,
-                                       dst3);
-        HEVC_UNIW_RND_CLIP2_MAX_SATU_H(dst4, dst5, weight_vec, offset_vec,
-                                       rnd_vec, dst4, dst5);
-
-        PCKEV_B3_UB(dst1, dst0, dst3, dst2, dst5, dst4, out0, out1, out2);
+        HEVC_FILT_8TAP_4W_SH(src10_r, src32_r, src54_r, src76_r, filt0,
+                             filt1, filt2, filt3, dst0, dst1);
+        HEVC_FILT_8TAP_4W_SH(src21_r, src43_r, src65_r, src87_r, filt0,
+                             filt1, filt2, filt3, dst2, dst3);
+        HEVC_FILT_8TAP_4W_SH(src32_r, src54_r, src76_r, src98_r,
+                             filt0, filt1, filt2, filt3, dst4, dst5);
+        HEVC_FILT_8TAP_4W_SH(src43_r, src65_r, src87_r, src109_r,
+                             filt0, filt1, filt2, filt3, dst6, dst7);
+        HEVC_FILT_8TAP_4W_SH(src2110, src4332, src6554, src8776,
+                             filt0, filt1, filt2, filt3, dst8, dst9);
+        HEVC_FILT_8TAP_4W_SH(src4332, src6554, src8776, src10998,
+                             filt0, filt1, filt2, filt3, dst10, dst11);
+
+        MUL4(dst0, weight_vec, dst1, weight_vec, dst2, weight_vec, dst3,
+             weight_vec, dst0, dst1, dst2, dst3)
+        MUL4(dst4, weight_vec, dst5, weight_vec, dst6, weight_vec, dst7,
+             weight_vec, dst4, dst5, dst6, dst7);
+        MUL4(dst8, weight_vec, dst9, weight_vec, dst10, weight_vec, dst11,
+             weight_vec, dst8, dst9, dst10, dst11);
+        SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
+        SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
+        SRAR_W4_SW(dst8, dst9, dst10, dst11, rnd_vec);
+        ADD4(dst0, offset_vec, dst1, offset_vec, dst2, offset_vec, dst3,
+             offset_vec, dst0, dst1, dst2, dst3);
+        ADD4(dst4, offset_vec, dst5, offset_vec, dst6, offset_vec, dst7,
+             offset_vec, dst4, dst5, dst6, dst7);
+        ADD4(dst8, offset_vec, dst9, offset_vec, dst10, offset_vec, dst11,
+             offset_vec, dst8, dst9, dst10, dst11);
+        CLIP_SW8_0_255(dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7);
+        CLIP_SW4_0_255(dst8, dst9, dst10, dst11);
+        PCKEV_H4_SH(dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
+                    vec0, vec1, vec2, vec3);
+        PCKEV_H2_SH(dst9, dst8, dst11, dst10, vec4, vec5);
+        PCKEV_B3_UB(vec1, vec0, vec3, vec2, vec5, vec4, out0, out1, out2);
         ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
         ST_W4(out2, 0, 1, 2, 3, dst + 8, dst_stride);
         dst += (4 * dst_stride);
@@ -1579,6 +1781,7 @@ static void hevc_vt_uniwgt_8t_16multx4mult_msa(uint8_t *src,
     uint8_t *src_tmp;
     uint8_t *dst_tmp;
     int32_t loop_cnt, cnt;
+    int32_t res = height & 0x03;
     v16u8 out0, out1, out2, out3;
     v16i8 src0, src1, src2, src3, src4, src5, src6, src7, src8, src9, src10;
     v16i8 src10_r, src32_r, src54_r, src76_r;
@@ -1587,28 +1790,22 @@ static void hevc_vt_uniwgt_8t_16multx4mult_msa(uint8_t *src,
     v16i8 src21_l, src43_l, src65_l, src87_l;
     v16i8 src98_r, src109_r, src98_l, src109_l;
     v8i16 filt0, filt1, filt2, filt3;
-    v8i16 filter_vec;
-    v8i16 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
-    v8i16 weight_vec_h, offset_vec, denom_vec;
-    v4i32 weight_vec, rnd_vec;
+    v8i16 filter_vec, vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
+    v4i32 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    v4i32 dst8, dst9, dst10, dst11, dst12, dst13, dst14, dst15;
+    v4i32 weight_vec, rnd_vec, offset_vec;
+    v8i16 zero = { 0 };
 
     src -= (3 * src_stride);
 
     weight_vec = __msa_fill_w(weight);
     rnd_vec = __msa_fill_w(rnd_val);
+    offset_vec = __msa_fill_w(offset);
 
-    weight *= 128;
-    rnd_val -= 6;
-
-    weight_vec_h = __msa_fill_h(weight);
-    offset_vec = __msa_fill_h(offset);
-    denom_vec = __msa_fill_h(rnd_val);
-
-    weight_vec_h = __msa_srar_h(weight_vec_h, denom_vec);
-    offset_vec = __msa_adds_s_h(offset_vec, weight_vec_h);
 
     filter_vec = LD_SH(filter);
-    SPLATI_H4_SH(filter_vec, 0, 1, 2, 3, filt0, filt1, filt2, filt3);
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W4_SH(filter_vec, filt0, filt1, filt2, filt3);
 
     for (cnt = weightmul16; cnt--;) {
         src_tmp = src;
@@ -1616,12 +1813,10 @@ static void hevc_vt_uniwgt_8t_16multx4mult_msa(uint8_t *src,
 
         LD_SB7(src_tmp, src_stride, src0, src1, src2, src3, src4, src5, src6);
         src_tmp += (7 * src_stride);
-        XORI_B7_128_SB(src0, src1, src2, src3, src4, src5, src6);
 
         for (loop_cnt = (height >> 2); loop_cnt--;) {
             LD_SB4(src_tmp, src_stride, src7, src8, src9, src10);
             src_tmp += (4 * src_stride);
-            XORI_B4_128_SB(src7, src8, src9, src10);
 
             ILVR_B4_SB(src1, src0, src3, src2, src5, src4, src2, src1,
                        src10_r, src32_r, src54_r, src21_r);
@@ -1634,31 +1829,52 @@ static void hevc_vt_uniwgt_8t_16multx4mult_msa(uint8_t *src,
             ILVL_B4_SB(src7, src6, src8, src7, src9, src8, src10, src9,
                        src76_l, src87_l, src98_l, src109_l);
 
-            dst0 = HEVC_FILT_8TAP_SH(src10_r, src32_r, src54_r, src76_r, filt0,
-                                     filt1, filt2, filt3);
-            dst1 = HEVC_FILT_8TAP_SH(src10_l, src32_l, src54_l, src76_l, filt0,
-                                     filt1, filt2, filt3);
-            dst2 = HEVC_FILT_8TAP_SH(src21_r, src43_r, src65_r, src87_r, filt0,
-                                     filt1, filt2, filt3);
-            dst3 = HEVC_FILT_8TAP_SH(src21_l, src43_l, src65_l, src87_l, filt0,
-                                     filt1, filt2, filt3);
-            dst4 = HEVC_FILT_8TAP_SH(src32_r, src54_r, src76_r, src98_r, filt0,
-                                     filt1, filt2, filt3);
-            dst5 = HEVC_FILT_8TAP_SH(src32_l, src54_l, src76_l, src98_l, filt0,
-                                     filt1, filt2, filt3);
-            dst6 = HEVC_FILT_8TAP_SH(src43_r, src65_r, src87_r, src109_r, filt0,
-                                     filt1, filt2, filt3);
-            dst7 = HEVC_FILT_8TAP_SH(src43_l, src65_l, src87_l, src109_l, filt0,
-                                     filt1, filt2, filt3);
+            HEVC_FILT_8TAP_4W_SH(src10_r, src32_r, src54_r, src76_r, filt0,
+                                 filt1, filt2, filt3, dst0, dst1);
+            HEVC_FILT_8TAP_4W_SH(src10_l, src32_l, src54_l, src76_l, filt0,
+                                 filt1, filt2, filt3, dst2, dst3);
+            HEVC_FILT_8TAP_4W_SH(src21_r, src43_r, src65_r, src87_r, filt0,
+                                 filt1, filt2, filt3, dst4, dst5);
+            HEVC_FILT_8TAP_4W_SH(src21_l, src43_l, src65_l, src87_l, filt0,
+                                 filt1, filt2, filt3, dst6, dst7);
+            HEVC_FILT_8TAP_4W_SH(src32_r, src54_r, src76_r, src98_r, filt0,
+                                 filt1, filt2, filt3, dst8, dst9);
+            HEVC_FILT_8TAP_4W_SH(src32_l, src54_l, src76_l, src98_l, filt0,
+                                 filt1, filt2, filt3, dst10, dst11);
+            HEVC_FILT_8TAP_4W_SH(src43_r, src65_r, src87_r, src109_r, filt0,
+                                 filt1, filt2, filt3, dst12, dst13);
+            HEVC_FILT_8TAP_4W_SH(src43_l, src65_l, src87_l, src109_l, filt0,
+                                 filt1, filt2, filt3, dst14, dst15);
+
+            MUL4(dst0, weight_vec, dst1, weight_vec, dst2, weight_vec, dst3,
+                 weight_vec, dst0, dst1, dst2, dst3)
+            MUL4(dst4, weight_vec, dst5, weight_vec, dst6, weight_vec, dst7,
+                 weight_vec, dst4, dst5, dst6, dst7);
+            MUL4(dst8, weight_vec, dst9, weight_vec, dst10, weight_vec, dst11,
+                 weight_vec, dst8, dst9, dst10, dst11);
+            MUL4(dst12, weight_vec, dst13, weight_vec, dst14, weight_vec, dst15,
+                 weight_vec, dst12, dst13, dst14, dst15);
+            SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
+            SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
+            SRAR_W4_SW(dst8, dst9, dst10, dst11, rnd_vec);
+            SRAR_W4_SW(dst12, dst13, dst14, dst15, rnd_vec);
+            ADD4(dst0, offset_vec, dst1, offset_vec, dst2, offset_vec, dst3,
+                 offset_vec, dst0, dst1, dst2, dst3);
+            ADD4(dst4, offset_vec, dst5, offset_vec, dst6, offset_vec, dst7,
+                 offset_vec, dst4, dst5, dst6, dst7);
+            ADD4(dst8, offset_vec, dst9, offset_vec, dst10, offset_vec, dst11,
+                 offset_vec, dst8, dst9, dst10, dst11);
+            ADD4(dst12, offset_vec, dst13, offset_vec, dst14, offset_vec, dst15,
+                 offset_vec, dst12, dst13, dst14, dst15);
+            CLIP_SW8_0_255(dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7);
+            CLIP_SW8_0_255(dst8, dst9, dst10, dst11, dst12, dst13, dst14, dst15);
+            PCKEV_H4_SH(dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
+                        vec0, vec1, vec2, vec3);
+            PCKEV_H4_SH(dst9, dst8, dst11, dst10, dst13, dst12, dst15,
+                        dst14, vec4, vec5, vec6, vec7);
+            PCKEV_B4_UB(vec1, vec0, vec3, vec2, vec5, vec4, vec7, vec6,
+                        out0, out1, out2, out3);
 
-            HEVC_UNIW_RND_CLIP4_MAX_SATU_H(dst0, dst1, dst2, dst3, weight_vec,
-                                           offset_vec, rnd_vec, dst0, dst1,
-                                           dst2, dst3);
-            HEVC_UNIW_RND_CLIP4_MAX_SATU_H(dst4, dst5, dst6, dst7, weight_vec,
-                                           offset_vec, rnd_vec, dst4, dst5,
-                                           dst6, dst7);
-            PCKEV_B2_UB(dst1, dst0, dst3, dst2, out0, out1);
-            PCKEV_B2_UB(dst5, dst4, dst7, dst6, out2, out3);
             ST_UB4(out0, out1, out2, out3, dst_tmp, dst_stride);
             dst_tmp += (4 * dst_stride);
 
@@ -1670,6 +1886,43 @@ static void hevc_vt_uniwgt_8t_16multx4mult_msa(uint8_t *src,
             src5 = src9;
             src6 = src10;
         }
+        if (res) {
+            LD_SB2(src_tmp, src_stride, src7, src8);
+
+            ILVR_B4_SB(src1, src0, src3, src2, src5, src4, src2, src1,
+                       src10_r, src32_r, src54_r, src21_r);
+            ILVR_B2_SB(src4, src3, src6, src5, src43_r, src65_r);
+            ILVL_B4_SB(src1, src0, src3, src2, src5, src4, src2, src1,
+                       src10_l, src32_l, src54_l, src21_l);
+            ILVL_B2_SB(src4, src3, src6, src5, src43_l, src65_l);
+            ILVR_B2_SB(src7, src6, src8, src7, src76_r, src87_r);
+            ILVL_B2_SB(src7, src6, src8, src7, src76_l, src87_l);
+
+            HEVC_FILT_8TAP_4W_SH(src10_r, src32_r, src54_r, src76_r, filt0,
+                                 filt1, filt2, filt3, dst0, dst1);
+            HEVC_FILT_8TAP_4W_SH(src10_l, src32_l, src54_l, src76_l, filt0,
+                                 filt1, filt2, filt3, dst2, dst3);
+            HEVC_FILT_8TAP_4W_SH(src21_r, src43_r, src65_r, src87_r, filt0,
+                                 filt1, filt2, filt3, dst4, dst5);
+            HEVC_FILT_8TAP_4W_SH(src21_l, src43_l, src65_l, src87_l, filt0,
+                                 filt1, filt2, filt3, dst6, dst7);
+            MUL4(dst0, weight_vec, dst1, weight_vec, dst2, weight_vec, dst3,
+                 weight_vec, dst0, dst1, dst2, dst3)
+            MUL4(dst4, weight_vec, dst5, weight_vec, dst6, weight_vec, dst7,
+                 weight_vec, dst4, dst5, dst6, dst7);
+            SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
+            SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
+            ADD4(dst0, offset_vec, dst1, offset_vec, dst2, offset_vec, dst3,
+                 offset_vec, dst0, dst1, dst2, dst3);
+            ADD4(dst4, offset_vec, dst5, offset_vec, dst6, offset_vec, dst7,
+                 offset_vec, dst4, dst5, dst6, dst7);
+            CLIP_SW8_0_255(dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7);
+            PCKEV_H4_SH(dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
+                        vec0, vec1, vec2, vec3);
+            PCKEV_B2_UB(vec1, vec0, vec3, vec2, out0, out1);
+
+            ST_UB2(out0, out1, dst_tmp, dst_stride);
+        }
 
         src += 16;
         dst += 16;
@@ -1774,20 +2027,21 @@ static void hevc_hv_uniwgt_8t_4w_msa(uint8_t *src,
     v8i16 filter_vec;
     v16i8 vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
     v16i8 vec8, vec9, vec10, vec11, vec12, vec13, vec14, vec15;
-    v8i16 dst30, dst41, dst52, dst63, dst66, dst97, dst108;
+    v4i32 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7, dst8, dst9, dst10;
     v8i16 dst10_r, dst32_r, dst54_r, dst76_r, dst98_r;
     v8i16 dst21_r, dst43_r, dst65_r, dst87_r, dst109_r;
     v4i32 dst0_r, dst1_r, dst2_r, dst3_r;
-    v4i32 weight_vec, offset_vec, rnd_vec, const_128, denom_vec;
+    v4i32 weight_vec, offset_vec, rnd_vec;
     v16i8 mask0 = LD_SB(ff_hevc_mask_arr + 16);
+    v8i16 zero = { 0 };
 
     src -= ((3 * src_stride) + 3);
     filter_vec = LD_SH(filter_x);
-    SPLATI_H4_SH(filter_vec, 0, 1, 2, 3, filt0, filt1, filt2, filt3);
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W4_SH(filter_vec, filt0, filt1, filt2, filt3);
 
     filter_vec = LD_SH(filter_y);
     UNPCK_R_SB_SH(filter_vec, filter_vec);
-
     SPLATI_W4_SH(filter_vec, filt_h0, filt_h1, filt_h2, filt_h3);
 
     mask1 = mask0 + 2;
@@ -1797,15 +2051,9 @@ static void hevc_hv_uniwgt_8t_4w_msa(uint8_t *src,
     weight_vec = __msa_fill_w(weight);
     offset_vec = __msa_fill_w(offset);
     rnd_vec = __msa_fill_w(rnd_val);
-    denom_vec = rnd_vec - 6;
-
-    const_128 = __msa_ldi_w(128);
-    const_128 *= weight_vec;
-    offset_vec += __msa_srar_w(const_128, denom_vec);
 
     LD_SB7(src, src_stride, src0, src1, src2, src3, src4, src5, src6);
     src += (7 * src_stride);
-    XORI_B7_128_SB(src0, src1, src2, src3, src4, src5, src6);
 
     /* row 0 row 1 row 2 row 3 */
     VSHF_B4_SB(src0, src3, mask0, mask1, mask2, mask3, vec0, vec1, vec2, vec3);
@@ -1814,39 +2062,41 @@ static void hevc_hv_uniwgt_8t_4w_msa(uint8_t *src,
                vec8, vec9, vec10, vec11);
     VSHF_B4_SB(src3, src6, mask0, mask1, mask2, mask3,
                vec12, vec13, vec14, vec15);
-    dst30 = HEVC_FILT_8TAP_SH(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
-                              filt3);
-    dst41 = HEVC_FILT_8TAP_SH(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
-                              filt3);
-    dst52 = HEVC_FILT_8TAP_SH(vec8, vec9, vec10, vec11, filt0, filt1, filt2,
-                              filt3);
-    dst63 = HEVC_FILT_8TAP_SH(vec12, vec13, vec14, vec15, filt0, filt1, filt2,
-                              filt3);
-
-    ILVRL_H2_SH(dst41, dst30, dst10_r, dst43_r);
-    ILVRL_H2_SH(dst52, dst41, dst21_r, dst54_r);
-    ILVRL_H2_SH(dst63, dst52, dst32_r, dst65_r);
-
-    dst66 = (v8i16) __msa_splati_d((v2i64) dst63, 1);
+    HEVC_FILT_8TAP_4W_SH(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
+                         filt3, dst0, dst3);
+    HEVC_FILT_8TAP_4W_SH(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
+                         filt3, dst1, dst4);
+    HEVC_FILT_8TAP_4W_SH(vec8, vec9, vec10, vec11, filt0, filt1, filt2,
+                         filt3, dst2, dst5);
+    vec0 = __msa_ilvl_b((v16i8) zero, (v16i8) vec12);
+    vec1 = __msa_ilvl_b((v16i8) zero, (v16i8) vec13);
+    vec2 = __msa_ilvl_b((v16i8) zero, (v16i8) vec14);
+    vec3 = __msa_ilvl_b((v16i8) zero, (v16i8) vec15);
+    dst6 = __msa_dotp_s_w((v8i16) vec0, (v8i16) filt0);
+    dst6 = __msa_dpadd_s_w((v4i32) dst6, (v8i16) vec1, (v8i16) filt1);
+    dst6 = __msa_dpadd_s_w((v4i32) dst6, (v8i16) vec2, (v8i16) filt2);
+    dst6 = __msa_dpadd_s_w((v4i32) dst6, (v8i16) vec3, (v8i16) filt3);
+
+    ILVEV_H2_SH(dst0, dst1, dst3, dst4, dst10_r, dst43_r);
+    ILVEV_H2_SH(dst1, dst2, dst4, dst5, dst21_r, dst54_r);
+    ILVEV_H2_SH(dst2, dst3, dst5, dst6, dst32_r, dst65_r);
 
     for (loop_cnt = height >> 2; loop_cnt--;) {
         LD_SB4(src, src_stride, src7, src8, src9, src10);
         src += (4 * src_stride);
-        XORI_B4_128_SB(src7, src8, src9, src10);
 
         VSHF_B4_SB(src7, src9, mask0, mask1, mask2, mask3,
                    vec0, vec1, vec2, vec3);
         VSHF_B4_SB(src8, src10, mask0, mask1, mask2, mask3,
                    vec4, vec5, vec6, vec7);
-        dst97 = HEVC_FILT_8TAP_SH(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
-                                  filt3);
-        dst108 = HEVC_FILT_8TAP_SH(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
-                                   filt3);
+        HEVC_FILT_8TAP_4W_SH(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
+                             filt3, dst7, dst9);
+        HEVC_FILT_8TAP_4W_SH(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
+                             filt3, dst8, dst10);
 
-        dst76_r = __msa_ilvr_h(dst97, dst66);
-        ILVRL_H2_SH(dst108, dst97, dst87_r, dst109_r);
-        dst66 = (v8i16) __msa_splati_d((v2i64) dst97, 1);
-        dst98_r = __msa_ilvr_h(dst66, dst108);
+        dst76_r = __msa_ilvev_h((v8i16) dst7, (v8i16) dst6);
+        ILVEV_H2_SH(dst7, dst8, dst9, dst10, dst87_r, dst109_r);
+        dst98_r = __msa_ilvev_h((v8i16) dst9, (v8i16) dst8);
 
         dst0_r = HEVC_FILT_8TAP(dst10_r, dst32_r, dst54_r, dst76_r, filt_h0,
                                 filt_h1, filt_h2, filt_h3);
@@ -1863,7 +2113,7 @@ static void hevc_hv_uniwgt_8t_4w_msa(uint8_t *src,
         SRAR_W4_SW(dst0_r, dst1_r, dst2_r, dst3_r, rnd_vec);
         ADD2(dst0_r, offset_vec, dst1_r, offset_vec, dst0_r, dst1_r);
         ADD2(dst2_r, offset_vec, dst3_r, offset_vec, dst2_r, dst3_r);
-        CLIP_SW4_0_255_MAX_SATU(dst0_r, dst1_r, dst2_r, dst3_r);
+        CLIP_SW4_0_255(dst0_r, dst1_r, dst2_r, dst3_r);
         PCKEV_H2_SW(dst1_r, dst0_r, dst3_r, dst2_r, dst0_r, dst1_r);
         out = (v16u8) __msa_pckev_b((v16i8) dst1_r, (v16i8) dst0_r);
         ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
@@ -1875,7 +2125,7 @@ static void hevc_hv_uniwgt_8t_4w_msa(uint8_t *src,
         dst21_r = dst65_r;
         dst43_r = dst87_r;
         dst65_r = dst109_r;
-        dst66 = (v8i16) __msa_splati_d((v2i64) dst108, 1);
+        dst6 = dst10;
     }
 }
 
@@ -1896,33 +2146,32 @@ static void hevc_hv_uniwgt_8t_8multx2mult_msa(uint8_t *src,
     uint8_t *dst_tmp;
     v16i8 src0, src1, src2, src3, src4, src5, src6, src7, src8;
     v8i16 filt0, filt1, filt2, filt3;
-    v4i32 filt_h0, filt_h1, filt_h2, filt_h3;
+    v8i16 filt_h0, filt_h1, filt_h2, filt_h3;
     v16i8 mask1, mask2, mask3;
     v8i16 filter_vec;
     v16i8 vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
     v16i8 vec8, vec9, vec10, vec11, vec12, vec13, vec14, vec15;
-    v8i16 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7, dst8;
     v4i32 dst0_r, dst0_l, dst1_r, dst1_l;
     v8i16 dst10_r, dst32_r, dst54_r, dst76_r;
     v8i16 dst10_l, dst32_l, dst54_l, dst76_l;
     v8i16 dst21_r, dst43_r, dst65_r, dst87_r;
     v8i16 dst21_l, dst43_l, dst65_l, dst87_l;
-    v4i32 weight_vec, offset_vec, rnd_vec, const_128, denom_vec;
+    v4i32 weight_vec, offset_vec, rnd_vec;
+    v4i32 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7, dst8, dst9, dst10;
+    v4i32 dst11, dst12, dst13, dst14, dst15;
     v16i8 mask0 = LD_SB(ff_hevc_mask_arr);
+    v8i16 zero = { 0 };
 
     src -= ((3 * src_stride) + 3);
 
     weight_vec = __msa_fill_w(weight);
     offset_vec = __msa_fill_w(offset);
     rnd_vec = __msa_fill_w(rnd_val);
-    denom_vec = rnd_vec - 6;
 
-    const_128 = __msa_ldi_w(128);
-    const_128 *= weight_vec;
-    offset_vec += __msa_srar_w(const_128, denom_vec);
 
     filter_vec = LD_SH(filter_x);
-    SPLATI_H4_SH(filter_vec, 0, 1, 2, 3, filt0, filt1, filt2, filt3);
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W4_SW(filter_vec, filt0, filt1, filt2, filt3);
 
     filter_vec = LD_SH(filter_y);
     UNPCK_R_SB_SH(filter_vec, filter_vec);
@@ -1938,7 +2187,6 @@ static void hevc_hv_uniwgt_8t_8multx2mult_msa(uint8_t *src,
 
         LD_SB7(src_tmp, src_stride, src0, src1, src2, src3, src4, src5, src6);
         src_tmp += (7 * src_stride);
-        XORI_B7_128_SB(src0, src1, src2, src3, src4, src5, src6);
 
         VSHF_B4_SB(src0, src0, mask0, mask1, mask2, mask3,
                    vec0, vec1, vec2, vec3);
@@ -1948,14 +2196,14 @@ static void hevc_hv_uniwgt_8t_8multx2mult_msa(uint8_t *src,
                    vec8, vec9, vec10, vec11);
         VSHF_B4_SB(src3, src3, mask0, mask1, mask2, mask3,
                    vec12, vec13, vec14, vec15);
-        dst0 = HEVC_FILT_8TAP_SH(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
-                                 filt3);
-        dst1 = HEVC_FILT_8TAP_SH(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
-                                 filt3);
-        dst2 = HEVC_FILT_8TAP_SH(vec8, vec9, vec10, vec11, filt0, filt1, filt2,
-                                 filt3);
-        dst3 = HEVC_FILT_8TAP_SH(vec12, vec13, vec14, vec15, filt0, filt1,
-                                 filt2, filt3);
+        HEVC_FILT_8TAP_4W_SH(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
+                             filt3, dst0, dst1);
+        HEVC_FILT_8TAP_4W_SH(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
+                             filt3, dst2, dst3);
+        HEVC_FILT_8TAP_4W_SH(vec8, vec9, vec10, vec11, filt0, filt1, filt2,
+                             filt3, dst4, dst5);
+        HEVC_FILT_8TAP_4W_SH(vec12, vec13, vec14, vec15, filt0, filt1,
+                             filt2, filt3, dst6, dst7);
 
         VSHF_B4_SB(src4, src4, mask0, mask1, mask2, mask3,
                    vec0, vec1, vec2, vec3);
@@ -1963,31 +2211,30 @@ static void hevc_hv_uniwgt_8t_8multx2mult_msa(uint8_t *src,
                    vec4, vec5, vec6, vec7);
         VSHF_B4_SB(src6, src6, mask0, mask1, mask2, mask3,
                    vec8, vec9, vec10, vec11);
-        dst4 = HEVC_FILT_8TAP_SH(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
-                                 filt3);
-        dst5 = HEVC_FILT_8TAP_SH(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
-                                 filt3);
-        dst6 = HEVC_FILT_8TAP_SH(vec8, vec9, vec10, vec11, filt0, filt1, filt2,
-                                 filt3);
-
-        ILVR_H4_SH(dst1, dst0, dst3, dst2, dst5, dst4, dst2, dst1,
-                   dst10_r, dst32_r, dst54_r, dst21_r);
-        ILVR_H2_SH(dst4, dst3, dst6, dst5, dst43_r, dst65_r);
-        ILVL_H4_SH(dst1, dst0, dst3, dst2, dst5, dst4, dst2, dst1,
-                   dst10_l, dst32_l, dst54_l, dst21_l);
-        ILVL_H2_SH(dst4, dst3, dst6, dst5, dst43_l, dst65_l);
+        HEVC_FILT_8TAP_4W_SH(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
+                             filt3, dst8, dst9);
+        HEVC_FILT_8TAP_4W_SH(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
+                             filt3, dst10, dst11);
+        HEVC_FILT_8TAP_4W_SH(vec8, vec9, vec10, vec11, filt0, filt1, filt2,
+                             filt3, dst12, dst13);
+
+        ILVEV_H2_SH(dst0, dst2, dst1, dst3, dst10_r, dst10_l);
+        ILVEV_H2_SH(dst2, dst4, dst3, dst5, dst21_r, dst21_l);
+        ILVEV_H2_SH(dst4, dst6, dst5, dst7, dst32_r, dst32_l);
+        ILVEV_H2_SH(dst6, dst8, dst7, dst9, dst43_r, dst43_l);
+        ILVEV_H2_SH(dst8, dst10, dst9, dst11, dst54_r, dst54_l);
+        ILVEV_H2_SH(dst10, dst12, dst11, dst13, dst65_r, dst65_l);
 
         for (loop_cnt = height >> 1; loop_cnt--;) {
             LD_SB2(src_tmp, src_stride, src7, src8);
             src_tmp += 2 * src_stride;
-            XORI_B2_128_SB(src7, src8);
 
             VSHF_B4_SB(src7, src7, mask0, mask1, mask2, mask3,
                        vec0, vec1, vec2, vec3);
-            dst7 = HEVC_FILT_8TAP_SH(vec0, vec1, vec2, vec3, filt0, filt1,
-                                     filt2, filt3);
+            HEVC_FILT_8TAP_4W_SH(vec0, vec1, vec2, vec3, filt0, filt1,
+                                 filt2, filt3, dst14, dst15);
 
-            ILVRL_H2_SH(dst7, dst6, dst76_r, dst76_l);
+            ILVEV_H2_SH(dst12, dst14, dst13, dst15, dst76_r, dst76_l);
             dst0_r = HEVC_FILT_8TAP(dst10_r, dst32_r, dst54_r, dst76_r,
                                     filt_h0, filt_h1, filt_h2, filt_h3);
             dst0_l = HEVC_FILT_8TAP(dst10_l, dst32_l, dst54_l, dst76_l,
@@ -1998,10 +2245,10 @@ static void hevc_hv_uniwgt_8t_8multx2mult_msa(uint8_t *src,
             /* row 8 */
             VSHF_B4_SB(src8, src8, mask0, mask1, mask2, mask3,
                        vec0, vec1, vec2, vec3);
-            dst8 = HEVC_FILT_8TAP_SH(vec0, vec1, vec2, vec3, filt0, filt1,
-                                     filt2, filt3);
+            HEVC_FILT_8TAP_4W_SH(vec0, vec1, vec2, vec3, filt0, filt1,
+                                 filt2, filt3, dst0, dst1);
 
-            ILVRL_H2_SH(dst8, dst7, dst87_r, dst87_l);
+            ILVEV_H2_SH(dst14, dst0, dst15, dst1, dst87_r, dst87_l);
             dst1_r = HEVC_FILT_8TAP(dst21_r, dst43_r, dst65_r, dst87_r,
                                     filt_h0, filt_h1, filt_h2, filt_h3);
             dst1_l = HEVC_FILT_8TAP(dst21_l, dst43_l, dst65_l, dst87_l,
@@ -2014,7 +2261,7 @@ static void hevc_hv_uniwgt_8t_8multx2mult_msa(uint8_t *src,
             SRAR_W4_SW(dst0_r, dst1_r, dst0_l, dst1_l, rnd_vec);
             ADD2(dst0_r, offset_vec, dst0_l, offset_vec, dst0_r, dst0_l);
             ADD2(dst1_r, offset_vec, dst1_l, offset_vec, dst1_r, dst1_l);
-            CLIP_SW4_0_255_MAX_SATU(dst0_r, dst1_r, dst0_l, dst1_l);
+            CLIP_SW4_0_255(dst0_r, dst1_r, dst0_l, dst1_l);
 
             PCKEV_H2_SW(dst0_l, dst0_r, dst1_l, dst1_r, dst0_r, dst1_r);
             dst0_r = (v4i32) __msa_pckev_b((v16i8) dst1_r, (v16i8) dst0_r);
@@ -2033,7 +2280,8 @@ static void hevc_hv_uniwgt_8t_8multx2mult_msa(uint8_t *src,
             dst21_l = dst43_l;
             dst43_l = dst65_l;
             dst65_l = dst87_l;
-            dst6 = dst8;
+            dst12 = dst0;
+            dst13 = dst1;
         }
 
         src += 8;
@@ -2075,19 +2323,21 @@ static void hevc_hv_uniwgt_8t_12w_msa(uint8_t *src,
     v16i8 mask0, mask1, mask2, mask3, mask4, mask5, mask6, mask7;
     v16i8 vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
     v16i8 vec8, vec9, vec10, vec11, vec12, vec13, vec14, vec15;
-    v8i16 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
-    v8i16 dst30, dst41, dst52, dst63, dst66, dst97, dst108;
+    v4i32 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    v4i32 dst8, dst9, dst10, dst11, dst12, dst13, dst14, dst15;
     v8i16 filt0, filt1, filt2, filt3, filt_h0, filt_h1, filt_h2, filt_h3;
     v8i16 dst10_r, dst32_r, dst54_r, dst76_r, dst10_l, dst32_l, dst54_l;
     v8i16 dst98_r, dst21_r, dst43_r, dst65_r, dst87_r, dst109_r;
     v8i16 dst76_l, filter_vec;
     v4i32 dst0_r, dst0_l, dst1_r, dst2_r, dst3_r;
-    v4i32 weight_vec, offset_vec, rnd_vec, const_128, denom_vec;
+    v4i32 weight_vec, offset_vec, rnd_vec;
+    v8i16 zero = { 0 };
 
     src -= ((3 * src_stride) + 3);
 
     filter_vec = LD_SH(filter_x);
-    SPLATI_H4_SH(filter_vec, 0, 1, 2, 3, filt0, filt1, filt2, filt3);
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W4_SH(filter_vec, filt0, filt1, filt2, filt3);
 
     filter_vec = LD_SH(filter_y);
     UNPCK_R_SB_SH(filter_vec, filter_vec);
@@ -2097,11 +2347,6 @@ static void hevc_hv_uniwgt_8t_12w_msa(uint8_t *src,
     weight_vec = __msa_fill_w(weight);
     offset_vec = __msa_fill_w(offset);
     rnd_vec = __msa_fill_w(rnd_val);
-    denom_vec = rnd_vec - 6;
-
-    const_128 = __msa_ldi_w(128);
-    const_128 *= weight_vec;
-    offset_vec += __msa_srar_w(const_128, denom_vec);
 
     mask0 = LD_SB(ff_hevc_mask_arr);
     mask1 = mask0 + 2;
@@ -2113,7 +2358,6 @@ static void hevc_hv_uniwgt_8t_12w_msa(uint8_t *src,
 
     LD_SB7(src_tmp, src_stride, src0, src1, src2, src3, src4, src5, src6);
     src_tmp += (7 * src_stride);
-    XORI_B7_128_SB(src0, src1, src2, src3, src4, src5, src6);
 
     /* row 0 row 1 row 2 row 3 */
     VSHF_B4_SB(src0, src0, mask0, mask1, mask2, mask3, vec0, vec1, vec2, vec3);
@@ -2122,38 +2366,37 @@ static void hevc_hv_uniwgt_8t_12w_msa(uint8_t *src,
                vec11);
     VSHF_B4_SB(src3, src3, mask0, mask1, mask2, mask3, vec12, vec13, vec14,
                vec15);
-    dst0 = HEVC_FILT_8TAP_SH(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
-                             filt3);
-    dst1 = HEVC_FILT_8TAP_SH(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
-                             filt3);
-    dst2 = HEVC_FILT_8TAP_SH(vec8, vec9, vec10, vec11, filt0, filt1, filt2,
-                             filt3);
-    dst3 = HEVC_FILT_8TAP_SH(vec12, vec13, vec14, vec15, filt0, filt1,
-                             filt2, filt3);
+    HEVC_FILT_8TAP_4W_SH(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
+                         filt3, dst0, dst1);
+    HEVC_FILT_8TAP_4W_SH(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
+                         filt3, dst2, dst3);
+    HEVC_FILT_8TAP_4W_SH(vec8, vec9, vec10, vec11, filt0, filt1, filt2,
+                         filt3, dst4, dst5);
+    HEVC_FILT_8TAP_4W_SH(vec12, vec13, vec14, vec15, filt0, filt1,
+                         filt2, filt3, dst6, dst7);
     VSHF_B4_SB(src4, src4, mask0, mask1, mask2, mask3, vec0, vec1, vec2, vec3);
     VSHF_B4_SB(src5, src5, mask0, mask1, mask2, mask3, vec4, vec5, vec6, vec7);
     VSHF_B4_SB(src6, src6, mask0, mask1, mask2, mask3, vec8, vec9, vec10,
                vec11);
-    dst4 = HEVC_FILT_8TAP_SH(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
-                             filt3);
-    dst5 = HEVC_FILT_8TAP_SH(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
-                             filt3);
-    dst6 = HEVC_FILT_8TAP_SH(vec8, vec9, vec10, vec11, filt0, filt1, filt2,
-                             filt3);
+    HEVC_FILT_8TAP_4W_SH(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
+                         filt3, dst8, dst9);
+    HEVC_FILT_8TAP_4W_SH(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
+                         filt3, dst10, dst11);
+    HEVC_FILT_8TAP_4W_SH(vec8, vec9, vec10, vec11, filt0, filt1, filt2,
+                         filt3, dst12, dst13);
 
     for (loop_cnt = 16; loop_cnt--;) {
         src7 = LD_SB(src_tmp);
-        src7 = (v16i8) __msa_xori_b((v16u8) src7, 128);
         src_tmp += src_stride;
 
         VSHF_B4_SB(src7, src7, mask0, mask1, mask2, mask3, vec0, vec1, vec2,
                    vec3);
-        dst7 = HEVC_FILT_8TAP_SH(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
-                                 filt3);
-        ILVRL_H2_SH(dst1, dst0, dst10_r, dst10_l);
-        ILVRL_H2_SH(dst3, dst2, dst32_r, dst32_l);
-        ILVRL_H2_SH(dst5, dst4, dst54_r, dst54_l);
-        ILVRL_H2_SH(dst7, dst6, dst76_r, dst76_l);
+        HEVC_FILT_8TAP_4W_SH(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
+                             filt3, dst14, dst15);
+        ILVEV_H2_SH(dst0, dst2, dst1, dst3, dst10_r, dst10_l);
+        ILVEV_H2_SH(dst4, dst6, dst5, dst7, dst32_r, dst32_l);
+        ILVEV_H2_SH(dst8, dst10, dst9, dst11, dst54_r, dst54_l);
+        ILVEV_H2_SH(dst12, dst14, dst13, dst15, dst76_r, dst76_l);
 
         dst0_r = HEVC_FILT_8TAP(dst10_r, dst32_r, dst54_r, dst76_r,
                                 filt_h0, filt_h1, filt_h2, filt_h3);
@@ -2165,19 +2408,26 @@ static void hevc_hv_uniwgt_8t_12w_msa(uint8_t *src,
         MUL2(dst0_r, weight_vec, dst0_l, weight_vec, dst0_r, dst0_l);
         SRAR_W2_SW(dst0_r, dst0_l, rnd_vec);
         ADD2(dst0_r, offset_vec, dst0_l, offset_vec, dst0_r, dst0_l);
-        CLIP_SW2_0_255_MAX_SATU(dst0_r, dst0_l);
+        CLIP_SW2_0_255(dst0_r, dst0_l);
         dst0_r = (v4i32) __msa_pckev_h((v8i16) dst0_l, (v8i16) dst0_r);
         out = (v16u8) __msa_pckev_b((v16i8) dst0_r, (v16i8) dst0_r);
         ST_D1(out, 0, dst_tmp);
         dst_tmp += dst_stride;
 
-        dst0 = dst1;
-        dst1 = dst2;
-        dst2 = dst3;
-        dst3 = dst4;
-        dst4 = dst5;
-        dst5 = dst6;
-        dst6 = dst7;
+        dst0 = dst2;
+        dst1 = dst3;
+        dst2 = dst4;
+        dst3 = dst5;
+        dst4 = dst6;
+        dst5 = dst7;
+        dst6 = dst8;
+        dst7 = dst9;
+        dst8 = dst10;
+        dst9 = dst11;
+        dst10 = dst12;
+        dst11 = dst13;
+        dst12 = dst14;
+        dst13 = dst15;
     }
 
     src += 8;
@@ -2190,7 +2440,6 @@ static void hevc_hv_uniwgt_8t_12w_msa(uint8_t *src,
 
     LD_SB7(src, src_stride, src0, src1, src2, src3, src4, src5, src6);
     src += (7 * src_stride);
-    XORI_B7_128_SB(src0, src1, src2, src3, src4, src5, src6);
 
     VSHF_B4_SB(src0, src3, mask4, mask5, mask6, mask7, vec0, vec1, vec2, vec3);
     VSHF_B4_SB(src1, src4, mask4, mask5, mask6, mask7, vec4, vec5, vec6, vec7);
@@ -2198,38 +2447,33 @@ static void hevc_hv_uniwgt_8t_12w_msa(uint8_t *src,
                vec11);
     VSHF_B4_SB(src3, src6, mask4, mask5, mask6, mask7, vec12, vec13, vec14,
                vec15);
-    dst30 = HEVC_FILT_8TAP_SH(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
-                              filt3);
-    dst41 = HEVC_FILT_8TAP_SH(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
-                              filt3);
-    dst52 = HEVC_FILT_8TAP_SH(vec8, vec9, vec10, vec11, filt0, filt1, filt2,
-                              filt3);
-    dst63 = HEVC_FILT_8TAP_SH(vec12, vec13, vec14, vec15, filt0, filt1, filt2,
-                              filt3);
-    ILVRL_H2_SH(dst41, dst30, dst10_r, dst43_r);
-    ILVRL_H2_SH(dst52, dst41, dst21_r, dst54_r);
-    ILVRL_H2_SH(dst63, dst52, dst32_r, dst65_r);
-
-    dst66 = (v8i16) __msa_splati_d((v2i64) dst63, 1);
+    HEVC_FILT_8TAP_4W_SH(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
+                         filt3, dst0, dst3);
+    HEVC_FILT_8TAP_4W_SH(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
+                         filt3, dst1, dst4);
+    HEVC_FILT_8TAP_4W_SH(vec8, vec9, vec10, vec11, filt0, filt1, filt2,
+                         filt3, dst2, dst5);
+    HEVC_FILT_8TAP_4W_SH(vec12, vec13, vec14, vec15, filt0, filt1, filt2,
+                         filt3, dst3, dst6);
+    ILVEV_H2_SH(dst0, dst1, dst3, dst4, dst10_r, dst43_r);
+    ILVEV_H2_SH(dst1, dst2, dst4, dst5, dst21_r, dst54_r);
+    ILVEV_H2_SH(dst2, dst3, dst5, dst6, dst32_r, dst65_r);
 
     for (loop_cnt = 4; loop_cnt--;) {
         LD_SB4(src, src_stride, src7, src8, src9, src10);
         src += (4 * src_stride);
-        XORI_B4_128_SB(src7, src8, src9, src10);
 
         VSHF_B4_SB(src7, src9, mask4, mask5, mask6, mask7, vec0, vec1, vec2,
                    vec3);
         VSHF_B4_SB(src8, src10, mask4, mask5, mask6, mask7, vec4, vec5, vec6,
                    vec7);
-        dst97 = HEVC_FILT_8TAP_SH(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
-                                  filt3);
-        dst108 = HEVC_FILT_8TAP_SH(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
-                                   filt3);
+        HEVC_FILT_8TAP_4W_SH(vec0, vec1, vec2, vec3, filt0, filt1, filt2,
+                             filt3, dst7, dst9);
+        HEVC_FILT_8TAP_4W_SH(vec4, vec5, vec6, vec7, filt0, filt1, filt2,
+                             filt3, dst8, dst10);
 
-        dst76_r = __msa_ilvr_h(dst97, dst66);
-        ILVRL_H2_SH(dst108, dst97, dst87_r, dst109_r);
-        dst66 = (v8i16) __msa_splati_d((v2i64) dst97, 1);
-        dst98_r = __msa_ilvr_h(dst66, dst108);
+        ILVEV_H2_SH(dst6, dst7, dst7, dst8, dst76_r, dst87_r);
+        ILVEV_H2_SH(dst9, dst10, dst8, dst9, dst109_r, dst98_r);
 
         dst0_r = HEVC_FILT_8TAP(dst10_r, dst32_r, dst54_r, dst76_r, filt_h0,
                                 filt_h1, filt_h2, filt_h3);
@@ -2246,7 +2490,7 @@ static void hevc_hv_uniwgt_8t_12w_msa(uint8_t *src,
         SRAR_W4_SW(dst0_r, dst1_r, dst2_r, dst3_r, rnd_vec);
         ADD2(dst0_r, offset_vec, dst1_r, offset_vec, dst0_r, dst1_r);
         ADD2(dst2_r, offset_vec, dst3_r, offset_vec, dst2_r, dst3_r);
-        CLIP_SW4_0_255_MAX_SATU(dst0_r, dst1_r, dst2_r, dst3_r);
+        CLIP_SW4_0_255(dst0_r, dst1_r, dst2_r, dst3_r);
         PCKEV_H2_SW(dst1_r, dst0_r, dst3_r, dst2_r, dst0_r, dst1_r);
         out = (v16u8) __msa_pckev_b((v16i8) dst1_r, (v16i8) dst0_r);
         ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
@@ -2258,7 +2502,7 @@ static void hevc_hv_uniwgt_8t_12w_msa(uint8_t *src,
         dst21_r = dst65_r;
         dst43_r = dst87_r;
         dst65_r = dst109_r;
-        dst66 = (v8i16) __msa_splati_d((v2i64) dst108, 1);
+        dst6 = dst10;
     }
 }
 
@@ -2352,50 +2596,41 @@ static void hevc_hz_uniwgt_4t_4x2_msa(uint8_t *src,
                                       int32_t rnd_val)
 {
     v16u8 out;
-    v8i16 filt0, filt1;
+    v8i16 filt0, filt1, filter_vec;
     v16i8 src0, src1, vec0, vec1;
+    v8i16 tmp0, tmp1, tmp2, tmp3;
     v16i8 mask1;
-    v8i16 dst0;
-    v4i32 dst0_r, dst0_l;
-    v8i16 filter_vec, weight_vec_h, offset_vec, denom_vec;
-    v4i32 weight_vec, rnd_vec;
+    v4i32 dst0, dst1;
+    v4i32 weight_vec, rnd_vec, offset_vec;
     v16i8 mask0 = LD_SB(&ff_hevc_mask_arr[16]);
+    v8i16 zero = { 0 };
 
     src -= 1;
 
     filter_vec = LD_SH(filter);
-    SPLATI_H2_SH(filter_vec, 0, 1, filt0, filt1);
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W2_SH(filter_vec, 0, filt0, filt1);
 
     mask1 = mask0 + 2;
 
-    weight = weight & 0x0000FFFF;
-
     weight_vec = __msa_fill_w(weight);
     rnd_vec = __msa_fill_w(rnd_val);
-
-    weight *= 128;
-    rnd_val -= 6;
-
-    weight_vec_h = __msa_fill_h(weight);
-    offset_vec = __msa_fill_h(offset);
-    denom_vec = __msa_fill_h(rnd_val);
-
-    weight_vec_h = __msa_srar_h(weight_vec_h, denom_vec);
-    offset_vec = __msa_adds_s_h(offset_vec, weight_vec_h);
+    offset_vec = __msa_fill_w(offset);
 
     LD_SB2(src, src_stride, src0, src1);
-    XORI_B2_128_SB(src0, src1);
 
     VSHF_B2_SB(src0, src1, src0, src1, mask0, mask1, vec0, vec1);
-    dst0 = HEVC_FILT_4TAP_SH(vec0, vec1, filt0, filt1);
+    ILVRL_B2_SH(zero, vec0, tmp0, tmp1);
+    ILVRL_B2_SH(zero, vec1, tmp2, tmp3);
+    dst0 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+    dst1 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
 
-    ILVRL_H2_SW(dst0, dst0, dst0_r, dst0_l);
-    DOTP_SH2_SW(dst0_r, dst0_l, weight_vec, weight_vec, dst0_r, dst0_l);
-    SRAR_W2_SW(dst0_r, dst0_l, rnd_vec);
-    dst0 = __msa_pckev_h((v8i16) dst0_l, (v8i16) dst0_r);
-    dst0 = __msa_adds_s_h(dst0, offset_vec);
-    dst0 = CLIP_SH_0_255_MAX_SATU(dst0);
-    out = (v16u8) __msa_pckev_b((v16i8) dst0, (v16i8) dst0);
+    MUL2(dst0, weight_vec, dst1, weight_vec, dst0, dst1);
+    SRAR_W2_SW(dst0, dst1, rnd_vec);
+    ADD2(dst0, offset_vec, dst1, offset_vec, dst0, dst1);
+    CLIP_SW2_0_255(dst0, dst1);
+    vec0 = __msa_pckev_h((v8i16) dst1, (v8i16) dst0);
+    out = (v16u8) __msa_pckev_b((v16i8) vec0, (v16i8) vec0);
     ST_W2(out, 0, 1, dst, dst_stride);
     dst += (4 * dst_stride);
 }
@@ -2413,48 +2648,51 @@ static void hevc_hz_uniwgt_4t_4x4_msa(uint8_t *src,
     v8i16 filt0, filt1;
     v16i8 src0, src1, src2, src3;
     v16i8 mask1, vec0, vec1, vec2, vec3;
-    v8i16 dst0, dst1;
-    v8i16 filter_vec, weight_vec_h, offset_vec, denom_vec;
-    v4i32 weight_vec, rnd_vec;
+    v8i16 tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
+    v4i32 dst0, dst1, dst2, dst3;
+    v8i16 filter_vec;
+    v4i32 weight_vec, rnd_vec, offset_vec;
     v16i8 mask0 = LD_SB(&ff_hevc_mask_arr[16]);
+    v8i16 zero = { 0 };
 
     src -= 1;
 
     /* rearranging filter */
     filter_vec = LD_SH(filter);
-    SPLATI_H2_SH(filter_vec, 0, 1, filt0, filt1);
 
-    mask1 = mask0 + 2;
-
-    weight = weight & 0x0000FFFF;
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W2_SH(filter_vec, 0, filt0, filt1);
 
     weight_vec = __msa_fill_w(weight);
     rnd_vec = __msa_fill_w(rnd_val);
 
-    weight *= 128;
-    rnd_val -= 6;
-
-    weight_vec_h = __msa_fill_h(weight);
-    offset_vec = __msa_fill_h(offset);
-    denom_vec = __msa_fill_h(rnd_val);
-
-    weight_vec_h = __msa_srar_h(weight_vec_h, denom_vec);
-    offset_vec = __msa_adds_s_h(offset_vec, weight_vec_h);
+    offset_vec = __msa_fill_w(offset);
+    mask1 = mask0 + 2;
 
     LD_SB4(src, src_stride, src0, src1, src2, src3);
-    XORI_B4_128_SB(src0, src1, src2, src3);
 
     VSHF_B2_SB(src0, src1, src0, src1, mask0, mask1, vec0, vec1);
     VSHF_B2_SB(src2, src3, src2, src3, mask0, mask1, vec2, vec3);
-    dst0 = HEVC_FILT_4TAP_SH(vec0, vec1, filt0, filt1);
-    dst1 = HEVC_FILT_4TAP_SH(vec2, vec3, filt0, filt1);
-
-    HEVC_UNIW_RND_CLIP2_MAX_SATU_H(dst0, dst1, weight_vec, offset_vec, rnd_vec,
-                                   dst0, dst1);
+    ILVRL_B2_SH(zero, vec0, tmp0, tmp1);
+    ILVRL_B2_SH(zero, vec1, tmp2, tmp3);
+    ILVRL_B2_SH(zero, vec2, tmp4, tmp5);
+    ILVRL_B2_SH(zero, vec3, tmp6, tmp7);
+    dst0 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+    dst1 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+    dst2 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+    dst3 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+
+    MUL4(dst0, weight_vec, dst1, weight_vec, dst2, weight_vec, dst3,
+         weight_vec, dst0, dst1, dst2, dst3);
+    SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
+    ADD4(dst0, offset_vec, dst1, offset_vec, dst2, offset_vec, dst3,
+         offset_vec, dst0, dst1, dst2, dst3);
+    CLIP_SW4_0_255(dst0, dst1, dst2, dst3);
+    tmp0 = __msa_pckev_h((v8i16) dst1, (v8i16) dst0);
+    tmp1 = __msa_pckev_h((v8i16) dst3, (v8i16) dst2);
 
-    out = (v16u8) __msa_pckev_b((v16i8) dst1, (v16i8) dst0);
+    out = (v16u8) __msa_pckev_b((v16i8) tmp1, (v16i8) tmp0);
     ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
-    dst += (4 * dst_stride);
 }
 
 static void hevc_hz_uniwgt_4t_4x8multiple_msa(uint8_t *src,
@@ -2472,31 +2710,22 @@ static void hevc_hz_uniwgt_4t_4x8multiple_msa(uint8_t *src,
     v8i16 filt0, filt1;
     v16i8 src0, src1, src2, src3, src4, src5, src6, src7;
     v16i8 mask1, vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
-    v8i16 dst0, dst1, dst2, dst3;
+    v8i16 tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
+    v4i32 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
     v8i16 filter_vec;
-    v8i16 weight_vec_h, offset_vec, denom_vec;
-    v4i32 weight_vec, rnd_vec;
+    v4i32 weight_vec, rnd_vec, offset_vec;
     v16i8 mask0 = LD_SB(&ff_hevc_mask_arr[16]);
+    v8i16 zero = { 0 };
 
     src -= 1;
 
     filter_vec = LD_SH(filter);
-    SPLATI_H2_SH(filter_vec, 0, 1, filt0, filt1);
-
-    weight = weight & 0x0000FFFF;
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W2_SH(filter_vec, 0, filt0, filt1);
 
     weight_vec = __msa_fill_w(weight);
     rnd_vec = __msa_fill_w(rnd_val);
-
-    weight *= 128;
-    rnd_val -= 6;
-
-    weight_vec_h = __msa_fill_h(weight);
-    offset_vec = __msa_fill_h(offset);
-    denom_vec = __msa_fill_h(rnd_val);
-
-    weight_vec_h = __msa_srar_h(weight_vec_h, denom_vec);
-    offset_vec = __msa_adds_s_h(offset_vec, weight_vec_h);
+    offset_vec = __msa_fill_w(offset);
 
     mask1 = mask0 + 2;
 
@@ -2504,22 +2733,43 @@ static void hevc_hz_uniwgt_4t_4x8multiple_msa(uint8_t *src,
         LD_SB8(src, src_stride, src0, src1, src2, src3, src4, src5, src6, src7);
         src += (8 * src_stride);
 
-        XORI_B8_128_SB(src0, src1, src2, src3, src4, src5, src6, src7);
-
         VSHF_B2_SB(src0, src1, src0, src1, mask0, mask1, vec0, vec1);
         VSHF_B2_SB(src2, src3, src2, src3, mask0, mask1, vec2, vec3);
         VSHF_B2_SB(src4, src5, src4, src5, mask0, mask1, vec4, vec5);
         VSHF_B2_SB(src6, src7, src6, src7, mask0, mask1, vec6, vec7);
-        dst0 = HEVC_FILT_4TAP_SH(vec0, vec1, filt0, filt1);
-        dst1 = HEVC_FILT_4TAP_SH(vec2, vec3, filt0, filt1);
-        dst2 = HEVC_FILT_4TAP_SH(vec4, vec5, filt0, filt1);
-        dst3 = HEVC_FILT_4TAP_SH(vec6, vec7, filt0, filt1);
-
-        HEVC_UNIW_RND_CLIP4_MAX_SATU_H(dst0, dst1, dst2, dst3,
-                                       weight_vec, offset_vec, rnd_vec,
-                                       dst0, dst1, dst2, dst3);
-
-        PCKEV_B2_UB(dst1, dst0, dst3, dst2, out0, out1);
+        ILVRL_B2_SH(zero, vec0, tmp0, tmp1);
+        ILVRL_B2_SH(zero, vec1, tmp2, tmp3);
+        ILVRL_B2_SH(zero, vec2, tmp4, tmp5);
+        ILVRL_B2_SH(zero, vec3, tmp6, tmp7);
+        dst0 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+        dst1 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+        dst2 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+        dst3 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+        ILVRL_B2_SH(zero, vec4, tmp0, tmp1);
+        ILVRL_B2_SH(zero, vec5, tmp2, tmp3);
+        ILVRL_B2_SH(zero, vec6, tmp4, tmp5);
+        ILVRL_B2_SH(zero, vec7, tmp6, tmp7);
+        dst4 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+        dst5 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+        dst6 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+        dst7 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+
+        MUL4(dst0, weight_vec, dst1, weight_vec, dst2, weight_vec, dst3,
+             weight_vec, dst0, dst1, dst2, dst3);
+        MUL4(dst4, weight_vec, dst5, weight_vec, dst6, weight_vec,
+             dst7, weight_vec, dst4, dst5, dst6, dst7);
+        SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
+        SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
+        ADD4(dst0, offset_vec, dst1, offset_vec, dst2, offset_vec, dst3,
+             offset_vec, dst0, dst1, dst2, dst3);
+        ADD4(dst4, offset_vec, dst5, offset_vec, dst6, offset_vec,
+             dst7, offset_vec, dst4, dst5, dst6, dst7);
+        CLIP_SW8_0_255(dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7);
+        tmp0 = __msa_pckev_h((v8i16) dst1, (v8i16) dst0);
+        tmp1 = __msa_pckev_h((v8i16) dst3, (v8i16) dst2);
+        tmp2 = __msa_pckev_h((v8i16) dst5, (v8i16) dst4);
+        tmp3 = __msa_pckev_h((v8i16) dst7, (v8i16) dst6);
+        PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out0, out1);
         ST_W8(out0, out1, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
         dst += (8 * dst_stride);
     }
@@ -2559,65 +2809,101 @@ static void hevc_hz_uniwgt_4t_6w_msa(uint8_t *src,
                                      int32_t rnd_val)
 {
     v16u8 out0, out1, out2, out3;
-    v8i16 filt0, filt1;
+    v8i16 filter_vec, filt0, filt1;
     v16i8 src0, src1, src2, src3, src4, src5, src6, src7;
     v16i8 mask0 = LD_SB(ff_hevc_mask_arr);
     v16i8 mask1;
     v16i8 vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
-    v8i16 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
-    v8i16 filter_vec, weight_vec_h, offset_vec, denom_vec;
-    v4i32 weight_vec, rnd_vec;
+    v8i16 tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
+    v4i32 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    v4i32 dst8, dst9, dst10, dst11, dst12, dst13, dst14, dst15;
+    v4i32 weight_vec, rnd_vec, offset_vec;
+    v8i16 zero = { 0 };
 
     src -= 1;
 
     filter_vec = LD_SH(filter);
-    SPLATI_H2_SH(filter_vec, 0, 1, filt0, filt1);
-
-    weight = weight & 0x0000FFFF;
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W2_SH(filter_vec, 0, filt0, filt1);
 
     weight_vec = __msa_fill_w(weight);
     rnd_vec = __msa_fill_w(rnd_val);
-
-    weight *= 128;
-    rnd_val -= 6;
-
-    weight_vec_h = __msa_fill_h(weight);
-    offset_vec = __msa_fill_h(offset);
-    denom_vec = __msa_fill_h(rnd_val);
-
-    weight_vec_h = __msa_srar_h(weight_vec_h, denom_vec);
-    offset_vec = __msa_adds_s_h(offset_vec, weight_vec_h);
+    offset_vec = __msa_fill_w(offset);
 
     mask1 = mask0 + 2;
 
     LD_SB8(src, src_stride, src0, src1, src2, src3, src4, src5, src6, src7);
-    XORI_B8_128_SB(src0, src1, src2, src3, src4, src5, src6, src7);
     VSHF_B2_SB(src0, src0, src0, src0, mask0, mask1, vec0, vec1);
     VSHF_B2_SB(src1, src1, src1, src1, mask0, mask1, vec2, vec3);
     VSHF_B2_SB(src2, src2, src2, src2, mask0, mask1, vec4, vec5);
     VSHF_B2_SB(src3, src3, src3, src3, mask0, mask1, vec6, vec7);
-    dst0 = HEVC_FILT_4TAP_SH(vec0, vec1, filt0, filt1);
-    dst1 = HEVC_FILT_4TAP_SH(vec2, vec3, filt0, filt1);
-    dst2 = HEVC_FILT_4TAP_SH(vec4, vec5, filt0, filt1);
-    dst3 = HEVC_FILT_4TAP_SH(vec6, vec7, filt0, filt1);
+    ILVRL_B2_SH(zero, vec0, tmp0, tmp1);
+    ILVRL_B2_SH(zero, vec1, tmp2, tmp3);
+    ILVRL_B2_SH(zero, vec2, tmp4, tmp5);
+    ILVRL_B2_SH(zero, vec3, tmp6, tmp7);
+    dst0 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+    dst1 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+    dst2 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+    dst3 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+    ILVRL_B2_SH(zero, vec4, tmp0, tmp1);
+    ILVRL_B2_SH(zero, vec5, tmp2, tmp3);
+    ILVRL_B2_SH(zero, vec6, tmp4, tmp5);
+    ILVRL_B2_SH(zero, vec7, tmp6, tmp7);
+    dst4 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+    dst5 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+    dst6 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+    dst7 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
     VSHF_B2_SB(src4, src4, src4, src4, mask0, mask1, vec0, vec1);
     VSHF_B2_SB(src5, src5, src5, src5, mask0, mask1, vec2, vec3);
     VSHF_B2_SB(src6, src6, src6, src6, mask0, mask1, vec4, vec5);
     VSHF_B2_SB(src7, src7, src7, src7, mask0, mask1, vec6, vec7);
-    dst4 = HEVC_FILT_4TAP_SH(vec0, vec1, filt0, filt1);
-    dst5 = HEVC_FILT_4TAP_SH(vec2, vec3, filt0, filt1);
-    dst6 = HEVC_FILT_4TAP_SH(vec4, vec5, filt0, filt1);
-    dst7 = HEVC_FILT_4TAP_SH(vec6, vec7, filt0, filt1);
-
-    HEVC_UNIW_RND_CLIP4_MAX_SATU_H(dst0, dst1, dst2, dst3,
-                                   weight_vec, offset_vec, rnd_vec,
-                                   dst0, dst1, dst2, dst3);
-    HEVC_UNIW_RND_CLIP4_MAX_SATU_H(dst4, dst5, dst6, dst7,
-                                   weight_vec, offset_vec, rnd_vec,
-                                   dst4, dst5, dst6, dst7);
-
-    PCKEV_B2_UB(dst1, dst0, dst3, dst2, out0, out1);
-    PCKEV_B2_UB(dst5, dst4, dst7, dst6, out2, out3);
+    ILVRL_B2_SH(zero, vec0, tmp0, tmp1);
+    ILVRL_B2_SH(zero, vec1, tmp2, tmp3);
+    ILVRL_B2_SH(zero, vec2, tmp4, tmp5);
+    ILVRL_B2_SH(zero, vec3, tmp6, tmp7);
+    dst8  = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+    dst9  = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+    dst10 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+    dst11 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+    ILVRL_B2_SH(zero, vec4, tmp0, tmp1);
+    ILVRL_B2_SH(zero, vec5, tmp2, tmp3);
+    ILVRL_B2_SH(zero, vec6, tmp4, tmp5);
+    ILVRL_B2_SH(zero, vec7, tmp6, tmp7);
+    dst12 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+    dst13 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+    dst14 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+    dst15 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+
+    MUL4(dst0, weight_vec, dst1, weight_vec, dst2, weight_vec, dst3,
+         weight_vec, dst0, dst1, dst2, dst3);
+    MUL4(dst4, weight_vec, dst5, weight_vec, dst6, weight_vec,
+         dst7, weight_vec, dst4, dst5, dst6, dst7);
+    SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
+    SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
+    ADD4(dst0, offset_vec, dst1, offset_vec, dst2, offset_vec, dst3,
+         offset_vec, dst0, dst1, dst2, dst3);
+    ADD4(dst4, offset_vec, dst5, offset_vec, dst6, offset_vec,
+         dst7, offset_vec, dst4, dst5, dst6, dst7);
+    CLIP_SW8_0_255(dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7);
+
+    MUL4(dst8, weight_vec, dst9, weight_vec, dst10, weight_vec, dst11,
+         weight_vec, dst8, dst9, dst10, dst11);
+    MUL4(dst12, weight_vec, dst13, weight_vec, dst14, weight_vec,
+         dst15, weight_vec, dst12, dst13, dst14, dst15);
+    SRAR_W4_SW(dst8, dst9, dst10, dst11, rnd_vec);
+    SRAR_W4_SW(dst12, dst13, dst14, dst15, rnd_vec);
+    ADD4(dst8, offset_vec, dst9, offset_vec, dst10, offset_vec, dst11,
+         offset_vec, dst8, dst9, dst10, dst11);
+    ADD4(dst12, offset_vec, dst13, offset_vec, dst14, offset_vec,
+         dst15, offset_vec, dst12, dst13, dst14, dst15);
+    CLIP_SW8_0_255(dst8, dst9, dst10, dst11, dst12, dst13, dst14, dst15);
+
+    PCKEV_H2_SH(dst1, dst0, dst3, dst2, tmp0, tmp1);
+    PCKEV_H2_SH(dst5, dst4, dst7, dst6, tmp2, tmp3);
+    PCKEV_H2_SH(dst9, dst8, dst11, dst10, tmp4, tmp5);
+    PCKEV_H2_SH(dst13, dst12, dst15, dst14, tmp6, tmp7);
+    PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out0, out1);
+    PCKEV_B2_UB(tmp5, tmp4, tmp7, tmp6, out2, out3);
     ST_W2(out0, 0, 2, dst, dst_stride);
     ST_H2(out0, 2, 6, dst + 4, dst_stride);
     ST_W2(out1, 0, 2, dst + 2 * dst_stride, dst_stride);
@@ -2639,48 +2925,49 @@ static void hevc_hz_uniwgt_4t_8x2_msa(uint8_t *src,
                                       int32_t rnd_val)
 {
     v16u8 out;
-    v8i16 filt0, filt1, dst0, dst1;
+    v8i16 filter_vec, filt0, filt1;
     v16i8 src0, src1;
     v16i8 mask0 = LD_SB(ff_hevc_mask_arr);
     v16i8 mask1;
     v16i8 vec0, vec1, vec2, vec3;
-    v8i16 filter_vec, weight_vec_h, offset_vec, denom_vec;
-    v4i32 weight_vec, rnd_vec;
+    v8i16 tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
+    v4i32 weight_vec, rnd_vec, offset_vec;
+    v4i32 dst0, dst1, dst2, dst3;
+    v8i16 zero = { 0 };
 
     src -= 1;
 
     filter_vec = LD_SH(filter);
-    SPLATI_H2_SH(filter_vec, 0, 1, filt0, filt1);
-
-    weight = weight & 0x0000FFFF;
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W2_SH(filter_vec, 0, filt0, filt1);
 
     weight_vec = __msa_fill_w(weight);
     rnd_vec = __msa_fill_w(rnd_val);
-
-    weight *= 128;
-    rnd_val -= 6;
-
-    weight_vec_h = __msa_fill_h(weight);
-    offset_vec = __msa_fill_h(offset);
-    denom_vec = __msa_fill_h(rnd_val);
-
-    weight_vec_h = __msa_srar_h(weight_vec_h, denom_vec);
-    offset_vec = __msa_adds_s_h(offset_vec, weight_vec_h);
-
+    offset_vec = __msa_fill_w(offset);
     mask1 = mask0 + 2;
 
     LD_SB2(src, src_stride, src0, src1);
-    XORI_B2_128_SB(src0, src1);
 
     VSHF_B2_SB(src0, src0, src0, src0, mask0, mask1, vec0, vec1);
     VSHF_B2_SB(src1, src1, src1, src1, mask0, mask1, vec2, vec3);
-    dst0 = HEVC_FILT_4TAP_SH(vec0, vec1, filt0, filt1);
-    dst1 = HEVC_FILT_4TAP_SH(vec2, vec3, filt0, filt1);
-
-    HEVC_UNIW_RND_CLIP2_MAX_SATU_H(dst0, dst1, weight_vec, offset_vec, rnd_vec,
-                                   dst0, dst1);
+    ILVRL_B2_SH(zero, vec0, tmp0, tmp1);
+    ILVRL_B2_SH(zero, vec1, tmp2, tmp3);
+    ILVRL_B2_SH(zero, vec2, tmp4, tmp5);
+    ILVRL_B2_SH(zero, vec3, tmp6, tmp7);
+    dst0 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+    dst1 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+    dst2 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+    dst3 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+
+    MUL4(dst0, weight_vec, dst1, weight_vec, dst2, weight_vec, dst3,
+         weight_vec, dst0, dst1, dst2, dst3);
+    SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
+    ADD4(dst0, offset_vec, dst1, offset_vec, dst2, offset_vec, dst3,
+         offset_vec, dst0, dst1, dst2, dst3);
+    CLIP_SW4_0_255(dst0, dst1, dst2, dst3);
+    PCKEV_H2_SH(dst1, dst0, dst3, dst2, tmp0, tmp1);
 
-    out = (v16u8) __msa_pckev_b((v16i8) dst1, (v16i8) dst0);
+    out = (v16u8) __msa_pckev_b((v16i8) tmp1, (v16i8) tmp0);
     ST_D2(out, 0, 1, dst, dst_stride);
 }
 
@@ -2696,48 +2983,63 @@ static void hevc_hz_uniwgt_4t_8x4_msa(uint8_t *src,
     v16u8 out0, out1;
     v16i8 src0, src1, src2, src3;
     v16i8 mask0, mask1, vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
-    v8i16 filt0, filt1, dst0, dst1, dst2, dst3;
-    v8i16 filter_vec, weight_vec_h, offset_vec, denom_vec;
-    v4i32 weight_vec, rnd_vec;
+    v8i16 filter_vec, filt0, filt1;
+    v4i32 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    v4i32 tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
+    v4i32 weight_vec, rnd_vec, offset_vec;
+    v8i16 zero = { 0 };
 
     src -= 1;
 
     filter_vec = LD_SH(filter);
-    SPLATI_H2_SH(filter_vec, 0, 1, filt0, filt1);
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W2_SH(filter_vec, 0, filt0, filt1);
 
-    weight = weight & 0x0000FFFF;
     weight_vec = __msa_fill_w(weight);
     rnd_vec = __msa_fill_w(rnd_val);
-
-    weight *= 128;
-    rnd_val -= 6;
-
-    weight_vec_h = __msa_fill_h(weight);
-    offset_vec = __msa_fill_h(offset);
-    denom_vec = __msa_fill_h(rnd_val);
-
-    weight_vec_h = __msa_srar_h(weight_vec_h, denom_vec);
-    offset_vec = __msa_adds_s_h(offset_vec, weight_vec_h);
+    offset_vec = __msa_fill_w(offset);
 
     mask0 = LD_SB(&ff_hevc_mask_arr[0]);
     mask1 = mask0 + 2;
 
     LD_SB4(src, src_stride, src0, src1, src2, src3);
-    XORI_B4_128_SB(src0, src1, src2, src3);
     VSHF_B2_SB(src0, src0, src0, src0, mask0, mask1, vec0, vec1);
     VSHF_B2_SB(src1, src1, src1, src1, mask0, mask1, vec2, vec3);
     VSHF_B2_SB(src2, src2, src2, src2, mask0, mask1, vec4, vec5);
     VSHF_B2_SB(src3, src3, src3, src3, mask0, mask1, vec6, vec7);
-    dst0 = HEVC_FILT_4TAP_SH(vec0, vec1, filt0, filt1);
-    dst1 = HEVC_FILT_4TAP_SH(vec2, vec3, filt0, filt1);
-    dst2 = HEVC_FILT_4TAP_SH(vec4, vec5, filt0, filt1);
-    dst3 = HEVC_FILT_4TAP_SH(vec6, vec7, filt0, filt1);
+    ILVRL_B2_SH(zero, vec0, tmp0, tmp1);
+    ILVRL_B2_SH(zero, vec1, tmp2, tmp3);
+    ILVRL_B2_SH(zero, vec2, tmp4, tmp5);
+    ILVRL_B2_SH(zero, vec3, tmp6, tmp7);
+    dst0 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+    dst1 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+    dst2 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+    dst3 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+    ILVRL_B2_SH(zero, vec4, tmp0, tmp1);
+    ILVRL_B2_SH(zero, vec5, tmp2, tmp3);
+    ILVRL_B2_SH(zero, vec6, tmp4, tmp5);
+    ILVRL_B2_SH(zero, vec7, tmp6, tmp7);
+    dst4 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+    dst5 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+    dst6 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+    dst7 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+
+    MUL4(dst0, weight_vec, dst1, weight_vec, dst2, weight_vec, dst3,
+         weight_vec, dst0, dst1, dst2, dst3);
+    MUL4(dst4, weight_vec, dst5, weight_vec, dst6, weight_vec,
+         dst7, weight_vec, dst4, dst5, dst6, dst7);
+    SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
+    SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
+    ADD4(dst0, offset_vec, dst1, offset_vec, dst2, offset_vec, dst3,
+         offset_vec, dst0, dst1, dst2, dst3);
+    ADD4(dst4, offset_vec, dst5, offset_vec, dst6, offset_vec,
+         dst7, offset_vec, dst4, dst5, dst6, dst7);
+    CLIP_SW8_0_255(dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7);
 
-    HEVC_UNIW_RND_CLIP4_MAX_SATU_H(dst0, dst1, dst2, dst3,
-                                   weight_vec, offset_vec, rnd_vec,
-                                   dst0, dst1, dst2, dst3);
+    PCKEV_H2_SH(dst1, dst0, dst3, dst2, tmp0, tmp1);
+    PCKEV_H2_SH(dst5, dst4, dst7, dst6, tmp2, tmp3);
 
-    PCKEV_B2_UB(dst1, dst0, dst3, dst2, out0, out1);
+    PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out0, out1);
     ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
 }
 
@@ -2751,40 +3053,31 @@ static void hevc_hz_uniwgt_4t_8x6_msa(uint8_t *src,
                                       int32_t rnd_val)
 {
     v16u8 out0, out1, out2;
-    v8i16 filt0, filt1;
+    v8i16 filter_vec, filt0, filt1;
     v16i8 src0, src1, src2, src3, src4, src5;
     v16i8 mask0 = LD_SB(&ff_hevc_mask_arr[0]);
     v16i8 mask1;
     v16i8 vec11;
     v16i8 vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7, vec8, vec9, vec10;
-    v8i16 dst0, dst1, dst2, dst3, dst4, dst5;
-    v8i16 filter_vec, weight_vec_h, offset_vec, denom_vec;
-    v4i32 weight_vec, rnd_vec;
+    v4i32 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    v4i32 dst8, dst9, dst10, dst11;
+    v8i16 tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
+    v4i32 weight_vec, rnd_vec, offset_vec;
+    v8i16 zero = { 0 };
 
     src -= 1;
 
     filter_vec = LD_SH(filter);
-    SPLATI_H2_SH(filter_vec, 0, 1, filt0, filt1);
-
-    weight = weight & 0x0000FFFF;
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W2_SH(filter_vec, 0, filt0, filt1);
 
     weight_vec = __msa_fill_w(weight);
     rnd_vec = __msa_fill_w(rnd_val);
-
-    weight *= 128;
-    rnd_val -= 6;
-
-    weight_vec_h = __msa_fill_h(weight);
-    offset_vec = __msa_fill_h(offset);
-    denom_vec = __msa_fill_h(rnd_val);
-
-    weight_vec_h = __msa_srar_h(weight_vec_h, denom_vec);
-    offset_vec = __msa_adds_s_h(offset_vec, weight_vec_h);
+    offset_vec = __msa_fill_w(offset);
 
     mask1 = mask0 + 2;
 
     LD_SB6(src, src_stride, src0, src1, src2, src3, src4, src5);
-    XORI_B6_128_SB(src0, src1, src2, src3, src4, src5);
 
     VSHF_B2_SB(src0, src0, src0, src0, mask0, mask1, vec0, vec1);
     VSHF_B2_SB(src1, src1, src1, src1, mask0, mask1, vec2, vec3);
@@ -2792,21 +3085,55 @@ static void hevc_hz_uniwgt_4t_8x6_msa(uint8_t *src,
     VSHF_B2_SB(src3, src3, src3, src3, mask0, mask1, vec6, vec7);
     VSHF_B2_SB(src4, src4, src4, src4, mask0, mask1, vec8, vec9);
     VSHF_B2_SB(src5, src5, src5, src5, mask0, mask1, vec10, vec11);
-    dst0 = HEVC_FILT_4TAP_SH(vec0, vec1, filt0, filt1);
-    dst1 = HEVC_FILT_4TAP_SH(vec2, vec3, filt0, filt1);
-    dst2 = HEVC_FILT_4TAP_SH(vec4, vec5, filt0, filt1);
-    dst3 = HEVC_FILT_4TAP_SH(vec6, vec7, filt0, filt1);
-    dst4 = HEVC_FILT_4TAP_SH(vec8, vec9, filt0, filt1);
-    dst5 = HEVC_FILT_4TAP_SH(vec10, vec11, filt0, filt1);
-
-    HEVC_UNIW_RND_CLIP4_MAX_SATU_H(dst0, dst1, dst2, dst3,
-                                   weight_vec, offset_vec, rnd_vec,
-                                   dst0, dst1, dst2, dst3);
+    ILVRL_B2_SH(zero, vec0, tmp0, tmp1);
+    ILVRL_B2_SH(zero, vec1, tmp2, tmp3);
+    ILVRL_B2_SH(zero, vec2, tmp4, tmp5);
+    ILVRL_B2_SH(zero, vec3, tmp6, tmp7);
+    dst0 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+    dst1 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+    dst2 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+    dst3 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+    ILVRL_B2_SH(zero, vec4, tmp0, tmp1);
+    ILVRL_B2_SH(zero, vec5, tmp2, tmp3);
+    ILVRL_B2_SH(zero, vec6, tmp4, tmp5);
+    ILVRL_B2_SH(zero, vec7, tmp6, tmp7);
+    dst4 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+    dst5 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+    dst6 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+    dst7 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+    ILVRL_B2_SH(zero, vec8, tmp0, tmp1);
+    ILVRL_B2_SH(zero, vec9, tmp2, tmp3);
+    ILVRL_B2_SH(zero, vec10, tmp4, tmp5);
+    ILVRL_B2_SH(zero, vec11, tmp6, tmp7);
+    dst8  = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+    dst9  = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+    dst10 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+    dst11 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+
+    MUL4(dst0, weight_vec, dst1, weight_vec, dst2, weight_vec, dst3,
+         weight_vec, dst0, dst1, dst2, dst3);
+    MUL4(dst4, weight_vec, dst5, weight_vec, dst6, weight_vec,
+         dst7, weight_vec, dst4, dst5, dst6, dst7);
+    SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
+    SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
+    ADD4(dst0, offset_vec, dst1, offset_vec, dst2, offset_vec, dst3,
+         offset_vec, dst0, dst1, dst2, dst3);
+    ADD4(dst4, offset_vec, dst5, offset_vec, dst6, offset_vec,
+         dst7, offset_vec, dst4, dst5, dst6, dst7);
+    CLIP_SW8_0_255(dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7);
+
+    MUL4(dst8, weight_vec, dst9, weight_vec, dst10, weight_vec, dst11,
+         weight_vec, dst8, dst9, dst10, dst11);
+    SRAR_W4_SW(dst8, dst9, dst10, dst11, rnd_vec);
+    ADD4(dst8, offset_vec, dst9, offset_vec, dst10, offset_vec, dst11,
+         offset_vec, dst8, dst9, dst10, dst11);
+    CLIP_SW4_0_255(dst8, dst9, dst10, dst11);
 
-    HEVC_UNIW_RND_CLIP2_MAX_SATU_H(dst4, dst5, weight_vec, offset_vec, rnd_vec,
-                                   dst4, dst5);
+    PCKEV_H2_SH(dst1, dst0, dst3, dst2, tmp0, tmp1);
+    PCKEV_H2_SH(dst5, dst4, dst7, dst6, tmp2, tmp3);
+    PCKEV_H2_SH(dst9, dst8, dst11, dst10, tmp4, tmp5);
 
-    PCKEV_B3_UB(dst1, dst0, dst3, dst2, dst5, dst4, out0, out1, out2);
+    PCKEV_B3_UB(tmp1, tmp0, tmp3, tmp2, tmp5, tmp4, out0, out1, out2);
     ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
     ST_D2(out2, 0, 1, dst + 4 * dst_stride, dst_stride);
 }
@@ -2822,70 +3149,107 @@ static void hevc_hz_uniwgt_4t_8x8multiple_msa(uint8_t *src,
                                               int32_t rnd_val)
 {
     uint32_t loop_cnt;
-    v8i16 filt0, filt1;
+    v8i16 filter_vec, filt0, filt1;
     v16u8 out0, out1, out2, out3;
     v16i8 src0, src1, src2, src3, src4, src5, src6, src7;
     v16i8 mask0 = LD_SB(&ff_hevc_mask_arr[0]);
     v16i8 mask1;
     v16i8 vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
-    v8i16 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
-    v8i16 filter_vec, weight_vec_h, offset_vec, denom_vec;
-    v4i32 weight_vec, rnd_vec;
+    v8i16 tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
+    v4i32 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    v4i32 dst8, dst9, dst10, dst11, dst12, dst13, dst14, dst15;
+    v4i32 weight_vec, rnd_vec, offset_vec;
+    v8i16 zero = { 0 };
 
     src -= 1;
 
     filter_vec = LD_SH(filter);
-    SPLATI_H2_SH(filter_vec, 0, 1, filt0, filt1);
-
-    weight = weight & 0x0000FFFF;
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W2_SH(filter_vec, 0, filt0, filt1);
 
     weight_vec = __msa_fill_w(weight);
     rnd_vec = __msa_fill_w(rnd_val);
-
-    weight *= 128;
-    rnd_val -= 6;
-
-    weight_vec_h = __msa_fill_h(weight);
-    offset_vec = __msa_fill_h(offset);
-    denom_vec = __msa_fill_h(rnd_val);
-
-    weight_vec_h = __msa_srar_h(weight_vec_h, denom_vec);
-    offset_vec = __msa_adds_s_h(offset_vec, weight_vec_h);
+    offset_vec = __msa_fill_w(offset);
 
     mask1 = mask0 + 2;
 
     for (loop_cnt = (height >> 3); loop_cnt--;) {
         LD_SB8(src, src_stride, src0, src1, src2, src3, src4, src5, src6, src7);
         src += (8 * src_stride);
-        XORI_B8_128_SB(src0, src1, src2, src3, src4, src5, src6, src7);
 
         VSHF_B2_SB(src0, src0, src0, src0, mask0, mask1, vec0, vec1);
         VSHF_B2_SB(src1, src1, src1, src1, mask0, mask1, vec2, vec3);
         VSHF_B2_SB(src2, src2, src2, src2, mask0, mask1, vec4, vec5);
         VSHF_B2_SB(src3, src3, src3, src3, mask0, mask1, vec6, vec7);
-        dst0 = HEVC_FILT_4TAP_SH(vec0, vec1, filt0, filt1);
-        dst1 = HEVC_FILT_4TAP_SH(vec2, vec3, filt0, filt1);
-        dst2 = HEVC_FILT_4TAP_SH(vec4, vec5, filt0, filt1);
-        dst3 = HEVC_FILT_4TAP_SH(vec6, vec7, filt0, filt1);
+
+        ILVRL_B2_SH(zero, vec0, tmp0, tmp1);
+        ILVRL_B2_SH(zero, vec1, tmp2, tmp3);
+        ILVRL_B2_SH(zero, vec2, tmp4, tmp5);
+        ILVRL_B2_SH(zero, vec3, tmp6, tmp7);
+        dst0 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+        dst1 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+        dst2 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+        dst3 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+        ILVRL_B2_SH(zero, vec4, tmp0, tmp1);
+        ILVRL_B2_SH(zero, vec5, tmp2, tmp3);
+        ILVRL_B2_SH(zero, vec6, tmp4, tmp5);
+        ILVRL_B2_SH(zero, vec7, tmp6, tmp7);
+        dst4 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+        dst5 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+        dst6 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+        dst7 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
         VSHF_B2_SB(src4, src4, src4, src4, mask0, mask1, vec0, vec1);
         VSHF_B2_SB(src5, src5, src5, src5, mask0, mask1, vec2, vec3);
         VSHF_B2_SB(src6, src6, src6, src6, mask0, mask1, vec4, vec5);
         VSHF_B2_SB(src7, src7, src7, src7, mask0, mask1, vec6, vec7);
-        dst4 = HEVC_FILT_4TAP_SH(vec0, vec1, filt0, filt1);
-        dst5 = HEVC_FILT_4TAP_SH(vec2, vec3, filt0, filt1);
-        dst6 = HEVC_FILT_4TAP_SH(vec4, vec5, filt0, filt1);
-        dst7 = HEVC_FILT_4TAP_SH(vec6, vec7, filt0, filt1);
-
-        HEVC_UNIW_RND_CLIP4_MAX_SATU_H(dst0, dst1, dst2, dst3,
-                                       weight_vec, offset_vec, rnd_vec,
-                                       dst0, dst1, dst2, dst3);
-
-        HEVC_UNIW_RND_CLIP4_MAX_SATU_H(dst4, dst5, dst6, dst7,
-                                       weight_vec, offset_vec, rnd_vec,
-                                       dst4, dst5, dst6, dst7);
+        ILVRL_B2_SH(zero, vec0, tmp0, tmp1);
+        ILVRL_B2_SH(zero, vec1, tmp2, tmp3);
+        ILVRL_B2_SH(zero, vec2, tmp4, tmp5);
+        ILVRL_B2_SH(zero, vec3, tmp6, tmp7);
+        dst8  = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+        dst9  = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+        dst10 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+        dst11 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+        ILVRL_B2_SH(zero, vec4, tmp0, tmp1);
+        ILVRL_B2_SH(zero, vec5, tmp2, tmp3);
+        ILVRL_B2_SH(zero, vec6, tmp4, tmp5);
+        ILVRL_B2_SH(zero, vec7, tmp6, tmp7);
+        dst12 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+        dst13 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+        dst14 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+        dst15 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+
+        MUL4(dst0, weight_vec, dst1, weight_vec, dst2, weight_vec, dst3,
+             weight_vec, dst0, dst1, dst2, dst3);
+        MUL4(dst4, weight_vec, dst5, weight_vec, dst6, weight_vec,
+             dst7, weight_vec, dst4, dst5, dst6, dst7);
+        SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
+        SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
+        ADD4(dst0, offset_vec, dst1, offset_vec, dst2, offset_vec, dst3,
+             offset_vec, dst0, dst1, dst2, dst3);
+        ADD4(dst4, offset_vec, dst5, offset_vec, dst6, offset_vec,
+             dst7, offset_vec, dst4, dst5, dst6, dst7);
+        CLIP_SW8_0_255(dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7);
+
+        MUL4(dst8, weight_vec, dst9, weight_vec, dst10, weight_vec, dst11,
+             weight_vec, dst8, dst9, dst10, dst11);
+        MUL4(dst12, weight_vec, dst13, weight_vec, dst14, weight_vec,
+             dst15, weight_vec, dst12, dst13, dst14, dst15);
+        SRAR_W4_SW(dst8, dst9, dst10, dst11, rnd_vec);
+        SRAR_W4_SW(dst12, dst13, dst14, dst15, rnd_vec);
+        ADD4(dst8, offset_vec, dst9, offset_vec, dst10, offset_vec, dst11,
+             offset_vec, dst8, dst9, dst10, dst11);
+        ADD4(dst12, offset_vec, dst13, offset_vec, dst14, offset_vec,
+             dst15, offset_vec, dst12, dst13, dst14, dst15);
+        CLIP_SW8_0_255(dst8, dst9, dst10, dst11, dst12, dst13, dst14, dst15);
+
+        PCKEV_H2_SH(dst1, dst0, dst3, dst2, tmp0, tmp1);
+        PCKEV_H2_SH(dst5, dst4, dst7, dst6, tmp2, tmp3);
+        PCKEV_H2_SH(dst9, dst8, dst11, dst10, tmp4, tmp5);
+        PCKEV_H2_SH(dst13, dst12, dst15, dst14, tmp6, tmp7);
 
-        PCKEV_B2_UB(dst1, dst0, dst3, dst2, out0, out1);
-        PCKEV_B2_UB(dst5, dst4, dst7, dst6, out2, out3);
+        PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out0, out1);
+        PCKEV_B2_UB(tmp5, tmp4, tmp7, tmp6, out2, out3);
         ST_D8(out0, out1, out2, out3, 0, 1, 0, 1, 0, 1, 0, 1, dst, dst_stride);
         dst += (8 * dst_stride);
     }
@@ -2929,37 +3293,29 @@ static void hevc_hz_uniwgt_4t_12w_msa(uint8_t *src,
 {
     uint32_t loop_cnt;
     v16u8 out0, out1, out2;
-    v8i16 filt0, filt1;
+    v8i16 filter_vec, filt0, filt1;
     v16i8 src0, src1, src2, src3;
     v16i8 mask0 = LD_SB(&ff_hevc_mask_arr[0]);
     v16i8 mask2 = { 8, 9, 9, 10, 10, 11, 11, 12, 24, 25, 25, 26, 26, 27, 27, 28
     };
     v16i8 mask1;
     v16i8 vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7, vec8, vec9, vec10;
-    v8i16 dst0, dst1, dst2, dst3, dst4, dst5;
-    v8i16 filter_vec, weight_vec_h, offset_vec, denom_vec;
+    v4i32 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    v4i32 dst8, dst9, dst10, dst11;
+    v8i16 tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
     v16i8 mask3, vec11;
-    v4i32 weight_vec, rnd_vec;
+    v4i32 weight_vec, rnd_vec, offset_vec;
+    v8i16 zero = { 0 };
 
     src -= 1;
 
     filter_vec = LD_SH(filter);
-    SPLATI_H2_SH(filter_vec, 0, 1, filt0, filt1);
-
-    weight = weight & 0x0000FFFF;
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W2_SH(filter_vec, 0, filt0, filt1);
 
     weight_vec = __msa_fill_w(weight);
     rnd_vec = __msa_fill_w(rnd_val);
-
-    weight *= 128;
-    rnd_val -= 6;
-
-    weight_vec_h = __msa_fill_h(weight);
-    offset_vec = __msa_fill_h(offset);
-    denom_vec = __msa_fill_h(rnd_val);
-
-    weight_vec_h = __msa_srar_h(weight_vec_h, denom_vec);
-    offset_vec = __msa_adds_s_h(offset_vec, weight_vec_h);
+    offset_vec = __msa_fill_w(offset);
 
     mask1 = mask0 + 2;
     mask3 = mask2 + 2;
@@ -2968,29 +3324,62 @@ static void hevc_hz_uniwgt_4t_12w_msa(uint8_t *src,
         LD_SB4(src, src_stride, src0, src1, src2, src3);
         src += (4 * src_stride);
 
-        XORI_B4_128_SB(src0, src1, src2, src3);
-
         VSHF_B2_SB(src0, src0, src0, src0, mask0, mask1, vec0, vec1);
         VSHF_B2_SB(src1, src1, src1, src1, mask0, mask1, vec2, vec3);
         VSHF_B2_SB(src2, src2, src2, src2, mask0, mask1, vec4, vec5);
         VSHF_B2_SB(src3, src3, src3, src3, mask0, mask1, vec6, vec7);
         VSHF_B2_SB(src0, src1, src0, src1, mask2, mask3, vec8, vec9);
         VSHF_B2_SB(src2, src3, src2, src3, mask2, mask3, vec10, vec11);
-        dst0 = HEVC_FILT_4TAP_SH(vec0, vec1, filt0, filt1);
-        dst1 = HEVC_FILT_4TAP_SH(vec2, vec3, filt0, filt1);
-        dst2 = HEVC_FILT_4TAP_SH(vec4, vec5, filt0, filt1);
-        dst3 = HEVC_FILT_4TAP_SH(vec6, vec7, filt0, filt1);
-        dst4 = HEVC_FILT_4TAP_SH(vec8, vec9, filt0, filt1);
-        dst5 = HEVC_FILT_4TAP_SH(vec10, vec11, filt0, filt1);
-
-        HEVC_UNIW_RND_CLIP4_MAX_SATU_H(dst0, dst1, dst2, dst3,
-                                       weight_vec, offset_vec, rnd_vec,
-                                       dst0, dst1, dst2, dst3);
-
-        HEVC_UNIW_RND_CLIP2_MAX_SATU_H(dst4, dst5, weight_vec, offset_vec,
-                                       rnd_vec, dst4, dst5);
 
-        PCKEV_B3_UB(dst1, dst0, dst3, dst2, dst5, dst4, out0, out1, out2);
+        ILVRL_B2_SH(zero, vec0, tmp0, tmp1);
+        ILVRL_B2_SH(zero, vec1, tmp2, tmp3);
+        ILVRL_B2_SH(zero, vec2, tmp4, tmp5);
+        ILVRL_B2_SH(zero, vec3, tmp6, tmp7);
+        dst0 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+        dst1 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+        dst2 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+        dst3 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+        ILVRL_B2_SH(zero, vec4, tmp0, tmp1);
+        ILVRL_B2_SH(zero, vec5, tmp2, tmp3);
+        ILVRL_B2_SH(zero, vec6, tmp4, tmp5);
+        ILVRL_B2_SH(zero, vec7, tmp6, tmp7);
+        dst4 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+        dst5 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+        dst6 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+        dst7 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+        ILVRL_B2_SH(zero, vec8, tmp0, tmp1);
+        ILVRL_B2_SH(zero, vec9, tmp2, tmp3);
+        ILVRL_B2_SH(zero, vec10, tmp4, tmp5);
+        ILVRL_B2_SH(zero, vec11, tmp6, tmp7);
+        dst8  = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+        dst9  = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+        dst10 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+        dst11 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+
+        MUL4(dst0, weight_vec, dst1, weight_vec, dst2, weight_vec, dst3,
+             weight_vec, dst0, dst1, dst2, dst3);
+        MUL4(dst4, weight_vec, dst5, weight_vec, dst6, weight_vec,
+             dst7, weight_vec, dst4, dst5, dst6, dst7);
+        SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
+        SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
+        ADD4(dst0, offset_vec, dst1, offset_vec, dst2, offset_vec, dst3,
+             offset_vec, dst0, dst1, dst2, dst3);
+        ADD4(dst4, offset_vec, dst5, offset_vec, dst6, offset_vec,
+             dst7, offset_vec, dst4, dst5, dst6, dst7);
+        CLIP_SW8_0_255(dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7);
+
+        MUL4(dst8, weight_vec, dst9, weight_vec, dst10, weight_vec, dst11,
+             weight_vec, dst8, dst9, dst10, dst11);
+        SRAR_W4_SW(dst8, dst9, dst10, dst11, rnd_vec);
+        ADD4(dst8, offset_vec, dst9, offset_vec, dst10, offset_vec, dst11,
+             offset_vec, dst8, dst9, dst10, dst11);
+        CLIP_SW4_0_255(dst8, dst9, dst10, dst11);
+
+        PCKEV_H2_SH(dst1, dst0, dst3, dst2, tmp0, tmp1);
+        PCKEV_H2_SH(dst5, dst4, dst7, dst6, tmp2, tmp3);
+        PCKEV_H2_SH(dst9, dst8, dst11, dst10, tmp4, tmp5);
+
+        PCKEV_B3_UB(tmp1, tmp0, tmp3, tmp2, tmp5, tmp4, out0, out1, out2);
         ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
         ST_W4(out2, 0, 1, 2, 3, dst + 8, dst_stride);
         dst += (4 * dst_stride);
@@ -3010,33 +3399,25 @@ static void hevc_hz_uniwgt_4t_16w_msa(uint8_t *src,
     uint32_t loop_cnt;
     v16u8 out0, out1, out2, out3;
     v16i8 src0, src1, src2, src3, src4, src5, src6, src7;
-    v8i16 filt0, filt1;
+    v8i16 filter_vec, filt0, filt1;
     v16i8 mask0 = LD_SB(&ff_hevc_mask_arr[0]);
     v16i8 mask1;
     v16i8 vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
-    v8i16 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
-    v8i16 filter_vec, weight_vec_h, offset_vec, denom_vec;
-    v4i32 weight_vec, rnd_vec;
+    v4i32 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    v4i32 dst8, dst9, dst10, dst11, dst12, dst13, dst14, dst15;
+    v8i16 tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
+    v4i32 weight_vec, rnd_vec, offset_vec;
+    v8i16 zero = { 0 };
 
     src -= 1;
 
     filter_vec = LD_SH(filter);
-    SPLATI_H2_SH(filter_vec, 0, 1, filt0, filt1);
-
-    weight = weight & 0x0000FFFF;
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W2_SH(filter_vec, 0, filt0, filt1);
 
     weight_vec = __msa_fill_w(weight);
     rnd_vec = __msa_fill_w(rnd_val);
-
-    weight *= 128;
-    rnd_val -= 6;
-
-    weight_vec_h = __msa_fill_h(weight);
-    offset_vec = __msa_fill_h(offset);
-    denom_vec = __msa_fill_h(rnd_val);
-
-    weight_vec_h = __msa_srar_h(weight_vec_h, denom_vec);
-    offset_vec = __msa_adds_s_h(offset_vec, weight_vec_h);
+    offset_vec = __msa_fill_w(offset);
 
     mask1 = mask0 + 2;
 
@@ -3045,34 +3426,77 @@ static void hevc_hz_uniwgt_4t_16w_msa(uint8_t *src,
         LD_SB4(src + 8, src_stride, src1, src3, src5, src7);
         src += (4 * src_stride);
 
-        XORI_B8_128_SB(src0, src1, src2, src3, src4, src5, src6, src7);
-
         VSHF_B2_SB(src0, src0, src0, src0, mask0, mask1, vec0, vec1);
         VSHF_B2_SB(src1, src1, src1, src1, mask0, mask1, vec2, vec3);
         VSHF_B2_SB(src2, src2, src2, src2, mask0, mask1, vec4, vec5);
         VSHF_B2_SB(src3, src3, src3, src3, mask0, mask1, vec6, vec7);
-        dst0 = HEVC_FILT_4TAP_SH(vec0, vec1, filt0, filt1);
-        dst1 = HEVC_FILT_4TAP_SH(vec2, vec3, filt0, filt1);
-        dst2 = HEVC_FILT_4TAP_SH(vec4, vec5, filt0, filt1);
-        dst3 = HEVC_FILT_4TAP_SH(vec6, vec7, filt0, filt1);
+        ILVRL_B2_SH(zero, vec0, tmp0, tmp1);
+        ILVRL_B2_SH(zero, vec1, tmp2, tmp3);
+        ILVRL_B2_SH(zero, vec2, tmp4, tmp5);
+        ILVRL_B2_SH(zero, vec3, tmp6, tmp7);
+        dst0 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+        dst1 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+        dst2 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+        dst3 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+        ILVRL_B2_SH(zero, vec4, tmp0, tmp1);
+        ILVRL_B2_SH(zero, vec5, tmp2, tmp3);
+        ILVRL_B2_SH(zero, vec6, tmp4, tmp5);
+        ILVRL_B2_SH(zero, vec7, tmp6, tmp7);
+        dst4 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+        dst5 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+        dst6 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+        dst7 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+
         VSHF_B2_SB(src4, src4, src4, src4, mask0, mask1, vec0, vec1);
         VSHF_B2_SB(src5, src5, src5, src5, mask0, mask1, vec2, vec3);
         VSHF_B2_SB(src6, src6, src6, src6, mask0, mask1, vec4, vec5);
         VSHF_B2_SB(src7, src7, src7, src7, mask0, mask1, vec6, vec7);
-        dst4 = HEVC_FILT_4TAP_SH(vec0, vec1, filt0, filt1);
-        dst5 = HEVC_FILT_4TAP_SH(vec2, vec3, filt0, filt1);
-        dst6 = HEVC_FILT_4TAP_SH(vec4, vec5, filt0, filt1);
-        dst7 = HEVC_FILT_4TAP_SH(vec6, vec7, filt0, filt1);
-
-        HEVC_UNIW_RND_CLIP4_MAX_SATU_H(dst0, dst1, dst2, dst3,
-                                       weight_vec, offset_vec, rnd_vec,
-                                       dst0, dst1, dst2, dst3);
-
-        HEVC_UNIW_RND_CLIP4_MAX_SATU_H(dst4, dst5, dst6, dst7,
-                                       weight_vec, offset_vec, rnd_vec,
-                                       dst4, dst5, dst6, dst7);
-
-        PCKEV_B4_UB(dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6,
+        ILVRL_B2_SH(zero, vec0, tmp0, tmp1);
+        ILVRL_B2_SH(zero, vec1, tmp2, tmp3);
+        ILVRL_B2_SH(zero, vec2, tmp4, tmp5);
+        ILVRL_B2_SH(zero, vec3, tmp6, tmp7);
+        dst8  = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+        dst9  = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+        dst10 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+        dst11 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+        ILVRL_B2_SH(zero, vec4, tmp0, tmp1);
+        ILVRL_B2_SH(zero, vec5, tmp2, tmp3);
+        ILVRL_B2_SH(zero, vec6, tmp4, tmp5);
+        ILVRL_B2_SH(zero, vec7, tmp6, tmp7);
+        dst12 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+        dst13 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+        dst14 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+        dst15 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+
+        MUL4(dst0, weight_vec, dst1, weight_vec, dst2, weight_vec, dst3,
+             weight_vec, dst0, dst1, dst2, dst3);
+        MUL4(dst4, weight_vec, dst5, weight_vec, dst6, weight_vec,
+             dst7, weight_vec, dst4, dst5, dst6, dst7);
+        SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
+        SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
+        ADD4(dst0, offset_vec, dst1, offset_vec, dst2, offset_vec, dst3,
+             offset_vec, dst0, dst1, dst2, dst3);
+        ADD4(dst4, offset_vec, dst5, offset_vec, dst6, offset_vec,
+             dst7, offset_vec, dst4, dst5, dst6, dst7);
+        CLIP_SW8_0_255(dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7);
+
+        MUL4(dst8, weight_vec, dst9, weight_vec, dst10, weight_vec, dst11,
+             weight_vec, dst8, dst9, dst10, dst11);
+        MUL4(dst12, weight_vec, dst13, weight_vec, dst14, weight_vec,
+             dst15, weight_vec, dst12, dst13, dst14, dst15);
+        SRAR_W4_SW(dst8, dst9, dst10, dst11, rnd_vec);
+        SRAR_W4_SW(dst12, dst13, dst14, dst15, rnd_vec);
+        ADD4(dst8, offset_vec, dst9, offset_vec, dst10, offset_vec, dst11,
+             offset_vec, dst8, dst9, dst10, dst11);
+        ADD4(dst12, offset_vec, dst13, offset_vec, dst14, offset_vec,
+             dst15, offset_vec, dst12, dst13, dst14, dst15);
+        CLIP_SW8_0_255(dst8, dst9, dst10, dst11, dst12, dst13, dst14, dst15);
+
+        PCKEV_H2_SH(dst1, dst0, dst3, dst2, tmp0, tmp1);
+        PCKEV_H2_SH(dst5, dst4, dst7, dst6, tmp2, tmp3);
+        PCKEV_H2_SH(dst9, dst8, dst11, dst10, tmp4, tmp5);
+        PCKEV_H2_SH(dst13, dst12, dst15, dst14, tmp6, tmp7);
+        PCKEV_B4_UB(tmp1, tmp0, tmp3, tmp2, tmp5, tmp4, tmp7, tmp6,
                     out0, out1, out2, out3);
 
         ST_UB4(out0, out1, out2, out3, dst, dst_stride);
@@ -3093,31 +3517,24 @@ static void hevc_hz_uniwgt_4t_24w_msa(uint8_t *src,
     uint32_t loop_cnt;
     v16u8 out0, out1, out2;
     v16i8 src0, src1, src2, src3;
-    v8i16 filt0, filt1;
+    v8i16 filter_vec, filt0, filt1;
     v16i8 mask0, mask1, mask2, mask3;
     v16i8 vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
-    v8i16 dst0, dst1, dst2, dst3, dst4, dst5;
-    v8i16 filter_vec, weight_vec_h, offset_vec, denom_vec;
-    v4i32 weight_vec, rnd_vec;
+    v8i16 tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
+    v4i32 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    v4i32 dst8, dst9, dst10, dst11;
+    v4i32 weight_vec, rnd_vec, offset_vec;
+    v8i16 zero = { 0 };
 
     src -= 1;
 
     filter_vec = LD_SH(filter);
-    SPLATI_H2_SH(filter_vec, 0, 1, filt0, filt1);
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W2_SH(filter_vec, 0, filt0, filt1);
 
-    weight = weight & 0x0000FFFF;
     weight_vec = __msa_fill_w(weight);
     rnd_vec = __msa_fill_w(rnd_val);
-
-    weight *= 128;
-    rnd_val -= 6;
-
-    weight_vec_h = __msa_fill_h(weight);
-    offset_vec = __msa_fill_h(offset);
-    denom_vec = __msa_fill_h(rnd_val);
-
-    weight_vec_h = __msa_srar_h(weight_vec_h, denom_vec);
-    offset_vec = __msa_adds_s_h(offset_vec, weight_vec_h);
+    offset_vec = __msa_fill_w(offset);
 
     mask0 = LD_SB(&ff_hevc_mask_arr[0]);
     mask1 = mask0 + 2;
@@ -3129,29 +3546,62 @@ static void hevc_hz_uniwgt_4t_24w_msa(uint8_t *src,
         LD_SB2(src + 16, src_stride, src1, src3);
         src += (2 * src_stride);
 
-        XORI_B4_128_SB(src0, src1, src2, src3);
-
         VSHF_B2_SB(src0, src0, src0, src0, mask0, mask1, vec0, vec1);
         VSHF_B2_SB(src0, src1, src0, src1, mask2, mask3, vec2, vec3);
         VSHF_B2_SB(src2, src2, src2, src2, mask0, mask1, vec4, vec5);
         VSHF_B2_SB(src2, src3, src2, src3, mask2, mask3, vec6, vec7);
-        dst0 = HEVC_FILT_4TAP_SH(vec0, vec1, filt0, filt1);
-        dst1 = HEVC_FILT_4TAP_SH(vec2, vec3, filt0, filt1);
-        dst2 = HEVC_FILT_4TAP_SH(vec4, vec5, filt0, filt1);
-        dst3 = HEVC_FILT_4TAP_SH(vec6, vec7, filt0, filt1);
+        ILVRL_B2_SH(zero, vec0, tmp0, tmp1);
+        ILVRL_B2_SH(zero, vec1, tmp2, tmp3);
+        ILVRL_B2_SH(zero, vec2, tmp4, tmp5);
+        ILVRL_B2_SH(zero, vec3, tmp6, tmp7);
+        dst0 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+        dst1 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+        dst2 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+        dst3 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+        ILVRL_B2_SH(zero, vec4, tmp0, tmp1);
+        ILVRL_B2_SH(zero, vec5, tmp2, tmp3);
+        ILVRL_B2_SH(zero, vec6, tmp4, tmp5);
+        ILVRL_B2_SH(zero, vec7, tmp6, tmp7);
+        dst4 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+        dst5 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+        dst6 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+        dst7 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+
         VSHF_B2_SB(src1, src1, src1, src1, mask0, mask1, vec0, vec1);
         VSHF_B2_SB(src3, src3, src3, src3, mask0, mask1, vec2, vec3);
-        dst4 = HEVC_FILT_4TAP_SH(vec0, vec1, filt0, filt1);
-        dst5 = HEVC_FILT_4TAP_SH(vec2, vec3, filt0, filt1);
-
-        HEVC_UNIW_RND_CLIP4_MAX_SATU_H(dst0, dst1, dst2, dst3,
-                                       weight_vec, offset_vec, rnd_vec,
-                                       dst0, dst1, dst2, dst3);
-
-        HEVC_UNIW_RND_CLIP2_MAX_SATU_H(dst4, dst5, weight_vec, offset_vec,
-                                       rnd_vec, dst4, dst5);
-
-        PCKEV_B3_UB(dst1, dst0, dst3, dst2, dst5, dst4, out0, out1, out2);
+        ILVRL_B2_SH(zero, vec0, tmp0, tmp1);
+        ILVRL_B2_SH(zero, vec1, tmp2, tmp3);
+        ILVRL_B2_SH(zero, vec2, tmp4, tmp5);
+        ILVRL_B2_SH(zero, vec3, tmp6, tmp7);
+        dst8  = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+        dst9  = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+        dst10 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+        dst11 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+
+        MUL4(dst0, weight_vec, dst1, weight_vec, dst2, weight_vec, dst3,
+             weight_vec, dst0, dst1, dst2, dst3);
+        MUL4(dst4, weight_vec, dst5, weight_vec, dst6, weight_vec,
+             dst7, weight_vec, dst4, dst5, dst6, dst7);
+        SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
+        SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
+        ADD4(dst0, offset_vec, dst1, offset_vec, dst2, offset_vec, dst3,
+             offset_vec, dst0, dst1, dst2, dst3);
+        ADD4(dst4, offset_vec, dst5, offset_vec, dst6, offset_vec,
+             dst7, offset_vec, dst4, dst5, dst6, dst7);
+        CLIP_SW8_0_255(dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7);
+
+        MUL4(dst8, weight_vec, dst9, weight_vec, dst10, weight_vec, dst11,
+             weight_vec, dst8, dst9, dst10, dst11);
+        SRAR_W4_SW(dst8, dst9, dst10, dst11, rnd_vec);
+        ADD4(dst8, offset_vec, dst9, offset_vec, dst10, offset_vec, dst11,
+             offset_vec, dst8, dst9, dst10, dst11);
+        CLIP_SW4_0_255(dst8, dst9, dst10, dst11);
+
+        PCKEV_H2_SH(dst1, dst0, dst3, dst2, tmp0, tmp1);
+        PCKEV_H2_SH(dst5, dst4, dst7, dst6, tmp2, tmp3);
+        PCKEV_H2_SH(dst9, dst8, dst11, dst10, tmp4, tmp5);
+
+        PCKEV_B3_UB(tmp1, tmp0, tmp3, tmp2, tmp5, tmp4, out0, out1, out2);
         ST_UB2(out0, out1, dst, dst_stride);
         ST_D2(out2, 0, 1, dst + 16, dst_stride);
         dst += (2 * dst_stride);
@@ -3171,33 +3621,25 @@ static void hevc_hz_uniwgt_4t_32w_msa(uint8_t *src,
     uint32_t loop_cnt;
     v16u8 out0, out1, out2, out3;
     v16i8 src0, src1, src2, src3, src4, src5;
-    v8i16 filt0, filt1;
+    v8i16 filter_vec, filt0, filt1;
     v16i8 mask0 = LD_SB(&ff_hevc_mask_arr[0]);
     v16i8 mask1, mask2, mask3;
     v16i8 vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
-    v8i16 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
-    v8i16 filter_vec, weight_vec_h, offset_vec, denom_vec;
-    v4i32 weight_vec, rnd_vec;
+    v8i16 tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
+    v4i32 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    v4i32 dst8, dst9, dst10, dst11, dst12, dst13, dst14, dst15;
+    v4i32 weight_vec, rnd_vec, offset_vec;
+    v8i16 zero = { 0 };
 
     src -= 1;
 
     filter_vec = LD_SH(filter);
-    SPLATI_H2_SH(filter_vec, 0, 1, filt0, filt1);
-
-    weight = weight & 0x0000FFFF;
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W2_SH(filter_vec, 0, filt0, filt1);
 
     weight_vec = __msa_fill_w(weight);
     rnd_vec = __msa_fill_w(rnd_val);
-
-    weight *= 128;
-    rnd_val -= 6;
-
-    weight_vec_h = __msa_fill_h(weight);
-    offset_vec = __msa_fill_h(offset);
-    denom_vec = __msa_fill_h(rnd_val);
-
-    weight_vec_h = __msa_srar_h(weight_vec_h, denom_vec);
-    offset_vec = __msa_adds_s_h(offset_vec, weight_vec_h);
+    offset_vec = __msa_fill_w(offset);
 
     mask1 = mask0 + 2;
     mask2 = mask0 + 8;
@@ -3210,34 +3652,78 @@ static void hevc_hz_uniwgt_4t_32w_msa(uint8_t *src,
         LD_SB2(src, 16, src3, src4);
         src5 = LD_SB(src + 24);
         src += src_stride;
-        XORI_B6_128_SB(src0, src1, src2, src3, src4, src5);
         VSHF_B2_SB(src0, src0, src0, src0, mask0, mask1, vec0, vec1);
         VSHF_B2_SB(src0, src1, src0, src1, mask2, mask3, vec2, vec3);
         VSHF_B2_SB(src1, src1, src1, src1, mask0, mask1, vec4, vec5);
         VSHF_B2_SB(src2, src2, src2, src2, mask0, mask1, vec6, vec7);
-        dst0 = HEVC_FILT_4TAP_SH(vec0, vec1, filt0, filt1);
-        dst1 = HEVC_FILT_4TAP_SH(vec2, vec3, filt0, filt1);
-        dst2 = HEVC_FILT_4TAP_SH(vec4, vec5, filt0, filt1);
-        dst3 = HEVC_FILT_4TAP_SH(vec6, vec7, filt0, filt1);
+        ILVRL_B2_SH(zero, vec0, tmp0, tmp1);
+        ILVRL_B2_SH(zero, vec1, tmp2, tmp3);
+        ILVRL_B2_SH(zero, vec2, tmp4, tmp5);
+        ILVRL_B2_SH(zero, vec3, tmp6, tmp7);
+        dst0 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+        dst1 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+        dst2 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+        dst3 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+        ILVRL_B2_SH(zero, vec4, tmp0, tmp1);
+        ILVRL_B2_SH(zero, vec5, tmp2, tmp3);
+        ILVRL_B2_SH(zero, vec6, tmp4, tmp5);
+        ILVRL_B2_SH(zero, vec7, tmp6, tmp7);
+        dst4 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+        dst5 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+        dst6 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+        dst7 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+
         VSHF_B2_SB(src3, src3, src3, src3, mask0, mask1, vec0, vec1);
         VSHF_B2_SB(src3, src4, src3, src4, mask2, mask3, vec2, vec3);
         VSHF_B2_SB(src4, src4, src4, src4, mask0, mask1, vec4, vec5);
         VSHF_B2_SB(src5, src5, src5, src5, mask0, mask1, vec6, vec7);
-        dst4 = HEVC_FILT_4TAP_SH(vec0, vec1, filt0, filt1);
-        dst5 = HEVC_FILT_4TAP_SH(vec2, vec3, filt0, filt1);
-        dst6 = HEVC_FILT_4TAP_SH(vec4, vec5, filt0, filt1);
-        dst7 = HEVC_FILT_4TAP_SH(vec6, vec7, filt0, filt1);
-
-        HEVC_UNIW_RND_CLIP4_MAX_SATU_H(dst0, dst1, dst2, dst3,
-                                       weight_vec, offset_vec, rnd_vec,
-                                       dst0, dst1, dst2, dst3);
-
-        HEVC_UNIW_RND_CLIP4_MAX_SATU_H(dst4, dst5, dst6, dst7,
-                                       weight_vec, offset_vec, rnd_vec,
-                                       dst4, dst5, dst6, dst7);
-
-        PCKEV_B2_UB(dst1, dst0, dst3, dst2, out0, out1);
-        PCKEV_B2_UB(dst5, dst4, dst7, dst6, out2, out3);
+        ILVRL_B2_SH(zero, vec0, tmp0, tmp1);
+        ILVRL_B2_SH(zero, vec1, tmp2, tmp3);
+        ILVRL_B2_SH(zero, vec2, tmp4, tmp5);
+        ILVRL_B2_SH(zero, vec3, tmp6, tmp7);
+        dst8  = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+        dst9  = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+        dst10 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+        dst11 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+        ILVRL_B2_SH(zero, vec4, tmp0, tmp1);
+        ILVRL_B2_SH(zero, vec5, tmp2, tmp3);
+        ILVRL_B2_SH(zero, vec6, tmp4, tmp5);
+        ILVRL_B2_SH(zero, vec7, tmp6, tmp7);
+        dst12 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+        dst13 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+        dst14 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+        dst15 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+
+        MUL4(dst0, weight_vec, dst1, weight_vec, dst2, weight_vec, dst3,
+             weight_vec, dst0, dst1, dst2, dst3);
+        MUL4(dst4, weight_vec, dst5, weight_vec, dst6, weight_vec,
+             dst7, weight_vec, dst4, dst5, dst6, dst7);
+        SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
+        SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
+        ADD4(dst0, offset_vec, dst1, offset_vec, dst2, offset_vec, dst3,
+             offset_vec, dst0, dst1, dst2, dst3);
+        ADD4(dst4, offset_vec, dst5, offset_vec, dst6, offset_vec,
+             dst7, offset_vec, dst4, dst5, dst6, dst7);
+        CLIP_SW8_0_255(dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7);
+
+        MUL4(dst8, weight_vec, dst9, weight_vec, dst10, weight_vec, dst11,
+             weight_vec, dst8, dst9, dst10, dst11);
+        MUL4(dst12, weight_vec, dst13, weight_vec, dst14, weight_vec,
+             dst15, weight_vec, dst12, dst13, dst14, dst15);
+        SRAR_W4_SW(dst8, dst9, dst10, dst11, rnd_vec);
+        SRAR_W4_SW(dst12, dst13, dst14, dst15, rnd_vec);
+        ADD4(dst8, offset_vec, dst9, offset_vec, dst10, offset_vec, dst11,
+             offset_vec, dst8, dst9, dst10, dst11);
+        ADD4(dst12, offset_vec, dst13, offset_vec, dst14, offset_vec,
+             dst15, offset_vec, dst12, dst13, dst14, dst15);
+        CLIP_SW8_0_255(dst8, dst9, dst10, dst11, dst12, dst13, dst14, dst15);
+
+        PCKEV_H2_SH(dst1, dst0, dst3, dst2, tmp0, tmp1);
+        PCKEV_H2_SH(dst5, dst4, dst7, dst6, tmp2, tmp3);
+        PCKEV_H2_SH(dst9, dst8, dst11, dst10, tmp4, tmp5);
+        PCKEV_H2_SH(dst13, dst12, dst15, dst14, tmp6, tmp7);
+        PCKEV_B4_UB(tmp1, tmp0, tmp3, tmp2, tmp5, tmp4, tmp7, tmp6,
+                    out0, out1, out2, out3);
         ST_UB2(out0, out1, dst, 16);
         dst += dst_stride;
         ST_UB2(out2, out3, dst, 16);
@@ -3260,42 +3746,36 @@ static void hevc_vt_uniwgt_4t_4x2_msa(uint8_t *src,
     v16i8 src2110, src4332;
     v8i16 dst0;
     v4i32 dst0_r, dst0_l;
-    v8i16 filt0, filt1;
-    v8i16 filter_vec, weight_vec_h, offset_vec, denom_vec;
-    v4i32 weight_vec, rnd_vec;
+    v8i16 filter_vec, filt0, filt1;
+    v8i16 tmp0, tmp1, tmp2, tmp3;
+    v4i32 weight_vec, rnd_vec, offset_vec;
+    v8i16 zero = { 0 };
 
     src -= src_stride;
 
-    weight = weight & 0x0000FFFF;
-
     weight_vec = __msa_fill_w(weight);
     rnd_vec = __msa_fill_w(rnd_val);
-
-    weight *= 128;
-    rnd_val -= 6;
-
-    weight_vec_h = __msa_fill_h(weight);
-    offset_vec = __msa_fill_h(offset);
-    denom_vec = __msa_fill_h(rnd_val);
-
-    weight_vec_h = __msa_srar_h(weight_vec_h, denom_vec);
-    offset_vec = __msa_adds_s_h(offset_vec, weight_vec_h);
+    offset_vec = __msa_fill_w(offset);
 
     filter_vec = LD_SH(filter);
-    SPLATI_H2_SH(filter_vec, 0, 1, filt0, filt1);
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W2_SH(filter_vec, 0, filt0, filt1);
 
     LD_SB5(src, src_stride, src0, src1, src2, src3, src4);
     ILVR_B2_SB(src1, src0, src2, src1, src10_r, src21_r);
     ILVR_B2_SB(src3, src2, src4, src3, src32_r, src43_r);
     ILVR_D2_SB(src21_r, src10_r, src43_r, src32_r, src2110, src4332);
-    XORI_B2_128_SB(src2110, src4332);
-    dst0 = HEVC_FILT_4TAP_SH(src2110, src4332, filt0, filt1);
-    ILVRL_H2_SW(dst0, dst0, dst0_r, dst0_l);
-    DOTP_SH2_SW(dst0_r, dst0_l, weight_vec, weight_vec, dst0_r, dst0_l);
+    ILVRL_B2_SH(zero, src2110, tmp0, tmp1);
+    ILVRL_B2_SH(zero, src4332, tmp2, tmp3);
+
+    dst0_r = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+    dst0_l = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+
+    MUL2(dst0_r, weight_vec, dst0_l, weight_vec, dst0_r, dst0_l);
     SRAR_W2_SW(dst0_r, dst0_l, rnd_vec);
+    ADD2(dst0_r, offset_vec, dst0_l, offset_vec, dst0_r, dst0_l);
+    CLIP_SW2_0_255(dst0_r, dst0_l);
     dst0 = __msa_pckev_h((v8i16) dst0_l, (v8i16) dst0_r);
-    dst0 = __msa_adds_s_h(dst0, offset_vec);
-    dst0 = CLIP_SH_0_255_MAX_SATU(dst0);
     out = (v16u8) __msa_pckev_b((v16i8) dst0, (v16i8) dst0);
     ST_W2(out, 0, 1, dst, dst_stride);
 }
@@ -3313,30 +3793,21 @@ static void hevc_vt_uniwgt_4t_4x4_msa(uint8_t *src,
     v16i8 src0, src1, src2, src3, src4, src5, src6;
     v16i8 src10_r, src32_r, src54_r, src21_r, src43_r, src65_r;
     v16i8 src2110, src4332, src6554;
-    v8i16 dst0, dst1;
-    v8i16 filt0, filt1;
-    v8i16 filter_vec, weight_vec_h, offset_vec, denom_vec;
-    v4i32 weight_vec, rnd_vec;
+    v4i32 dst0, dst1, dst2, dst3;
+    v8i16 tmp0, tmp1, tmp2, tmp3, tmp4, tmp5;
+    v8i16 filter_vec, filt0, filt1;
+    v4i32 weight_vec, rnd_vec, offset_vec;
+    v8i16 zero = { 0 };
 
     src -= src_stride;
 
-    weight = weight & 0x0000FFFF;
-
     weight_vec = __msa_fill_w(weight);
     rnd_vec = __msa_fill_w(rnd_val);
-
-    weight *= 128;
-    rnd_val -= 6;
-
-    weight_vec_h = __msa_fill_h(weight);
-    offset_vec = __msa_fill_h(offset);
-    denom_vec = __msa_fill_h(rnd_val);
-
-    weight_vec_h = __msa_srar_h(weight_vec_h, denom_vec);
-    offset_vec = __msa_adds_s_h(offset_vec, weight_vec_h);
+    offset_vec = __msa_fill_w(offset);
 
     filter_vec = LD_SH(filter);
-    SPLATI_H2_SH(filter_vec, 0, 1, filt0, filt1);
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W2_SH(filter_vec, 0, filt0, filt1);
 
     LD_SB7(src, src_stride, src0, src1, src2, src3, src4, src5, src6);
     ILVR_B2_SB(src1, src0, src2, src1, src10_r, src21_r);
@@ -3344,13 +3815,25 @@ static void hevc_vt_uniwgt_4t_4x4_msa(uint8_t *src,
                src32_r, src43_r, src54_r, src65_r);
     ILVR_D3_SB(src21_r, src10_r, src43_r, src32_r, src65_r, src54_r,
                src2110, src4332, src6554);
-    XORI_B3_128_SB(src2110, src4332, src6554);
-    dst0 = HEVC_FILT_4TAP_SH(src2110, src4332, filt0, filt1);
-    dst1 = HEVC_FILT_4TAP_SH(src4332, src6554, filt0, filt1);
-    HEVC_UNIW_RND_CLIP2_MAX_SATU_H(dst0, dst1, weight_vec, offset_vec, rnd_vec,
-                                   dst0, dst1);
 
-    out = (v16u8) __msa_pckev_b((v16i8) dst1, (v16i8) dst0);
+    ILVRL_B2_SH(zero, src2110, tmp0, tmp1);
+    ILVRL_B2_SH(zero, src4332, tmp2, tmp3);
+    ILVRL_B2_SH(zero, src6554, tmp4, tmp5);
+
+    dst0 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+    dst1 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+    dst2 = HEVC_FILT_4TAP_SW(tmp2, tmp4, filt0, filt1);
+    dst3 = HEVC_FILT_4TAP_SW(tmp3, tmp5, filt0, filt1);
+
+    MUL4(dst0, weight_vec, dst1, weight_vec, dst2, weight_vec, dst3,
+         weight_vec, dst0, dst1, dst2, dst3);
+    SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
+    ADD4(dst0, offset_vec, dst1, offset_vec, dst2, offset_vec, dst3,
+         offset_vec, dst0, dst1, dst2, dst3);
+    CLIP_SW4_0_255(dst0, dst1, dst2, dst3);
+
+    PCKEV_H2_SH(dst1, dst0, dst3, dst2, tmp0, tmp1);
+    out = (v16u8) __msa_pckev_b((v16i8) tmp1, (v16i8) tmp0);
     ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
 }
 
@@ -3371,35 +3854,26 @@ static void hevc_vt_uniwgt_4t_4x8multiple_msa(uint8_t *src,
     v16i8 src21_r, src43_r, src65_r, src87_r, src109_r;
     v16i8 src2110, src4332, src6554, src8776;
     v16i8 src10998;
-    v8i16 dst0, dst1, dst2, dst3, filt0, filt1;
-    v8i16 filter_vec, weight_vec_h, offset_vec, denom_vec;
-    v4i32 weight_vec, rnd_vec;
+    v4i32 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    v8i16 tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
+    v8i16 filter_vec, filt0, filt1;
+    v4i32 weight_vec, rnd_vec, offset_vec;
+    v8i16 zero = { 0 };
 
     src -= src_stride;
 
-    weight = weight & 0x0000FFFF;
-
     weight_vec = __msa_fill_w(weight);
     rnd_vec = __msa_fill_w(rnd_val);
-
-    weight *= 128;
-    rnd_val -= 6;
-
-    weight_vec_h = __msa_fill_h(weight);
-    offset_vec = __msa_fill_h(offset);
-    denom_vec = __msa_fill_h(rnd_val);
-
-    weight_vec_h = __msa_srar_h(weight_vec_h, denom_vec);
-    offset_vec = __msa_adds_s_h(offset_vec, weight_vec_h);
+    offset_vec = __msa_fill_w(offset);
 
     filter_vec = LD_SH(filter);
-    SPLATI_H2_SH(filter_vec, 0, 1, filt0, filt1);
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W2_SH(filter_vec, 0, filt0, filt1);
 
     LD_SB3(src, src_stride, src0, src1, src2);
     src += (3 * src_stride);
     ILVR_B2_SB(src1, src0, src2, src1, src10_r, src21_r);
     src2110 = (v16i8) __msa_ilvr_d((v2i64) src21_r, (v2i64) src10_r);
-    src2110 = (v16i8) __msa_xori_b((v16u8) src2110, 128);
 
     for (loop_cnt = (height >> 3); loop_cnt--;) {
         LD_SB8(src, src_stride,
@@ -3411,17 +3885,36 @@ static void hevc_vt_uniwgt_4t_4x8multiple_msa(uint8_t *src,
         ILVR_B2_SB(src9, src8, src10, src9, src98_r, src109_r);
         ILVR_D4_SB(src43_r, src32_r, src65_r, src54_r, src87_r, src76_r,
                    src109_r, src98_r, src4332, src6554, src8776, src10998);
-        XORI_B4_128_SB(src4332, src6554, src8776, src10998);
-        dst0 = HEVC_FILT_4TAP_SH(src2110, src4332, filt0, filt1);
-        dst1 = HEVC_FILT_4TAP_SH(src4332, src6554, filt0, filt1);
-        dst2 = HEVC_FILT_4TAP_SH(src6554, src8776, filt0, filt1);
-        dst3 = HEVC_FILT_4TAP_SH(src8776, src10998, filt0, filt1);
 
-        HEVC_UNIW_RND_CLIP4_MAX_SATU_H(dst0, dst1, dst2, dst3,
-                                       weight_vec, offset_vec, rnd_vec,
-                                       dst0, dst1, dst2, dst3);
+        ILVRL_B2_SH(zero, src2110, tmp0, tmp1);
+        ILVRL_B2_SH(zero, src4332, tmp2, tmp3);
+        ILVRL_B2_SH(zero, src6554, tmp4, tmp5);
+        ILVRL_B2_SH(zero, src8776, tmp6, tmp7);
+        dst0 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+        dst1 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+        dst2 = HEVC_FILT_4TAP_SW(tmp2, tmp4, filt0, filt1);
+        dst3 = HEVC_FILT_4TAP_SW(tmp3, tmp5, filt0, filt1);
+        ILVRL_B2_SH(zero, src10998, tmp0, tmp1);
+        dst4 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+        dst5 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+        dst6 = HEVC_FILT_4TAP_SW(tmp6, tmp0, filt0, filt1);
+        dst7 = HEVC_FILT_4TAP_SW(tmp7, tmp1, filt0, filt1);
+
+        MUL4(dst0, weight_vec, dst1, weight_vec, dst2, weight_vec, dst3,
+             weight_vec, dst0, dst1, dst2, dst3);
+        MUL4(dst4, weight_vec, dst5, weight_vec, dst6, weight_vec,
+             dst7, weight_vec, dst4, dst5, dst6, dst7);
+        SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
+        SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
+        ADD4(dst0, offset_vec, dst1, offset_vec, dst2, offset_vec, dst3,
+             offset_vec, dst0, dst1, dst2, dst3);
+        ADD4(dst4, offset_vec, dst5, offset_vec, dst6, offset_vec,
+             dst7, offset_vec, dst4, dst5, dst6, dst7);
+        CLIP_SW8_0_255(dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7);
+        PCKEV_H2_SH(dst1, dst0, dst3, dst2, tmp0, tmp1);
+        PCKEV_H2_SH(dst5, dst4, dst7, dst6, tmp2, tmp3);
 
-        PCKEV_B2_UB(dst1, dst0, dst3, dst2, out0, out1);
+        PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out0, out1);
         ST_W8(out0, out1, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
         dst += (8 * dst_stride);
 
@@ -3467,59 +3960,90 @@ static void hevc_vt_uniwgt_4t_6w_msa(uint8_t *src,
     v16i8 src0, src1, src2, src3, src4, src5, src6, src7, src8, src9, src10;
     v16i8 src10_r, src32_r, src21_r, src43_r;
     v16i8 src54_r, src65_r, src76_r, src87_r, src98_r, src109_r;
-    v8i16 filt0, filt1;
-    v8i16 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
-    v8i16 filter_vec, weight_vec_h, offset_vec, denom_vec;
-    v4i32 weight_vec, rnd_vec;
+    v8i16 filter_vec, filt0, filt1;
+    v8i16 tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
+    v4i32 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    v4i32 dst8, dst9, dst10, dst11, dst12, dst13, dst14, dst15;
+    v4i32 weight_vec, rnd_vec, offset_vec;
+    v8i16 zero = { 0 };
 
     src -= src_stride;
 
-    weight = weight & 0x0000FFFF;
-
     weight_vec = __msa_fill_w(weight);
     rnd_vec = __msa_fill_w(rnd_val);
-
-    weight *= 128;
-    rnd_val -= 6;
-
-    weight_vec_h = __msa_fill_h(weight);
-    offset_vec = __msa_fill_h(offset);
-    denom_vec = __msa_fill_h(rnd_val);
-
-    weight_vec_h = __msa_srar_h(weight_vec_h, denom_vec);
-    offset_vec = __msa_adds_s_h(offset_vec, weight_vec_h);
+    offset_vec = __msa_fill_w(offset);
 
     filter_vec = LD_SH(filter);
-    SPLATI_H2_SH(filter_vec, 0, 1, filt0, filt1);
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W2_SH(filter_vec, 0, filt0, filt1);
 
     LD_SB3(src, src_stride, src0, src1, src2);
     src += (3 * src_stride);
     LD_SB8(src, src_stride, src3, src4, src5, src6, src7, src8, src9, src10);
-    XORI_B3_128_SB(src0, src1, src2);
-    XORI_B8_128_SB(src3, src4, src5, src6, src7, src8, src9, src10);
     ILVR_B2_SB(src1, src0, src2, src1, src10_r, src21_r);
     ILVR_B2_SB(src3, src2, src4, src3, src32_r, src43_r);
     ILVR_B2_SB(src5, src4, src6, src5, src54_r, src65_r);
     ILVR_B2_SB(src7, src6, src8, src7, src76_r, src87_r);
     ILVR_B2_SB(src9, src8, src10, src9, src98_r, src109_r);
-    dst0 = HEVC_FILT_4TAP_SH(src10_r, src32_r, filt0, filt1);
-    dst1 = HEVC_FILT_4TAP_SH(src21_r, src43_r, filt0, filt1);
-    dst2 = HEVC_FILT_4TAP_SH(src32_r, src54_r, filt0, filt1);
-    dst3 = HEVC_FILT_4TAP_SH(src43_r, src65_r, filt0, filt1);
-    dst4 = HEVC_FILT_4TAP_SH(src54_r, src76_r, filt0, filt1);
-    dst5 = HEVC_FILT_4TAP_SH(src65_r, src87_r, filt0, filt1);
-    dst6 = HEVC_FILT_4TAP_SH(src76_r, src98_r, filt0, filt1);
-    dst7 = HEVC_FILT_4TAP_SH(src87_r, src109_r, filt0, filt1);
-
-    HEVC_UNIW_RND_CLIP4_MAX_SATU_H(dst0, dst1, dst2, dst3,
-                                   weight_vec, offset_vec, rnd_vec,
-                                   dst0, dst1, dst2, dst3);
-    HEVC_UNIW_RND_CLIP4_MAX_SATU_H(dst4, dst5, dst6, dst7,
-                                   weight_vec, offset_vec, rnd_vec,
-                                   dst4, dst5, dst6, dst7);
-
-    PCKEV_B2_UB(dst1, dst0, dst3, dst2, out0, out1);
-    PCKEV_B2_UB(dst5, dst4, dst7, dst6, out2, out3);
+
+    ILVRL_B2_SH(zero, src10_r, tmp0, tmp1);
+    ILVRL_B2_SH(zero, src21_r, tmp2, tmp3);
+    ILVRL_B2_SH(zero, src32_r, tmp4, tmp5);
+    ILVRL_B2_SH(zero, src43_r, tmp6, tmp7);
+    dst0 = HEVC_FILT_4TAP_SW(tmp0, tmp4, filt0, filt1);
+    dst1 = HEVC_FILT_4TAP_SW(tmp1, tmp5, filt0, filt1);
+    dst2 = HEVC_FILT_4TAP_SW(tmp2, tmp6, filt0, filt1);
+    dst3 = HEVC_FILT_4TAP_SW(tmp3, tmp7, filt0, filt1);
+    ILVRL_B2_SH(zero, src54_r, tmp0, tmp1);
+    ILVRL_B2_SH(zero, src65_r, tmp2, tmp3);
+    dst4 = HEVC_FILT_4TAP_SW(tmp4, tmp0, filt0, filt1);
+    dst5 = HEVC_FILT_4TAP_SW(tmp5, tmp1, filt0, filt1);
+    dst6 = HEVC_FILT_4TAP_SW(tmp6, tmp2, filt0, filt1);
+    dst7 = HEVC_FILT_4TAP_SW(tmp7, tmp3, filt0, filt1);
+    ILVRL_B2_SH(zero, src76_r, tmp4, tmp5);
+    ILVRL_B2_SH(zero, src87_r, tmp6, tmp7);
+    dst8  = HEVC_FILT_4TAP_SW(tmp0, tmp4, filt0, filt1);
+    dst9  = HEVC_FILT_4TAP_SW(tmp1, tmp5, filt0, filt1);
+    dst10 = HEVC_FILT_4TAP_SW(tmp2, tmp6, filt0, filt1);
+    dst11 = HEVC_FILT_4TAP_SW(tmp3, tmp7, filt0, filt1);
+    ILVRL_B2_SH(zero, src98_r, tmp0, tmp1);
+    ILVRL_B2_SH(zero, src109_r, tmp2, tmp3);
+    dst12 = HEVC_FILT_4TAP_SW(tmp4, tmp0, filt0, filt1);
+    dst13 = HEVC_FILT_4TAP_SW(tmp5, tmp1, filt0, filt1);
+    dst14 = HEVC_FILT_4TAP_SW(tmp6, tmp2, filt0, filt1);
+    dst15 = HEVC_FILT_4TAP_SW(tmp7, tmp3, filt0, filt1);
+
+    MUL4(dst0, weight_vec, dst1, weight_vec, dst2, weight_vec, dst3,
+         weight_vec, dst0, dst1, dst2, dst3);
+    MUL4(dst4, weight_vec, dst5, weight_vec, dst6, weight_vec,
+         dst7, weight_vec, dst4, dst5, dst6, dst7);
+    SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
+    SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
+    ADD4(dst0, offset_vec, dst1, offset_vec, dst2, offset_vec, dst3,
+         offset_vec, dst0, dst1, dst2, dst3);
+    ADD4(dst4, offset_vec, dst5, offset_vec, dst6, offset_vec,
+         dst7, offset_vec, dst4, dst5, dst6, dst7);
+    CLIP_SW8_0_255(dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7);
+
+    MUL4(dst8, weight_vec, dst9, weight_vec, dst10, weight_vec, dst11,
+         weight_vec, dst8, dst9, dst10, dst11);
+    MUL4(dst12, weight_vec, dst13, weight_vec, dst14, weight_vec,
+         dst15, weight_vec, dst12, dst13, dst14, dst15);
+    SRAR_W4_SW(dst8, dst9, dst10, dst11, rnd_vec);
+    SRAR_W4_SW(dst12, dst13, dst14, dst15, rnd_vec);
+    ADD4(dst8, offset_vec, dst9, offset_vec, dst10, offset_vec, dst11,
+         offset_vec, dst8, dst9, dst10, dst11);
+    ADD4(dst12, offset_vec, dst13, offset_vec, dst14, offset_vec,
+         dst15, offset_vec, dst12, dst13, dst14, dst15);
+    CLIP_SW8_0_255(dst8, dst9, dst10, dst11, dst12, dst13, dst14, dst15);
+
+    PCKEV_H2_SH(dst1, dst0, dst3, dst2, tmp0, tmp1);
+    PCKEV_H2_SH(dst5, dst4, dst7, dst6, tmp2, tmp3);
+    PCKEV_H2_SH(dst9, dst8, dst11, dst10, tmp4, tmp5);
+    PCKEV_H2_SH(dst13, dst12, dst15, dst14, tmp6, tmp7);
+
+    PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out0, out1);
+    PCKEV_B2_UB(tmp5, tmp4, tmp7, tmp6, out2, out3);
     ST_W2(out0, 0, 2, dst, dst_stride);
     ST_H2(out0, 2, 6, dst + 4, dst_stride);
     ST_W2(out1, 0, 2, dst + 2 * dst_stride, dst_stride);
@@ -3543,42 +4067,44 @@ static void hevc_vt_uniwgt_4t_8x2_msa(uint8_t *src,
     v16u8 out;
     v16i8 src0, src1, src2, src3, src4;
     v16i8 src10_r, src32_r, src21_r, src43_r;
-    v8i16 dst0, dst1;
-    v8i16 filt0, filt1;
-    v8i16 filter_vec, weight_vec_h, offset_vec, denom_vec;
-    v4i32 weight_vec, rnd_vec;
+    v4i32 dst0, dst1, dst2, dst3;
+    v8i16 tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
+    v8i16 filter_vec, filt0, filt1;
+    v4i32 weight_vec, rnd_vec, offset_vec;
+    v8i16 zero = { 0 };
 
     src -= src_stride;
 
-    weight = weight & 0x0000FFFF;
-
     weight_vec = __msa_fill_w(weight);
     rnd_vec = __msa_fill_w(rnd_val);
-
-    weight *= 128;
-    rnd_val -= 6;
-
-    weight_vec_h = __msa_fill_h(weight);
-    offset_vec = __msa_fill_h(offset);
-    denom_vec = __msa_fill_h(rnd_val);
-
-    weight_vec_h = __msa_srar_h(weight_vec_h, denom_vec);
-    offset_vec = __msa_adds_s_h(offset_vec, weight_vec_h);
+    offset_vec = __msa_fill_w(offset);
 
     filter_vec = LD_SH(filter);
-    SPLATI_H2_SH(filter_vec, 0, 1, filt0, filt1);
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W2_SH(filter_vec, 0, filt0, filt1);
 
     LD_SB5(src, src_stride, src0, src1, src2, src3, src4);
-    XORI_B5_128_SB(src0, src1, src2, src3, src4);
     ILVR_B2_SB(src1, src0, src2, src1, src10_r, src21_r);
     ILVR_B2_SB(src3, src2, src4, src3, src32_r, src43_r);
-    dst0 = HEVC_FILT_4TAP_SH(src10_r, src32_r, filt0, filt1);
-    dst1 = HEVC_FILT_4TAP_SH(src21_r, src43_r, filt0, filt1);
 
-    HEVC_UNIW_RND_CLIP2_MAX_SATU_H(dst0, dst1, weight_vec, offset_vec, rnd_vec,
-                                   dst0, dst1);
+    ILVRL_B2_SH(zero, src10_r, tmp0, tmp1);
+    ILVRL_B2_SH(zero, src21_r, tmp2, tmp3);
+    ILVRL_B2_SH(zero, src32_r, tmp4, tmp5);
+    ILVRL_B2_SH(zero, src43_r, tmp6, tmp7);
+
+    dst0 = HEVC_FILT_4TAP_SW(tmp0, tmp4, filt0, filt1);
+    dst1 = HEVC_FILT_4TAP_SW(tmp1, tmp5, filt0, filt1);
+    dst2 = HEVC_FILT_4TAP_SW(tmp2, tmp6, filt0, filt1);
+    dst3 = HEVC_FILT_4TAP_SW(tmp3, tmp7, filt0, filt1);
+    MUL4(dst0, weight_vec, dst1, weight_vec, dst2, weight_vec, dst3,
+         weight_vec, dst0, dst1, dst2, dst3);
+    SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
+    ADD4(dst0, offset_vec, dst1, offset_vec, dst2, offset_vec, dst3,
+         offset_vec, dst0, dst1, dst2, dst3);
+    CLIP_SW4_0_255(dst0, dst1, dst2, dst3);
+    PCKEV_H2_SH(dst1, dst0, dst3, dst2, tmp0, tmp1);
 
-    out = (v16u8) __msa_pckev_b((v16i8) dst1, (v16i8) dst0);
+    out = (v16u8) __msa_pckev_b((v16i8) tmp1, (v16i8) tmp0);
     ST_D2(out, 0, 1, dst, dst_stride);
 }
 
@@ -3595,45 +4121,56 @@ static void hevc_vt_uniwgt_4t_8x4_msa(uint8_t *src,
     v16i8 src0, src1, src2, src3, src4;
     v16i8 src10_r, src32_r, src21_r, src43_r;
     v16i8 src5, src6, src54_r, src65_r;
-    v8i16 filt0, filt1;
-    v8i16 dst0, dst1, dst2, dst3;
-    v8i16 filter_vec, weight_vec_h, offset_vec, denom_vec;
-    v4i32 weight_vec, rnd_vec;
+    v8i16 filter_vec, filt0, filt1;
+    v4i32 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    v8i16 tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
+    v4i32 weight_vec, rnd_vec, offset_vec;
+    v8i16 zero = { 0 };
 
     src -= src_stride;
 
-    weight = weight & 0x0000FFFF;
-
     weight_vec = __msa_fill_w(weight);
     rnd_vec = __msa_fill_w(rnd_val);
-
-    weight *= 128;
-    rnd_val -= 6;
-
-    weight_vec_h = __msa_fill_h(weight);
-    offset_vec = __msa_fill_h(offset);
-    denom_vec = __msa_fill_h(rnd_val);
-
-    weight_vec_h = __msa_srar_h(weight_vec_h, denom_vec);
-    offset_vec = __msa_adds_s_h(offset_vec, weight_vec_h);
+    offset_vec = __msa_fill_w(offset);
 
     filter_vec = LD_SH(filter);
-    SPLATI_H2_SH(filter_vec, 0, 1, filt0, filt1);
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W2_SH(filter_vec, 0, filt0, filt1);
 
     LD_SB7(src, src_stride, src0, src1, src2, src3, src4, src5, src6);
-    src += (3 * src_stride);
-    XORI_B7_128_SB(src0, src1, src2, src3, src4, src5, src6);
     ILVR_B2_SB(src1, src0, src2, src1, src10_r, src21_r);
     ILVR_B2_SB(src3, src2, src4, src3, src32_r, src43_r);
     ILVR_B2_SB(src5, src4, src6, src5, src54_r, src65_r);
-    dst0 = HEVC_FILT_4TAP_SH(src10_r, src32_r, filt0, filt1);
-    dst1 = HEVC_FILT_4TAP_SH(src21_r, src43_r, filt0, filt1);
-    dst2 = HEVC_FILT_4TAP_SH(src32_r, src54_r, filt0, filt1);
-    dst3 = HEVC_FILT_4TAP_SH(src43_r, src65_r, filt0, filt1);
-    HEVC_UNIW_RND_CLIP4_MAX_SATU_H(dst0, dst1, dst2, dst3, weight_vec,
-                                   offset_vec, rnd_vec, dst0, dst1, dst2,
-                                   dst3);
-    PCKEV_B2_UB(dst1, dst0, dst3, dst2, out0, out1);
+
+    ILVRL_B2_SH(zero, src10_r, tmp0, tmp1);
+    ILVRL_B2_SH(zero, src21_r, tmp2, tmp3);
+    ILVRL_B2_SH(zero, src32_r, tmp4, tmp5);
+    ILVRL_B2_SH(zero, src43_r, tmp6, tmp7);
+    dst0 = HEVC_FILT_4TAP_SW(tmp0, tmp4, filt0, filt1);
+    dst1 = HEVC_FILT_4TAP_SW(tmp1, tmp5, filt0, filt1);
+    dst2 = HEVC_FILT_4TAP_SW(tmp2, tmp6, filt0, filt1);
+    dst3 = HEVC_FILT_4TAP_SW(tmp3, tmp7, filt0, filt1);
+    ILVRL_B2_SH(zero, src54_r, tmp0, tmp1);
+    ILVRL_B2_SH(zero, src65_r, tmp2, tmp3);
+    dst4 = HEVC_FILT_4TAP_SW(tmp4, tmp0, filt0, filt1);
+    dst5 = HEVC_FILT_4TAP_SW(tmp5, tmp1, filt0, filt1);
+    dst6 = HEVC_FILT_4TAP_SW(tmp6, tmp2, filt0, filt1);
+    dst7 = HEVC_FILT_4TAP_SW(tmp7, tmp3, filt0, filt1);
+
+    MUL4(dst0, weight_vec, dst1, weight_vec, dst2, weight_vec, dst3,
+         weight_vec, dst0, dst1, dst2, dst3);
+    MUL4(dst4, weight_vec, dst5, weight_vec, dst6, weight_vec,
+         dst7, weight_vec, dst4, dst5, dst6, dst7);
+    SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
+    SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
+    ADD4(dst0, offset_vec, dst1, offset_vec, dst2, offset_vec, dst3,
+         offset_vec, dst0, dst1, dst2, dst3);
+    ADD4(dst4, offset_vec, dst5, offset_vec, dst6, offset_vec,
+         dst7, offset_vec, dst4, dst5, dst6, dst7);
+    CLIP_SW8_0_255(dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7);
+    PCKEV_H2_SH(dst1, dst0, dst3, dst2, tmp0, tmp1);
+    PCKEV_H2_SH(dst5, dst4, dst7, dst6, tmp2, tmp3);
+    PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out0, out1);
     ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
 }
 
@@ -3650,52 +4187,74 @@ static void hevc_vt_uniwgt_4t_8x6_msa(uint8_t *src,
     v16i8 src0, src1, src2, src3, src4, src5, src6, src7, src8;
     v16i8 src10_r, src32_r, src54_r, src76_r;
     v16i8 src21_r, src43_r, src65_r, src87_r;
-    v8i16 dst0, dst1, dst2, dst3, dst4, dst5;
-    v8i16 filt0, filt1;
-    v8i16 filter_vec, weight_vec_h, offset_vec, denom_vec;
-    v4i32 weight_vec, rnd_vec;
+    v4i32 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    v4i32 dst8, dst9, dst10, dst11;
+    v8i16 tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
+    v8i16 filter_vec, filt0, filt1;
+    v4i32 weight_vec, rnd_vec, offset_vec;
+    v8i16 zero = { 0 };
 
     src -= src_stride;
-
-    weight = weight & 0x0000FFFF;
-
     weight_vec = __msa_fill_w(weight);
     rnd_vec = __msa_fill_w(rnd_val);
-
-    weight *= 128;
-    rnd_val -= 6;
-
-    weight_vec_h = __msa_fill_h(weight);
-    offset_vec = __msa_fill_h(offset);
-    denom_vec = __msa_fill_h(rnd_val);
-
-    weight_vec_h = __msa_srar_h(weight_vec_h, denom_vec);
-    offset_vec = __msa_adds_s_h(offset_vec, weight_vec_h);
+    offset_vec = __msa_fill_w(offset);
 
     filter_vec = LD_SH(filter);
-    SPLATI_H2_SH(filter_vec, 0, 1, filt0, filt1);
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W2_SH(filter_vec, 0, filt0, filt1);
 
     LD_SB3(src, src_stride, src0, src1, src2);
     src += (3 * src_stride);
     LD_SB6(src, src_stride, src3, src4, src5, src6, src7, src8);
 
-    XORI_B3_128_SB(src0, src1, src2);
-    XORI_B6_128_SB(src3, src4, src5, src6, src7, src8);
     ILVR_B4_SB(src1, src0, src2, src1, src3, src2, src4, src3, src10_r, src21_r,
                src32_r, src43_r);
     ILVR_B4_SB(src5, src4, src6, src5, src7, src6, src8, src7, src54_r, src65_r,
                src76_r, src87_r);
-    dst0 = HEVC_FILT_4TAP_SH(src10_r, src32_r, filt0, filt1);
-    dst1 = HEVC_FILT_4TAP_SH(src21_r, src43_r, filt0, filt1);
-    dst2 = HEVC_FILT_4TAP_SH(src32_r, src54_r, filt0, filt1);
-    dst3 = HEVC_FILT_4TAP_SH(src43_r, src65_r, filt0, filt1);
-    dst4 = HEVC_FILT_4TAP_SH(src54_r, src76_r, filt0, filt1);
-    dst5 = HEVC_FILT_4TAP_SH(src65_r, src87_r, filt0, filt1);
-    HEVC_UNIW_RND_CLIP4_MAX_SATU_H(dst0, dst1, dst2, dst3, weight_vec,
-                                   offset_vec, rnd_vec, dst0, dst1, dst2, dst3);
-    HEVC_UNIW_RND_CLIP2_MAX_SATU_H(dst4, dst5, weight_vec, offset_vec, rnd_vec,
-                                   dst4, dst5);
-    PCKEV_B3_UB(dst1, dst0, dst3, dst2, dst5, dst4, out0, out1, out2);
+
+    ILVRL_B2_SH(zero, src10_r, tmp0, tmp1);
+    ILVRL_B2_SH(zero, src21_r, tmp2, tmp3);
+    ILVRL_B2_SH(zero, src32_r, tmp4, tmp5);
+    ILVRL_B2_SH(zero, src43_r, tmp6, tmp7);
+    dst0 = HEVC_FILT_4TAP_SW(tmp0, tmp4, filt0, filt1);
+    dst1 = HEVC_FILT_4TAP_SW(tmp1, tmp5, filt0, filt1);
+    dst2 = HEVC_FILT_4TAP_SW(tmp2, tmp6, filt0, filt1);
+    dst3 = HEVC_FILT_4TAP_SW(tmp3, tmp7, filt0, filt1);
+    ILVRL_B2_SH(zero, src54_r, tmp0, tmp1);
+    ILVRL_B2_SH(zero, src65_r, tmp2, tmp3);
+    dst4 = HEVC_FILT_4TAP_SW(tmp4, tmp0, filt0, filt1);
+    dst5 = HEVC_FILT_4TAP_SW(tmp5, tmp1, filt0, filt1);
+    dst6 = HEVC_FILT_4TAP_SW(tmp6, tmp2, filt0, filt1);
+    dst7 = HEVC_FILT_4TAP_SW(tmp7, tmp3, filt0, filt1);
+    ILVRL_B2_SH(zero, src76_r, tmp4, tmp5);
+    ILVRL_B2_SH(zero, src87_r, tmp6, tmp7);
+    dst8  = HEVC_FILT_4TAP_SW(tmp0, tmp4, filt0, filt1);
+    dst9  = HEVC_FILT_4TAP_SW(tmp1, tmp5, filt0, filt1);
+    dst10 = HEVC_FILT_4TAP_SW(tmp2, tmp6, filt0, filt1);
+    dst11 = HEVC_FILT_4TAP_SW(tmp3, tmp7, filt0, filt1);
+
+    MUL4(dst0, weight_vec, dst1, weight_vec, dst2, weight_vec, dst3,
+         weight_vec, dst0, dst1, dst2, dst3);
+    MUL4(dst4, weight_vec, dst5, weight_vec, dst6, weight_vec,
+         dst7, weight_vec, dst4, dst5, dst6, dst7);
+    SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
+    SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
+    ADD4(dst0, offset_vec, dst1, offset_vec, dst2, offset_vec, dst3,
+         offset_vec, dst0, dst1, dst2, dst3);
+    ADD4(dst4, offset_vec, dst5, offset_vec, dst6, offset_vec,
+         dst7, offset_vec, dst4, dst5, dst6, dst7);
+    CLIP_SW8_0_255(dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7);
+    MUL4(dst8, weight_vec, dst9, weight_vec, dst10, weight_vec, dst11,
+         weight_vec, dst8, dst9, dst10, dst11);
+    SRAR_W4_SW(dst8, dst9, dst10, dst11, rnd_vec);
+    ADD4(dst8, offset_vec, dst9, offset_vec, dst10, offset_vec, dst11,
+         offset_vec, dst8, dst9, dst10, dst11);
+    CLIP_SW4_0_255(dst8, dst9, dst10, dst11);
+
+    PCKEV_H2_SH(dst1, dst0, dst3, dst2, tmp0, tmp1);
+    PCKEV_H2_SH(dst5, dst4, dst7, dst6, tmp2, tmp3);
+    PCKEV_H2_SH(dst9, dst8, dst11, dst10, tmp4, tmp5);
+    PCKEV_B3_UB(tmp1, tmp0, tmp3, tmp2, tmp5, tmp4, out0, out1, out2);
     ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
     ST_D2(out2, 0, 1, dst + 4 * dst_stride, dst_stride);
 }
@@ -3715,61 +4274,94 @@ static void hevc_vt_uniwgt_4t_8x8mult_msa(uint8_t *src,
     v16i8 src0, src1, src2, src3, src4, src5, src6, src7, src8, src9, src10;
     v16i8 src10_r, src32_r, src21_r, src43_r;
     v16i8 src54_r, src65_r, src76_r, src87_r, src98_r, src109_r;
-    v8i16 filt0, filt1;
-    v8i16 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
-    v8i16 filter_vec, weight_vec_h, offset_vec, denom_vec;
-    v4i32 weight_vec, rnd_vec;
+    v8i16 filter_vec, filt0, filt1;
+    v8i16 tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
+    v4i32 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    v4i32 dst8, dst9, dst10, dst11, dst12, dst13, dst14, dst15;
+    v4i32 weight_vec, rnd_vec, offset_vec;
+    v8i16 zero = { 0 };
 
     src -= src_stride;
 
-    weight = weight & 0x0000FFFF;
-
     weight_vec = __msa_fill_w(weight);
     rnd_vec = __msa_fill_w(rnd_val);
-
-    weight *= 128;
-    rnd_val -= 6;
-
-    weight_vec_h = __msa_fill_h(weight);
-    offset_vec = __msa_fill_h(offset);
-    denom_vec = __msa_fill_h(rnd_val);
-
-    weight_vec_h = __msa_srar_h(weight_vec_h, denom_vec);
-    offset_vec = __msa_adds_s_h(offset_vec, weight_vec_h);
+    offset_vec = __msa_fill_w(offset);
 
     filter_vec = LD_SH(filter);
-    SPLATI_H2_SH(filter_vec, 0, 1, filt0, filt1);
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W2_SH(filter_vec, 0, filt0, filt1);
 
     LD_SB3(src, src_stride, src0, src1, src2);
     src += (3 * src_stride);
-    XORI_B3_128_SB(src0, src1, src2);
     ILVR_B2_SB(src1, src0, src2, src1, src10_r, src21_r);
 
     for (loop_cnt = (height >> 3); loop_cnt--;) {
         LD_SB8(src, src_stride,
                src3, src4, src5, src6, src7, src8, src9, src10);
         src += (8 * src_stride);
-        XORI_B8_128_SB(src3, src4, src5, src6, src7, src8, src9, src10);
         ILVR_B2_SB(src3, src2, src4, src3, src32_r, src43_r);
         ILVR_B2_SB(src5, src4, src6, src5, src54_r, src65_r);
         ILVR_B2_SB(src7, src6, src8, src7, src76_r, src87_r);
         ILVR_B2_SB(src9, src8, src10, src9, src98_r, src109_r);
-        dst0 = HEVC_FILT_4TAP_SH(src10_r, src32_r, filt0, filt1);
-        dst1 = HEVC_FILT_4TAP_SH(src21_r, src43_r, filt0, filt1);
-        dst2 = HEVC_FILT_4TAP_SH(src32_r, src54_r, filt0, filt1);
-        dst3 = HEVC_FILT_4TAP_SH(src43_r, src65_r, filt0, filt1);
-        dst4 = HEVC_FILT_4TAP_SH(src54_r, src76_r, filt0, filt1);
-        dst5 = HEVC_FILT_4TAP_SH(src65_r, src87_r, filt0, filt1);
-        dst6 = HEVC_FILT_4TAP_SH(src76_r, src98_r, filt0, filt1);
-        dst7 = HEVC_FILT_4TAP_SH(src87_r, src109_r, filt0, filt1);
-        HEVC_UNIW_RND_CLIP4_MAX_SATU_H(dst0, dst1, dst2, dst3, weight_vec,
-                                       offset_vec, rnd_vec, dst0, dst1, dst2,
-                                       dst3);
-        HEVC_UNIW_RND_CLIP4_MAX_SATU_H(dst4, dst5, dst6, dst7, weight_vec,
-                                       offset_vec, rnd_vec, dst4, dst5, dst6,
-                                       dst7);
-        PCKEV_B2_UB(dst1, dst0, dst3, dst2, out0, out1);
-        PCKEV_B2_UB(dst5, dst4, dst7, dst6, out2, out3);
+
+        ILVRL_B2_SH(zero, src10_r, tmp0, tmp1);
+        ILVRL_B2_SH(zero, src21_r, tmp2, tmp3);
+        ILVRL_B2_SH(zero, src32_r, tmp4, tmp5);
+        ILVRL_B2_SH(zero, src43_r, tmp6, tmp7);
+        dst0 = HEVC_FILT_4TAP_SW(tmp0, tmp4, filt0, filt1);
+        dst1 = HEVC_FILT_4TAP_SW(tmp1, tmp5, filt0, filt1);
+        dst2 = HEVC_FILT_4TAP_SW(tmp2, tmp6, filt0, filt1);
+        dst3 = HEVC_FILT_4TAP_SW(tmp3, tmp7, filt0, filt1);
+        ILVRL_B2_SH(zero, src54_r, tmp0, tmp1);
+        ILVRL_B2_SH(zero, src65_r, tmp2, tmp3);
+        dst4 = HEVC_FILT_4TAP_SW(tmp4, tmp0, filt0, filt1);
+        dst5 = HEVC_FILT_4TAP_SW(tmp5, tmp1, filt0, filt1);
+        dst6 = HEVC_FILT_4TAP_SW(tmp6, tmp2, filt0, filt1);
+        dst7 = HEVC_FILT_4TAP_SW(tmp7, tmp3, filt0, filt1);
+        ILVRL_B2_SH(zero, src76_r, tmp4, tmp5);
+        ILVRL_B2_SH(zero, src87_r, tmp6, tmp7);
+        dst8  = HEVC_FILT_4TAP_SW(tmp0, tmp4, filt0, filt1);
+        dst9  = HEVC_FILT_4TAP_SW(tmp1, tmp5, filt0, filt1);
+        dst10 = HEVC_FILT_4TAP_SW(tmp2, tmp6, filt0, filt1);
+        dst11 = HEVC_FILT_4TAP_SW(tmp3, tmp7, filt0, filt1);
+        ILVRL_B2_SH(zero, src98_r, tmp0, tmp1);
+        ILVRL_B2_SH(zero, src109_r, tmp2, tmp3);
+        dst12 = HEVC_FILT_4TAP_SW(tmp4, tmp0, filt0, filt1);
+        dst13 = HEVC_FILT_4TAP_SW(tmp5, tmp1, filt0, filt1);
+        dst14 = HEVC_FILT_4TAP_SW(tmp6, tmp2, filt0, filt1);
+        dst15 = HEVC_FILT_4TAP_SW(tmp7, tmp3, filt0, filt1);
+
+        MUL4(dst0, weight_vec, dst1, weight_vec, dst2, weight_vec, dst3,
+             weight_vec, dst0, dst1, dst2, dst3);
+        MUL4(dst4, weight_vec, dst5, weight_vec, dst6, weight_vec,
+             dst7, weight_vec, dst4, dst5, dst6, dst7);
+        SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
+        SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
+        ADD4(dst0, offset_vec, dst1, offset_vec, dst2, offset_vec, dst3,
+             offset_vec, dst0, dst1, dst2, dst3);
+        ADD4(dst4, offset_vec, dst5, offset_vec, dst6, offset_vec,
+             dst7, offset_vec, dst4, dst5, dst6, dst7);
+        CLIP_SW8_0_255(dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7);
+
+        MUL4(dst8, weight_vec, dst9, weight_vec, dst10, weight_vec, dst11,
+             weight_vec, dst8, dst9, dst10, dst11);
+        MUL4(dst12, weight_vec, dst13, weight_vec, dst14, weight_vec,
+             dst15, weight_vec, dst12, dst13, dst14, dst15);
+        SRAR_W4_SW(dst8, dst9, dst10, dst11, rnd_vec);
+        SRAR_W4_SW(dst12, dst13, dst14, dst15, rnd_vec);
+        ADD4(dst8, offset_vec, dst9, offset_vec, dst10, offset_vec, dst11,
+             offset_vec, dst8, dst9, dst10, dst11);
+        ADD4(dst12, offset_vec, dst13, offset_vec, dst14, offset_vec,
+             dst15, offset_vec, dst12, dst13, dst14, dst15);
+        CLIP_SW8_0_255(dst8, dst9, dst10, dst11, dst12, dst13, dst14, dst15);
+
+        PCKEV_H2_SH(dst1, dst0, dst3, dst2, tmp0, tmp1);
+        PCKEV_H2_SH(dst5, dst4, dst7, dst6, tmp2, tmp3);
+        PCKEV_H2_SH(dst9, dst8, dst11, dst10, tmp4, tmp5);
+        PCKEV_H2_SH(dst13, dst12, dst15, dst14, tmp6, tmp7);
+
+        PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out0, out1);
+        PCKEV_B2_UB(tmp5, tmp4, tmp7, tmp6, out2, out3);
         ST_D8(out0, out1, out2, out3, 0, 1, 0, 1, 0, 1, 0, 1, dst, dst_stride);
         dst += (8 * dst_stride);
 
@@ -3823,34 +4415,25 @@ static void hevc_vt_uniwgt_4t_12w_msa(uint8_t *src,
     v16i8 src2110, src4332;
     v16i8 src54_r, src76_r, src98_r, src65_r, src87_r, src109_r;
     v16i8 src76_l, src98_l, src87_l, src109_l, src6554, src8776, src10998;
-    v8i16 filt0, filt1;
-    v8i16 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7, dst8;
-    v8i16 dst9, dst10, dst11, filter_vec, weight_vec_h, offset_vec, denom_vec;
-    v4i32 weight_vec, rnd_vec;
+    v8i16 filter_vec, filt0, filt1;
+    v4i32 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7, dst8;
+    v4i32 dst9, dst10, dst11;
+    v8i16 tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7, tmp8, tmp9;
+    v4i32 weight_vec, rnd_vec, offset_vec;
+    v8i16 zero = { 0 };
 
     src -= (1 * src_stride);
 
-    weight = weight & 0x0000FFFF;
-
     weight_vec = __msa_fill_w(weight);
     rnd_vec = __msa_fill_w(rnd_val);
-
-    weight *= 128;
-    rnd_val -= 6;
-
-    weight_vec_h = __msa_fill_h(weight);
-    offset_vec = __msa_fill_h(offset);
-    denom_vec = __msa_fill_h(rnd_val);
-
-    weight_vec_h = __msa_srar_h(weight_vec_h, denom_vec);
-    offset_vec = __msa_adds_s_h(offset_vec, weight_vec_h);
+    offset_vec = __msa_fill_w(offset);
 
     filter_vec = LD_SH(filter);
-    SPLATI_H2_SH(filter_vec, 0, 1, filt0, filt1);
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W2_SH(filter_vec, 0, filt0, filt1);
 
     LD_SB3(src, src_stride, src0, src1, src2);
     src += (3 * src_stride);
-    XORI_B3_128_SB(src0, src1, src2);
     ILVR_B2_SB(src1, src0, src2, src1, src10_r, src21_r);
     ILVL_B2_SB(src1, src0, src2, src1, src10_l, src21_l);
     src2110 = (v16i8) __msa_ilvr_d((v2i64) src21_l, (v2i64) src10_l);
@@ -3858,24 +4441,58 @@ static void hevc_vt_uniwgt_4t_12w_msa(uint8_t *src,
     for (loop_cnt = 2; loop_cnt--;) {
         LD_SB8(src, src_stride, src3, src4, src5, src6, src7, src8, src9, src10);
         src += (8 * src_stride);
-        XORI_B8_128_SB(src3, src4, src5, src6, src7, src8, src9, src10);
         ILVRL_B2_SB(src3, src2, src32_r, src32_l);
         ILVRL_B2_SB(src4, src3, src43_r, src43_l);
         ILVRL_B2_SB(src5, src4, src54_r, src54_l);
         ILVRL_B2_SB(src6, src5, src65_r, src65_l);
         src4332 = (v16i8) __msa_ilvr_d((v2i64) src43_l, (v2i64) src32_l);
         src6554 = (v16i8) __msa_ilvr_d((v2i64) src65_l, (v2i64) src54_l);
-        dst0 = HEVC_FILT_4TAP_SH(src10_r, src32_r, filt0, filt1);
-        dst1 = HEVC_FILT_4TAP_SH(src21_r, src43_r, filt0, filt1);
-        dst2 = HEVC_FILT_4TAP_SH(src32_r, src54_r, filt0, filt1);
-        dst3 = HEVC_FILT_4TAP_SH(src43_r, src65_r, filt0, filt1);
-        dst4 = HEVC_FILT_4TAP_SH(src2110, src4332, filt0, filt1);
-        dst5 = HEVC_FILT_4TAP_SH(src4332, src6554, filt0, filt1);
-        HEVC_UNIW_RND_CLIP4_MAX_SATU_H(dst0, dst1, dst2, dst3, weight_vec,
-                                       offset_vec, rnd_vec, dst0, dst1, dst2,
-                                       dst3);
-        HEVC_UNIW_RND_CLIP2_MAX_SATU_H(dst4, dst5, weight_vec, offset_vec,
-                                       rnd_vec, dst4, dst5);
+
+        ILVRL_B2_SH(zero, src10_r, tmp0, tmp1);
+        ILVRL_B2_SH(zero, src21_r, tmp2, tmp3);
+        ILVRL_B2_SH(zero, src32_r, tmp4, tmp5);
+        ILVRL_B2_SH(zero, src43_r, tmp6, tmp7);
+        dst0 = HEVC_FILT_4TAP_SW(tmp0, tmp4, filt0, filt1);
+        dst1 = HEVC_FILT_4TAP_SW(tmp1, tmp5, filt0, filt1);
+        dst2 = HEVC_FILT_4TAP_SW(tmp2, tmp6, filt0, filt1);
+        dst3 = HEVC_FILT_4TAP_SW(tmp3, tmp7, filt0, filt1);
+        ILVRL_B2_SH(zero, src54_r, tmp0, tmp1);
+        ILVRL_B2_SH(zero, src65_r, tmp2, tmp3);
+        dst4 = HEVC_FILT_4TAP_SW(tmp4, tmp0, filt0, filt1);
+        dst5 = HEVC_FILT_4TAP_SW(tmp5, tmp1, filt0, filt1);
+        dst6 = HEVC_FILT_4TAP_SW(tmp6, tmp2, filt0, filt1);
+        dst7 = HEVC_FILT_4TAP_SW(tmp7, tmp3, filt0, filt1);
+        ILVRL_B2_SH(zero, src2110, tmp4, tmp5);
+        ILVRL_B2_SH(zero, src4332, tmp6, tmp7);
+        ILVRL_B2_SH(zero, src6554, tmp8, tmp9);
+        dst8  = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+        dst9  = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+        dst10 = HEVC_FILT_4TAP_SW(tmp6, tmp8, filt0, filt1);
+        dst11 = HEVC_FILT_4TAP_SW(tmp7, tmp9, filt0, filt1);
+
+        MUL4(dst0, weight_vec, dst1, weight_vec, dst2, weight_vec, dst3,
+             weight_vec, dst0, dst1, dst2, dst3);
+        MUL4(dst4, weight_vec, dst5, weight_vec, dst6, weight_vec,
+             dst7, weight_vec, dst4, dst5, dst6, dst7);
+        SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
+        SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
+        ADD4(dst0, offset_vec, dst1, offset_vec, dst2, offset_vec, dst3,
+             offset_vec, dst0, dst1, dst2, dst3);
+        ADD4(dst4, offset_vec, dst5, offset_vec, dst6, offset_vec,
+             dst7, offset_vec, dst4, dst5, dst6, dst7);
+        CLIP_SW8_0_255(dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7);
+
+        MUL4(dst8, weight_vec, dst9, weight_vec, dst10, weight_vec, dst11,
+             weight_vec, dst8, dst9, dst10, dst11);
+        SRAR_W4_SW(dst8, dst9, dst10, dst11, rnd_vec);
+        ADD4(dst8, offset_vec, dst9, offset_vec, dst10, offset_vec, dst11,
+             offset_vec, dst8, dst9, dst10, dst11);
+        CLIP_SW4_0_255(dst8, dst9, dst10, dst11);
+
+        PCKEV_H2_SH(dst1, dst0, dst3, dst2, dst0, dst1);
+        PCKEV_H2_SH(dst5, dst4, dst7, dst6, dst2, dst3);
+        PCKEV_H2_SH(dst9, dst8, dst11, dst10, dst4, dst5);
+
         PCKEV_B3_UB(dst1, dst0, dst3, dst2, dst5, dst4, out0, out1, out2);
         ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
         ST_W4(out2, 0, 1, 2, 3, dst + 8, dst_stride);
@@ -3887,18 +4504,51 @@ static void hevc_vt_uniwgt_4t_12w_msa(uint8_t *src,
         ILVRL_B2_SB(src10, src9, src109_r, src109_l);
         src8776 = (v16i8) __msa_ilvr_d((v2i64) src87_l, (v2i64) src76_l);
         src10998 = (v16i8) __msa_ilvr_d((v2i64) src109_l, (v2i64) src98_l);
-        dst6 = HEVC_FILT_4TAP_SH(src54_r, src76_r, filt0, filt1);
-        dst7 = HEVC_FILT_4TAP_SH(src65_r, src87_r, filt0, filt1);
-        dst8 = HEVC_FILT_4TAP_SH(src76_r, src98_r, filt0, filt1);
-        dst9 = HEVC_FILT_4TAP_SH(src87_r, src109_r, filt0, filt1);
-        dst10 = HEVC_FILT_4TAP_SH(src6554, src8776, filt0, filt1);
-        dst11 = HEVC_FILT_4TAP_SH(src8776, src10998, filt0, filt1);
-        HEVC_UNIW_RND_CLIP4_MAX_SATU_H(dst6, dst7, dst8, dst9, weight_vec,
-                                       offset_vec, rnd_vec, dst6, dst7, dst8,
-                                       dst9);
-        HEVC_UNIW_RND_CLIP2_MAX_SATU_H(dst10, dst11, weight_vec, offset_vec,
-                                       rnd_vec, dst10, dst11);
-        PCKEV_B3_UB(dst7, dst6, dst9, dst8, dst11, dst10, out3, out4, out5);
+
+        ILVRL_B2_SH(zero, src76_r, tmp4, tmp5);
+        ILVRL_B2_SH(zero, src87_r, tmp6, tmp7);
+        dst0 = HEVC_FILT_4TAP_SW(tmp0, tmp4, filt0, filt1);
+        dst1 = HEVC_FILT_4TAP_SW(tmp1, tmp5, filt0, filt1);
+        dst2 = HEVC_FILT_4TAP_SW(tmp2, tmp6, filt0, filt1);
+        dst3 = HEVC_FILT_4TAP_SW(tmp3, tmp7, filt0, filt1);
+        ILVRL_B2_SH(zero, src98_r, tmp0, tmp1);
+        ILVRL_B2_SH(zero, src109_r, tmp2, tmp3);
+        dst4 = HEVC_FILT_4TAP_SW(tmp4, tmp0, filt0, filt1);
+        dst5 = HEVC_FILT_4TAP_SW(tmp5, tmp1, filt0, filt1);
+        dst6 = HEVC_FILT_4TAP_SW(tmp6, tmp2, filt0, filt1);
+        dst7 = HEVC_FILT_4TAP_SW(tmp7, tmp3, filt0, filt1);
+        ILVRL_B2_SH(zero, src8776, tmp4, tmp5);
+        ILVRL_B2_SH(zero, src10998, tmp6, tmp7);
+        ILVRL_B2_SH(zero, src6554, tmp8, tmp9);
+        dst8  = HEVC_FILT_4TAP_SW(tmp8, tmp4, filt0, filt1);
+        dst9  = HEVC_FILT_4TAP_SW(tmp9, tmp5, filt0, filt1);
+        dst10 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+        dst11 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+
+        MUL4(dst0, weight_vec, dst1, weight_vec, dst2, weight_vec, dst3,
+             weight_vec, dst0, dst1, dst2, dst3);
+        MUL4(dst4, weight_vec, dst5, weight_vec, dst6, weight_vec,
+             dst7, weight_vec, dst4, dst5, dst6, dst7);
+        SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
+        SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
+        ADD4(dst0, offset_vec, dst1, offset_vec, dst2, offset_vec, dst3,
+             offset_vec, dst0, dst1, dst2, dst3);
+        ADD4(dst4, offset_vec, dst5, offset_vec, dst6, offset_vec,
+             dst7, offset_vec, dst4, dst5, dst6, dst7);
+        CLIP_SW8_0_255(dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7);
+
+        MUL4(dst8, weight_vec, dst9, weight_vec, dst10, weight_vec, dst11,
+             weight_vec, dst8, dst9, dst10, dst11);
+        SRAR_W4_SW(dst8, dst9, dst10, dst11, rnd_vec);
+        ADD4(dst8, offset_vec, dst9, offset_vec, dst10, offset_vec, dst11,
+             offset_vec, dst8, dst9, dst10, dst11);
+        CLIP_SW4_0_255(dst8, dst9, dst10, dst11);
+
+        PCKEV_H2_SH(dst1, dst0, dst3, dst2, dst0, dst1);
+        PCKEV_H2_SH(dst5, dst4, dst7, dst6, dst2, dst3);
+        PCKEV_H2_SH(dst9, dst8, dst11, dst10, dst4, dst5);
+
+        PCKEV_B3_UB(dst1, dst0, dst3, dst2, dst5, dst4, out3, out4, out5);
         ST_D4(out3, out4, 0, 1, 0, 1, dst, dst_stride);
         ST_W4(out5, 0, 1, 2, 3, dst + 8, dst_stride);
         dst += (4 * dst_stride);
@@ -3926,60 +4576,94 @@ static void hevc_vt_uniwgt_4t_16w_msa(uint8_t *src,
     v16i8 src10_r, src32_r, src21_r, src43_r;
     v16i8 src10_l, src32_l, src21_l, src43_l;
     v16i8 src54_r, src54_l, src65_r, src65_l, src6;
-    v8i16 filt0, filt1;
-    v8i16 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
-    v8i16 filter_vec, weight_vec_h, offset_vec, denom_vec;
-    v4i32 weight_vec, rnd_vec;
+    v8i16 filter_vec, filt0, filt1;
+    v8i16 tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
+    v4i32 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    v4i32 dst8, dst9, dst10, dst11, dst12, dst13, dst14, dst15;
+    v4i32 weight_vec, rnd_vec, offset_vec;
+    v8i16 zero = { 0 };
 
     src -= src_stride;
 
-    weight = weight & 0x0000FFFF;
-
     weight_vec = __msa_fill_w(weight);
     rnd_vec = __msa_fill_w(rnd_val);
-
-    weight *= 128;
-    rnd_val -= 6;
-
-    weight_vec_h = __msa_fill_h(weight);
-    offset_vec = __msa_fill_h(offset);
-    denom_vec = __msa_fill_h(rnd_val);
-
-    weight_vec_h = __msa_srar_h(weight_vec_h, denom_vec);
-    offset_vec = __msa_adds_s_h(offset_vec, weight_vec_h);
+    offset_vec = __msa_fill_w(offset);
 
     filter_vec = LD_SH(filter);
-    SPLATI_H2_SH(filter_vec, 0, 1, filt0, filt1);
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W2_SH(filter_vec, 0, filt0, filt1);
 
     LD_SB3(src, src_stride, src0, src1, src2);
     src += (3 * src_stride);
-    XORI_B3_128_SB(src0, src1, src2);
     ILVR_B2_SB(src1, src0, src2, src1, src10_r, src21_r);
     ILVL_B2_SB(src1, src0, src2, src1, src10_l, src21_l);
 
     for (loop_cnt = (height >> 2); loop_cnt--;) {
         LD_SB4(src, src_stride, src3, src4, src5, src6);
         src += (4 * src_stride);
-        XORI_B4_128_SB(src3, src4, src5, src6);
         ILVRL_B2_SB(src3, src2, src32_r, src32_l);
         ILVRL_B2_SB(src4, src3, src43_r, src43_l);
         ILVRL_B2_SB(src5, src4, src54_r, src54_l);
         ILVRL_B2_SB(src6, src5, src65_r, src65_l);
-        dst0 = HEVC_FILT_4TAP_SH(src10_r, src32_r, filt0, filt1);
-        dst1 = HEVC_FILT_4TAP_SH(src21_r, src43_r, filt0, filt1);
-        dst2 = HEVC_FILT_4TAP_SH(src32_r, src54_r, filt0, filt1);
-        dst3 = HEVC_FILT_4TAP_SH(src43_r, src65_r, filt0, filt1);
-        dst4 = HEVC_FILT_4TAP_SH(src10_l, src32_l, filt0, filt1);
-        dst5 = HEVC_FILT_4TAP_SH(src21_l, src43_l, filt0, filt1);
-        dst6 = HEVC_FILT_4TAP_SH(src32_l, src54_l, filt0, filt1);
-        dst7 = HEVC_FILT_4TAP_SH(src43_l, src65_l, filt0, filt1);
-        HEVC_UNIW_RND_CLIP4_MAX_SATU_H(dst0, dst1, dst2, dst3, weight_vec,
-                                       offset_vec, rnd_vec, dst0, dst1, dst2,
-                                       dst3);
-        HEVC_UNIW_RND_CLIP4_MAX_SATU_H(dst4, dst5, dst6, dst7, weight_vec,
-                                       offset_vec, rnd_vec, dst4, dst5, dst6,
-                                       dst7);
-        PCKEV_B4_UB(dst4, dst0, dst5, dst1, dst6, dst2, dst7, dst3, out0, out1,
+
+        ILVRL_B2_SH(zero, src10_r, tmp0, tmp1);
+        ILVRL_B2_SH(zero, src21_r, tmp2, tmp3);
+        ILVRL_B2_SH(zero, src32_r, tmp4, tmp5);
+        ILVRL_B2_SH(zero, src43_r, tmp6, tmp7);
+        dst0 = HEVC_FILT_4TAP_SW(tmp0, tmp4, filt0, filt1);
+        dst1 = HEVC_FILT_4TAP_SW(tmp1, tmp5, filt0, filt1);
+        dst2 = HEVC_FILT_4TAP_SW(tmp2, tmp6, filt0, filt1);
+        dst3 = HEVC_FILT_4TAP_SW(tmp3, tmp7, filt0, filt1);
+        ILVRL_B2_SH(zero, src54_r, tmp0, tmp1);
+        ILVRL_B2_SH(zero, src65_r, tmp2, tmp3);
+        dst4 = HEVC_FILT_4TAP_SW(tmp4, tmp0, filt0, filt1);
+        dst5 = HEVC_FILT_4TAP_SW(tmp5, tmp1, filt0, filt1);
+        dst6 = HEVC_FILT_4TAP_SW(tmp6, tmp2, filt0, filt1);
+        dst7 = HEVC_FILT_4TAP_SW(tmp7, tmp3, filt0, filt1);
+        ILVRL_B2_SH(zero, src10_l, tmp0, tmp1);
+        ILVRL_B2_SH(zero, src21_l, tmp2, tmp3);
+        ILVRL_B2_SH(zero, src32_l, tmp4, tmp5);
+        ILVRL_B2_SH(zero, src43_l, tmp6, tmp7);
+        dst8  = HEVC_FILT_4TAP_SW(tmp0, tmp4, filt0, filt1);
+        dst9  = HEVC_FILT_4TAP_SW(tmp1, tmp5, filt0, filt1);
+        dst10 = HEVC_FILT_4TAP_SW(tmp2, tmp6, filt0, filt1);
+        dst11 = HEVC_FILT_4TAP_SW(tmp3, tmp7, filt0, filt1);
+        ILVRL_B2_SH(zero, src54_l, tmp0, tmp1);
+        ILVRL_B2_SH(zero, src65_l, tmp2, tmp3);
+        dst12 = HEVC_FILT_4TAP_SW(tmp4, tmp0, filt0, filt1);
+        dst13 = HEVC_FILT_4TAP_SW(tmp5, tmp1, filt0, filt1);
+        dst14 = HEVC_FILT_4TAP_SW(tmp6, tmp2, filt0, filt1);
+        dst15 = HEVC_FILT_4TAP_SW(tmp7, tmp3, filt0, filt1);
+
+        MUL4(dst0, weight_vec, dst1, weight_vec, dst2, weight_vec, dst3,
+             weight_vec, dst0, dst1, dst2, dst3);
+        MUL4(dst4, weight_vec, dst5, weight_vec, dst6, weight_vec,
+             dst7, weight_vec, dst4, dst5, dst6, dst7);
+        SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
+        SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
+        ADD4(dst0, offset_vec, dst1, offset_vec, dst2, offset_vec, dst3,
+             offset_vec, dst0, dst1, dst2, dst3);
+        ADD4(dst4, offset_vec, dst5, offset_vec, dst6, offset_vec,
+             dst7, offset_vec, dst4, dst5, dst6, dst7);
+        CLIP_SW8_0_255(dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7);
+
+        MUL4(dst8, weight_vec, dst9, weight_vec, dst10, weight_vec, dst11,
+             weight_vec, dst8, dst9, dst10, dst11);
+        MUL4(dst12, weight_vec, dst13, weight_vec, dst14, weight_vec,
+             dst15, weight_vec, dst12, dst13, dst14, dst15);
+        SRAR_W4_SW(dst8, dst9, dst10, dst11, rnd_vec);
+        SRAR_W4_SW(dst12, dst13, dst14, dst15, rnd_vec);
+        ADD4(dst8, offset_vec, dst9, offset_vec, dst10, offset_vec, dst11,
+             offset_vec, dst8, dst9, dst10, dst11);
+        ADD4(dst12, offset_vec, dst13, offset_vec, dst14, offset_vec,
+             dst15, offset_vec, dst12, dst13, dst14, dst15);
+        CLIP_SW8_0_255(dst8, dst9, dst10, dst11, dst12, dst13, dst14, dst15);
+
+        PCKEV_H2_SH(dst1, dst0, dst3, dst2, tmp0, tmp1);
+        PCKEV_H2_SH(dst5, dst4, dst7, dst6, tmp2, tmp3);
+        PCKEV_H2_SH(dst9, dst8, dst11, dst10, tmp4, tmp5);
+        PCKEV_H2_SH(dst13, dst12, dst15, dst14, tmp6, tmp7);
+        PCKEV_B4_UB(tmp4, tmp0, tmp5, tmp1, tmp6, tmp2, tmp7, tmp3, out0, out1,
                     out2, out3);
         ST_UB4(out0, out1, out2, out3, dst, dst_stride);
         dst += (4 * dst_stride);
@@ -4009,36 +4693,27 @@ static void hevc_vt_uniwgt_4t_24w_msa(uint8_t *src,
     v16i8 src10_r, src32_r, src54_r, src21_r, src43_r, src65_r;
     v16i8 src10_l, src32_l, src54_l, src21_l, src43_l, src65_l;
     v16i8 src87_r, src98_r, src109_r, src1110_r, src1211_r, src1312_r;
-    v8i16 filt0, filt1;
-    v8i16 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7, dst8, dst9, dst10;
-    v8i16 filter_vec, weight_vec_h, offset_vec, denom_vec, dst11;
-    v4i32 weight_vec, rnd_vec;
+    v8i16 filter_vec, filt0, filt1;
+    v8i16 tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
+    v4i32 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    v4i32 dst8, dst9, dst10, dst11, dst12, dst13, dst14, dst15;
+    v4i32 dst16, dst17, dst18, dst19, dst20, dst21, dst22, dst23;
+    v4i32 weight_vec, rnd_vec, offset_vec;
+    v8i16 zero = { 0 };
 
     src -= src_stride;
 
-    weight = weight & 0x0000FFFF;
-
     weight_vec = __msa_fill_w(weight);
     rnd_vec = __msa_fill_w(rnd_val);
-
-    weight *= 128;
-    rnd_val -= 6;
-
-    weight_vec_h = __msa_fill_h(weight);
-    offset_vec = __msa_fill_h(offset);
-    denom_vec = __msa_fill_h(rnd_val);
-
-    weight_vec_h = __msa_srar_h(weight_vec_h, denom_vec);
-    offset_vec = __msa_adds_s_h(offset_vec, weight_vec_h);
+    offset_vec = __msa_fill_w(offset);
 
     filter_vec = LD_SH(filter);
-    SPLATI_H2_SH(filter_vec, 0, 1, filt0, filt1);
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W2_SH(filter_vec, 0, filt0, filt1);
 
     LD_SB3(src, src_stride, src0, src1, src2);
     LD_SB3(src + 16, src_stride, src7, src8, src9);
     src += (3 * src_stride);
-    XORI_B3_128_SB(src0, src1, src2);
-    XORI_B3_128_SB(src7, src8, src9);
     ILVR_B2_SB(src1, src0, src2, src1, src10_r, src21_r);
     ILVL_B2_SB(src1, src0, src2, src1, src10_l, src21_l);
     ILVR_B2_SB(src8, src7, src9, src8, src87_r, src98_r);
@@ -4047,38 +4722,103 @@ static void hevc_vt_uniwgt_4t_24w_msa(uint8_t *src,
         LD_SB4(src, src_stride, src3, src4, src5, src6);
         LD_SB4(src + 16, src_stride, src10, src11, src12, src13);
         src += (4 * src_stride);
-        XORI_B4_128_SB(src3, src4, src5, src6);
-        XORI_B4_128_SB(src10, src11, src12, src13);
         ILVR_B2_SB(src3, src2, src4, src3, src32_r, src43_r);
         ILVL_B2_SB(src3, src2, src4, src3, src32_l, src43_l);
         ILVRL_B2_SB(src5, src4, src54_r, src54_l);
         ILVRL_B2_SB(src6, src5, src65_r, src65_l);
         ILVR_B2_SB(src10, src9, src11, src10, src109_r, src1110_r);
         ILVR_B2_SB(src12, src11, src13, src12, src1211_r, src1312_r);
-        dst0 = HEVC_FILT_4TAP_SH(src10_r, src32_r, filt0, filt1);
-        dst1 = HEVC_FILT_4TAP_SH(src21_r, src43_r, filt0, filt1);
-        dst2 = HEVC_FILT_4TAP_SH(src32_r, src54_r, filt0, filt1);
-        dst3 = HEVC_FILT_4TAP_SH(src43_r, src65_r, filt0, filt1);
-        dst4 = HEVC_FILT_4TAP_SH(src10_l, src32_l, filt0, filt1);
-        dst5 = HEVC_FILT_4TAP_SH(src21_l, src43_l, filt0, filt1);
-        dst6 = HEVC_FILT_4TAP_SH(src32_l, src54_l, filt0, filt1);
-        dst7 = HEVC_FILT_4TAP_SH(src43_l, src65_l, filt0, filt1);
-        dst8 = HEVC_FILT_4TAP_SH(src87_r, src109_r, filt0, filt1);
-        dst9 = HEVC_FILT_4TAP_SH(src98_r, src1110_r, filt0, filt1);
-        dst10 = HEVC_FILT_4TAP_SH(src109_r, src1211_r, filt0, filt1);
-        dst11 = HEVC_FILT_4TAP_SH(src1110_r, src1312_r, filt0, filt1);
-        HEVC_UNIW_RND_CLIP4_MAX_SATU_H(dst0, dst1, dst2, dst3, weight_vec,
-                                       offset_vec, rnd_vec, dst0, dst1, dst2,
-                                       dst3);
-        HEVC_UNIW_RND_CLIP4_MAX_SATU_H(dst4, dst5, dst6, dst7, weight_vec,
-                                       offset_vec, rnd_vec, dst4, dst5, dst6,
-                                       dst7);
-        HEVC_UNIW_RND_CLIP4_MAX_SATU_H(dst8, dst9, dst10, dst11, weight_vec,
-                                       offset_vec, rnd_vec, dst8, dst9, dst10,
-                                       dst11);
-        PCKEV_B4_UB(dst4, dst0, dst5, dst1, dst6, dst2, dst7, dst3, out0, out1,
+
+        ILVRL_B2_SH(zero, src10_r, tmp0, tmp1);
+        ILVRL_B2_SH(zero, src21_r, tmp2, tmp3);
+        ILVRL_B2_SH(zero, src32_r, tmp4, tmp5);
+        ILVRL_B2_SH(zero, src43_r, tmp6, tmp7);
+        dst0 = HEVC_FILT_4TAP_SW(tmp0, tmp4, filt0, filt1);
+        dst1 = HEVC_FILT_4TAP_SW(tmp1, tmp5, filt0, filt1);
+        dst2 = HEVC_FILT_4TAP_SW(tmp2, tmp6, filt0, filt1);
+        dst3 = HEVC_FILT_4TAP_SW(tmp3, tmp7, filt0, filt1);
+        ILVRL_B2_SH(zero, src54_r, tmp0, tmp1);
+        ILVRL_B2_SH(zero, src65_r, tmp2, tmp3);
+        dst4 = HEVC_FILT_4TAP_SW(tmp4, tmp0, filt0, filt1);
+        dst5 = HEVC_FILT_4TAP_SW(tmp5, tmp1, filt0, filt1);
+        dst6 = HEVC_FILT_4TAP_SW(tmp6, tmp2, filt0, filt1);
+        dst7 = HEVC_FILT_4TAP_SW(tmp7, tmp3, filt0, filt1);
+        ILVRL_B2_SH(zero, src10_l, tmp0, tmp1);
+        ILVRL_B2_SH(zero, src21_l, tmp2, tmp3);
+        ILVRL_B2_SH(zero, src32_l, tmp4, tmp5);
+        ILVRL_B2_SH(zero, src43_l, tmp6, tmp7);
+        dst8  = HEVC_FILT_4TAP_SW(tmp0, tmp4, filt0, filt1);
+        dst9  = HEVC_FILT_4TAP_SW(tmp1, tmp5, filt0, filt1);
+        dst10 = HEVC_FILT_4TAP_SW(tmp2, tmp6, filt0, filt1);
+        dst11 = HEVC_FILT_4TAP_SW(tmp3, tmp7, filt0, filt1);
+        ILVRL_B2_SH(zero, src54_l, tmp0, tmp1);
+        ILVRL_B2_SH(zero, src65_l, tmp2, tmp3);
+        dst12 = HEVC_FILT_4TAP_SW(tmp4, tmp0, filt0, filt1);
+        dst13 = HEVC_FILT_4TAP_SW(tmp5, tmp1, filt0, filt1);
+        dst14 = HEVC_FILT_4TAP_SW(tmp6, tmp2, filt0, filt1);
+        dst15 = HEVC_FILT_4TAP_SW(tmp7, tmp3, filt0, filt1);
+        ILVRL_B2_SH(zero, src87_r, tmp0, tmp1);
+        ILVRL_B2_SH(zero, src98_r, tmp2, tmp3);
+        ILVRL_B2_SH(zero, src109_r, tmp4, tmp5);
+        ILVRL_B2_SH(zero, src1110_r, tmp6, tmp7);
+        dst16 = HEVC_FILT_4TAP_SW(tmp0, tmp4, filt0, filt1);
+        dst17 = HEVC_FILT_4TAP_SW(tmp1, tmp5, filt0, filt1);
+        dst18 = HEVC_FILT_4TAP_SW(tmp2, tmp6, filt0, filt1);
+        dst19 = HEVC_FILT_4TAP_SW(tmp3, tmp7, filt0, filt1);
+        ILVRL_B2_SH(zero, src1211_r, tmp0, tmp1);
+        ILVRL_B2_SH(zero, src1312_r, tmp2, tmp3);
+        dst20 = HEVC_FILT_4TAP_SW(tmp4, tmp0, filt0, filt1);
+        dst21 = HEVC_FILT_4TAP_SW(tmp5, tmp1, filt0, filt1);
+        dst22 = HEVC_FILT_4TAP_SW(tmp6, tmp2, filt0, filt1);
+        dst23 = HEVC_FILT_4TAP_SW(tmp7, tmp3, filt0, filt1);
+
+        MUL4(dst0, weight_vec, dst1, weight_vec, dst2, weight_vec, dst3,
+             weight_vec, dst0, dst1, dst2, dst3);
+        MUL4(dst4, weight_vec, dst5, weight_vec, dst6, weight_vec,
+             dst7, weight_vec, dst4, dst5, dst6, dst7);
+        SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
+        SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
+        ADD4(dst0, offset_vec, dst1, offset_vec, dst2, offset_vec, dst3,
+             offset_vec, dst0, dst1, dst2, dst3);
+        ADD4(dst4, offset_vec, dst5, offset_vec, dst6, offset_vec,
+             dst7, offset_vec, dst4, dst5, dst6, dst7);
+        CLIP_SW8_0_255(dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7);
+
+        MUL4(dst8, weight_vec, dst9, weight_vec, dst10, weight_vec, dst11,
+             weight_vec, dst8, dst9, dst10, dst11);
+        MUL4(dst12, weight_vec, dst13, weight_vec, dst14, weight_vec,
+             dst15, weight_vec, dst12, dst13, dst14, dst15);
+        SRAR_W4_SW(dst8, dst9, dst10, dst11, rnd_vec);
+        SRAR_W4_SW(dst12, dst13, dst14, dst15, rnd_vec);
+        ADD4(dst8, offset_vec, dst9, offset_vec, dst10, offset_vec, dst11,
+             offset_vec, dst8, dst9, dst10, dst11);
+        ADD4(dst12, offset_vec, dst13, offset_vec, dst14, offset_vec,
+             dst15, offset_vec, dst12, dst13, dst14, dst15);
+        CLIP_SW8_0_255(dst8, dst9, dst10, dst11, dst12, dst13, dst14, dst15);
+
+        MUL4(dst16, weight_vec, dst17, weight_vec, dst18, weight_vec, dst19,
+             weight_vec, dst16, dst17, dst18, dst19);
+        MUL4(dst20, weight_vec, dst21, weight_vec, dst22, weight_vec,
+             dst23, weight_vec, dst20, dst21, dst22, dst23);
+        SRAR_W4_SW(dst16, dst17, dst18, dst19, rnd_vec);
+        SRAR_W4_SW(dst20, dst21, dst22, dst23, rnd_vec);
+        ADD4(dst16, offset_vec, dst17, offset_vec, dst18, offset_vec, dst19,
+             offset_vec, dst16, dst17, dst18, dst19);
+        ADD4(dst20, offset_vec, dst21, offset_vec, dst22, offset_vec,
+             dst23, offset_vec, dst20, dst21, dst22, dst23);
+        CLIP_SW8_0_255(dst16, dst17, dst18, dst19, dst20, dst21, dst22, dst23);
+
+        PCKEV_H2_SH(dst1, dst0, dst3, dst2, tmp0, tmp1);
+        PCKEV_H2_SH(dst5, dst4, dst7, dst6, tmp2, tmp3);
+        PCKEV_H2_SH(dst9, dst8, dst11, dst10, tmp4, tmp5);
+        PCKEV_H2_SH(dst13, dst12, dst15, dst14, tmp6, tmp7);
+        PCKEV_B4_UB(tmp4, tmp0, tmp5, tmp1, tmp6, tmp2, tmp7, tmp3, out0, out1,
                     out2, out3);
-        PCKEV_B2_UB(dst9, dst8, dst11, dst10, out4, out5);
+
+        PCKEV_H2_SH(dst17, dst16, dst19, dst18, tmp0, tmp1);
+        PCKEV_H2_SH(dst21, dst20, dst23, dst22, tmp2, tmp3);
+
+        PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out4, out5);
         ST_UB4(out0, out1, out2, out3, dst, dst_stride);
         ST_D4(out4, out5, 0, 1, 0, 1, dst + 16, dst_stride);
         dst += (4 * dst_stride);
@@ -4109,37 +4849,28 @@ static void hevc_vt_uniwgt_4t_32w_msa(uint8_t *src,
     v16i8 src0, src1, src2, src3, src4, src5, src6, src7, src8, src9;
     v16i8 src10_r, src32_r, src76_r, src98_r;
     v16i8 src21_r, src43_r, src65_r, src87_r;
-    v8i16 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
     v16i8 src10_l, src32_l, src76_l, src98_l;
     v16i8 src21_l, src43_l, src65_l, src87_l;
-    v8i16 filt0, filt1;
-    v8i16 filter_vec, weight_vec_h, offset_vec, denom_vec;
-    v4i32 weight_vec, rnd_vec;
+    v8i16 filter_vec, filt0, filt1;
+    v4i32 weight_vec, rnd_vec, offset_vec;
+    v4i32 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    v4i32 dst8, dst9, dst10, dst11, dst12, dst13, dst14, dst15;
+    v8i16 tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
+    v8i16 zero = { 0 };
 
     src -= src_stride;
 
-    weight = weight & 0x0000FFFF;
-
     weight_vec = __msa_fill_w(weight);
     rnd_vec = __msa_fill_w(rnd_val);
-
-    weight *= 128;
-    rnd_val -= 6;
-
-    weight_vec_h = __msa_fill_h(weight);
-    offset_vec = __msa_fill_h(offset);
-    denom_vec = __msa_fill_h(rnd_val);
-
-    weight_vec_h = __msa_srar_h(weight_vec_h, denom_vec);
-    offset_vec = __msa_adds_s_h(offset_vec, weight_vec_h);
+    offset_vec = __msa_fill_w(offset);
 
     filter_vec = LD_SH(filter);
-    SPLATI_H2_SH(filter_vec, 0, 1, filt0, filt1);
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W2_SH(filter_vec, 0, filt0, filt1);
 
     LD_SB3(src, src_stride, src0, src1, src2);
     LD_SB3(src + 16, src_stride, src5, src6, src7);
     src += (3 * src_stride);
-    XORI_B6_128_SB(src0, src1, src2, src5, src6, src7);
     ILVR_B2_SB(src1, src0, src2, src1, src10_r, src21_r);
     ILVL_B2_SB(src1, src0, src2, src1, src10_l, src21_l);
     ILVR_B2_SB(src6, src5, src7, src6, src65_r, src76_r);
@@ -4149,26 +4880,75 @@ static void hevc_vt_uniwgt_4t_32w_msa(uint8_t *src,
         LD_SB2(src, src_stride, src3, src4);
         LD_SB2(src + 16, src_stride, src8, src9);
         src += (2 * src_stride);
-        XORI_B4_128_SB(src3, src4, src8, src9);
         ILVR_B2_SB(src3, src2, src4, src3, src32_r, src43_r);
         ILVL_B2_SB(src3, src2, src4, src3, src32_l, src43_l);
         ILVRL_B2_SB(src8, src7, src87_r, src87_l);
         ILVRL_B2_SB(src9, src8, src98_r, src98_l);
-        dst0 = HEVC_FILT_4TAP_SH(src10_r, src32_r, filt0, filt1);
-        dst1 = HEVC_FILT_4TAP_SH(src21_r, src43_r, filt0, filt1);
-        dst2 = HEVC_FILT_4TAP_SH(src10_l, src32_l, filt0, filt1);
-        dst3 = HEVC_FILT_4TAP_SH(src21_l, src43_l, filt0, filt1);
-        dst4 = HEVC_FILT_4TAP_SH(src65_r, src87_r, filt0, filt1);
-        dst5 = HEVC_FILT_4TAP_SH(src76_r, src98_r, filt0, filt1);
-        dst6 = HEVC_FILT_4TAP_SH(src65_l, src87_l, filt0, filt1);
-        dst7 = HEVC_FILT_4TAP_SH(src76_l, src98_l, filt0, filt1);
-        HEVC_UNIW_RND_CLIP4_MAX_SATU_H(dst0, dst1, dst2, dst3, weight_vec,
-                                       offset_vec, rnd_vec, dst0, dst1, dst2,
-                                       dst3);
-        HEVC_UNIW_RND_CLIP4_MAX_SATU_H(dst4, dst5, dst6, dst7, weight_vec,
-                                       offset_vec, rnd_vec, dst4, dst5, dst6,
-                                       dst7);
-        PCKEV_B4_UB(dst2, dst0, dst3, dst1, dst6, dst4, dst7, dst5, out0, out1,
+
+        ILVRL_B2_SH(zero, src10_r, tmp0, tmp1);
+        ILVRL_B2_SH(zero, src21_r, tmp2, tmp3);
+        ILVRL_B2_SH(zero, src32_r, tmp4, tmp5);
+        ILVRL_B2_SH(zero, src43_r, tmp6, tmp7);
+        dst0 = HEVC_FILT_4TAP_SW(tmp0, tmp4, filt0, filt1);
+        dst1 = HEVC_FILT_4TAP_SW(tmp1, tmp5, filt0, filt1);
+        dst2 = HEVC_FILT_4TAP_SW(tmp2, tmp6, filt0, filt1);
+        dst3 = HEVC_FILT_4TAP_SW(tmp3, tmp7, filt0, filt1);
+
+        ILVRL_B2_SH(zero, src10_l, tmp0, tmp1);
+        ILVRL_B2_SH(zero, src21_l, tmp2, tmp3);
+        ILVRL_B2_SH(zero, src32_l, tmp4, tmp5);
+        ILVRL_B2_SH(zero, src43_l, tmp6, tmp7);
+        dst4 = HEVC_FILT_4TAP_SW(tmp0, tmp4, filt0, filt1);
+        dst5 = HEVC_FILT_4TAP_SW(tmp1, tmp5, filt0, filt1);
+        dst6 = HEVC_FILT_4TAP_SW(tmp2, tmp6, filt0, filt1);
+        dst7 = HEVC_FILT_4TAP_SW(tmp3, tmp7, filt0, filt1);
+
+        ILVRL_B2_SH(zero, src65_r, tmp0, tmp1);
+        ILVRL_B2_SH(zero, src76_r, tmp2, tmp3);
+        ILVRL_B2_SH(zero, src87_r, tmp4, tmp5);
+        ILVRL_B2_SH(zero, src98_r, tmp6, tmp7);
+        dst8  = HEVC_FILT_4TAP_SW(tmp0, tmp4, filt0, filt1);
+        dst9  = HEVC_FILT_4TAP_SW(tmp1, tmp5, filt0, filt1);
+        dst10 = HEVC_FILT_4TAP_SW(tmp2, tmp6, filt0, filt1);
+        dst11 = HEVC_FILT_4TAP_SW(tmp3, tmp7, filt0, filt1);
+        ILVRL_B2_SH(zero, src65_l, tmp0, tmp1);
+        ILVRL_B2_SH(zero, src76_l, tmp2, tmp3);
+        ILVRL_B2_SH(zero, src87_l, tmp4, tmp5);
+        ILVRL_B2_SH(zero, src98_l, tmp6, tmp7);
+        dst12 = HEVC_FILT_4TAP_SW(tmp0, tmp4, filt0, filt1);
+        dst13 = HEVC_FILT_4TAP_SW(tmp1, tmp5, filt0, filt1);
+        dst14 = HEVC_FILT_4TAP_SW(tmp2, tmp6, filt0, filt1);
+        dst15 = HEVC_FILT_4TAP_SW(tmp3, tmp7, filt0, filt1);
+
+        MUL4(dst0, weight_vec, dst1, weight_vec, dst2, weight_vec, dst3,
+             weight_vec, dst0, dst1, dst2, dst3);
+        MUL4(dst4, weight_vec, dst5, weight_vec, dst6, weight_vec,
+             dst7, weight_vec, dst4, dst5, dst6, dst7);
+        SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
+        SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
+        ADD4(dst0, offset_vec, dst1, offset_vec, dst2, offset_vec, dst3,
+             offset_vec, dst0, dst1, dst2, dst3);
+        ADD4(dst4, offset_vec, dst5, offset_vec, dst6, offset_vec,
+             dst7, offset_vec, dst4, dst5, dst6, dst7);
+        CLIP_SW8_0_255(dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7);
+
+        MUL4(dst8, weight_vec, dst9, weight_vec, dst10, weight_vec, dst11,
+             weight_vec, dst8, dst9, dst10, dst11);
+        MUL4(dst12, weight_vec, dst13, weight_vec, dst14, weight_vec,
+             dst15, weight_vec, dst12, dst13, dst14, dst15);
+        SRAR_W4_SW(dst8, dst9, dst10, dst11, rnd_vec);
+        SRAR_W4_SW(dst12, dst13, dst14, dst15, rnd_vec);
+        ADD4(dst8, offset_vec, dst9, offset_vec, dst10, offset_vec, dst11,
+             offset_vec, dst8, dst9, dst10, dst11);
+        ADD4(dst12, offset_vec, dst13, offset_vec, dst14, offset_vec,
+             dst15, offset_vec, dst12, dst13, dst14, dst15);
+        CLIP_SW8_0_255(dst8, dst9, dst10, dst11, dst12, dst13, dst14, dst15);
+
+        PCKEV_H2_SH(dst1, dst0, dst3, dst2, tmp0, tmp1);
+        PCKEV_H2_SH(dst5, dst4, dst7, dst6, tmp2, tmp3);
+        PCKEV_H2_SH(dst9, dst8, dst11, dst10, tmp4, tmp5);
+        PCKEV_H2_SH(dst13, dst12, dst15, dst14, tmp6, tmp7);
+        PCKEV_B4_UB(tmp2, tmp0, tmp3, tmp1, tmp6, tmp4, tmp7, tmp5, out0, out1,
                     out2, out3);
         ST_UB2(out0, out2, dst, 16);
         dst += dst_stride;
@@ -4205,49 +4985,54 @@ static void hevc_hv_uniwgt_4t_4x2_msa(uint8_t *src,
     v16i8 mask1;
     v8i16 filt_h0, filt_h1, filter_vec, tmp;
     v16i8 vec0, vec1, vec2, vec3, vec4, vec5;
-    v8i16 dst20, dst31, dst42, dst10, dst32, dst21, dst43;
-    v8i16 offset_vec, const_128, denom_vec;
-    v4i32 dst0, dst1, weight_vec, rnd_vec;
+    v8i16 dst10, dst21, dst32, dst43;
+    v8i16 tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
+    v4i32 dst0, dst1, dst2, dst3, dst4;
+    v4i32 weight_vec, rnd_vec, offset_vec;
+    v8i16 zero = { 0 };
 
     src -= (src_stride + 1);
 
     filter_vec = LD_SH(filter_x);
-    SPLATI_H2_SH(filter_vec, 0, 1, filt0, filt1);
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W2_SH(filter_vec, 0, filt0, filt1);
 
     filter_vec = LD_SH(filter_y);
     UNPCK_R_SB_SH(filter_vec, filter_vec);
-
     SPLATI_W2_SH(filter_vec, 0, filt_h0, filt_h1);
 
     mask1 = mask0 + 2;
 
     weight_vec = __msa_fill_w(weight);
     rnd_vec = __msa_fill_w(rnd_val);
-
-    offset_vec = __msa_fill_h(offset);
-    denom_vec = __msa_fill_h(rnd_val - 6);
-    const_128 = __msa_fill_h((128 * weight));
-    offset_vec += __msa_srar_h(const_128, denom_vec);
+    offset_vec = __msa_fill_w(offset);
 
     LD_SB5(src, src_stride, src0, src1, src2, src3, src4);
-    XORI_B5_128_SB(src0, src1, src2, src3, src4);
     VSHF_B2_SB(src0, src2, src0, src2, mask0, mask1, vec0, vec1);
     VSHF_B2_SB(src1, src3, src1, src3, mask0, mask1, vec2, vec3);
     VSHF_B2_SB(src2, src4, src2, src4, mask0, mask1, vec4, vec5);
-    dst20 = HEVC_FILT_4TAP_SH(vec0, vec1, filt0, filt1);
-    dst31 = HEVC_FILT_4TAP_SH(vec2, vec3, filt0, filt1);
-    dst42 = HEVC_FILT_4TAP_SH(vec4, vec5, filt0, filt1);
-    ILVRL_H2_SH(dst31, dst20, dst10, dst32);
-    ILVRL_H2_SH(dst42, dst31, dst21, dst43);
+    ILVRL_B2_SH(zero, vec0, tmp0, tmp1);
+    ILVRL_B2_SH(zero, vec1, tmp2, tmp3);
+    ILVRL_B2_SH(zero, vec2, tmp4, tmp5);
+    ILVRL_B2_SH(zero, vec3, tmp6, tmp7);
+    dst0 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+    dst2 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+    dst1 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+    dst3 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+    ILVL_B2_SH(zero, vec4, zero, vec5, tmp1, tmp3);
+    dst4 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+
+    ILVEV_H2_SH(dst0, dst1, dst2, dst3, dst10, dst32);
+    ILVEV_H2_SH(dst1, dst2, dst3, dst4, dst21, dst43);
     dst0 = HEVC_FILT_4TAP(dst10, dst32, filt_h0, filt_h1);
     dst1 = HEVC_FILT_4TAP(dst21, dst43, filt_h0, filt_h1);
     dst0 >>= 6;
     dst1 >>= 6;
     MUL2(dst0, weight_vec, dst1, weight_vec, dst0, dst1);
     SRAR_W2_SW(dst0, dst1, rnd_vec);
+    ADD2(dst0, offset_vec, dst1, offset_vec, dst0, dst1);
+    CLIP_SW2_0_255(dst0, dst1);
     tmp = __msa_pckev_h((v8i16) dst1, (v8i16) dst0);
-    tmp += offset_vec;
-    tmp = CLIP_SH_0_255_MAX_SATU(tmp);
     out = (v16u8) __msa_pckev_b((v16i8) tmp, (v16i8) tmp);
     ST_W2(out, 0, 1, dst, dst_stride);
 }
@@ -4265,47 +5050,55 @@ static void hevc_hv_uniwgt_4t_4x4_msa(uint8_t *src,
     v16u8 out;
     v16i8 src0, src1, src2, src3, src4, src5, src6;
     v8i16 filt0, filt1;
-    v8i16 filt_h0, filt_h1, filter_vec, tmp0, tmp1;
+    v8i16 filt_h0, filt_h1, filter_vec;
     v16i8 mask0 = LD_SB(ff_hevc_mask_arr + 16);
     v16i8 mask1;
     v16i8 vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
-    v8i16 dst30, dst41, dst52, dst63, dst10, dst32, dst54, dst21, dst43, dst65;
-    v8i16 offset_vec, const_128, denom_vec;
-    v4i32 dst0, dst1, dst2, dst3, weight_vec, rnd_vec;
+    v8i16 dst10, dst32, dst54, dst21, dst43, dst65;
+    v4i32 dst0, dst1, dst2, dst3, dst4, dst5, dst6;
+    v8i16 tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
+    v4i32 weight_vec, rnd_vec, offset_vec;
+    v8i16 zero = { 0 };
 
     src -= (src_stride + 1);
 
     filter_vec = LD_SH(filter_x);
-    SPLATI_H2_SH(filter_vec, 0, 1, filt0, filt1);
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W2_SH(filter_vec, 0, filt0, filt1);
 
     filter_vec = LD_SH(filter_y);
     UNPCK_R_SB_SH(filter_vec, filter_vec);
-
     SPLATI_W2_SH(filter_vec, 0, filt_h0, filt_h1);
 
     mask1 = mask0 + 2;
 
     weight_vec = __msa_fill_w(weight);
     rnd_vec = __msa_fill_w(rnd_val);
-
-    offset_vec = __msa_fill_h(offset);
-    denom_vec = __msa_fill_h(rnd_val - 6);
-    const_128 = __msa_fill_h((128 * weight));
-    offset_vec += __msa_srar_h(const_128, denom_vec);
+    offset_vec = __msa_fill_w(offset);
 
     LD_SB7(src, src_stride, src0, src1, src2, src3, src4, src5, src6);
-    XORI_B7_128_SB(src0, src1, src2, src3, src4, src5, src6);
     VSHF_B2_SB(src0, src3, src0, src3, mask0, mask1, vec0, vec1);
     VSHF_B2_SB(src1, src4, src1, src4, mask0, mask1, vec2, vec3);
     VSHF_B2_SB(src2, src5, src2, src5, mask0, mask1, vec4, vec5);
     VSHF_B2_SB(src3, src6, src3, src6, mask0, mask1, vec6, vec7);
-    dst30 = HEVC_FILT_4TAP_SH(vec0, vec1, filt0, filt1);
-    dst41 = HEVC_FILT_4TAP_SH(vec2, vec3, filt0, filt1);
-    dst52 = HEVC_FILT_4TAP_SH(vec4, vec5, filt0, filt1);
-    dst63 = HEVC_FILT_4TAP_SH(vec6, vec7, filt0, filt1);
-    ILVRL_H2_SH(dst41, dst30, dst10, dst43);
-    ILVRL_H2_SH(dst52, dst41, dst21, dst54);
-    ILVRL_H2_SH(dst63, dst52, dst32, dst65);
+
+    ILVRL_B2_SH(zero, vec0, tmp0, tmp1);
+    ILVRL_B2_SH(zero, vec1, tmp2, tmp3);
+    ILVRL_B2_SH(zero, vec2, tmp4, tmp5);
+    ILVRL_B2_SH(zero, vec3, tmp6, tmp7);
+    dst0 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+    dst3 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+    dst1 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+    dst4 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+    ILVRL_B2_SH(zero, vec4, tmp0, tmp1);
+    ILVRL_B2_SH(zero, vec5, tmp2, tmp3);
+    ILVL_B2_SH(zero, vec6, zero, vec7, tmp5, tmp7);
+    dst2 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+    dst5 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+    dst6 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+    ILVEV_H2_SH(dst0, dst1, dst3, dst4, dst10, dst43);
+    ILVEV_H2_SH(dst1, dst2, dst4, dst5, dst21, dst54);
+    ILVEV_H2_SH(dst2, dst3, dst5, dst6, dst32, dst65);
     dst0 = HEVC_FILT_4TAP(dst10, dst32, filt_h0, filt_h1);
     dst1 = HEVC_FILT_4TAP(dst21, dst43, filt_h0, filt_h1);
     dst2 = HEVC_FILT_4TAP(dst32, dst54, filt_h0, filt_h1);
@@ -4314,9 +5107,10 @@ static void hevc_hv_uniwgt_4t_4x4_msa(uint8_t *src,
     MUL2(dst0, weight_vec, dst1, weight_vec, dst0, dst1);
     MUL2(dst2, weight_vec, dst3, weight_vec, dst2, dst3);
     SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
+    ADD4(dst0, offset_vec, dst1, offset_vec, dst2, offset_vec, dst3,
+         offset_vec, dst0, dst1, dst2, dst3);
+    CLIP_SW4_0_255(dst0, dst1, dst2, dst3);
     PCKEV_H2_SH(dst1, dst0, dst3, dst2, tmp0, tmp1);
-    ADD2(tmp0, offset_vec, tmp1, offset_vec, tmp0, tmp1);
-    CLIP_SH2_0_255_MAX_SATU(tmp0, tmp1);
     out = (v16u8) __msa_pckev_b((v16i8) tmp1, (v16i8) tmp0);
     ST_W4(out, 0, 1, 2, 3, dst, dst_stride);
 }
@@ -4338,65 +5132,78 @@ static void hevc_hv_uniwgt_4t_4multx8mult_msa(uint8_t *src,
     v8i16 filt0, filt1;
     v16i8 mask0 = LD_SB(ff_hevc_mask_arr + 16);
     v16i8 mask1;
-    v8i16 filt_h0, filt_h1, filter_vec, tmp0, tmp1, tmp2, tmp3;
+    v8i16 filter_vec, filt_h0, filt_h1;
     v16i8 vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
-    v8i16 dst10, dst21, dst22, dst73, dst84, dst95, dst106;
     v8i16 dst10_r, dst32_r, dst54_r, dst76_r;
     v8i16 dst21_r, dst43_r, dst65_r, dst87_r;
-    v8i16 dst98_r, dst109_r, offset_vec, const_128, denom_vec;
-    v4i32 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7, weight_vec, rnd_vec;
+    v8i16 dst98_r, dst109_r;
+    v8i16 tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
+    v4i32 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7, dst8, dst9, dst10;
+    v4i32 offset_vec, weight_vec, rnd_vec;
+    v8i16 zero = { 0 };
 
     src -= (src_stride + 1);
 
     filter_vec = LD_SH(filter_x);
-    SPLATI_H2_SH(filter_vec, 0, 1, filt0, filt1);
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W2_SH(filter_vec, 0, filt0, filt1);
 
     filter_vec = LD_SH(filter_y);
     UNPCK_R_SB_SH(filter_vec, filter_vec);
-
     SPLATI_W2_SH(filter_vec, 0, filt_h0, filt_h1);
 
     mask1 = mask0 + 2;
 
     weight_vec = __msa_fill_w(weight);
     rnd_vec = __msa_fill_w(rnd_val);
-
-    offset_vec = __msa_fill_h(offset);
-    denom_vec = __msa_fill_h(rnd_val - 6);
-    const_128 = __msa_fill_h((128 * weight));
-    offset_vec += __msa_srar_h(const_128, denom_vec);
+    offset_vec = __msa_fill_w(offset);
 
     LD_SB3(src, src_stride, src0, src1, src2);
     src += (3 * src_stride);
-    XORI_B3_128_SB(src0, src1, src2);
 
     VSHF_B2_SB(src0, src1, src0, src1, mask0, mask1, vec0, vec1);
     VSHF_B2_SB(src1, src2, src1, src2, mask0, mask1, vec2, vec3);
-    dst10 = HEVC_FILT_4TAP_SH(vec0, vec1, filt0, filt1);
-    dst21 = HEVC_FILT_4TAP_SH(vec2, vec3, filt0, filt1);
-    ILVRL_H2_SH(dst21, dst10, dst10_r, dst21_r);
-    dst22 = (v8i16) __msa_splati_d((v2i64) dst21, 1);
+    ILVRL_B2_SH(zero, vec0, tmp0, tmp1);
+    ILVRL_B2_SH(zero, vec1, tmp2, tmp3);
+    ILVL_B2_SH(zero, vec2, zero, vec3, tmp5, tmp7);
+    dst0 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+    dst1 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+    dst2 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+
+    ILVEV_H2_SH(dst0, dst1, dst1, dst2, dst10_r, dst21_r);
 
     for (loop_cnt = height >> 3; loop_cnt--;) {
         LD_SB8(src, src_stride,
                src3, src4, src5, src6, src7, src8, src9, src10);
         src += (8 * src_stride);
-        XORI_B8_128_SB(src3, src4, src5, src6, src7, src8, src9, src10);
 
         VSHF_B2_SB(src3, src7, src3, src7, mask0, mask1, vec0, vec1);
         VSHF_B2_SB(src4, src8, src4, src8, mask0, mask1, vec2, vec3);
         VSHF_B2_SB(src5, src9, src5, src9, mask0, mask1, vec4, vec5);
         VSHF_B2_SB(src6, src10, src6, src10, mask0, mask1, vec6, vec7);
-        dst73 = HEVC_FILT_4TAP_SH(vec0, vec1, filt0, filt1);
-        dst84 = HEVC_FILT_4TAP_SH(vec2, vec3, filt0, filt1);
-        dst95 = HEVC_FILT_4TAP_SH(vec4, vec5, filt0, filt1);
-        dst106 = HEVC_FILT_4TAP_SH(vec6, vec7, filt0, filt1);
-        dst32_r = __msa_ilvr_h(dst73, dst22);
-        ILVRL_H2_SH(dst84, dst73, dst43_r, dst87_r);
-        ILVRL_H2_SH(dst95, dst84, dst54_r, dst98_r);
-        ILVRL_H2_SH(dst106, dst95, dst65_r, dst109_r);
-        dst22 = (v8i16) __msa_splati_d((v2i64) dst73, 1);
-        dst76_r = __msa_ilvr_h(dst22, dst106);
+
+        ILVRL_B2_SH(zero, vec0, tmp0, tmp1);
+        ILVRL_B2_SH(zero, vec1, tmp2, tmp3);
+        ILVRL_B2_SH(zero, vec2, tmp4, tmp5);
+        ILVRL_B2_SH(zero, vec3, tmp6, tmp7);
+        dst3 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+        dst7 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+        dst4 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+        dst8 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+        ILVRL_B2_SH(zero, vec4, tmp0, tmp1);
+        ILVRL_B2_SH(zero, vec5, tmp2, tmp3);
+        ILVRL_B2_SH(zero, vec6, tmp4, tmp5);
+        ILVRL_B2_SH(zero, vec7, tmp6, tmp7);
+        dst5  = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+        dst9  = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+        dst6  = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+        dst10 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+
+        dst32_r = __msa_ilvev_h(dst3, dst2);
+        ILVEV_H2_SH(dst3, dst4, dst7, dst8, dst43_r, dst87_r);
+        ILVEV_H2_SH(dst4, dst5, dst8, dst9, dst54_r, dst98_r);
+        ILVEV_H2_SH(dst5, dst6, dst9, dst10, dst65_r, dst109_r);
+        dst76_r = __msa_ilvev_h(dst7, dst6);
         dst0 = HEVC_FILT_4TAP(dst10_r, dst32_r, filt_h0, filt_h1);
         dst1 = HEVC_FILT_4TAP(dst21_r, dst43_r, filt_h0, filt_h1);
         dst2 = HEVC_FILT_4TAP(dst32_r, dst54_r, filt_h0, filt_h1);
@@ -4413,18 +5220,20 @@ static void hevc_hv_uniwgt_4t_4multx8mult_msa(uint8_t *src,
         MUL2(dst6, weight_vec, dst7, weight_vec, dst6, dst7);
         SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
         SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
+        ADD4(dst0, offset_vec, dst1, offset_vec, dst2, offset_vec, dst3,
+             offset_vec, dst0, dst1, dst2, dst3);
+        ADD4(dst4, offset_vec, dst5, offset_vec, dst6, offset_vec, dst7,
+             offset_vec, dst4, dst5, dst6, dst7);
+        CLIP_SW8_0_255(dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7);
         PCKEV_H4_SH(dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6, tmp0, tmp1,
                     tmp2, tmp3);
-        ADD2(tmp0, offset_vec, tmp1, offset_vec, tmp0, tmp1);
-        ADD2(tmp2, offset_vec, tmp3, offset_vec, tmp2, tmp3);
-        CLIP_SH4_0_255_MAX_SATU(tmp0, tmp1, tmp2, tmp3);
         PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out0, out1);
         ST_W8(out0, out1, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
         dst += (8 * dst_stride);
 
         dst10_r = dst98_r;
         dst21_r = dst109_r;
-        dst22 = (v8i16) __msa_splati_d((v2i64) dst106, 1);
+        dst2 = dst10;
     }
 }
 
@@ -4472,75 +5281,103 @@ static void hevc_hv_uniwgt_4t_6w_msa(uint8_t *src,
     v16i8 mask1;
     v8i16 filt_h0, filt_h1, filter_vec;
     v16i8 vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
-    v8i16 dsth0, dsth1, dsth2, dsth3, dsth4, dsth5, dsth6, dsth7, dsth8, dsth9;
-    v8i16 dsth10, tmp0, tmp1, tmp2, tmp3, tmp4, tmp5;
+    v8i16 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7, dst8, dst9;
+    v8i16 tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
     v8i16 dst10_r, dst32_r, dst54_r, dst76_r, dst98_r, dst21_r, dst43_r;
     v8i16 dst65_r, dst87_r, dst109_r, dst10_l, dst32_l, dst54_l, dst76_l;
     v8i16 dst98_l, dst21_l, dst43_l, dst65_l, dst87_l, dst109_l;
     v8i16 dst1021_l, dst3243_l, dst5465_l, dst7687_l, dst98109_l;
-    v8i16 offset_vec, const_128, denom_vec;
     v4i32 dst0_r, dst1_r, dst2_r, dst3_r, dst4_r, dst5_r, dst6_r, dst7_r;
-    v4i32 dst0_l, dst1_l, dst2_l, dst3_l, weight_vec, rnd_vec;
+    v4i32 dst0_l, dst1_l, dst2_l, dst3_l, weight_vec, rnd_vec, offset_vec;
+    v8i16 zero = { 0 };
 
     src -= (src_stride + 1);
 
     filter_vec = LD_SH(filter_x);
-    SPLATI_H2_SH(filter_vec, 0, 1, filt0, filt1);
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W2_SH(filter_vec, 0, filt0, filt1);
 
     filter_vec = LD_SH(filter_y);
     UNPCK_R_SB_SH(filter_vec, filter_vec);
-
     SPLATI_W2_SH(filter_vec, 0, filt_h0, filt_h1);
 
     mask1 = mask0 + 2;
 
     weight_vec = __msa_fill_w(weight);
     rnd_vec = __msa_fill_w(rnd_val);
-
-    offset_vec = __msa_fill_h(offset);
-    denom_vec = __msa_fill_h(rnd_val - 6);
-    const_128 = __msa_fill_h((128 * weight));
-    offset_vec += __msa_srar_h(const_128, denom_vec);
+    offset_vec = __msa_fill_w(offset);
 
     LD_SB3(src, src_stride, src0, src1, src2);
     src += (3 * src_stride);
-    XORI_B3_128_SB(src0, src1, src2);
 
     VSHF_B2_SB(src0, src0, src0, src0, mask0, mask1, vec0, vec1);
     VSHF_B2_SB(src1, src1, src1, src1, mask0, mask1, vec2, vec3);
     VSHF_B2_SB(src2, src2, src2, src2, mask0, mask1, vec4, vec5);
-    dsth0 = HEVC_FILT_4TAP_SH(vec0, vec1, filt0, filt1);
-    dsth1 = HEVC_FILT_4TAP_SH(vec2, vec3, filt0, filt1);
-    dsth2 = HEVC_FILT_4TAP_SH(vec4, vec5, filt0, filt1);
-    ILVRL_H2_SH(dsth1, dsth0, dst10_r, dst10_l);
-    ILVRL_H2_SH(dsth2, dsth1, dst21_r, dst21_l);
+    ILVRL_B2_SH(zero, vec0, tmp0, tmp1);
+    ILVRL_B2_SH(zero, vec1, tmp2, tmp3);
+    ILVRL_B2_SH(zero, vec2, tmp4, tmp5);
+    ILVRL_B2_SH(zero, vec3, tmp6, tmp7);
+    dst0 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+    dst1 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+    dst2 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+    dst3 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+    ILVRL_B2_SH(zero, vec4, tmp0, tmp1);
+    ILVRL_B2_SH(zero, vec5, tmp2, tmp3);
+    dst4 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+    dst5 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+
+    ILVEV_H2_SH(dst0, dst2, dst1, dst3, dst10_r, dst10_l);
+    ILVEV_H2_SH(dst2, dst4, dst3, dst5, dst21_r, dst21_l);
 
     LD_SB8(src, src_stride, src3, src4, src5, src6, src7, src8, src9, src10);
-    XORI_B8_128_SB(src3, src4, src5, src6, src7, src8, src9, src10);
     VSHF_B2_SB(src3, src3, src3, src3, mask0, mask1, vec0, vec1);
     VSHF_B2_SB(src4, src4, src4, src4, mask0, mask1, vec2, vec3);
     VSHF_B2_SB(src5, src5, src5, src5, mask0, mask1, vec4, vec5);
     VSHF_B2_SB(src6, src6, src6, src6, mask0, mask1, vec6, vec7);
-    dsth3 = HEVC_FILT_4TAP_SH(vec0, vec1, filt0, filt1);
-    dsth4 = HEVC_FILT_4TAP_SH(vec2, vec3, filt0, filt1);
-    dsth5 = HEVC_FILT_4TAP_SH(vec4, vec5, filt0, filt1);
-    dsth6 = HEVC_FILT_4TAP_SH(vec6, vec7, filt0, filt1);
+    ILVRL_B2_SH(zero, vec0, tmp0, tmp1);
+    ILVRL_B2_SH(zero, vec1, tmp2, tmp3);
+    ILVRL_B2_SH(zero, vec2, tmp4, tmp5);
+    ILVRL_B2_SH(zero, vec3, tmp6, tmp7);
+    dst6 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+    dst7 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+    dst8 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+    dst9 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+    ILVEV_H2_SH(dst4, dst6, dst5, dst7, dst32_r, dst32_l);
+    ILVEV_H2_SH(dst6, dst8, dst7, dst9, dst43_r, dst43_l);
+    ILVRL_B2_SH(zero, vec4, tmp0, tmp1);
+    ILVRL_B2_SH(zero, vec5, tmp2, tmp3);
+    ILVRL_B2_SH(zero, vec6, tmp4, tmp5);
+    ILVRL_B2_SH(zero, vec7, tmp6, tmp7);
+    dst0 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+    dst1 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+    dst2 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+    dst3 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+    ILVEV_H2_SH(dst8, dst0, dst9, dst1, dst54_r, dst54_l);
+    ILVEV_H2_SH(dst0, dst2, dst1, dst3, dst65_r, dst65_l);
     VSHF_B2_SB(src7, src7, src7, src7, mask0, mask1, vec0, vec1);
     VSHF_B2_SB(src8, src8, src8, src8, mask0, mask1, vec2, vec3);
     VSHF_B2_SB(src9, src9, src9, src9, mask0, mask1, vec4, vec5);
     VSHF_B2_SB(src10, src10, src10, src10, mask0, mask1, vec6, vec7);
-    dsth7 = HEVC_FILT_4TAP_SH(vec0, vec1, filt0, filt1);
-    dsth8 = HEVC_FILT_4TAP_SH(vec2, vec3, filt0, filt1);
-    dsth9 = HEVC_FILT_4TAP_SH(vec4, vec5, filt0, filt1);
-    dsth10 = HEVC_FILT_4TAP_SH(vec6, vec7, filt0, filt1);
-    ILVRL_H2_SH(dsth3, dsth2, dst32_r, dst32_l);
-    ILVRL_H2_SH(dsth4, dsth3, dst43_r, dst43_l);
-    ILVRL_H2_SH(dsth5, dsth4, dst54_r, dst54_l);
-    ILVRL_H2_SH(dsth6, dsth5, dst65_r, dst65_l);
-    ILVRL_H2_SH(dsth7, dsth6, dst76_r, dst76_l);
-    ILVRL_H2_SH(dsth8, dsth7, dst87_r, dst87_l);
-    ILVRL_H2_SH(dsth9, dsth8, dst98_r, dst98_l);
-    ILVRL_H2_SH(dsth10, dsth9, dst109_r, dst109_l);
+    ILVRL_B2_SH(zero, vec0, tmp0, tmp1);
+    ILVRL_B2_SH(zero, vec1, tmp2, tmp3);
+    ILVRL_B2_SH(zero, vec2, tmp4, tmp5);
+    ILVRL_B2_SH(zero, vec3, tmp6, tmp7);
+    dst4 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+    dst5 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+    dst6 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+    dst7 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+    ILVEV_H2_SH(dst2, dst4, dst3, dst5, dst76_r, dst76_l);
+    ILVEV_H2_SH(dst4, dst6, dst5, dst7, dst87_r, dst87_l);
+    ILVRL_B2_SH(zero, vec4, tmp0, tmp1);
+    ILVRL_B2_SH(zero, vec5, tmp2, tmp3);
+    ILVRL_B2_SH(zero, vec6, tmp4, tmp5);
+    ILVRL_B2_SH(zero, vec7, tmp6, tmp7);
+    dst0 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+    dst1 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+    dst2 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+    dst3 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+    ILVEV_H2_SH(dst6, dst0, dst7, dst1, dst98_r, dst98_l);
+    ILVEV_H2_SH(dst0, dst2, dst1, dst3, dst109_r, dst109_l);
     PCKEV_D2_SH(dst21_l, dst10_l, dst43_l, dst32_l, dst1021_l, dst3243_l);
     PCKEV_D2_SH(dst65_l, dst54_l, dst87_l, dst76_l, dst5465_l, dst7687_l);
     dst98109_l = (v8i16) __msa_pckev_d((v2i64) dst109_l, (v2i64) dst98_l);
@@ -4568,14 +5405,18 @@ static void hevc_hv_uniwgt_4t_6w_msa(uint8_t *src,
     SRAR_W4_SW(dst0_r, dst1_r, dst2_r, dst3_r, rnd_vec);
     SRAR_W4_SW(dst4_r, dst5_r, dst6_r, dst7_r, rnd_vec);
     SRAR_W4_SW(dst0_l, dst1_l, dst2_l, dst3_l, rnd_vec);
+    ADD4(dst0_r, offset_vec, dst1_r, offset_vec, dst2_r, offset_vec, dst3_r,
+         offset_vec, dst0_r, dst1_r, dst2_r, dst3_r);
+    ADD4(dst4_r, offset_vec, dst5_r, offset_vec, dst6_r, offset_vec, dst7_r,
+         offset_vec, dst4_r, dst5_r, dst6_r, dst7_r);
+    ADD4(dst0_l, offset_vec, dst1_l, offset_vec, dst2_l, offset_vec, dst3_l,
+         offset_vec, dst0_l, dst1_l, dst2_l, dst3_l);
+    CLIP_SW8_0_255(dst1_r, dst0_r, dst3_r, dst2_r,
+                   dst4_r, dst5_r, dst6_r, dst7_r);
+    CLIP_SW4_0_255(dst0_l, dst1_l, dst2_l, dst3_l);
     PCKEV_H2_SH(dst1_r, dst0_r, dst3_r, dst2_r, tmp0, tmp1);
     PCKEV_H2_SH(dst5_r, dst4_r, dst7_r, dst6_r, tmp2, tmp3);
     PCKEV_H2_SH(dst1_l, dst0_l, dst3_l, dst2_l, tmp4, tmp5);
-    ADD2(tmp0, offset_vec, tmp1, offset_vec, tmp0, tmp1);
-    ADD2(tmp2, offset_vec, tmp3, offset_vec, tmp2, tmp3);
-    ADD2(tmp4, offset_vec, tmp5, offset_vec, tmp4, tmp5);
-    CLIP_SH4_0_255_MAX_SATU(tmp0, tmp1, tmp2, tmp3);
-    CLIP_SH2_0_255_MAX_SATU(tmp4, tmp5);
     PCKEV_B3_UB(tmp1, tmp0, tmp3, tmp2, tmp5, tmp4, out0, out1, out2);
     ST_W8(out0, out1, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
     ST_H8(out2, 0, 1, 2, 3, 4, 5, 6, 7, dst + 4, dst_stride);
@@ -4598,50 +5439,63 @@ static void hevc_hv_uniwgt_4t_8x2_msa(uint8_t *src,
     v16i8 mask0 = LD_SB(ff_hevc_mask_arr);
     v16i8 mask1;
     v16i8 vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7, vec8, vec9;
-    v8i16 dst0, dst1, dst2, dst3, dst4;
+    v4i32 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    v8i16 tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
     v4i32 dst0_r, dst0_l, dst1_r, dst1_l;
     v8i16 dst10_r, dst32_r, dst21_r, dst43_r;
     v8i16 dst10_l, dst32_l, dst21_l, dst43_l;
-    v8i16 tmp0, tmp1;
-    v8i16 offset_vec, const_128, denom_vec;
-    v4i32 weight_vec, rnd_vec;
+    v4i32 weight_vec, rnd_vec, offset_vec;
+    v8i16 zero = { 0 };
 
     src -= (src_stride + 1);
 
     filter_vec = LD_SH(filter_x);
-    SPLATI_H2_SH(filter_vec, 0, 1, filt0, filt1);
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W2_SH(filter_vec, 0, filt0, filt1);
 
     filter_vec = LD_SH(filter_y);
     UNPCK_R_SB_SH(filter_vec, filter_vec);
-
     SPLATI_W2_SH(filter_vec, 0, filt_h0, filt_h1);
 
     mask1 = mask0 + 2;
 
     weight_vec = __msa_fill_w(weight);
     rnd_vec = __msa_fill_w(rnd_val);
-
-    offset_vec = __msa_fill_h(offset);
-    denom_vec = __msa_fill_h(rnd_val - 6);
-    const_128 = __msa_fill_h((128 * weight));
-    offset_vec += __msa_srar_h(const_128, denom_vec);
+    offset_vec = __msa_fill_w(offset);
 
     LD_SB5(src, src_stride, src0, src1, src2, src3, src4);
-    XORI_B5_128_SB(src0, src1, src2, src3, src4);
     VSHF_B2_SB(src0, src0, src0, src0, mask0, mask1, vec0, vec1);
     VSHF_B2_SB(src1, src1, src1, src1, mask0, mask1, vec2, vec3);
     VSHF_B2_SB(src2, src2, src2, src2, mask0, mask1, vec4, vec5);
     VSHF_B2_SB(src3, src3, src3, src3, mask0, mask1, vec6, vec7);
     VSHF_B2_SB(src4, src4, src4, src4, mask0, mask1, vec8, vec9);
-    dst0 = HEVC_FILT_4TAP_SH(vec0, vec1, filt0, filt1);
-    dst1 = HEVC_FILT_4TAP_SH(vec2, vec3, filt0, filt1);
-    dst2 = HEVC_FILT_4TAP_SH(vec4, vec5, filt0, filt1);
-    dst3 = HEVC_FILT_4TAP_SH(vec6, vec7, filt0, filt1);
-    dst4 = HEVC_FILT_4TAP_SH(vec8, vec9, filt0, filt1);
-    ILVRL_H2_SH(dst1, dst0, dst10_r, dst10_l);
-    ILVRL_H2_SH(dst2, dst1, dst21_r, dst21_l);
-    ILVRL_H2_SH(dst3, dst2, dst32_r, dst32_l);
-    ILVRL_H2_SH(dst4, dst3, dst43_r, dst43_l);
+
+    ILVRL_B2_SH(zero, vec0, tmp0, tmp1);
+    ILVRL_B2_SH(zero, vec1, tmp2, tmp3);
+    ILVRL_B2_SH(zero, vec2, tmp4, tmp5);
+    ILVRL_B2_SH(zero, vec3, tmp6, tmp7);
+    dst0 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+    dst1 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+    dst2 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+    dst3 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+    ILVRL_B2_SH(zero, vec4, tmp0, tmp1);
+    ILVRL_B2_SH(zero, vec5, tmp2, tmp3);
+    ILVRL_B2_SH(zero, vec6, tmp4, tmp5);
+    ILVRL_B2_SH(zero, vec7, tmp6, tmp7);
+    dst4 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+    dst5 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+    dst6 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+    dst7 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+
+    ILVEV_H2_SH(dst0, dst2, dst1, dst3, dst10_r, dst10_l);
+    ILVEV_H2_SH(dst2, dst4, dst3, dst5, dst21_r, dst21_l);
+    ILVEV_H2_SH(dst4, dst6, dst5, dst7, dst32_r, dst32_l);
+    ILVRL_B2_SH(zero, vec8, tmp0, tmp1);
+    ILVRL_B2_SH(zero, vec9, tmp2, tmp3);
+    dst0 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+    dst1 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+    ILVEV_H2_SH(dst6, dst0, dst7, dst1, dst43_r, dst43_l);
+
     dst0_r = HEVC_FILT_4TAP(dst10_r, dst32_r, filt_h0, filt_h1);
     dst0_l = HEVC_FILT_4TAP(dst10_l, dst32_l, filt_h0, filt_h1);
     dst1_r = HEVC_FILT_4TAP(dst21_r, dst43_r, filt_h0, filt_h1);
@@ -4650,9 +5504,10 @@ static void hevc_hv_uniwgt_4t_8x2_msa(uint8_t *src,
     MUL2(dst0_r, weight_vec, dst1_r, weight_vec, dst0_r, dst1_r);
     MUL2(dst0_l, weight_vec, dst1_l, weight_vec, dst0_l, dst1_l);
     SRAR_W4_SW(dst0_r, dst0_l, dst1_r, dst1_l, rnd_vec);
+    ADD4(dst0_r, offset_vec, dst0_l, offset_vec, dst1_r, offset_vec,
+         dst1_l, offset_vec, dst0_r, dst0_l, dst1_r, dst1_l);
+    CLIP_SW4_0_255(dst0_r, dst0_l, dst1_r, dst1_l);
     PCKEV_H2_SH(dst0_l, dst0_r, dst1_l, dst1_r, tmp0, tmp1);
-    ADD2(tmp0, offset_vec, tmp1, offset_vec, tmp0, tmp1);
-    CLIP_SH2_0_255_MAX_SATU(tmp0, tmp1);
     out = (v16u8) __msa_pckev_b((v16i8) tmp1, (v16i8) tmp0);
     ST_D2(out, 0, 1, dst, dst_stride);
 }
@@ -4673,21 +5528,22 @@ static void hevc_hv_uniwgt_4t_8multx4_msa(uint8_t *src,
     v16i8 src0, src1, src2, src3, src4, src5, src6, mask0, mask1;
     v16i8 vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
     v8i16 filt0, filt1, filt_h0, filt_h1, filter_vec;
-    v8i16 dst0, dst1, dst2, dst3, dst4, dst5, dst6, tmp0, tmp1, tmp2, tmp3;
+    v4i32 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    v8i16 tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
     v8i16 dst10_r, dst32_r, dst54_r, dst21_r, dst43_r, dst65_r;
     v8i16 dst10_l, dst32_l, dst54_l, dst21_l, dst43_l, dst65_l;
-    v8i16 offset_vec, const_128, denom_vec;
     v4i32 dst0_r, dst0_l, dst1_r, dst1_l, dst2_r, dst2_l, dst3_r, dst3_l;
-    v4i32 weight_vec, rnd_vec;
+    v4i32 weight_vec, rnd_vec, offset_vec;
+    v8i16 zero = { 0 };
 
     src -= (src_stride + 1);
 
     filter_vec = LD_SH(filter_x);
-    SPLATI_H2_SH(filter_vec, 0, 1, filt0, filt1);
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W2_SH(filter_vec, 0, filt0, filt1);
 
     filter_vec = LD_SH(filter_y);
     UNPCK_R_SB_SH(filter_vec, filter_vec);
-
     SPLATI_W2_SH(filter_vec, 0, filt_h0, filt_h1);
 
     mask0 = LD_SB(ff_hevc_mask_arr);
@@ -4695,36 +5551,53 @@ static void hevc_hv_uniwgt_4t_8multx4_msa(uint8_t *src,
 
     weight_vec = __msa_fill_w(weight);
     rnd_vec = __msa_fill_w(rnd_val);
-
-    offset_vec = __msa_fill_h(offset);
-    denom_vec = __msa_fill_h(rnd_val - 6);
-    const_128 = __msa_fill_h((128 * weight));
-    offset_vec += __msa_srar_h(const_128, denom_vec);
+    offset_vec = __msa_fill_w(offset);
 
     for (cnt = width8mult; cnt--;) {
         LD_SB7(src, src_stride, src0, src1, src2, src3, src4, src5, src6);
         src += 8;
-        XORI_B7_128_SB(src0, src1, src2, src3, src4, src5, src6);
         VSHF_B2_SB(src0, src0, src0, src0, mask0, mask1, vec0, vec1);
         VSHF_B2_SB(src1, src1, src1, src1, mask0, mask1, vec2, vec3);
         VSHF_B2_SB(src2, src2, src2, src2, mask0, mask1, vec4, vec5);
-        dst0 = HEVC_FILT_4TAP_SH(vec0, vec1, filt0, filt1);
-        dst1 = HEVC_FILT_4TAP_SH(vec2, vec3, filt0, filt1);
-        dst2 = HEVC_FILT_4TAP_SH(vec4, vec5, filt0, filt1);
-        ILVRL_H2_SH(dst1, dst0, dst10_r, dst10_l);
-        ILVRL_H2_SH(dst2, dst1, dst21_r, dst21_l);
+        ILVRL_B2_SH(zero, vec0, tmp0, tmp1);
+        ILVRL_B2_SH(zero, vec1, tmp2, tmp3);
+        ILVRL_B2_SH(zero, vec2, tmp4, tmp5);
+        ILVRL_B2_SH(zero, vec3, tmp6, tmp7);
+        dst0 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+        dst1 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+        dst2 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+        dst3 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+        ILVRL_B2_SH(zero, vec4, tmp0, tmp1);
+        ILVRL_B2_SH(zero, vec5, tmp2, tmp3);
+        dst4 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+        dst5 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+        ILVEV_H2_SH(dst0, dst2, dst1, dst3, dst10_r, dst10_l);
+        ILVEV_H2_SH(dst2, dst4, dst3, dst5, dst21_r, dst21_l);
         VSHF_B2_SB(src3, src3, src3, src3, mask0, mask1, vec0, vec1);
         VSHF_B2_SB(src4, src4, src4, src4, mask0, mask1, vec2, vec3);
         VSHF_B2_SB(src5, src5, src5, src5, mask0, mask1, vec4, vec5);
         VSHF_B2_SB(src6, src6, src6, src6, mask0, mask1, vec6, vec7);
-        dst3 = HEVC_FILT_4TAP_SH(vec0, vec1, filt0, filt1);
-        dst4 = HEVC_FILT_4TAP_SH(vec2, vec3, filt0, filt1);
-        dst5 = HEVC_FILT_4TAP_SH(vec4, vec5, filt0, filt1);
-        dst6 = HEVC_FILT_4TAP_SH(vec6, vec7, filt0, filt1);
-        ILVRL_H2_SH(dst3, dst2, dst32_r, dst32_l);
-        ILVRL_H2_SH(dst4, dst3, dst43_r, dst43_l);
-        ILVRL_H2_SH(dst5, dst4, dst54_r, dst54_l);
-        ILVRL_H2_SH(dst6, dst5, dst65_r, dst65_l);
+        ILVRL_B2_SH(zero, vec0, tmp0, tmp1);
+        ILVRL_B2_SH(zero, vec1, tmp2, tmp3);
+        ILVRL_B2_SH(zero, vec2, tmp4, tmp5);
+        ILVRL_B2_SH(zero, vec3, tmp6, tmp7);
+        dst0 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+        dst1 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+        dst2 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+        dst3 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+        ILVEV_H2_SH(dst4, dst0, dst5, dst1, dst32_r, dst32_l);
+        ILVEV_H2_SH(dst0, dst2, dst1, dst3, dst43_r, dst43_l);
+        ILVRL_B2_SH(zero, vec4, tmp0, tmp1);
+        ILVRL_B2_SH(zero, vec5, tmp2, tmp3);
+        ILVRL_B2_SH(zero, vec6, tmp4, tmp5);
+        ILVRL_B2_SH(zero, vec7, tmp6, tmp7);
+        dst4 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+        dst5 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+        dst6 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+        dst7 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+        ILVEV_H2_SH(dst2, dst4, dst3, dst5, dst54_r, dst54_l);
+        ILVEV_H2_SH(dst4, dst6, dst5, dst7, dst65_r, dst65_l);
+
         dst0_r = HEVC_FILT_4TAP(dst10_r, dst32_r, filt_h0, filt_h1);
         dst0_l = HEVC_FILT_4TAP(dst10_l, dst32_l, filt_h0, filt_h1);
         dst1_r = HEVC_FILT_4TAP(dst21_r, dst43_r, filt_h0, filt_h1);
@@ -4741,11 +5614,14 @@ static void hevc_hv_uniwgt_4t_8multx4_msa(uint8_t *src,
         MUL2(dst2_l, weight_vec, dst3_l, weight_vec, dst2_l, dst3_l);
         SRAR_W4_SW(dst0_r, dst0_l, dst1_r, dst1_l, rnd_vec);
         SRAR_W4_SW(dst2_r, dst2_l, dst3_r, dst3_l, rnd_vec);
+        ADD2(dst0_r, offset_vec, dst1_r, offset_vec, dst0_r, dst1_r);
+        ADD2(dst2_r, offset_vec, dst3_r, offset_vec, dst2_r, dst3_r);
+        ADD2(dst0_l, offset_vec, dst1_l, offset_vec, dst0_l, dst1_l);
+        ADD2(dst2_l, offset_vec, dst3_l, offset_vec, dst2_l, dst3_l);
+        CLIP_SW8_0_255(dst0_r, dst0_l, dst1_r, dst1_l,
+                       dst2_r, dst2_l, dst3_r, dst3_l);
         PCKEV_H4_SH(dst0_l, dst0_r, dst1_l, dst1_r, dst2_l, dst2_r, dst3_l,
                     dst3_r, tmp0, tmp1, tmp2, tmp3);
-        ADD2(tmp0, offset_vec, tmp1, offset_vec, tmp0, tmp1);
-        ADD2(tmp2, offset_vec, tmp3, offset_vec, tmp2, tmp3);
-        CLIP_SH4_0_255_MAX_SATU(tmp0, tmp1, tmp2, tmp3);
         PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out0, out1);
         ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
         dst += 8;
@@ -4770,41 +5646,35 @@ static void hevc_hv_uniwgt_4t_8x6_msa(uint8_t *src,
     v16i8 mask1;
     v16i8 vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7, vec8, vec9;
     v16i8 vec10, vec11, vec12, vec13, vec14, vec15, vec16, vec17;
-    v8i16 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7, dst8;
+    v4i32 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
     v4i32 dst0_r, dst0_l, dst1_r, dst1_l, dst2_r, dst2_l, dst3_r, dst3_l;
-    v4i32 dst4_r, dst4_l, dst5_r, dst5_l, weight_vec, rnd_vec;
+    v4i32 dst4_r, dst4_l, dst5_r, dst5_l, weight_vec, rnd_vec, offset_vec;
     v8i16 dst10_r, dst32_r, dst10_l, dst32_l;
     v8i16 dst21_r, dst43_r, dst21_l, dst43_l;
     v8i16 dst54_r, dst54_l, dst65_r, dst65_l;
     v8i16 dst76_r, dst76_l, dst87_r, dst87_l;
-    v8i16 tmp0, tmp1, tmp2, tmp3, tmp4, tmp5;
-    v8i16 offset_vec, const_128, denom_vec;
+    v8i16 tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
+    v8i16 zero = { 0 };
 
     src -= (src_stride + 1);
 
     filter_vec = LD_SH(filter_x);
-    SPLATI_H2_SH(filter_vec, 0, 1, filt0, filt1);
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W2_SH(filter_vec, 0, filt0, filt1);
 
     filter_vec = LD_SH(filter_y);
     UNPCK_R_SB_SH(filter_vec, filter_vec);
-
     SPLATI_W2_SH(filter_vec, 0, filt_h0, filt_h1);
 
     mask1 = mask0 + 2;
 
     weight_vec = __msa_fill_w(weight);
     rnd_vec = __msa_fill_w(rnd_val);
-
-    offset_vec = __msa_fill_h(offset);
-    denom_vec = __msa_fill_h(rnd_val - 6);
-    const_128 = __msa_fill_h((128 * weight));
-    offset_vec += __msa_srar_h(const_128, denom_vec);
+    offset_vec = __msa_fill_w(offset);
 
     LD_SB5(src, src_stride, src0, src1, src2, src3, src4);
     src += (5 * src_stride);
     LD_SB4(src, src_stride, src5, src6, src7, src8);
-    XORI_B5_128_SB(src0, src1, src2, src3, src4);
-    XORI_B4_128_SB(src5, src6, src7, src8);
     VSHF_B2_SB(src0, src0, src0, src0, mask0, mask1, vec0, vec1);
     VSHF_B2_SB(src1, src1, src1, src1, mask0, mask1, vec2, vec3);
     VSHF_B2_SB(src2, src2, src2, src2, mask0, mask1, vec4, vec5);
@@ -4814,23 +5684,51 @@ static void hevc_hv_uniwgt_4t_8x6_msa(uint8_t *src,
     VSHF_B2_SB(src6, src6, src6, src6, mask0, mask1, vec12, vec13);
     VSHF_B2_SB(src7, src7, src7, src7, mask0, mask1, vec14, vec15);
     VSHF_B2_SB(src8, src8, src8, src8, mask0, mask1, vec16, vec17);
-    dst0 = HEVC_FILT_4TAP_SH(vec0, vec1, filt0, filt1);
-    dst1 = HEVC_FILT_4TAP_SH(vec2, vec3, filt0, filt1);
-    dst2 = HEVC_FILT_4TAP_SH(vec4, vec5, filt0, filt1);
-    dst3 = HEVC_FILT_4TAP_SH(vec6, vec7, filt0, filt1);
-    dst4 = HEVC_FILT_4TAP_SH(vec8, vec9, filt0, filt1);
-    dst5 = HEVC_FILT_4TAP_SH(vec10, vec11, filt0, filt1);
-    dst6 = HEVC_FILT_4TAP_SH(vec12, vec13, filt0, filt1);
-    dst7 = HEVC_FILT_4TAP_SH(vec14, vec15, filt0, filt1);
-    dst8 = HEVC_FILT_4TAP_SH(vec16, vec17, filt0, filt1);
-    ILVRL_H2_SH(dst1, dst0, dst10_r, dst10_l);
-    ILVRL_H2_SH(dst2, dst1, dst21_r, dst21_l);
-    ILVRL_H2_SH(dst3, dst2, dst32_r, dst32_l);
-    ILVRL_H2_SH(dst4, dst3, dst43_r, dst43_l);
-    ILVRL_H2_SH(dst5, dst4, dst54_r, dst54_l);
-    ILVRL_H2_SH(dst6, dst5, dst65_r, dst65_l);
-    ILVRL_H2_SH(dst7, dst6, dst76_r, dst76_l);
-    ILVRL_H2_SH(dst8, dst7, dst87_r, dst87_l);
+
+    ILVRL_B2_SH(zero, vec0, tmp0, tmp1);
+    ILVRL_B2_SH(zero, vec1, tmp2, tmp3);
+    ILVRL_B2_SH(zero, vec2, tmp4, tmp5);
+    ILVRL_B2_SH(zero, vec3, tmp6, tmp7);
+    dst0 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+    dst1 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+    dst2 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+    dst3 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+    ILVRL_B2_SH(zero, vec4, tmp0, tmp1);
+    ILVRL_B2_SH(zero, vec5, tmp2, tmp3);
+    ILVRL_B2_SH(zero, vec6, tmp4, tmp5);
+    ILVRL_B2_SH(zero, vec7, tmp6, tmp7);
+    dst4 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+    dst5 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+    dst6 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+    dst7 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+    ILVEV_H2_SH(dst0, dst2, dst1, dst3, dst10_r, dst10_l);
+    ILVEV_H2_SH(dst2, dst4, dst3, dst5, dst21_r, dst21_l);
+    ILVEV_H2_SH(dst4, dst6, dst5, dst7, dst32_r, dst32_l);
+    ILVRL_B2_SH(zero, vec8, tmp0, tmp1);
+    ILVRL_B2_SH(zero, vec9, tmp2, tmp3);
+    ILVRL_B2_SH(zero, vec10, tmp4, tmp5);
+    ILVRL_B2_SH(zero, vec11, tmp6, tmp7);
+    dst0 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+    dst1 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+    dst2 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+    dst3 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+    ILVEV_H2_SH(dst6, dst0, dst7, dst1, dst43_r, dst43_l);
+    ILVEV_H2_SH(dst0, dst2, dst1, dst3, dst54_r, dst54_l);
+    ILVRL_B2_SH(zero, vec12, tmp0, tmp1);
+    ILVRL_B2_SH(zero, vec13, tmp2, tmp3);
+    ILVRL_B2_SH(zero, vec14, tmp4, tmp5);
+    ILVRL_B2_SH(zero, vec15, tmp6, tmp7);
+    dst4 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+    dst5 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+    dst6 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+    dst7 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+    ILVEV_H2_SH(dst2, dst4, dst3, dst5, dst65_r, dst65_l);
+    ILVEV_H2_SH(dst4, dst6, dst5, dst7, dst76_r, dst76_l);
+    ILVRL_B2_SH(zero, vec16, tmp0, tmp1);
+    ILVRL_B2_SH(zero, vec17, tmp2, tmp3);
+    dst0 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+    dst1 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+    ILVEV_H2_SH(dst6, dst0, dst7, dst1, dst87_r, dst87_l);
     dst0_r = HEVC_FILT_4TAP(dst10_r, dst32_r, filt_h0, filt_h1);
     dst0_l = HEVC_FILT_4TAP(dst10_l, dst32_l, filt_h0, filt_h1);
     dst1_r = HEVC_FILT_4TAP(dst21_r, dst43_r, filt_h0, filt_h1);
@@ -4855,14 +5753,18 @@ static void hevc_hv_uniwgt_4t_8x6_msa(uint8_t *src,
     SRAR_W4_SW(dst0_r, dst0_l, dst1_r, dst1_l, rnd_vec);
     SRAR_W4_SW(dst2_r, dst2_l, dst3_r, dst3_l, rnd_vec);
     SRAR_W4_SW(dst4_r, dst4_l, dst5_r, dst5_l, rnd_vec);
+    ADD2(dst0_r, offset_vec, dst1_r, offset_vec, dst0_r, dst1_r);
+    ADD2(dst2_r, offset_vec, dst3_r, offset_vec, dst2_r, dst3_r);
+    ADD2(dst4_r, offset_vec, dst5_r, offset_vec, dst4_r, dst5_r);
+    ADD2(dst0_l, offset_vec, dst1_l, offset_vec, dst0_l, dst1_l);
+    ADD2(dst2_l, offset_vec, dst3_l, offset_vec, dst2_l, dst3_l);
+    ADD2(dst4_l, offset_vec, dst5_l, offset_vec, dst4_l, dst5_l);
+    CLIP_SW8_0_255(dst0_r, dst1_r, dst2_r, dst3_r,
+                   dst4_r, dst5_r, dst0_l, dst1_l);
+    CLIP_SW4_0_255(dst2_l, dst3_l, dst4_l, dst5_l);
     PCKEV_H4_SH(dst0_l, dst0_r, dst1_l, dst1_r, dst2_l, dst2_r, dst3_l, dst3_r,
                 tmp0, tmp1, tmp2, tmp3);
     PCKEV_H2_SH(dst4_l, dst4_r, dst5_l, dst5_r, tmp4, tmp5);
-    ADD2(tmp0, offset_vec, tmp1, offset_vec, tmp0, tmp1);
-    ADD2(tmp2, offset_vec, tmp3, offset_vec, tmp2, tmp3);
-    ADD2(tmp4, offset_vec, tmp5, offset_vec, tmp4, tmp5);
-    CLIP_SH4_0_255_MAX_SATU(tmp0, tmp1, tmp2, tmp3);
-    CLIP_SH2_0_255_MAX_SATU(tmp4, tmp5);
     PCKEV_B3_UB(tmp1, tmp0, tmp3, tmp2, tmp5, tmp4, out0, out1, out2);
     ST_D4(out0, out1, 0, 1, 0, 1, dst, dst_stride);
     ST_D2(out2, 0, 1, dst + 4 * dst_stride, dst_stride);
@@ -4890,33 +5792,30 @@ static void hevc_hv_uniwgt_4t_8multx4mult_msa(uint8_t *src,
     v16i8 mask0 = LD_SB(ff_hevc_mask_arr);
     v16i8 mask1;
     v16i8 vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
-    v8i16 dst0, dst1, dst2, dst3, dst4, dst5, dst6, tmp0, tmp1, tmp2, tmp3;
+    v4i32 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    v8i16 tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
     v8i16 dst10_r, dst32_r, dst54_r, dst21_r, dst43_r, dst65_r;
     v8i16 dst10_l, dst32_l, dst54_l, dst21_l, dst43_l, dst65_l;
     v4i32 dst0_r, dst0_l, dst1_r, dst1_l;
-    v8i16 offset_vec, const_128, denom_vec;
     v4i32 dst2_r, dst2_l, dst3_r, dst3_l;
-    v4i32 weight_vec, rnd_vec;
+    v4i32 weight_vec, rnd_vec, offset_vec;
+    v8i16 zero = { 0 };
 
     src -= (src_stride + 1);
 
     filter_vec = LD_SH(filter_x);
-    SPLATI_H2_SH(filter_vec, 0, 1, filt0, filt1);
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W2_SH(filter_vec, 0, filt0, filt1);
 
     filter_vec = LD_SH(filter_y);
     UNPCK_R_SB_SH(filter_vec, filter_vec);
-
     SPLATI_W2_SH(filter_vec, 0, filt_h0, filt_h1);
 
     mask1 = mask0 + 2;
 
     weight_vec = __msa_fill_w(weight);
     rnd_vec = __msa_fill_w(rnd_val);
-
-    offset_vec = __msa_fill_h(offset);
-    denom_vec = __msa_fill_h(rnd_val - 6);
-    const_128 = __msa_fill_h((128 * weight));
-    offset_vec += __msa_srar_h(const_128, denom_vec);
+    offset_vec = __msa_fill_w(offset);
 
     for (cnt = width8mult; cnt--;) {
         src_tmp = src;
@@ -4924,35 +5823,55 @@ static void hevc_hv_uniwgt_4t_8multx4mult_msa(uint8_t *src,
 
         LD_SB3(src_tmp, src_stride, src0, src1, src2);
         src_tmp += (3 * src_stride);
-        XORI_B3_128_SB(src0, src1, src2);
 
         VSHF_B2_SB(src0, src0, src0, src0, mask0, mask1, vec0, vec1);
         VSHF_B2_SB(src1, src1, src1, src1, mask0, mask1, vec2, vec3);
         VSHF_B2_SB(src2, src2, src2, src2, mask0, mask1, vec4, vec5);
-        dst0 = HEVC_FILT_4TAP_SH(vec0, vec1, filt0, filt1);
-        dst1 = HEVC_FILT_4TAP_SH(vec2, vec3, filt0, filt1);
-        dst2 = HEVC_FILT_4TAP_SH(vec4, vec5, filt0, filt1);
 
-        ILVRL_H2_SH(dst1, dst0, dst10_r, dst10_l);
-        ILVRL_H2_SH(dst2, dst1, dst21_r, dst21_l);
+        ILVRL_B2_SH(zero, vec0, tmp0, tmp1);
+        ILVRL_B2_SH(zero, vec1, tmp2, tmp3);
+        ILVRL_B2_SH(zero, vec2, tmp4, tmp5);
+        ILVRL_B2_SH(zero, vec3, tmp6, tmp7);
+        dst0 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+        dst1 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+        dst2 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+        dst3 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+        ILVRL_B2_SH(zero, vec4, tmp0, tmp1);
+        ILVRL_B2_SH(zero, vec5, tmp2, tmp3);
+        dst4 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+        dst5 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+        ILVEV_H2_SH(dst0, dst2, dst1, dst3, dst10_r, dst10_l);
+        ILVEV_H2_SH(dst2, dst4, dst3, dst5, dst21_r, dst21_l);
 
         for (loop_cnt = height >> 2; loop_cnt--;) {
             LD_SB4(src_tmp, src_stride, src3, src4, src5, src6);
             src_tmp += (4 * src_stride);
-            XORI_B4_128_SB(src3, src4, src5, src6);
 
             VSHF_B2_SB(src3, src3, src3, src3, mask0, mask1, vec0, vec1);
             VSHF_B2_SB(src4, src4, src4, src4, mask0, mask1, vec2, vec3);
             VSHF_B2_SB(src5, src5, src5, src5, mask0, mask1, vec4, vec5);
             VSHF_B2_SB(src6, src6, src6, src6, mask0, mask1, vec6, vec7);
-            dst3 = HEVC_FILT_4TAP_SH(vec0, vec1, filt0, filt1);
-            dst4 = HEVC_FILT_4TAP_SH(vec2, vec3, filt0, filt1);
-            dst5 = HEVC_FILT_4TAP_SH(vec4, vec5, filt0, filt1);
-            dst6 = HEVC_FILT_4TAP_SH(vec6, vec7, filt0, filt1);
-            ILVRL_H2_SH(dst3, dst2, dst32_r, dst32_l);
-            ILVRL_H2_SH(dst4, dst3, dst43_r, dst43_l);
-            ILVRL_H2_SH(dst5, dst4, dst54_r, dst54_l);
-            ILVRL_H2_SH(dst6, dst5, dst65_r, dst65_l);
+
+            ILVRL_B2_SH(zero, vec0, tmp0, tmp1);
+            ILVRL_B2_SH(zero, vec1, tmp2, tmp3);
+            ILVRL_B2_SH(zero, vec2, tmp4, tmp5);
+            ILVRL_B2_SH(zero, vec3, tmp6, tmp7);
+            dst0 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+            dst1 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+            dst2 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+            dst3 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+            ILVEV_H2_SH(dst4, dst0, dst5, dst1, dst32_r, dst32_l);
+            ILVEV_H2_SH(dst0, dst2, dst1, dst3, dst43_r, dst43_l);
+            ILVRL_B2_SH(zero, vec4, tmp0, tmp1);
+            ILVRL_B2_SH(zero, vec5, tmp2, tmp3);
+            ILVRL_B2_SH(zero, vec6, tmp4, tmp5);
+            ILVRL_B2_SH(zero, vec7, tmp6, tmp7);
+            dst4 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+            dst5 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+            dst6 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+            dst7 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+            ILVEV_H2_SH(dst2, dst4, dst3, dst5, dst54_r, dst54_l);
+            ILVEV_H2_SH(dst4, dst6, dst5, dst7, dst65_r, dst65_l);
             dst0_r = HEVC_FILT_4TAP(dst10_r, dst32_r, filt_h0, filt_h1);
             dst0_l = HEVC_FILT_4TAP(dst10_l, dst32_l, filt_h0, filt_h1);
             dst1_r = HEVC_FILT_4TAP(dst21_r, dst43_r, filt_h0, filt_h1);
@@ -4969,11 +5888,14 @@ static void hevc_hv_uniwgt_4t_8multx4mult_msa(uint8_t *src,
             MUL2(dst2_l, weight_vec, dst3_l, weight_vec, dst2_l, dst3_l);
             SRAR_W4_SW(dst0_r, dst0_l, dst1_r, dst1_l, rnd_vec);
             SRAR_W4_SW(dst2_r, dst2_l, dst3_r, dst3_l, rnd_vec);
+            ADD2(dst0_r, offset_vec, dst1_r, offset_vec, dst0_r, dst1_r);
+            ADD2(dst2_r, offset_vec, dst3_r, offset_vec, dst2_r, dst3_r);
+            ADD2(dst0_l, offset_vec, dst1_l, offset_vec, dst0_l, dst1_l);
+            ADD2(dst2_l, offset_vec, dst3_l, offset_vec, dst2_l, dst3_l);
+            CLIP_SW8_0_255(dst0_r, dst0_l, dst1_r, dst1_l,
+                           dst2_r, dst2_l, dst3_r, dst3_l);
             PCKEV_H4_SH(dst0_l, dst0_r, dst1_l, dst1_r, dst2_l, dst2_r, dst3_l,
                         dst3_r, tmp0, tmp1, tmp2, tmp3);
-            ADD2(tmp0, offset_vec, tmp1, offset_vec, tmp0, tmp1);
-            ADD2(tmp2, offset_vec, tmp3, offset_vec, tmp2, tmp3);
-            CLIP_SH4_0_255_MAX_SATU(tmp0, tmp1, tmp2, tmp3);
             PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out0, out1);
             ST_D4(out0, out1, 0, 1, 0, 1, dst_tmp, dst_stride);
             dst_tmp += (4 * dst_stride);
@@ -4982,7 +5904,8 @@ static void hevc_hv_uniwgt_4t_8multx4mult_msa(uint8_t *src,
             dst10_l = dst54_l;
             dst21_r = dst65_r;
             dst21_l = dst65_l;
-            dst2 = dst6;
+            dst4 = dst6;
+            dst5 = dst7;
         }
 
         src += 8;
@@ -5038,24 +5961,25 @@ static void hevc_hv_uniwgt_4t_12w_msa(uint8_t *src,
     v16i8 src0, src1, src2, src3, src4, src5, src6, src7, src8, src9, src10;
     v16i8 vec0, vec1, vec2, vec3, vec4, vec5, vec6, vec7;
     v16i8 mask0, mask1, mask2, mask3;
-    v8i16 filt0, filt1, filt_h0, filt_h1, filter_vec, tmp0, tmp1, tmp2, tmp3;
-    v8i16 dsth0, dsth1, dsth2, dsth3, dsth4, dsth5, dsth6;
-    v8i16 dst10, dst21, dst22, dst73, dst84, dst95, dst106;
+    v8i16 filt0, filt1, filt_h0, filt_h1, filter_vec;
+    v4i32 dsth0, dsth1, dsth2, dsth3, dsth4, dsth5, dsth6, dsth7;
     v8i16 dst76_r, dst98_r, dst87_r, dst109_r;
     v8i16 dst10_r, dst32_r, dst54_r, dst21_r, dst43_r, dst65_r;
     v8i16 dst10_l, dst32_l, dst54_l, dst21_l, dst43_l, dst65_l;
-    v8i16 offset_vec, const_128, denom_vec;
     v4i32 dst0_r, dst0_l, dst1_r, dst1_l, dst2_r, dst2_l, dst3_r, dst3_l;
     v4i32 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7, weight_vec, rnd_vec;
+    v4i32 dst8, dst9, dst10, offset_vec;
+    v8i16 tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
+    v8i16 zero = { 0 };
 
     src -= (src_stride + 1);
 
     filter_vec = LD_SH(filter_x);
-    SPLATI_H2_SH(filter_vec, 0, 1, filt0, filt1);
+    UNPCK_R_SB_SH(filter_vec, filter_vec);
+    SPLATI_W2_SH(filter_vec, 0, filt0, filt1);
 
     filter_vec = LD_SH(filter_y);
     UNPCK_R_SB_SH(filter_vec, filter_vec);
-
     SPLATI_W2_SH(filter_vec, 0, filt_h0, filt_h1);
 
     mask0 = LD_SB(ff_hevc_mask_arr);
@@ -5063,43 +5987,61 @@ static void hevc_hv_uniwgt_4t_12w_msa(uint8_t *src,
 
     weight_vec = __msa_fill_w(weight);
     rnd_vec = __msa_fill_w(rnd_val);
-
-    offset_vec = __msa_fill_h(offset);
-    denom_vec = __msa_fill_h(rnd_val - 6);
-    const_128 = __msa_fill_h((128 * weight));
-    offset_vec += __msa_srar_h(const_128, denom_vec);
+    offset_vec = __msa_fill_w(offset);
 
     src_tmp = src;
     dst_tmp = dst;
 
     LD_SB3(src_tmp, src_stride, src0, src1, src2);
     src_tmp += (3 * src_stride);
-    XORI_B3_128_SB(src0, src1, src2);
     VSHF_B2_SB(src0, src0, src0, src0, mask0, mask1, vec0, vec1);
     VSHF_B2_SB(src1, src1, src1, src1, mask0, mask1, vec2, vec3);
     VSHF_B2_SB(src2, src2, src2, src2, mask0, mask1, vec4, vec5);
-    dsth0 = HEVC_FILT_4TAP_SH(vec0, vec1, filt0, filt1);
-    dsth1 = HEVC_FILT_4TAP_SH(vec2, vec3, filt0, filt1);
-    dsth2 = HEVC_FILT_4TAP_SH(vec4, vec5, filt0, filt1);
-    ILVRL_H2_SH(dsth1, dsth0, dst10_r, dst10_l);
-    ILVRL_H2_SH(dsth2, dsth1, dst21_r, dst21_l);
+
+    ILVRL_B2_SH(zero, vec0, tmp0, tmp1);
+    ILVRL_B2_SH(zero, vec1, tmp2, tmp3);
+    ILVRL_B2_SH(zero, vec2, tmp4, tmp5);
+    ILVRL_B2_SH(zero, vec3, tmp6, tmp7);
+    dsth0 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+    dsth1 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+    dsth2 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+    dsth3 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+    ILVRL_B2_SH(zero, vec4, tmp0, tmp1);
+    ILVRL_B2_SH(zero, vec5, tmp2, tmp3);
+    dsth4 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+    dsth5 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+    ILVEV_H2_SH(dsth0, dsth2, dsth1, dsth3, dst10_r, dst10_l);
+    ILVEV_H2_SH(dsth2, dsth4, dsth3, dsth5, dst21_r, dst21_l);
 
     for (loop_cnt = 4; loop_cnt--;) {
         LD_SB4(src_tmp, src_stride, src3, src4, src5, src6);
         src_tmp += (4 * src_stride);
-        XORI_B4_128_SB(src3, src4, src5, src6);
         VSHF_B2_SB(src3, src3, src3, src3, mask0, mask1, vec0, vec1);
         VSHF_B2_SB(src4, src4, src4, src4, mask0, mask1, vec2, vec3);
         VSHF_B2_SB(src5, src5, src5, src5, mask0, mask1, vec4, vec5);
         VSHF_B2_SB(src6, src6, src6, src6, mask0, mask1, vec6, vec7);
-        dsth3 = HEVC_FILT_4TAP_SH(vec0, vec1, filt0, filt1);
-        dsth4 = HEVC_FILT_4TAP_SH(vec2, vec3, filt0, filt1);
-        dsth5 = HEVC_FILT_4TAP_SH(vec4, vec5, filt0, filt1);
-        dsth6 = HEVC_FILT_4TAP_SH(vec6, vec7, filt0, filt1);
-        ILVRL_H2_SH(dsth3, dsth2, dst32_r, dst32_l);
-        ILVRL_H2_SH(dsth4, dsth3, dst43_r, dst43_l);
-        ILVRL_H2_SH(dsth5, dsth4, dst54_r, dst54_l);
-        ILVRL_H2_SH(dsth6, dsth5, dst65_r, dst65_l);
+
+        ILVRL_B2_SH(zero, vec0, tmp0, tmp1);
+        ILVRL_B2_SH(zero, vec1, tmp2, tmp3);
+        ILVRL_B2_SH(zero, vec2, tmp4, tmp5);
+        ILVRL_B2_SH(zero, vec3, tmp6, tmp7);
+        dsth0 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+        dsth1 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+        dsth2 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+        dsth3 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+        ILVEV_H2_SH(dsth4, dsth0, dsth5, dsth1, dst32_r, dst32_l);
+        ILVEV_H2_SH(dsth0, dsth2, dsth1, dsth3, dst43_r, dst43_l);
+        ILVRL_B2_SH(zero, vec4, tmp0, tmp1);
+        ILVRL_B2_SH(zero, vec5, tmp2, tmp3);
+        ILVRL_B2_SH(zero, vec6, tmp4, tmp5);
+        ILVRL_B2_SH(zero, vec7, tmp6, tmp7);
+        dsth4 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+        dsth5 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+        dsth6 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+        dsth7 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+        ILVEV_H2_SH(dsth2, dsth4, dsth3, dsth5, dst54_r, dst54_l);
+        ILVEV_H2_SH(dsth4, dsth6, dsth5, dsth7, dst65_r, dst65_l);
+
         dst0_r = HEVC_FILT_4TAP(dst10_r, dst32_r, filt_h0, filt_h1);
         dst0_l = HEVC_FILT_4TAP(dst10_l, dst32_l, filt_h0, filt_h1);
         dst1_r = HEVC_FILT_4TAP(dst21_r, dst43_r, filt_h0, filt_h1);
@@ -5116,11 +6058,14 @@ static void hevc_hv_uniwgt_4t_12w_msa(uint8_t *src,
         MUL2(dst2_l, weight_vec, dst3_l, weight_vec, dst2_l, dst3_l);
         SRAR_W4_SW(dst0_r, dst0_l, dst1_r, dst1_l, rnd_vec);
         SRAR_W4_SW(dst2_r, dst2_l, dst3_r, dst3_l, rnd_vec);
+        ADD2(dst0_r, offset_vec, dst1_r, offset_vec, dst0_r, dst1_r);
+        ADD2(dst2_r, offset_vec, dst3_r, offset_vec, dst2_r, dst3_r);
+        ADD2(dst0_l, offset_vec, dst1_l, offset_vec, dst0_l, dst1_l);
+        ADD2(dst2_l, offset_vec, dst3_l, offset_vec, dst2_l, dst3_l);
+        CLIP_SW8_0_255(dst0_r, dst0_l, dst1_r, dst1_l,
+                       dst2_r, dst2_l, dst3_r, dst3_l);
         PCKEV_H4_SH(dst0_l, dst0_r, dst1_l, dst1_r, dst2_l, dst2_r, dst3_l,
                     dst3_r, tmp0, tmp1, tmp2, tmp3);
-        ADD2(tmp0, offset_vec, tmp1, offset_vec, tmp0, tmp1);
-        ADD2(tmp2, offset_vec, tmp3, offset_vec, tmp2, tmp3);
-        CLIP_SH4_0_255_MAX_SATU(tmp0, tmp1, tmp2, tmp3);
         PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out0, out1);
         ST_D4(out0, out1, 0, 1, 0, 1, dst_tmp, dst_stride);
         dst_tmp += (4 * dst_stride);
@@ -5129,7 +6074,8 @@ static void hevc_hv_uniwgt_4t_12w_msa(uint8_t *src,
         dst10_l = dst54_l;
         dst21_r = dst65_r;
         dst21_l = dst65_l;
-        dsth2 = dsth6;
+        dsth4 = dsth6;
+        dsth5 = dsth7;
     }
 
     src += 8;
@@ -5140,33 +6086,49 @@ static void hevc_hv_uniwgt_4t_12w_msa(uint8_t *src,
 
     LD_SB3(src, src_stride, src0, src1, src2);
     src += (3 * src_stride);
-    XORI_B3_128_SB(src0, src1, src2);
     VSHF_B2_SB(src0, src1, src0, src1, mask2, mask3, vec0, vec1);
     VSHF_B2_SB(src1, src2, src1, src2, mask2, mask3, vec2, vec3);
-    dst10 = HEVC_FILT_4TAP_SH(vec0, vec1, filt0, filt1);
-    dst21 = HEVC_FILT_4TAP_SH(vec2, vec3, filt0, filt1);
-    ILVRL_H2_SH(dst21, dst10, dst10_r, dst21_r);
-    dst22 = (v8i16) __msa_splati_d((v2i64) dst21, 1);
+
+    ILVRL_B2_SH(zero, vec0, tmp0, tmp1);
+    ILVRL_B2_SH(zero, vec1, tmp2, tmp3);
+    ILVL_B2_SH(zero, vec2, zero, vec3, tmp4, tmp6);
+    dst0 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+    dst1 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+    dst2 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+
+    ILVEV_H2_SH(dst0, dst1, dst1, dst2, dst10_r, dst21_r);
 
     for (loop_cnt = 2; loop_cnt--;) {
         LD_SB8(src, src_stride, src3, src4, src5, src6, src7, src8, src9,
                src10);
         src += (8 * src_stride);
-        XORI_B8_128_SB(src3, src4, src5, src6, src7, src8, src9, src10);
         VSHF_B2_SB(src3, src7, src3, src7, mask2, mask3, vec0, vec1);
         VSHF_B2_SB(src4, src8, src4, src8, mask2, mask3, vec2, vec3);
         VSHF_B2_SB(src5, src9, src5, src9, mask2, mask3, vec4, vec5);
         VSHF_B2_SB(src6, src10, src6, src10, mask2, mask3, vec6, vec7);
-        dst73 = HEVC_FILT_4TAP_SH(vec0, vec1, filt0, filt1);
-        dst84 = HEVC_FILT_4TAP_SH(vec2, vec3, filt0, filt1);
-        dst95 = HEVC_FILT_4TAP_SH(vec4, vec5, filt0, filt1);
-        dst106 = HEVC_FILT_4TAP_SH(vec6, vec7, filt0, filt1);
-        dst32_r = __msa_ilvr_h(dst73, dst22);
-        ILVRL_H2_SH(dst84, dst73, dst43_r, dst87_r);
-        ILVRL_H2_SH(dst95, dst84, dst54_r, dst98_r);
-        ILVRL_H2_SH(dst106, dst95, dst65_r, dst109_r);
-        dst22 = (v8i16) __msa_splati_d((v2i64) dst73, 1);
-        dst76_r = __msa_ilvr_h(dst22, dst106);
+
+        ILVRL_B2_SH(zero, vec0, tmp0, tmp1);
+        ILVRL_B2_SH(zero, vec1, tmp2, tmp3);
+        ILVRL_B2_SH(zero, vec2, tmp4, tmp5);
+        ILVRL_B2_SH(zero, vec3, tmp6, tmp7);
+        dst3 = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+        dst7 = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+        dst4 = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+        dst8 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+        ILVRL_B2_SH(zero, vec4, tmp0, tmp1);
+        ILVRL_B2_SH(zero, vec5, tmp2, tmp3);
+        ILVRL_B2_SH(zero, vec6, tmp4, tmp5);
+        ILVRL_B2_SH(zero, vec7, tmp6, tmp7);
+        dst5  = HEVC_FILT_4TAP_SW(tmp0, tmp2, filt0, filt1);
+        dst9  = HEVC_FILT_4TAP_SW(tmp1, tmp3, filt0, filt1);
+        dst6  = HEVC_FILT_4TAP_SW(tmp4, tmp6, filt0, filt1);
+        dst10 = HEVC_FILT_4TAP_SW(tmp5, tmp7, filt0, filt1);
+
+        dst32_r = __msa_ilvev_h(dst3, dst2);
+        ILVEV_H2_SH(dst3, dst4, dst7, dst8, dst43_r, dst87_r);
+        ILVEV_H2_SH(dst4, dst5, dst8, dst9, dst54_r, dst98_r);
+        ILVEV_H2_SH(dst5, dst6, dst9, dst10, dst65_r, dst109_r);
+        dst76_r = __msa_ilvev_h(dst7, dst6);
         dst0 = HEVC_FILT_4TAP(dst10_r, dst32_r, filt_h0, filt_h1);
         dst1 = HEVC_FILT_4TAP(dst21_r, dst43_r, filt_h0, filt_h1);
         dst2 = HEVC_FILT_4TAP(dst32_r, dst54_r, filt_h0, filt_h1);
@@ -5183,18 +6145,21 @@ static void hevc_hv_uniwgt_4t_12w_msa(uint8_t *src,
         MUL2(dst6, weight_vec, dst7, weight_vec, dst6, dst7);
         SRAR_W4_SW(dst0, dst1, dst2, dst3, rnd_vec);
         SRAR_W4_SW(dst4, dst5, dst6, dst7, rnd_vec);
+        ADD2(dst0, offset_vec, dst1, offset_vec, dst0, dst1);
+        ADD2(dst2, offset_vec, dst3, offset_vec, dst2, dst3);
+        ADD2(dst4, offset_vec, dst5, offset_vec, dst4, dst5);
+        ADD2(dst6, offset_vec, dst7, offset_vec, dst6, dst7);
+        CLIP_SW8_0_255(dst0, dst1, dst2, dst3,
+                       dst4, dst5, dst6, dst7);
         PCKEV_H4_SH(dst1, dst0, dst3, dst2, dst5, dst4, dst7, dst6, tmp0, tmp1,
                     tmp2, tmp3);
-        ADD2(tmp0, offset_vec, tmp1, offset_vec, tmp0, tmp1);
-        ADD2(tmp2, offset_vec, tmp3, offset_vec, tmp2, tmp3);
-        CLIP_SH4_0_255_MAX_SATU(tmp0, tmp1, tmp2, tmp3);
         PCKEV_B2_UB(tmp1, tmp0, tmp3, tmp2, out0, out1);
         ST_W8(out0, out1, 0, 1, 2, 3, 0, 1, 2, 3, dst, dst_stride);
         dst += (8 * dst_stride);
 
         dst10_r = dst98_r;
         dst21_r = dst109_r;
-        dst22 = (v8i16) __msa_splati_d((v2i64) dst106, 1);
+        dst2 = dst10;
     }
 }
 
diff --git a/libavcodec/mips/hevcdsp_init_mips.c b/libavcodec/mips/hevcdsp_init_mips.c
index 88337f462e..eb261e5adf 100644
--- a/libavcodec/mips/hevcdsp_init_mips.c
+++ b/libavcodec/mips/hevcdsp_init_mips.c
@@ -18,512 +18,500 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
+#include "libavutil/mips/cpu.h"
 #include "libavcodec/mips/hevcdsp_mips.h"
 
-#if HAVE_MMI
-static av_cold void hevc_dsp_init_mmi(HEVCDSPContext *c,
-                                      const int bit_depth)
+void ff_hevc_dsp_init_mips(HEVCDSPContext *c, const int bit_depth)
 {
-    if (8 == bit_depth) {
-        c->put_hevc_qpel[1][0][1] = ff_hevc_put_hevc_qpel_h4_8_mmi;
-        c->put_hevc_qpel[3][0][1] = ff_hevc_put_hevc_qpel_h8_8_mmi;
-        c->put_hevc_qpel[4][0][1] = ff_hevc_put_hevc_qpel_h12_8_mmi;
-        c->put_hevc_qpel[5][0][1] = ff_hevc_put_hevc_qpel_h16_8_mmi;
-        c->put_hevc_qpel[6][0][1] = ff_hevc_put_hevc_qpel_h24_8_mmi;
-        c->put_hevc_qpel[7][0][1] = ff_hevc_put_hevc_qpel_h32_8_mmi;
-        c->put_hevc_qpel[8][0][1] = ff_hevc_put_hevc_qpel_h48_8_mmi;
-        c->put_hevc_qpel[9][0][1] = ff_hevc_put_hevc_qpel_h64_8_mmi;
-
-        c->put_hevc_qpel[1][1][1] = ff_hevc_put_hevc_qpel_hv4_8_mmi;
-        c->put_hevc_qpel[3][1][1] = ff_hevc_put_hevc_qpel_hv8_8_mmi;
-        c->put_hevc_qpel[4][1][1] = ff_hevc_put_hevc_qpel_hv12_8_mmi;
-        c->put_hevc_qpel[5][1][1] = ff_hevc_put_hevc_qpel_hv16_8_mmi;
-        c->put_hevc_qpel[6][1][1] = ff_hevc_put_hevc_qpel_hv24_8_mmi;
-        c->put_hevc_qpel[7][1][1] = ff_hevc_put_hevc_qpel_hv32_8_mmi;
-        c->put_hevc_qpel[8][1][1] = ff_hevc_put_hevc_qpel_hv48_8_mmi;
-        c->put_hevc_qpel[9][1][1] = ff_hevc_put_hevc_qpel_hv64_8_mmi;
-
-        c->put_hevc_qpel_bi[1][0][1] = ff_hevc_put_hevc_qpel_bi_h4_8_mmi;
-        c->put_hevc_qpel_bi[3][0][1] = ff_hevc_put_hevc_qpel_bi_h8_8_mmi;
-        c->put_hevc_qpel_bi[4][0][1] = ff_hevc_put_hevc_qpel_bi_h12_8_mmi;
-        c->put_hevc_qpel_bi[5][0][1] = ff_hevc_put_hevc_qpel_bi_h16_8_mmi;
-        c->put_hevc_qpel_bi[6][0][1] = ff_hevc_put_hevc_qpel_bi_h24_8_mmi;
-        c->put_hevc_qpel_bi[7][0][1] = ff_hevc_put_hevc_qpel_bi_h32_8_mmi;
-        c->put_hevc_qpel_bi[8][0][1] = ff_hevc_put_hevc_qpel_bi_h48_8_mmi;
-        c->put_hevc_qpel_bi[9][0][1] = ff_hevc_put_hevc_qpel_bi_h64_8_mmi;
-
-        c->put_hevc_qpel_bi[1][1][1] = ff_hevc_put_hevc_qpel_bi_hv4_8_mmi;
-        c->put_hevc_qpel_bi[3][1][1] = ff_hevc_put_hevc_qpel_bi_hv8_8_mmi;
-        c->put_hevc_qpel_bi[4][1][1] = ff_hevc_put_hevc_qpel_bi_hv12_8_mmi;
-        c->put_hevc_qpel_bi[5][1][1] = ff_hevc_put_hevc_qpel_bi_hv16_8_mmi;
-        c->put_hevc_qpel_bi[6][1][1] = ff_hevc_put_hevc_qpel_bi_hv24_8_mmi;
-        c->put_hevc_qpel_bi[7][1][1] = ff_hevc_put_hevc_qpel_bi_hv32_8_mmi;
-        c->put_hevc_qpel_bi[8][1][1] = ff_hevc_put_hevc_qpel_bi_hv48_8_mmi;
-        c->put_hevc_qpel_bi[9][1][1] = ff_hevc_put_hevc_qpel_bi_hv64_8_mmi;
-
-        c->put_hevc_qpel_bi[3][0][0] = ff_hevc_put_hevc_pel_bi_pixels8_8_mmi;
-        c->put_hevc_qpel_bi[5][0][0] = ff_hevc_put_hevc_pel_bi_pixels16_8_mmi;
-        c->put_hevc_qpel_bi[6][0][0] = ff_hevc_put_hevc_pel_bi_pixels24_8_mmi;
-        c->put_hevc_qpel_bi[7][0][0] = ff_hevc_put_hevc_pel_bi_pixels32_8_mmi;
-        c->put_hevc_qpel_bi[8][0][0] = ff_hevc_put_hevc_pel_bi_pixels48_8_mmi;
-        c->put_hevc_qpel_bi[9][0][0] = ff_hevc_put_hevc_pel_bi_pixels64_8_mmi;
-
-        c->put_hevc_epel_bi[3][0][0] = ff_hevc_put_hevc_pel_bi_pixels8_8_mmi;
-        c->put_hevc_epel_bi[5][0][0] = ff_hevc_put_hevc_pel_bi_pixels16_8_mmi;
-        c->put_hevc_epel_bi[6][0][0] = ff_hevc_put_hevc_pel_bi_pixels24_8_mmi;
-        c->put_hevc_epel_bi[7][0][0] = ff_hevc_put_hevc_pel_bi_pixels32_8_mmi;
-
-        c->put_hevc_epel_bi[1][1][1] = ff_hevc_put_hevc_epel_bi_hv4_8_mmi;
-        c->put_hevc_epel_bi[3][1][1] = ff_hevc_put_hevc_epel_bi_hv8_8_mmi;
-        c->put_hevc_epel_bi[4][1][1] = ff_hevc_put_hevc_epel_bi_hv12_8_mmi;
-        c->put_hevc_epel_bi[5][1][1] = ff_hevc_put_hevc_epel_bi_hv16_8_mmi;
-        c->put_hevc_epel_bi[6][1][1] = ff_hevc_put_hevc_epel_bi_hv24_8_mmi;
-        c->put_hevc_epel_bi[7][1][1] = ff_hevc_put_hevc_epel_bi_hv32_8_mmi;
-
-        c->put_hevc_qpel_uni[1][1][1] = ff_hevc_put_hevc_qpel_uni_hv4_8_mmi;
-        c->put_hevc_qpel_uni[3][1][1] = ff_hevc_put_hevc_qpel_uni_hv8_8_mmi;
-        c->put_hevc_qpel_uni[4][1][1] = ff_hevc_put_hevc_qpel_uni_hv12_8_mmi;
-        c->put_hevc_qpel_uni[5][1][1] = ff_hevc_put_hevc_qpel_uni_hv16_8_mmi;
-        c->put_hevc_qpel_uni[6][1][1] = ff_hevc_put_hevc_qpel_uni_hv24_8_mmi;
-        c->put_hevc_qpel_uni[7][1][1] = ff_hevc_put_hevc_qpel_uni_hv32_8_mmi;
-        c->put_hevc_qpel_uni[8][1][1] = ff_hevc_put_hevc_qpel_uni_hv48_8_mmi;
-        c->put_hevc_qpel_uni[9][1][1] = ff_hevc_put_hevc_qpel_uni_hv64_8_mmi;
+    int cpu_flags = av_get_cpu_flags();
+
+    if (have_mmi(cpu_flags)) {
+        if (bit_depth == 8) {
+            c->put_hevc_qpel[1][0][1] = ff_hevc_put_hevc_qpel_h4_8_mmi;
+            c->put_hevc_qpel[3][0][1] = ff_hevc_put_hevc_qpel_h8_8_mmi;
+            c->put_hevc_qpel[4][0][1] = ff_hevc_put_hevc_qpel_h12_8_mmi;
+            c->put_hevc_qpel[5][0][1] = ff_hevc_put_hevc_qpel_h16_8_mmi;
+            c->put_hevc_qpel[6][0][1] = ff_hevc_put_hevc_qpel_h24_8_mmi;
+            c->put_hevc_qpel[7][0][1] = ff_hevc_put_hevc_qpel_h32_8_mmi;
+            c->put_hevc_qpel[8][0][1] = ff_hevc_put_hevc_qpel_h48_8_mmi;
+            c->put_hevc_qpel[9][0][1] = ff_hevc_put_hevc_qpel_h64_8_mmi;
+
+            c->put_hevc_qpel[1][1][1] = ff_hevc_put_hevc_qpel_hv4_8_mmi;
+            c->put_hevc_qpel[3][1][1] = ff_hevc_put_hevc_qpel_hv8_8_mmi;
+            c->put_hevc_qpel[4][1][1] = ff_hevc_put_hevc_qpel_hv12_8_mmi;
+            c->put_hevc_qpel[5][1][1] = ff_hevc_put_hevc_qpel_hv16_8_mmi;
+            c->put_hevc_qpel[6][1][1] = ff_hevc_put_hevc_qpel_hv24_8_mmi;
+            c->put_hevc_qpel[7][1][1] = ff_hevc_put_hevc_qpel_hv32_8_mmi;
+            c->put_hevc_qpel[8][1][1] = ff_hevc_put_hevc_qpel_hv48_8_mmi;
+            c->put_hevc_qpel[9][1][1] = ff_hevc_put_hevc_qpel_hv64_8_mmi;
+
+            c->put_hevc_qpel_bi[1][0][1] = ff_hevc_put_hevc_qpel_bi_h4_8_mmi;
+            c->put_hevc_qpel_bi[3][0][1] = ff_hevc_put_hevc_qpel_bi_h8_8_mmi;
+            c->put_hevc_qpel_bi[4][0][1] = ff_hevc_put_hevc_qpel_bi_h12_8_mmi;
+            c->put_hevc_qpel_bi[5][0][1] = ff_hevc_put_hevc_qpel_bi_h16_8_mmi;
+            c->put_hevc_qpel_bi[6][0][1] = ff_hevc_put_hevc_qpel_bi_h24_8_mmi;
+            c->put_hevc_qpel_bi[7][0][1] = ff_hevc_put_hevc_qpel_bi_h32_8_mmi;
+            c->put_hevc_qpel_bi[8][0][1] = ff_hevc_put_hevc_qpel_bi_h48_8_mmi;
+            c->put_hevc_qpel_bi[9][0][1] = ff_hevc_put_hevc_qpel_bi_h64_8_mmi;
+
+            c->put_hevc_qpel_bi[1][1][1] = ff_hevc_put_hevc_qpel_bi_hv4_8_mmi;
+            c->put_hevc_qpel_bi[3][1][1] = ff_hevc_put_hevc_qpel_bi_hv8_8_mmi;
+            c->put_hevc_qpel_bi[4][1][1] = ff_hevc_put_hevc_qpel_bi_hv12_8_mmi;
+            c->put_hevc_qpel_bi[5][1][1] = ff_hevc_put_hevc_qpel_bi_hv16_8_mmi;
+            c->put_hevc_qpel_bi[6][1][1] = ff_hevc_put_hevc_qpel_bi_hv24_8_mmi;
+            c->put_hevc_qpel_bi[7][1][1] = ff_hevc_put_hevc_qpel_bi_hv32_8_mmi;
+            c->put_hevc_qpel_bi[8][1][1] = ff_hevc_put_hevc_qpel_bi_hv48_8_mmi;
+            c->put_hevc_qpel_bi[9][1][1] = ff_hevc_put_hevc_qpel_bi_hv64_8_mmi;
+
+            c->put_hevc_qpel_bi[3][0][0] = ff_hevc_put_hevc_pel_bi_pixels8_8_mmi;
+            c->put_hevc_qpel_bi[5][0][0] = ff_hevc_put_hevc_pel_bi_pixels16_8_mmi;
+            c->put_hevc_qpel_bi[6][0][0] = ff_hevc_put_hevc_pel_bi_pixels24_8_mmi;
+            c->put_hevc_qpel_bi[7][0][0] = ff_hevc_put_hevc_pel_bi_pixels32_8_mmi;
+            c->put_hevc_qpel_bi[8][0][0] = ff_hevc_put_hevc_pel_bi_pixels48_8_mmi;
+            c->put_hevc_qpel_bi[9][0][0] = ff_hevc_put_hevc_pel_bi_pixels64_8_mmi;
+
+            c->put_hevc_epel_bi[3][0][0] = ff_hevc_put_hevc_pel_bi_pixels8_8_mmi;
+            c->put_hevc_epel_bi[5][0][0] = ff_hevc_put_hevc_pel_bi_pixels16_8_mmi;
+            c->put_hevc_epel_bi[6][0][0] = ff_hevc_put_hevc_pel_bi_pixels24_8_mmi;
+            c->put_hevc_epel_bi[7][0][0] = ff_hevc_put_hevc_pel_bi_pixels32_8_mmi;
+
+            c->put_hevc_epel_bi[1][1][1] = ff_hevc_put_hevc_epel_bi_hv4_8_mmi;
+            c->put_hevc_epel_bi[3][1][1] = ff_hevc_put_hevc_epel_bi_hv8_8_mmi;
+            c->put_hevc_epel_bi[4][1][1] = ff_hevc_put_hevc_epel_bi_hv12_8_mmi;
+            c->put_hevc_epel_bi[5][1][1] = ff_hevc_put_hevc_epel_bi_hv16_8_mmi;
+            c->put_hevc_epel_bi[6][1][1] = ff_hevc_put_hevc_epel_bi_hv24_8_mmi;
+            c->put_hevc_epel_bi[7][1][1] = ff_hevc_put_hevc_epel_bi_hv32_8_mmi;
+
+            c->put_hevc_qpel_uni[1][1][1] = ff_hevc_put_hevc_qpel_uni_hv4_8_mmi;
+            c->put_hevc_qpel_uni[3][1][1] = ff_hevc_put_hevc_qpel_uni_hv8_8_mmi;
+            c->put_hevc_qpel_uni[4][1][1] = ff_hevc_put_hevc_qpel_uni_hv12_8_mmi;
+            c->put_hevc_qpel_uni[5][1][1] = ff_hevc_put_hevc_qpel_uni_hv16_8_mmi;
+            c->put_hevc_qpel_uni[6][1][1] = ff_hevc_put_hevc_qpel_uni_hv24_8_mmi;
+            c->put_hevc_qpel_uni[7][1][1] = ff_hevc_put_hevc_qpel_uni_hv32_8_mmi;
+            c->put_hevc_qpel_uni[8][1][1] = ff_hevc_put_hevc_qpel_uni_hv48_8_mmi;
+            c->put_hevc_qpel_uni[9][1][1] = ff_hevc_put_hevc_qpel_uni_hv64_8_mmi;
+        }
     }
-}
-#endif // #if HAVE_MMI
 
-#if HAVE_MSA
-static av_cold void hevc_dsp_init_msa(HEVCDSPContext *c,
-                                      const int bit_depth)
-{
-    if (8 == bit_depth) {
-        c->put_hevc_qpel[1][0][0] = ff_hevc_put_hevc_pel_pixels4_8_msa;
-        c->put_hevc_qpel[2][0][0] = ff_hevc_put_hevc_pel_pixels6_8_msa;
-        c->put_hevc_qpel[3][0][0] = ff_hevc_put_hevc_pel_pixels8_8_msa;
-        c->put_hevc_qpel[4][0][0] = ff_hevc_put_hevc_pel_pixels12_8_msa;
-        c->put_hevc_qpel[5][0][0] = ff_hevc_put_hevc_pel_pixels16_8_msa;
-        c->put_hevc_qpel[6][0][0] = ff_hevc_put_hevc_pel_pixels24_8_msa;
-        c->put_hevc_qpel[7][0][0] = ff_hevc_put_hevc_pel_pixels32_8_msa;
-        c->put_hevc_qpel[8][0][0] = ff_hevc_put_hevc_pel_pixels48_8_msa;
-        c->put_hevc_qpel[9][0][0] = ff_hevc_put_hevc_pel_pixels64_8_msa;
-
-        c->put_hevc_qpel[1][0][1] = ff_hevc_put_hevc_qpel_h4_8_msa;
-        c->put_hevc_qpel[3][0][1] = ff_hevc_put_hevc_qpel_h8_8_msa;
-        c->put_hevc_qpel[4][0][1] = ff_hevc_put_hevc_qpel_h12_8_msa;
-        c->put_hevc_qpel[5][0][1] = ff_hevc_put_hevc_qpel_h16_8_msa;
-        c->put_hevc_qpel[6][0][1] = ff_hevc_put_hevc_qpel_h24_8_msa;
-        c->put_hevc_qpel[7][0][1] = ff_hevc_put_hevc_qpel_h32_8_msa;
-        c->put_hevc_qpel[8][0][1] = ff_hevc_put_hevc_qpel_h48_8_msa;
-        c->put_hevc_qpel[9][0][1] = ff_hevc_put_hevc_qpel_h64_8_msa;
-
-        c->put_hevc_qpel[1][1][0] = ff_hevc_put_hevc_qpel_v4_8_msa;
-        c->put_hevc_qpel[3][1][0] = ff_hevc_put_hevc_qpel_v8_8_msa;
-        c->put_hevc_qpel[4][1][0] = ff_hevc_put_hevc_qpel_v12_8_msa;
-        c->put_hevc_qpel[5][1][0] = ff_hevc_put_hevc_qpel_v16_8_msa;
-        c->put_hevc_qpel[6][1][0] = ff_hevc_put_hevc_qpel_v24_8_msa;
-        c->put_hevc_qpel[7][1][0] = ff_hevc_put_hevc_qpel_v32_8_msa;
-        c->put_hevc_qpel[8][1][0] = ff_hevc_put_hevc_qpel_v48_8_msa;
-        c->put_hevc_qpel[9][1][0] = ff_hevc_put_hevc_qpel_v64_8_msa;
-
-        c->put_hevc_qpel[1][1][1] = ff_hevc_put_hevc_qpel_hv4_8_msa;
-        c->put_hevc_qpel[3][1][1] = ff_hevc_put_hevc_qpel_hv8_8_msa;
-        c->put_hevc_qpel[4][1][1] = ff_hevc_put_hevc_qpel_hv12_8_msa;
-        c->put_hevc_qpel[5][1][1] = ff_hevc_put_hevc_qpel_hv16_8_msa;
-        c->put_hevc_qpel[6][1][1] = ff_hevc_put_hevc_qpel_hv24_8_msa;
-        c->put_hevc_qpel[7][1][1] = ff_hevc_put_hevc_qpel_hv32_8_msa;
-        c->put_hevc_qpel[8][1][1] = ff_hevc_put_hevc_qpel_hv48_8_msa;
-        c->put_hevc_qpel[9][1][1] = ff_hevc_put_hevc_qpel_hv64_8_msa;
-
-        c->put_hevc_epel[1][0][0] = ff_hevc_put_hevc_pel_pixels4_8_msa;
-        c->put_hevc_epel[2][0][0] = ff_hevc_put_hevc_pel_pixels6_8_msa;
-        c->put_hevc_epel[3][0][0] = ff_hevc_put_hevc_pel_pixels8_8_msa;
-        c->put_hevc_epel[4][0][0] = ff_hevc_put_hevc_pel_pixels12_8_msa;
-        c->put_hevc_epel[5][0][0] = ff_hevc_put_hevc_pel_pixels16_8_msa;
-        c->put_hevc_epel[6][0][0] = ff_hevc_put_hevc_pel_pixels24_8_msa;
-        c->put_hevc_epel[7][0][0] = ff_hevc_put_hevc_pel_pixels32_8_msa;
-
-        c->put_hevc_epel[1][0][1] = ff_hevc_put_hevc_epel_h4_8_msa;
-        c->put_hevc_epel[2][0][1] = ff_hevc_put_hevc_epel_h6_8_msa;
-        c->put_hevc_epel[3][0][1] = ff_hevc_put_hevc_epel_h8_8_msa;
-        c->put_hevc_epel[4][0][1] = ff_hevc_put_hevc_epel_h12_8_msa;
-        c->put_hevc_epel[5][0][1] = ff_hevc_put_hevc_epel_h16_8_msa;
-        c->put_hevc_epel[6][0][1] = ff_hevc_put_hevc_epel_h24_8_msa;
-        c->put_hevc_epel[7][0][1] = ff_hevc_put_hevc_epel_h32_8_msa;
-
-        c->put_hevc_epel[1][1][0] = ff_hevc_put_hevc_epel_v4_8_msa;
-        c->put_hevc_epel[2][1][0] = ff_hevc_put_hevc_epel_v6_8_msa;
-        c->put_hevc_epel[3][1][0] = ff_hevc_put_hevc_epel_v8_8_msa;
-        c->put_hevc_epel[4][1][0] = ff_hevc_put_hevc_epel_v12_8_msa;
-        c->put_hevc_epel[5][1][0] = ff_hevc_put_hevc_epel_v16_8_msa;
-        c->put_hevc_epel[6][1][0] = ff_hevc_put_hevc_epel_v24_8_msa;
-        c->put_hevc_epel[7][1][0] = ff_hevc_put_hevc_epel_v32_8_msa;
-
-        c->put_hevc_epel[1][1][1] = ff_hevc_put_hevc_epel_hv4_8_msa;
-        c->put_hevc_epel[2][1][1] = ff_hevc_put_hevc_epel_hv6_8_msa;
-        c->put_hevc_epel[3][1][1] = ff_hevc_put_hevc_epel_hv8_8_msa;
-        c->put_hevc_epel[4][1][1] = ff_hevc_put_hevc_epel_hv12_8_msa;
-        c->put_hevc_epel[5][1][1] = ff_hevc_put_hevc_epel_hv16_8_msa;
-        c->put_hevc_epel[6][1][1] = ff_hevc_put_hevc_epel_hv24_8_msa;
-        c->put_hevc_epel[7][1][1] = ff_hevc_put_hevc_epel_hv32_8_msa;
-
-        c->put_hevc_qpel_uni[3][0][0] = ff_hevc_put_hevc_uni_pel_pixels8_8_msa;
-        c->put_hevc_qpel_uni[4][0][0] = ff_hevc_put_hevc_uni_pel_pixels12_8_msa;
-        c->put_hevc_qpel_uni[5][0][0] = ff_hevc_put_hevc_uni_pel_pixels16_8_msa;
-        c->put_hevc_qpel_uni[6][0][0] = ff_hevc_put_hevc_uni_pel_pixels24_8_msa;
-        c->put_hevc_qpel_uni[7][0][0] = ff_hevc_put_hevc_uni_pel_pixels32_8_msa;
-        c->put_hevc_qpel_uni[8][0][0] = ff_hevc_put_hevc_uni_pel_pixels48_8_msa;
-        c->put_hevc_qpel_uni[9][0][0] = ff_hevc_put_hevc_uni_pel_pixels64_8_msa;
-
-        c->put_hevc_qpel_uni[1][0][1] = ff_hevc_put_hevc_uni_qpel_h4_8_msa;
-        c->put_hevc_qpel_uni[3][0][1] = ff_hevc_put_hevc_uni_qpel_h8_8_msa;
-        c->put_hevc_qpel_uni[4][0][1] = ff_hevc_put_hevc_uni_qpel_h12_8_msa;
-        c->put_hevc_qpel_uni[5][0][1] = ff_hevc_put_hevc_uni_qpel_h16_8_msa;
-        c->put_hevc_qpel_uni[6][0][1] = ff_hevc_put_hevc_uni_qpel_h24_8_msa;
-        c->put_hevc_qpel_uni[7][0][1] = ff_hevc_put_hevc_uni_qpel_h32_8_msa;
-        c->put_hevc_qpel_uni[8][0][1] = ff_hevc_put_hevc_uni_qpel_h48_8_msa;
-        c->put_hevc_qpel_uni[9][0][1] = ff_hevc_put_hevc_uni_qpel_h64_8_msa;
-
-        c->put_hevc_qpel_uni[1][1][0] = ff_hevc_put_hevc_uni_qpel_v4_8_msa;
-        c->put_hevc_qpel_uni[3][1][0] = ff_hevc_put_hevc_uni_qpel_v8_8_msa;
-        c->put_hevc_qpel_uni[4][1][0] = ff_hevc_put_hevc_uni_qpel_v12_8_msa;
-        c->put_hevc_qpel_uni[5][1][0] = ff_hevc_put_hevc_uni_qpel_v16_8_msa;
-        c->put_hevc_qpel_uni[6][1][0] = ff_hevc_put_hevc_uni_qpel_v24_8_msa;
-        c->put_hevc_qpel_uni[7][1][0] = ff_hevc_put_hevc_uni_qpel_v32_8_msa;
-        c->put_hevc_qpel_uni[8][1][0] = ff_hevc_put_hevc_uni_qpel_v48_8_msa;
-        c->put_hevc_qpel_uni[9][1][0] = ff_hevc_put_hevc_uni_qpel_v64_8_msa;
-
-        c->put_hevc_qpel_uni[1][1][1] = ff_hevc_put_hevc_uni_qpel_hv4_8_msa;
-        c->put_hevc_qpel_uni[3][1][1] = ff_hevc_put_hevc_uni_qpel_hv8_8_msa;
-        c->put_hevc_qpel_uni[4][1][1] = ff_hevc_put_hevc_uni_qpel_hv12_8_msa;
-        c->put_hevc_qpel_uni[5][1][1] = ff_hevc_put_hevc_uni_qpel_hv16_8_msa;
-        c->put_hevc_qpel_uni[6][1][1] = ff_hevc_put_hevc_uni_qpel_hv24_8_msa;
-        c->put_hevc_qpel_uni[7][1][1] = ff_hevc_put_hevc_uni_qpel_hv32_8_msa;
-        c->put_hevc_qpel_uni[8][1][1] = ff_hevc_put_hevc_uni_qpel_hv48_8_msa;
-        c->put_hevc_qpel_uni[9][1][1] = ff_hevc_put_hevc_uni_qpel_hv64_8_msa;
-
-        c->put_hevc_epel_uni[3][0][0] = ff_hevc_put_hevc_uni_pel_pixels8_8_msa;
-        c->put_hevc_epel_uni[4][0][0] = ff_hevc_put_hevc_uni_pel_pixels12_8_msa;
-        c->put_hevc_epel_uni[5][0][0] = ff_hevc_put_hevc_uni_pel_pixels16_8_msa;
-        c->put_hevc_epel_uni[6][0][0] = ff_hevc_put_hevc_uni_pel_pixels24_8_msa;
-        c->put_hevc_epel_uni[7][0][0] = ff_hevc_put_hevc_uni_pel_pixels32_8_msa;
-
-        c->put_hevc_epel_uni[1][0][1] = ff_hevc_put_hevc_uni_epel_h4_8_msa;
-        c->put_hevc_epel_uni[2][0][1] = ff_hevc_put_hevc_uni_epel_h6_8_msa;
-        c->put_hevc_epel_uni[3][0][1] = ff_hevc_put_hevc_uni_epel_h8_8_msa;
-        c->put_hevc_epel_uni[4][0][1] = ff_hevc_put_hevc_uni_epel_h12_8_msa;
-        c->put_hevc_epel_uni[5][0][1] = ff_hevc_put_hevc_uni_epel_h16_8_msa;
-        c->put_hevc_epel_uni[6][0][1] = ff_hevc_put_hevc_uni_epel_h24_8_msa;
-        c->put_hevc_epel_uni[7][0][1] = ff_hevc_put_hevc_uni_epel_h32_8_msa;
-
-        c->put_hevc_epel_uni[1][1][0] = ff_hevc_put_hevc_uni_epel_v4_8_msa;
-        c->put_hevc_epel_uni[2][1][0] = ff_hevc_put_hevc_uni_epel_v6_8_msa;
-        c->put_hevc_epel_uni[3][1][0] = ff_hevc_put_hevc_uni_epel_v8_8_msa;
-        c->put_hevc_epel_uni[4][1][0] = ff_hevc_put_hevc_uni_epel_v12_8_msa;
-        c->put_hevc_epel_uni[5][1][0] = ff_hevc_put_hevc_uni_epel_v16_8_msa;
-        c->put_hevc_epel_uni[6][1][0] = ff_hevc_put_hevc_uni_epel_v24_8_msa;
-        c->put_hevc_epel_uni[7][1][0] = ff_hevc_put_hevc_uni_epel_v32_8_msa;
-
-        c->put_hevc_epel_uni[1][1][1] = ff_hevc_put_hevc_uni_epel_hv4_8_msa;
-        c->put_hevc_epel_uni[2][1][1] = ff_hevc_put_hevc_uni_epel_hv6_8_msa;
-        c->put_hevc_epel_uni[3][1][1] = ff_hevc_put_hevc_uni_epel_hv8_8_msa;
-        c->put_hevc_epel_uni[4][1][1] = ff_hevc_put_hevc_uni_epel_hv12_8_msa;
-        c->put_hevc_epel_uni[5][1][1] = ff_hevc_put_hevc_uni_epel_hv16_8_msa;
-        c->put_hevc_epel_uni[6][1][1] = ff_hevc_put_hevc_uni_epel_hv24_8_msa;
-        c->put_hevc_epel_uni[7][1][1] = ff_hevc_put_hevc_uni_epel_hv32_8_msa;
-
-        c->put_hevc_qpel_uni_w[1][0][0] =
-            ff_hevc_put_hevc_uni_w_pel_pixels4_8_msa;
-        c->put_hevc_qpel_uni_w[3][0][0] =
-            ff_hevc_put_hevc_uni_w_pel_pixels8_8_msa;
-        c->put_hevc_qpel_uni_w[4][0][0] =
-            ff_hevc_put_hevc_uni_w_pel_pixels12_8_msa;
-        c->put_hevc_qpel_uni_w[5][0][0] =
-            ff_hevc_put_hevc_uni_w_pel_pixels16_8_msa;
-        c->put_hevc_qpel_uni_w[6][0][0] =
-            ff_hevc_put_hevc_uni_w_pel_pixels24_8_msa;
-        c->put_hevc_qpel_uni_w[7][0][0] =
-            ff_hevc_put_hevc_uni_w_pel_pixels32_8_msa;
-        c->put_hevc_qpel_uni_w[8][0][0] =
-            ff_hevc_put_hevc_uni_w_pel_pixels48_8_msa;
-        c->put_hevc_qpel_uni_w[9][0][0] =
-            ff_hevc_put_hevc_uni_w_pel_pixels64_8_msa;
-
-        c->put_hevc_qpel_uni_w[1][0][1] = ff_hevc_put_hevc_uni_w_qpel_h4_8_msa;
-        c->put_hevc_qpel_uni_w[3][0][1] = ff_hevc_put_hevc_uni_w_qpel_h8_8_msa;
-        c->put_hevc_qpel_uni_w[4][0][1] = ff_hevc_put_hevc_uni_w_qpel_h12_8_msa;
-        c->put_hevc_qpel_uni_w[5][0][1] = ff_hevc_put_hevc_uni_w_qpel_h16_8_msa;
-        c->put_hevc_qpel_uni_w[6][0][1] = ff_hevc_put_hevc_uni_w_qpel_h24_8_msa;
-        c->put_hevc_qpel_uni_w[7][0][1] = ff_hevc_put_hevc_uni_w_qpel_h32_8_msa;
-        c->put_hevc_qpel_uni_w[8][0][1] = ff_hevc_put_hevc_uni_w_qpel_h48_8_msa;
-        c->put_hevc_qpel_uni_w[9][0][1] = ff_hevc_put_hevc_uni_w_qpel_h64_8_msa;
-
-        c->put_hevc_qpel_uni_w[1][1][0] = ff_hevc_put_hevc_uni_w_qpel_v4_8_msa;
-        c->put_hevc_qpel_uni_w[3][1][0] = ff_hevc_put_hevc_uni_w_qpel_v8_8_msa;
-        c->put_hevc_qpel_uni_w[4][1][0] = ff_hevc_put_hevc_uni_w_qpel_v12_8_msa;
-        c->put_hevc_qpel_uni_w[5][1][0] = ff_hevc_put_hevc_uni_w_qpel_v16_8_msa;
-        c->put_hevc_qpel_uni_w[6][1][0] = ff_hevc_put_hevc_uni_w_qpel_v24_8_msa;
-        c->put_hevc_qpel_uni_w[7][1][0] = ff_hevc_put_hevc_uni_w_qpel_v32_8_msa;
-        c->put_hevc_qpel_uni_w[8][1][0] = ff_hevc_put_hevc_uni_w_qpel_v48_8_msa;
-        c->put_hevc_qpel_uni_w[9][1][0] = ff_hevc_put_hevc_uni_w_qpel_v64_8_msa;
-
-        c->put_hevc_qpel_uni_w[1][1][1] = ff_hevc_put_hevc_uni_w_qpel_hv4_8_msa;
-        c->put_hevc_qpel_uni_w[3][1][1] = ff_hevc_put_hevc_uni_w_qpel_hv8_8_msa;
-        c->put_hevc_qpel_uni_w[4][1][1] =
-            ff_hevc_put_hevc_uni_w_qpel_hv12_8_msa;
-        c->put_hevc_qpel_uni_w[5][1][1] =
-            ff_hevc_put_hevc_uni_w_qpel_hv16_8_msa;
-        c->put_hevc_qpel_uni_w[6][1][1] =
-            ff_hevc_put_hevc_uni_w_qpel_hv24_8_msa;
-        c->put_hevc_qpel_uni_w[7][1][1] =
-            ff_hevc_put_hevc_uni_w_qpel_hv32_8_msa;
-        c->put_hevc_qpel_uni_w[8][1][1] =
-            ff_hevc_put_hevc_uni_w_qpel_hv48_8_msa;
-        c->put_hevc_qpel_uni_w[9][1][1] =
-            ff_hevc_put_hevc_uni_w_qpel_hv64_8_msa;
-
-        c->put_hevc_epel_uni_w[1][0][0] =
-            ff_hevc_put_hevc_uni_w_pel_pixels4_8_msa;
-        c->put_hevc_epel_uni_w[2][0][0] =
-            ff_hevc_put_hevc_uni_w_pel_pixels6_8_msa;
-        c->put_hevc_epel_uni_w[3][0][0] =
-            ff_hevc_put_hevc_uni_w_pel_pixels8_8_msa;
-        c->put_hevc_epel_uni_w[4][0][0] =
-            ff_hevc_put_hevc_uni_w_pel_pixels12_8_msa;
-        c->put_hevc_epel_uni_w[5][0][0] =
-            ff_hevc_put_hevc_uni_w_pel_pixels16_8_msa;
-        c->put_hevc_epel_uni_w[6][0][0] =
-            ff_hevc_put_hevc_uni_w_pel_pixels24_8_msa;
-        c->put_hevc_epel_uni_w[7][0][0] =
-            ff_hevc_put_hevc_uni_w_pel_pixels32_8_msa;
-
-        c->put_hevc_epel_uni_w[1][0][1] = ff_hevc_put_hevc_uni_w_epel_h4_8_msa;
-        c->put_hevc_epel_uni_w[2][0][1] = ff_hevc_put_hevc_uni_w_epel_h6_8_msa;
-        c->put_hevc_epel_uni_w[3][0][1] = ff_hevc_put_hevc_uni_w_epel_h8_8_msa;
-        c->put_hevc_epel_uni_w[4][0][1] = ff_hevc_put_hevc_uni_w_epel_h12_8_msa;
-        c->put_hevc_epel_uni_w[5][0][1] = ff_hevc_put_hevc_uni_w_epel_h16_8_msa;
-        c->put_hevc_epel_uni_w[6][0][1] = ff_hevc_put_hevc_uni_w_epel_h24_8_msa;
-        c->put_hevc_epel_uni_w[7][0][1] = ff_hevc_put_hevc_uni_w_epel_h32_8_msa;
-
-        c->put_hevc_epel_uni_w[1][1][0] = ff_hevc_put_hevc_uni_w_epel_v4_8_msa;
-        c->put_hevc_epel_uni_w[2][1][0] = ff_hevc_put_hevc_uni_w_epel_v6_8_msa;
-        c->put_hevc_epel_uni_w[3][1][0] = ff_hevc_put_hevc_uni_w_epel_v8_8_msa;
-        c->put_hevc_epel_uni_w[4][1][0] = ff_hevc_put_hevc_uni_w_epel_v12_8_msa;
-        c->put_hevc_epel_uni_w[5][1][0] = ff_hevc_put_hevc_uni_w_epel_v16_8_msa;
-        c->put_hevc_epel_uni_w[6][1][0] = ff_hevc_put_hevc_uni_w_epel_v24_8_msa;
-        c->put_hevc_epel_uni_w[7][1][0] = ff_hevc_put_hevc_uni_w_epel_v32_8_msa;
-
-        c->put_hevc_epel_uni_w[1][1][1] = ff_hevc_put_hevc_uni_w_epel_hv4_8_msa;
-        c->put_hevc_epel_uni_w[2][1][1] = ff_hevc_put_hevc_uni_w_epel_hv6_8_msa;
-        c->put_hevc_epel_uni_w[3][1][1] = ff_hevc_put_hevc_uni_w_epel_hv8_8_msa;
-        c->put_hevc_epel_uni_w[4][1][1] =
-            ff_hevc_put_hevc_uni_w_epel_hv12_8_msa;
-        c->put_hevc_epel_uni_w[5][1][1] =
-            ff_hevc_put_hevc_uni_w_epel_hv16_8_msa;
-        c->put_hevc_epel_uni_w[6][1][1] =
-            ff_hevc_put_hevc_uni_w_epel_hv24_8_msa;
-        c->put_hevc_epel_uni_w[7][1][1] =
-            ff_hevc_put_hevc_uni_w_epel_hv32_8_msa;
-
-        c->put_hevc_qpel_bi[1][0][0] = ff_hevc_put_hevc_bi_pel_pixels4_8_msa;
-        c->put_hevc_qpel_bi[3][0][0] = ff_hevc_put_hevc_bi_pel_pixels8_8_msa;
-        c->put_hevc_qpel_bi[4][0][0] = ff_hevc_put_hevc_bi_pel_pixels12_8_msa;
-        c->put_hevc_qpel_bi[5][0][0] = ff_hevc_put_hevc_bi_pel_pixels16_8_msa;
-        c->put_hevc_qpel_bi[6][0][0] = ff_hevc_put_hevc_bi_pel_pixels24_8_msa;
-        c->put_hevc_qpel_bi[7][0][0] = ff_hevc_put_hevc_bi_pel_pixels32_8_msa;
-        c->put_hevc_qpel_bi[8][0][0] = ff_hevc_put_hevc_bi_pel_pixels48_8_msa;
-        c->put_hevc_qpel_bi[9][0][0] = ff_hevc_put_hevc_bi_pel_pixels64_8_msa;
-
-        c->put_hevc_qpel_bi[1][0][1] = ff_hevc_put_hevc_bi_qpel_h4_8_msa;
-        c->put_hevc_qpel_bi[3][0][1] = ff_hevc_put_hevc_bi_qpel_h8_8_msa;
-        c->put_hevc_qpel_bi[4][0][1] = ff_hevc_put_hevc_bi_qpel_h12_8_msa;
-        c->put_hevc_qpel_bi[5][0][1] = ff_hevc_put_hevc_bi_qpel_h16_8_msa;
-        c->put_hevc_qpel_bi[6][0][1] = ff_hevc_put_hevc_bi_qpel_h24_8_msa;
-        c->put_hevc_qpel_bi[7][0][1] = ff_hevc_put_hevc_bi_qpel_h32_8_msa;
-        c->put_hevc_qpel_bi[8][0][1] = ff_hevc_put_hevc_bi_qpel_h48_8_msa;
-        c->put_hevc_qpel_bi[9][0][1] = ff_hevc_put_hevc_bi_qpel_h64_8_msa;
-
-        c->put_hevc_qpel_bi[1][1][0] = ff_hevc_put_hevc_bi_qpel_v4_8_msa;
-        c->put_hevc_qpel_bi[3][1][0] = ff_hevc_put_hevc_bi_qpel_v8_8_msa;
-        c->put_hevc_qpel_bi[4][1][0] = ff_hevc_put_hevc_bi_qpel_v12_8_msa;
-        c->put_hevc_qpel_bi[5][1][0] = ff_hevc_put_hevc_bi_qpel_v16_8_msa;
-        c->put_hevc_qpel_bi[6][1][0] = ff_hevc_put_hevc_bi_qpel_v24_8_msa;
-        c->put_hevc_qpel_bi[7][1][0] = ff_hevc_put_hevc_bi_qpel_v32_8_msa;
-        c->put_hevc_qpel_bi[8][1][0] = ff_hevc_put_hevc_bi_qpel_v48_8_msa;
-        c->put_hevc_qpel_bi[9][1][0] = ff_hevc_put_hevc_bi_qpel_v64_8_msa;
-
-        c->put_hevc_qpel_bi[1][1][1] = ff_hevc_put_hevc_bi_qpel_hv4_8_msa;
-        c->put_hevc_qpel_bi[3][1][1] = ff_hevc_put_hevc_bi_qpel_hv8_8_msa;
-        c->put_hevc_qpel_bi[4][1][1] = ff_hevc_put_hevc_bi_qpel_hv12_8_msa;
-        c->put_hevc_qpel_bi[5][1][1] = ff_hevc_put_hevc_bi_qpel_hv16_8_msa;
-        c->put_hevc_qpel_bi[6][1][1] = ff_hevc_put_hevc_bi_qpel_hv24_8_msa;
-        c->put_hevc_qpel_bi[7][1][1] = ff_hevc_put_hevc_bi_qpel_hv32_8_msa;
-        c->put_hevc_qpel_bi[8][1][1] = ff_hevc_put_hevc_bi_qpel_hv48_8_msa;
-        c->put_hevc_qpel_bi[9][1][1] = ff_hevc_put_hevc_bi_qpel_hv64_8_msa;
-
-        c->put_hevc_epel_bi[1][0][0] = ff_hevc_put_hevc_bi_pel_pixels4_8_msa;
-        c->put_hevc_epel_bi[2][0][0] = ff_hevc_put_hevc_bi_pel_pixels6_8_msa;
-        c->put_hevc_epel_bi[3][0][0] = ff_hevc_put_hevc_bi_pel_pixels8_8_msa;
-        c->put_hevc_epel_bi[4][0][0] = ff_hevc_put_hevc_bi_pel_pixels12_8_msa;
-        c->put_hevc_epel_bi[5][0][0] = ff_hevc_put_hevc_bi_pel_pixels16_8_msa;
-        c->put_hevc_epel_bi[6][0][0] = ff_hevc_put_hevc_bi_pel_pixels24_8_msa;
-        c->put_hevc_epel_bi[7][0][0] = ff_hevc_put_hevc_bi_pel_pixels32_8_msa;
-
-        c->put_hevc_epel_bi[1][0][1] = ff_hevc_put_hevc_bi_epel_h4_8_msa;
-        c->put_hevc_epel_bi[2][0][1] = ff_hevc_put_hevc_bi_epel_h6_8_msa;
-        c->put_hevc_epel_bi[3][0][1] = ff_hevc_put_hevc_bi_epel_h8_8_msa;
-        c->put_hevc_epel_bi[4][0][1] = ff_hevc_put_hevc_bi_epel_h12_8_msa;
-        c->put_hevc_epel_bi[5][0][1] = ff_hevc_put_hevc_bi_epel_h16_8_msa;
-        c->put_hevc_epel_bi[6][0][1] = ff_hevc_put_hevc_bi_epel_h24_8_msa;
-        c->put_hevc_epel_bi[7][0][1] = ff_hevc_put_hevc_bi_epel_h32_8_msa;
-
-        c->put_hevc_epel_bi[1][1][0] = ff_hevc_put_hevc_bi_epel_v4_8_msa;
-        c->put_hevc_epel_bi[2][1][0] = ff_hevc_put_hevc_bi_epel_v6_8_msa;
-        c->put_hevc_epel_bi[3][1][0] = ff_hevc_put_hevc_bi_epel_v8_8_msa;
-        c->put_hevc_epel_bi[4][1][0] = ff_hevc_put_hevc_bi_epel_v12_8_msa;
-        c->put_hevc_epel_bi[5][1][0] = ff_hevc_put_hevc_bi_epel_v16_8_msa;
-        c->put_hevc_epel_bi[6][1][0] = ff_hevc_put_hevc_bi_epel_v24_8_msa;
-        c->put_hevc_epel_bi[7][1][0] = ff_hevc_put_hevc_bi_epel_v32_8_msa;
-
-        c->put_hevc_epel_bi[1][1][1] = ff_hevc_put_hevc_bi_epel_hv4_8_msa;
-        c->put_hevc_epel_bi[2][1][1] = ff_hevc_put_hevc_bi_epel_hv6_8_msa;
-        c->put_hevc_epel_bi[3][1][1] = ff_hevc_put_hevc_bi_epel_hv8_8_msa;
-        c->put_hevc_epel_bi[4][1][1] = ff_hevc_put_hevc_bi_epel_hv12_8_msa;
-        c->put_hevc_epel_bi[5][1][1] = ff_hevc_put_hevc_bi_epel_hv16_8_msa;
-        c->put_hevc_epel_bi[6][1][1] = ff_hevc_put_hevc_bi_epel_hv24_8_msa;
-        c->put_hevc_epel_bi[7][1][1] = ff_hevc_put_hevc_bi_epel_hv32_8_msa;
-
-        c->put_hevc_qpel_bi_w[1][0][0] =
-            ff_hevc_put_hevc_bi_w_pel_pixels4_8_msa;
-        c->put_hevc_qpel_bi_w[3][0][0] =
-            ff_hevc_put_hevc_bi_w_pel_pixels8_8_msa;
-        c->put_hevc_qpel_bi_w[4][0][0] =
-            ff_hevc_put_hevc_bi_w_pel_pixels12_8_msa;
-        c->put_hevc_qpel_bi_w[5][0][0] =
-            ff_hevc_put_hevc_bi_w_pel_pixels16_8_msa;
-        c->put_hevc_qpel_bi_w[6][0][0] =
-            ff_hevc_put_hevc_bi_w_pel_pixels24_8_msa;
-        c->put_hevc_qpel_bi_w[7][0][0] =
-            ff_hevc_put_hevc_bi_w_pel_pixels32_8_msa;
-        c->put_hevc_qpel_bi_w[8][0][0] =
-            ff_hevc_put_hevc_bi_w_pel_pixels48_8_msa;
-        c->put_hevc_qpel_bi_w[9][0][0] =
-            ff_hevc_put_hevc_bi_w_pel_pixels64_8_msa;
-
-        c->put_hevc_qpel_bi_w[1][0][1] = ff_hevc_put_hevc_bi_w_qpel_h4_8_msa;
-        c->put_hevc_qpel_bi_w[3][0][1] = ff_hevc_put_hevc_bi_w_qpel_h8_8_msa;
-        c->put_hevc_qpel_bi_w[4][0][1] = ff_hevc_put_hevc_bi_w_qpel_h12_8_msa;
-        c->put_hevc_qpel_bi_w[5][0][1] = ff_hevc_put_hevc_bi_w_qpel_h16_8_msa;
-        c->put_hevc_qpel_bi_w[6][0][1] = ff_hevc_put_hevc_bi_w_qpel_h24_8_msa;
-        c->put_hevc_qpel_bi_w[7][0][1] = ff_hevc_put_hevc_bi_w_qpel_h32_8_msa;
-        c->put_hevc_qpel_bi_w[8][0][1] = ff_hevc_put_hevc_bi_w_qpel_h48_8_msa;
-        c->put_hevc_qpel_bi_w[9][0][1] = ff_hevc_put_hevc_bi_w_qpel_h64_8_msa;
-
-        c->put_hevc_qpel_bi_w[1][1][0] = ff_hevc_put_hevc_bi_w_qpel_v4_8_msa;
-        c->put_hevc_qpel_bi_w[3][1][0] = ff_hevc_put_hevc_bi_w_qpel_v8_8_msa;
-        c->put_hevc_qpel_bi_w[4][1][0] = ff_hevc_put_hevc_bi_w_qpel_v12_8_msa;
-        c->put_hevc_qpel_bi_w[5][1][0] = ff_hevc_put_hevc_bi_w_qpel_v16_8_msa;
-        c->put_hevc_qpel_bi_w[6][1][0] = ff_hevc_put_hevc_bi_w_qpel_v24_8_msa;
-        c->put_hevc_qpel_bi_w[7][1][0] = ff_hevc_put_hevc_bi_w_qpel_v32_8_msa;
-        c->put_hevc_qpel_bi_w[8][1][0] = ff_hevc_put_hevc_bi_w_qpel_v48_8_msa;
-        c->put_hevc_qpel_bi_w[9][1][0] = ff_hevc_put_hevc_bi_w_qpel_v64_8_msa;
-
-        c->put_hevc_qpel_bi_w[1][1][1] = ff_hevc_put_hevc_bi_w_qpel_hv4_8_msa;
-        c->put_hevc_qpel_bi_w[3][1][1] = ff_hevc_put_hevc_bi_w_qpel_hv8_8_msa;
-        c->put_hevc_qpel_bi_w[4][1][1] = ff_hevc_put_hevc_bi_w_qpel_hv12_8_msa;
-        c->put_hevc_qpel_bi_w[5][1][1] = ff_hevc_put_hevc_bi_w_qpel_hv16_8_msa;
-        c->put_hevc_qpel_bi_w[6][1][1] = ff_hevc_put_hevc_bi_w_qpel_hv24_8_msa;
-        c->put_hevc_qpel_bi_w[7][1][1] = ff_hevc_put_hevc_bi_w_qpel_hv32_8_msa;
-        c->put_hevc_qpel_bi_w[8][1][1] = ff_hevc_put_hevc_bi_w_qpel_hv48_8_msa;
-        c->put_hevc_qpel_bi_w[9][1][1] = ff_hevc_put_hevc_bi_w_qpel_hv64_8_msa;
-
-        c->put_hevc_epel_bi_w[1][0][0] =
-            ff_hevc_put_hevc_bi_w_pel_pixels4_8_msa;
-        c->put_hevc_epel_bi_w[2][0][0] =
-            ff_hevc_put_hevc_bi_w_pel_pixels6_8_msa;
-        c->put_hevc_epel_bi_w[3][0][0] =
-            ff_hevc_put_hevc_bi_w_pel_pixels8_8_msa;
-        c->put_hevc_epel_bi_w[4][0][0] =
-            ff_hevc_put_hevc_bi_w_pel_pixels12_8_msa;
-        c->put_hevc_epel_bi_w[5][0][0] =
-            ff_hevc_put_hevc_bi_w_pel_pixels16_8_msa;
-        c->put_hevc_epel_bi_w[6][0][0] =
-            ff_hevc_put_hevc_bi_w_pel_pixels24_8_msa;
-        c->put_hevc_epel_bi_w[7][0][0] =
-            ff_hevc_put_hevc_bi_w_pel_pixels32_8_msa;
-
-        c->put_hevc_epel_bi_w[1][0][1] = ff_hevc_put_hevc_bi_w_epel_h4_8_msa;
-        c->put_hevc_epel_bi_w[2][0][1] = ff_hevc_put_hevc_bi_w_epel_h6_8_msa;
-        c->put_hevc_epel_bi_w[3][0][1] = ff_hevc_put_hevc_bi_w_epel_h8_8_msa;
-        c->put_hevc_epel_bi_w[4][0][1] = ff_hevc_put_hevc_bi_w_epel_h12_8_msa;
-        c->put_hevc_epel_bi_w[5][0][1] = ff_hevc_put_hevc_bi_w_epel_h16_8_msa;
-        c->put_hevc_epel_bi_w[6][0][1] = ff_hevc_put_hevc_bi_w_epel_h24_8_msa;
-        c->put_hevc_epel_bi_w[7][0][1] = ff_hevc_put_hevc_bi_w_epel_h32_8_msa;
-
-        c->put_hevc_epel_bi_w[1][1][0] = ff_hevc_put_hevc_bi_w_epel_v4_8_msa;
-        c->put_hevc_epel_bi_w[2][1][0] = ff_hevc_put_hevc_bi_w_epel_v6_8_msa;
-        c->put_hevc_epel_bi_w[3][1][0] = ff_hevc_put_hevc_bi_w_epel_v8_8_msa;
-        c->put_hevc_epel_bi_w[4][1][0] = ff_hevc_put_hevc_bi_w_epel_v12_8_msa;
-        c->put_hevc_epel_bi_w[5][1][0] = ff_hevc_put_hevc_bi_w_epel_v16_8_msa;
-        c->put_hevc_epel_bi_w[6][1][0] = ff_hevc_put_hevc_bi_w_epel_v24_8_msa;
-        c->put_hevc_epel_bi_w[7][1][0] = ff_hevc_put_hevc_bi_w_epel_v32_8_msa;
-
-        c->put_hevc_epel_bi_w[1][1][1] = ff_hevc_put_hevc_bi_w_epel_hv4_8_msa;
-        c->put_hevc_epel_bi_w[2][1][1] = ff_hevc_put_hevc_bi_w_epel_hv6_8_msa;
-        c->put_hevc_epel_bi_w[3][1][1] = ff_hevc_put_hevc_bi_w_epel_hv8_8_msa;
-        c->put_hevc_epel_bi_w[4][1][1] = ff_hevc_put_hevc_bi_w_epel_hv12_8_msa;
-        c->put_hevc_epel_bi_w[5][1][1] = ff_hevc_put_hevc_bi_w_epel_hv16_8_msa;
-        c->put_hevc_epel_bi_w[6][1][1] = ff_hevc_put_hevc_bi_w_epel_hv24_8_msa;
-        c->put_hevc_epel_bi_w[7][1][1] = ff_hevc_put_hevc_bi_w_epel_hv32_8_msa;
-
-        c->sao_band_filter[0] =
-        c->sao_band_filter[1] =
-        c->sao_band_filter[2] =
-        c->sao_band_filter[3] =
-        c->sao_band_filter[4] = ff_hevc_sao_band_filter_0_8_msa;
-
-        c->sao_edge_filter[0] =
-        c->sao_edge_filter[1] =
-        c->sao_edge_filter[2] =
-        c->sao_edge_filter[3] =
-        c->sao_edge_filter[4] = ff_hevc_sao_edge_filter_8_msa;
-
-        c->hevc_h_loop_filter_luma = ff_hevc_loop_filter_luma_h_8_msa;
-        c->hevc_v_loop_filter_luma = ff_hevc_loop_filter_luma_v_8_msa;
-
-        c->hevc_h_loop_filter_chroma = ff_hevc_loop_filter_chroma_h_8_msa;
-        c->hevc_v_loop_filter_chroma = ff_hevc_loop_filter_chroma_v_8_msa;
-
-        c->hevc_h_loop_filter_luma_c = ff_hevc_loop_filter_luma_h_8_msa;
-        c->hevc_v_loop_filter_luma_c = ff_hevc_loop_filter_luma_v_8_msa;
-
-        c->hevc_h_loop_filter_chroma_c =
-            ff_hevc_loop_filter_chroma_h_8_msa;
-        c->hevc_v_loop_filter_chroma_c =
-            ff_hevc_loop_filter_chroma_v_8_msa;
-
-        c->idct[0] = ff_hevc_idct_4x4_msa;
-        c->idct[1] = ff_hevc_idct_8x8_msa;
-        c->idct[2] = ff_hevc_idct_16x16_msa;
-        c->idct[3] = ff_hevc_idct_32x32_msa;
-        c->idct_dc[0] = ff_hevc_idct_dc_4x4_msa;
-        c->idct_dc[1] = ff_hevc_idct_dc_8x8_msa;
-        c->idct_dc[2] = ff_hevc_idct_dc_16x16_msa;
-        c->idct_dc[3] = ff_hevc_idct_dc_32x32_msa;
-        c->add_residual[0] = ff_hevc_addblk_4x4_msa;
-        c->add_residual[1] = ff_hevc_addblk_8x8_msa;
-        c->add_residual[2] = ff_hevc_addblk_16x16_msa;
-        c->add_residual[3] = ff_hevc_addblk_32x32_msa;
-        c->transform_4x4_luma = ff_hevc_idct_luma_4x4_msa;
+    if (have_msa(cpu_flags)) {
+        if (bit_depth == 8) {
+            c->put_hevc_qpel[1][0][0] = ff_hevc_put_hevc_pel_pixels4_8_msa;
+            c->put_hevc_qpel[2][0][0] = ff_hevc_put_hevc_pel_pixels6_8_msa;
+            c->put_hevc_qpel[3][0][0] = ff_hevc_put_hevc_pel_pixels8_8_msa;
+            c->put_hevc_qpel[4][0][0] = ff_hevc_put_hevc_pel_pixels12_8_msa;
+            c->put_hevc_qpel[5][0][0] = ff_hevc_put_hevc_pel_pixels16_8_msa;
+            c->put_hevc_qpel[6][0][0] = ff_hevc_put_hevc_pel_pixels24_8_msa;
+            c->put_hevc_qpel[7][0][0] = ff_hevc_put_hevc_pel_pixels32_8_msa;
+            c->put_hevc_qpel[8][0][0] = ff_hevc_put_hevc_pel_pixels48_8_msa;
+            c->put_hevc_qpel[9][0][0] = ff_hevc_put_hevc_pel_pixels64_8_msa;
+
+            c->put_hevc_qpel[1][0][1] = ff_hevc_put_hevc_qpel_h4_8_msa;
+            c->put_hevc_qpel[3][0][1] = ff_hevc_put_hevc_qpel_h8_8_msa;
+            c->put_hevc_qpel[4][0][1] = ff_hevc_put_hevc_qpel_h12_8_msa;
+            c->put_hevc_qpel[5][0][1] = ff_hevc_put_hevc_qpel_h16_8_msa;
+            c->put_hevc_qpel[6][0][1] = ff_hevc_put_hevc_qpel_h24_8_msa;
+            c->put_hevc_qpel[7][0][1] = ff_hevc_put_hevc_qpel_h32_8_msa;
+            c->put_hevc_qpel[8][0][1] = ff_hevc_put_hevc_qpel_h48_8_msa;
+            c->put_hevc_qpel[9][0][1] = ff_hevc_put_hevc_qpel_h64_8_msa;
+
+            c->put_hevc_qpel[1][1][0] = ff_hevc_put_hevc_qpel_v4_8_msa;
+            c->put_hevc_qpel[3][1][0] = ff_hevc_put_hevc_qpel_v8_8_msa;
+            c->put_hevc_qpel[4][1][0] = ff_hevc_put_hevc_qpel_v12_8_msa;
+            c->put_hevc_qpel[5][1][0] = ff_hevc_put_hevc_qpel_v16_8_msa;
+            c->put_hevc_qpel[6][1][0] = ff_hevc_put_hevc_qpel_v24_8_msa;
+            c->put_hevc_qpel[7][1][0] = ff_hevc_put_hevc_qpel_v32_8_msa;
+            c->put_hevc_qpel[8][1][0] = ff_hevc_put_hevc_qpel_v48_8_msa;
+            c->put_hevc_qpel[9][1][0] = ff_hevc_put_hevc_qpel_v64_8_msa;
+
+            c->put_hevc_qpel[1][1][1] = ff_hevc_put_hevc_qpel_hv4_8_msa;
+            c->put_hevc_qpel[3][1][1] = ff_hevc_put_hevc_qpel_hv8_8_msa;
+            c->put_hevc_qpel[4][1][1] = ff_hevc_put_hevc_qpel_hv12_8_msa;
+            c->put_hevc_qpel[5][1][1] = ff_hevc_put_hevc_qpel_hv16_8_msa;
+            c->put_hevc_qpel[6][1][1] = ff_hevc_put_hevc_qpel_hv24_8_msa;
+            c->put_hevc_qpel[7][1][1] = ff_hevc_put_hevc_qpel_hv32_8_msa;
+            c->put_hevc_qpel[8][1][1] = ff_hevc_put_hevc_qpel_hv48_8_msa;
+            c->put_hevc_qpel[9][1][1] = ff_hevc_put_hevc_qpel_hv64_8_msa;
+
+            c->put_hevc_epel[1][0][0] = ff_hevc_put_hevc_pel_pixels4_8_msa;
+            c->put_hevc_epel[2][0][0] = ff_hevc_put_hevc_pel_pixels6_8_msa;
+            c->put_hevc_epel[3][0][0] = ff_hevc_put_hevc_pel_pixels8_8_msa;
+            c->put_hevc_epel[4][0][0] = ff_hevc_put_hevc_pel_pixels12_8_msa;
+            c->put_hevc_epel[5][0][0] = ff_hevc_put_hevc_pel_pixels16_8_msa;
+            c->put_hevc_epel[6][0][0] = ff_hevc_put_hevc_pel_pixels24_8_msa;
+            c->put_hevc_epel[7][0][0] = ff_hevc_put_hevc_pel_pixels32_8_msa;
+
+            c->put_hevc_epel[1][0][1] = ff_hevc_put_hevc_epel_h4_8_msa;
+            c->put_hevc_epel[2][0][1] = ff_hevc_put_hevc_epel_h6_8_msa;
+            c->put_hevc_epel[3][0][1] = ff_hevc_put_hevc_epel_h8_8_msa;
+            c->put_hevc_epel[4][0][1] = ff_hevc_put_hevc_epel_h12_8_msa;
+            c->put_hevc_epel[5][0][1] = ff_hevc_put_hevc_epel_h16_8_msa;
+            c->put_hevc_epel[6][0][1] = ff_hevc_put_hevc_epel_h24_8_msa;
+            c->put_hevc_epel[7][0][1] = ff_hevc_put_hevc_epel_h32_8_msa;
+
+            c->put_hevc_epel[1][1][0] = ff_hevc_put_hevc_epel_v4_8_msa;
+            c->put_hevc_epel[2][1][0] = ff_hevc_put_hevc_epel_v6_8_msa;
+            c->put_hevc_epel[3][1][0] = ff_hevc_put_hevc_epel_v8_8_msa;
+            c->put_hevc_epel[4][1][0] = ff_hevc_put_hevc_epel_v12_8_msa;
+            c->put_hevc_epel[5][1][0] = ff_hevc_put_hevc_epel_v16_8_msa;
+            c->put_hevc_epel[6][1][0] = ff_hevc_put_hevc_epel_v24_8_msa;
+            c->put_hevc_epel[7][1][0] = ff_hevc_put_hevc_epel_v32_8_msa;
+
+            c->put_hevc_epel[1][1][1] = ff_hevc_put_hevc_epel_hv4_8_msa;
+            c->put_hevc_epel[2][1][1] = ff_hevc_put_hevc_epel_hv6_8_msa;
+            c->put_hevc_epel[3][1][1] = ff_hevc_put_hevc_epel_hv8_8_msa;
+            c->put_hevc_epel[4][1][1] = ff_hevc_put_hevc_epel_hv12_8_msa;
+            c->put_hevc_epel[5][1][1] = ff_hevc_put_hevc_epel_hv16_8_msa;
+            c->put_hevc_epel[6][1][1] = ff_hevc_put_hevc_epel_hv24_8_msa;
+            c->put_hevc_epel[7][1][1] = ff_hevc_put_hevc_epel_hv32_8_msa;
+
+            c->put_hevc_qpel_uni[3][0][0] = ff_hevc_put_hevc_uni_pel_pixels8_8_msa;
+            c->put_hevc_qpel_uni[4][0][0] = ff_hevc_put_hevc_uni_pel_pixels12_8_msa;
+            c->put_hevc_qpel_uni[5][0][0] = ff_hevc_put_hevc_uni_pel_pixels16_8_msa;
+            c->put_hevc_qpel_uni[6][0][0] = ff_hevc_put_hevc_uni_pel_pixels24_8_msa;
+            c->put_hevc_qpel_uni[7][0][0] = ff_hevc_put_hevc_uni_pel_pixels32_8_msa;
+            c->put_hevc_qpel_uni[8][0][0] = ff_hevc_put_hevc_uni_pel_pixels48_8_msa;
+            c->put_hevc_qpel_uni[9][0][0] = ff_hevc_put_hevc_uni_pel_pixels64_8_msa;
+
+            c->put_hevc_qpel_uni[1][0][1] = ff_hevc_put_hevc_uni_qpel_h4_8_msa;
+            c->put_hevc_qpel_uni[3][0][1] = ff_hevc_put_hevc_uni_qpel_h8_8_msa;
+            c->put_hevc_qpel_uni[4][0][1] = ff_hevc_put_hevc_uni_qpel_h12_8_msa;
+            c->put_hevc_qpel_uni[5][0][1] = ff_hevc_put_hevc_uni_qpel_h16_8_msa;
+            c->put_hevc_qpel_uni[6][0][1] = ff_hevc_put_hevc_uni_qpel_h24_8_msa;
+            c->put_hevc_qpel_uni[7][0][1] = ff_hevc_put_hevc_uni_qpel_h32_8_msa;
+            c->put_hevc_qpel_uni[8][0][1] = ff_hevc_put_hevc_uni_qpel_h48_8_msa;
+            c->put_hevc_qpel_uni[9][0][1] = ff_hevc_put_hevc_uni_qpel_h64_8_msa;
+
+            c->put_hevc_qpel_uni[1][1][0] = ff_hevc_put_hevc_uni_qpel_v4_8_msa;
+            c->put_hevc_qpel_uni[3][1][0] = ff_hevc_put_hevc_uni_qpel_v8_8_msa;
+            c->put_hevc_qpel_uni[4][1][0] = ff_hevc_put_hevc_uni_qpel_v12_8_msa;
+            c->put_hevc_qpel_uni[5][1][0] = ff_hevc_put_hevc_uni_qpel_v16_8_msa;
+            c->put_hevc_qpel_uni[6][1][0] = ff_hevc_put_hevc_uni_qpel_v24_8_msa;
+            c->put_hevc_qpel_uni[7][1][0] = ff_hevc_put_hevc_uni_qpel_v32_8_msa;
+            c->put_hevc_qpel_uni[8][1][0] = ff_hevc_put_hevc_uni_qpel_v48_8_msa;
+            c->put_hevc_qpel_uni[9][1][0] = ff_hevc_put_hevc_uni_qpel_v64_8_msa;
+
+            c->put_hevc_qpel_uni[1][1][1] = ff_hevc_put_hevc_uni_qpel_hv4_8_msa;
+            c->put_hevc_qpel_uni[3][1][1] = ff_hevc_put_hevc_uni_qpel_hv8_8_msa;
+            c->put_hevc_qpel_uni[4][1][1] = ff_hevc_put_hevc_uni_qpel_hv12_8_msa;
+            c->put_hevc_qpel_uni[5][1][1] = ff_hevc_put_hevc_uni_qpel_hv16_8_msa;
+            c->put_hevc_qpel_uni[6][1][1] = ff_hevc_put_hevc_uni_qpel_hv24_8_msa;
+            c->put_hevc_qpel_uni[7][1][1] = ff_hevc_put_hevc_uni_qpel_hv32_8_msa;
+            c->put_hevc_qpel_uni[8][1][1] = ff_hevc_put_hevc_uni_qpel_hv48_8_msa;
+            c->put_hevc_qpel_uni[9][1][1] = ff_hevc_put_hevc_uni_qpel_hv64_8_msa;
+
+            c->put_hevc_epel_uni[3][0][0] = ff_hevc_put_hevc_uni_pel_pixels8_8_msa;
+            c->put_hevc_epel_uni[4][0][0] = ff_hevc_put_hevc_uni_pel_pixels12_8_msa;
+            c->put_hevc_epel_uni[5][0][0] = ff_hevc_put_hevc_uni_pel_pixels16_8_msa;
+            c->put_hevc_epel_uni[6][0][0] = ff_hevc_put_hevc_uni_pel_pixels24_8_msa;
+            c->put_hevc_epel_uni[7][0][0] = ff_hevc_put_hevc_uni_pel_pixels32_8_msa;
+
+            c->put_hevc_epel_uni[1][0][1] = ff_hevc_put_hevc_uni_epel_h4_8_msa;
+            c->put_hevc_epel_uni[2][0][1] = ff_hevc_put_hevc_uni_epel_h6_8_msa;
+            c->put_hevc_epel_uni[3][0][1] = ff_hevc_put_hevc_uni_epel_h8_8_msa;
+            c->put_hevc_epel_uni[4][0][1] = ff_hevc_put_hevc_uni_epel_h12_8_msa;
+            c->put_hevc_epel_uni[5][0][1] = ff_hevc_put_hevc_uni_epel_h16_8_msa;
+            c->put_hevc_epel_uni[6][0][1] = ff_hevc_put_hevc_uni_epel_h24_8_msa;
+            c->put_hevc_epel_uni[7][0][1] = ff_hevc_put_hevc_uni_epel_h32_8_msa;
+
+            c->put_hevc_epel_uni[1][1][0] = ff_hevc_put_hevc_uni_epel_v4_8_msa;
+            c->put_hevc_epel_uni[2][1][0] = ff_hevc_put_hevc_uni_epel_v6_8_msa;
+            c->put_hevc_epel_uni[3][1][0] = ff_hevc_put_hevc_uni_epel_v8_8_msa;
+            c->put_hevc_epel_uni[4][1][0] = ff_hevc_put_hevc_uni_epel_v12_8_msa;
+            c->put_hevc_epel_uni[5][1][0] = ff_hevc_put_hevc_uni_epel_v16_8_msa;
+            c->put_hevc_epel_uni[6][1][0] = ff_hevc_put_hevc_uni_epel_v24_8_msa;
+            c->put_hevc_epel_uni[7][1][0] = ff_hevc_put_hevc_uni_epel_v32_8_msa;
+
+            c->put_hevc_epel_uni[1][1][1] = ff_hevc_put_hevc_uni_epel_hv4_8_msa;
+            c->put_hevc_epel_uni[2][1][1] = ff_hevc_put_hevc_uni_epel_hv6_8_msa;
+            c->put_hevc_epel_uni[3][1][1] = ff_hevc_put_hevc_uni_epel_hv8_8_msa;
+            c->put_hevc_epel_uni[4][1][1] = ff_hevc_put_hevc_uni_epel_hv12_8_msa;
+            c->put_hevc_epel_uni[5][1][1] = ff_hevc_put_hevc_uni_epel_hv16_8_msa;
+            c->put_hevc_epel_uni[6][1][1] = ff_hevc_put_hevc_uni_epel_hv24_8_msa;
+            c->put_hevc_epel_uni[7][1][1] = ff_hevc_put_hevc_uni_epel_hv32_8_msa;
+
+            c->put_hevc_qpel_uni_w[1][0][0] =
+                ff_hevc_put_hevc_uni_w_pel_pixels4_8_msa;
+            c->put_hevc_qpel_uni_w[3][0][0] =
+                ff_hevc_put_hevc_uni_w_pel_pixels8_8_msa;
+            c->put_hevc_qpel_uni_w[4][0][0] =
+                ff_hevc_put_hevc_uni_w_pel_pixels12_8_msa;
+            c->put_hevc_qpel_uni_w[5][0][0] =
+                ff_hevc_put_hevc_uni_w_pel_pixels16_8_msa;
+            c->put_hevc_qpel_uni_w[6][0][0] =
+                ff_hevc_put_hevc_uni_w_pel_pixels24_8_msa;
+            c->put_hevc_qpel_uni_w[7][0][0] =
+                ff_hevc_put_hevc_uni_w_pel_pixels32_8_msa;
+            c->put_hevc_qpel_uni_w[8][0][0] =
+                ff_hevc_put_hevc_uni_w_pel_pixels48_8_msa;
+            c->put_hevc_qpel_uni_w[9][0][0] =
+                ff_hevc_put_hevc_uni_w_pel_pixels64_8_msa;
+
+            c->put_hevc_qpel_uni_w[1][0][1] = ff_hevc_put_hevc_uni_w_qpel_h4_8_msa;
+            c->put_hevc_qpel_uni_w[3][0][1] = ff_hevc_put_hevc_uni_w_qpel_h8_8_msa;
+            c->put_hevc_qpel_uni_w[4][0][1] = ff_hevc_put_hevc_uni_w_qpel_h12_8_msa;
+            c->put_hevc_qpel_uni_w[5][0][1] = ff_hevc_put_hevc_uni_w_qpel_h16_8_msa;
+            c->put_hevc_qpel_uni_w[6][0][1] = ff_hevc_put_hevc_uni_w_qpel_h24_8_msa;
+            c->put_hevc_qpel_uni_w[7][0][1] = ff_hevc_put_hevc_uni_w_qpel_h32_8_msa;
+            c->put_hevc_qpel_uni_w[8][0][1] = ff_hevc_put_hevc_uni_w_qpel_h48_8_msa;
+            c->put_hevc_qpel_uni_w[9][0][1] = ff_hevc_put_hevc_uni_w_qpel_h64_8_msa;
+
+            c->put_hevc_qpel_uni_w[1][1][0] = ff_hevc_put_hevc_uni_w_qpel_v4_8_msa;
+            c->put_hevc_qpel_uni_w[3][1][0] = ff_hevc_put_hevc_uni_w_qpel_v8_8_msa;
+            c->put_hevc_qpel_uni_w[4][1][0] = ff_hevc_put_hevc_uni_w_qpel_v12_8_msa;
+            c->put_hevc_qpel_uni_w[5][1][0] = ff_hevc_put_hevc_uni_w_qpel_v16_8_msa;
+            c->put_hevc_qpel_uni_w[6][1][0] = ff_hevc_put_hevc_uni_w_qpel_v24_8_msa;
+            c->put_hevc_qpel_uni_w[7][1][0] = ff_hevc_put_hevc_uni_w_qpel_v32_8_msa;
+            c->put_hevc_qpel_uni_w[8][1][0] = ff_hevc_put_hevc_uni_w_qpel_v48_8_msa;
+            c->put_hevc_qpel_uni_w[9][1][0] = ff_hevc_put_hevc_uni_w_qpel_v64_8_msa;
+
+            c->put_hevc_qpel_uni_w[1][1][1] = ff_hevc_put_hevc_uni_w_qpel_hv4_8_msa;
+            c->put_hevc_qpel_uni_w[3][1][1] = ff_hevc_put_hevc_uni_w_qpel_hv8_8_msa;
+            c->put_hevc_qpel_uni_w[4][1][1] =
+                ff_hevc_put_hevc_uni_w_qpel_hv12_8_msa;
+            c->put_hevc_qpel_uni_w[5][1][1] =
+                ff_hevc_put_hevc_uni_w_qpel_hv16_8_msa;
+            c->put_hevc_qpel_uni_w[6][1][1] =
+                ff_hevc_put_hevc_uni_w_qpel_hv24_8_msa;
+            c->put_hevc_qpel_uni_w[7][1][1] =
+                ff_hevc_put_hevc_uni_w_qpel_hv32_8_msa;
+            c->put_hevc_qpel_uni_w[8][1][1] =
+                ff_hevc_put_hevc_uni_w_qpel_hv48_8_msa;
+            c->put_hevc_qpel_uni_w[9][1][1] =
+                ff_hevc_put_hevc_uni_w_qpel_hv64_8_msa;
+
+            c->put_hevc_epel_uni_w[1][0][0] =
+                ff_hevc_put_hevc_uni_w_pel_pixels4_8_msa;
+            c->put_hevc_epel_uni_w[2][0][0] =
+                ff_hevc_put_hevc_uni_w_pel_pixels6_8_msa;
+            c->put_hevc_epel_uni_w[3][0][0] =
+                ff_hevc_put_hevc_uni_w_pel_pixels8_8_msa;
+            c->put_hevc_epel_uni_w[4][0][0] =
+                ff_hevc_put_hevc_uni_w_pel_pixels12_8_msa;
+            c->put_hevc_epel_uni_w[5][0][0] =
+                ff_hevc_put_hevc_uni_w_pel_pixels16_8_msa;
+            c->put_hevc_epel_uni_w[6][0][0] =
+                ff_hevc_put_hevc_uni_w_pel_pixels24_8_msa;
+            c->put_hevc_epel_uni_w[7][0][0] =
+                ff_hevc_put_hevc_uni_w_pel_pixels32_8_msa;
+
+            c->put_hevc_epel_uni_w[1][0][1] = ff_hevc_put_hevc_uni_w_epel_h4_8_msa;
+            c->put_hevc_epel_uni_w[2][0][1] = ff_hevc_put_hevc_uni_w_epel_h6_8_msa;
+            c->put_hevc_epel_uni_w[3][0][1] = ff_hevc_put_hevc_uni_w_epel_h8_8_msa;
+            c->put_hevc_epel_uni_w[4][0][1] = ff_hevc_put_hevc_uni_w_epel_h12_8_msa;
+            c->put_hevc_epel_uni_w[5][0][1] = ff_hevc_put_hevc_uni_w_epel_h16_8_msa;
+            c->put_hevc_epel_uni_w[6][0][1] = ff_hevc_put_hevc_uni_w_epel_h24_8_msa;
+            c->put_hevc_epel_uni_w[7][0][1] = ff_hevc_put_hevc_uni_w_epel_h32_8_msa;
+
+            c->put_hevc_epel_uni_w[1][1][0] = ff_hevc_put_hevc_uni_w_epel_v4_8_msa;
+            c->put_hevc_epel_uni_w[2][1][0] = ff_hevc_put_hevc_uni_w_epel_v6_8_msa;
+            c->put_hevc_epel_uni_w[3][1][0] = ff_hevc_put_hevc_uni_w_epel_v8_8_msa;
+            c->put_hevc_epel_uni_w[4][1][0] = ff_hevc_put_hevc_uni_w_epel_v12_8_msa;
+            c->put_hevc_epel_uni_w[5][1][0] = ff_hevc_put_hevc_uni_w_epel_v16_8_msa;
+            c->put_hevc_epel_uni_w[6][1][0] = ff_hevc_put_hevc_uni_w_epel_v24_8_msa;
+            c->put_hevc_epel_uni_w[7][1][0] = ff_hevc_put_hevc_uni_w_epel_v32_8_msa;
+
+            c->put_hevc_epel_uni_w[1][1][1] = ff_hevc_put_hevc_uni_w_epel_hv4_8_msa;
+            c->put_hevc_epel_uni_w[2][1][1] = ff_hevc_put_hevc_uni_w_epel_hv6_8_msa;
+            c->put_hevc_epel_uni_w[3][1][1] = ff_hevc_put_hevc_uni_w_epel_hv8_8_msa;
+            c->put_hevc_epel_uni_w[4][1][1] =
+                ff_hevc_put_hevc_uni_w_epel_hv12_8_msa;
+            c->put_hevc_epel_uni_w[5][1][1] =
+                ff_hevc_put_hevc_uni_w_epel_hv16_8_msa;
+            c->put_hevc_epel_uni_w[6][1][1] =
+                ff_hevc_put_hevc_uni_w_epel_hv24_8_msa;
+            c->put_hevc_epel_uni_w[7][1][1] =
+                ff_hevc_put_hevc_uni_w_epel_hv32_8_msa;
+
+            c->put_hevc_qpel_bi[1][0][0] = ff_hevc_put_hevc_bi_pel_pixels4_8_msa;
+            c->put_hevc_qpel_bi[3][0][0] = ff_hevc_put_hevc_bi_pel_pixels8_8_msa;
+            c->put_hevc_qpel_bi[4][0][0] = ff_hevc_put_hevc_bi_pel_pixels12_8_msa;
+            c->put_hevc_qpel_bi[5][0][0] = ff_hevc_put_hevc_bi_pel_pixels16_8_msa;
+            c->put_hevc_qpel_bi[6][0][0] = ff_hevc_put_hevc_bi_pel_pixels24_8_msa;
+            c->put_hevc_qpel_bi[7][0][0] = ff_hevc_put_hevc_bi_pel_pixels32_8_msa;
+            c->put_hevc_qpel_bi[8][0][0] = ff_hevc_put_hevc_bi_pel_pixels48_8_msa;
+            c->put_hevc_qpel_bi[9][0][0] = ff_hevc_put_hevc_bi_pel_pixels64_8_msa;
+
+            c->put_hevc_qpel_bi[1][0][1] = ff_hevc_put_hevc_bi_qpel_h4_8_msa;
+            c->put_hevc_qpel_bi[3][0][1] = ff_hevc_put_hevc_bi_qpel_h8_8_msa;
+            c->put_hevc_qpel_bi[4][0][1] = ff_hevc_put_hevc_bi_qpel_h12_8_msa;
+            c->put_hevc_qpel_bi[5][0][1] = ff_hevc_put_hevc_bi_qpel_h16_8_msa;
+            c->put_hevc_qpel_bi[6][0][1] = ff_hevc_put_hevc_bi_qpel_h24_8_msa;
+            c->put_hevc_qpel_bi[7][0][1] = ff_hevc_put_hevc_bi_qpel_h32_8_msa;
+            c->put_hevc_qpel_bi[8][0][1] = ff_hevc_put_hevc_bi_qpel_h48_8_msa;
+            c->put_hevc_qpel_bi[9][0][1] = ff_hevc_put_hevc_bi_qpel_h64_8_msa;
+
+            c->put_hevc_qpel_bi[1][1][0] = ff_hevc_put_hevc_bi_qpel_v4_8_msa;
+            c->put_hevc_qpel_bi[3][1][0] = ff_hevc_put_hevc_bi_qpel_v8_8_msa;
+            c->put_hevc_qpel_bi[4][1][0] = ff_hevc_put_hevc_bi_qpel_v12_8_msa;
+            c->put_hevc_qpel_bi[5][1][0] = ff_hevc_put_hevc_bi_qpel_v16_8_msa;
+            c->put_hevc_qpel_bi[6][1][0] = ff_hevc_put_hevc_bi_qpel_v24_8_msa;
+            c->put_hevc_qpel_bi[7][1][0] = ff_hevc_put_hevc_bi_qpel_v32_8_msa;
+            c->put_hevc_qpel_bi[8][1][0] = ff_hevc_put_hevc_bi_qpel_v48_8_msa;
+            c->put_hevc_qpel_bi[9][1][0] = ff_hevc_put_hevc_bi_qpel_v64_8_msa;
+
+            c->put_hevc_qpel_bi[1][1][1] = ff_hevc_put_hevc_bi_qpel_hv4_8_msa;
+            c->put_hevc_qpel_bi[3][1][1] = ff_hevc_put_hevc_bi_qpel_hv8_8_msa;
+            c->put_hevc_qpel_bi[4][1][1] = ff_hevc_put_hevc_bi_qpel_hv12_8_msa;
+            c->put_hevc_qpel_bi[5][1][1] = ff_hevc_put_hevc_bi_qpel_hv16_8_msa;
+            c->put_hevc_qpel_bi[6][1][1] = ff_hevc_put_hevc_bi_qpel_hv24_8_msa;
+            c->put_hevc_qpel_bi[7][1][1] = ff_hevc_put_hevc_bi_qpel_hv32_8_msa;
+            c->put_hevc_qpel_bi[8][1][1] = ff_hevc_put_hevc_bi_qpel_hv48_8_msa;
+            c->put_hevc_qpel_bi[9][1][1] = ff_hevc_put_hevc_bi_qpel_hv64_8_msa;
+
+            c->put_hevc_epel_bi[1][0][0] = ff_hevc_put_hevc_bi_pel_pixels4_8_msa;
+            c->put_hevc_epel_bi[2][0][0] = ff_hevc_put_hevc_bi_pel_pixels6_8_msa;
+            c->put_hevc_epel_bi[3][0][0] = ff_hevc_put_hevc_bi_pel_pixels8_8_msa;
+            c->put_hevc_epel_bi[4][0][0] = ff_hevc_put_hevc_bi_pel_pixels12_8_msa;
+            c->put_hevc_epel_bi[5][0][0] = ff_hevc_put_hevc_bi_pel_pixels16_8_msa;
+            c->put_hevc_epel_bi[6][0][0] = ff_hevc_put_hevc_bi_pel_pixels24_8_msa;
+            c->put_hevc_epel_bi[7][0][0] = ff_hevc_put_hevc_bi_pel_pixels32_8_msa;
+
+            c->put_hevc_epel_bi[1][0][1] = ff_hevc_put_hevc_bi_epel_h4_8_msa;
+            c->put_hevc_epel_bi[2][0][1] = ff_hevc_put_hevc_bi_epel_h6_8_msa;
+            c->put_hevc_epel_bi[3][0][1] = ff_hevc_put_hevc_bi_epel_h8_8_msa;
+            c->put_hevc_epel_bi[4][0][1] = ff_hevc_put_hevc_bi_epel_h12_8_msa;
+            c->put_hevc_epel_bi[5][0][1] = ff_hevc_put_hevc_bi_epel_h16_8_msa;
+            c->put_hevc_epel_bi[6][0][1] = ff_hevc_put_hevc_bi_epel_h24_8_msa;
+            c->put_hevc_epel_bi[7][0][1] = ff_hevc_put_hevc_bi_epel_h32_8_msa;
+
+            c->put_hevc_epel_bi[1][1][0] = ff_hevc_put_hevc_bi_epel_v4_8_msa;
+            c->put_hevc_epel_bi[2][1][0] = ff_hevc_put_hevc_bi_epel_v6_8_msa;
+            c->put_hevc_epel_bi[3][1][0] = ff_hevc_put_hevc_bi_epel_v8_8_msa;
+            c->put_hevc_epel_bi[4][1][0] = ff_hevc_put_hevc_bi_epel_v12_8_msa;
+            c->put_hevc_epel_bi[5][1][0] = ff_hevc_put_hevc_bi_epel_v16_8_msa;
+            c->put_hevc_epel_bi[6][1][0] = ff_hevc_put_hevc_bi_epel_v24_8_msa;
+            c->put_hevc_epel_bi[7][1][0] = ff_hevc_put_hevc_bi_epel_v32_8_msa;
+
+            c->put_hevc_epel_bi[1][1][1] = ff_hevc_put_hevc_bi_epel_hv4_8_msa;
+            c->put_hevc_epel_bi[2][1][1] = ff_hevc_put_hevc_bi_epel_hv6_8_msa;
+            c->put_hevc_epel_bi[3][1][1] = ff_hevc_put_hevc_bi_epel_hv8_8_msa;
+            c->put_hevc_epel_bi[4][1][1] = ff_hevc_put_hevc_bi_epel_hv12_8_msa;
+            c->put_hevc_epel_bi[5][1][1] = ff_hevc_put_hevc_bi_epel_hv16_8_msa;
+            c->put_hevc_epel_bi[6][1][1] = ff_hevc_put_hevc_bi_epel_hv24_8_msa;
+            c->put_hevc_epel_bi[7][1][1] = ff_hevc_put_hevc_bi_epel_hv32_8_msa;
+
+            c->put_hevc_qpel_bi_w[1][0][0] =
+                ff_hevc_put_hevc_bi_w_pel_pixels4_8_msa;
+            c->put_hevc_qpel_bi_w[3][0][0] =
+                ff_hevc_put_hevc_bi_w_pel_pixels8_8_msa;
+            c->put_hevc_qpel_bi_w[4][0][0] =
+                ff_hevc_put_hevc_bi_w_pel_pixels12_8_msa;
+            c->put_hevc_qpel_bi_w[5][0][0] =
+                ff_hevc_put_hevc_bi_w_pel_pixels16_8_msa;
+            c->put_hevc_qpel_bi_w[6][0][0] =
+                ff_hevc_put_hevc_bi_w_pel_pixels24_8_msa;
+            c->put_hevc_qpel_bi_w[7][0][0] =
+                ff_hevc_put_hevc_bi_w_pel_pixels32_8_msa;
+            c->put_hevc_qpel_bi_w[8][0][0] =
+                ff_hevc_put_hevc_bi_w_pel_pixels48_8_msa;
+            c->put_hevc_qpel_bi_w[9][0][0] =
+                ff_hevc_put_hevc_bi_w_pel_pixels64_8_msa;
+
+            c->put_hevc_qpel_bi_w[1][0][1] = ff_hevc_put_hevc_bi_w_qpel_h4_8_msa;
+            c->put_hevc_qpel_bi_w[3][0][1] = ff_hevc_put_hevc_bi_w_qpel_h8_8_msa;
+            c->put_hevc_qpel_bi_w[4][0][1] = ff_hevc_put_hevc_bi_w_qpel_h12_8_msa;
+            c->put_hevc_qpel_bi_w[5][0][1] = ff_hevc_put_hevc_bi_w_qpel_h16_8_msa;
+            c->put_hevc_qpel_bi_w[6][0][1] = ff_hevc_put_hevc_bi_w_qpel_h24_8_msa;
+            c->put_hevc_qpel_bi_w[7][0][1] = ff_hevc_put_hevc_bi_w_qpel_h32_8_msa;
+            c->put_hevc_qpel_bi_w[8][0][1] = ff_hevc_put_hevc_bi_w_qpel_h48_8_msa;
+            c->put_hevc_qpel_bi_w[9][0][1] = ff_hevc_put_hevc_bi_w_qpel_h64_8_msa;
+
+            c->put_hevc_qpel_bi_w[1][1][0] = ff_hevc_put_hevc_bi_w_qpel_v4_8_msa;
+            c->put_hevc_qpel_bi_w[3][1][0] = ff_hevc_put_hevc_bi_w_qpel_v8_8_msa;
+            c->put_hevc_qpel_bi_w[4][1][0] = ff_hevc_put_hevc_bi_w_qpel_v12_8_msa;
+            c->put_hevc_qpel_bi_w[5][1][0] = ff_hevc_put_hevc_bi_w_qpel_v16_8_msa;
+            c->put_hevc_qpel_bi_w[6][1][0] = ff_hevc_put_hevc_bi_w_qpel_v24_8_msa;
+            c->put_hevc_qpel_bi_w[7][1][0] = ff_hevc_put_hevc_bi_w_qpel_v32_8_msa;
+            c->put_hevc_qpel_bi_w[8][1][0] = ff_hevc_put_hevc_bi_w_qpel_v48_8_msa;
+            c->put_hevc_qpel_bi_w[9][1][0] = ff_hevc_put_hevc_bi_w_qpel_v64_8_msa;
+
+            c->put_hevc_qpel_bi_w[1][1][1] = ff_hevc_put_hevc_bi_w_qpel_hv4_8_msa;
+            c->put_hevc_qpel_bi_w[3][1][1] = ff_hevc_put_hevc_bi_w_qpel_hv8_8_msa;
+            c->put_hevc_qpel_bi_w[4][1][1] = ff_hevc_put_hevc_bi_w_qpel_hv12_8_msa;
+            c->put_hevc_qpel_bi_w[5][1][1] = ff_hevc_put_hevc_bi_w_qpel_hv16_8_msa;
+            c->put_hevc_qpel_bi_w[6][1][1] = ff_hevc_put_hevc_bi_w_qpel_hv24_8_msa;
+            c->put_hevc_qpel_bi_w[7][1][1] = ff_hevc_put_hevc_bi_w_qpel_hv32_8_msa;
+            c->put_hevc_qpel_bi_w[8][1][1] = ff_hevc_put_hevc_bi_w_qpel_hv48_8_msa;
+            c->put_hevc_qpel_bi_w[9][1][1] = ff_hevc_put_hevc_bi_w_qpel_hv64_8_msa;
+
+            c->put_hevc_epel_bi_w[1][0][0] =
+                ff_hevc_put_hevc_bi_w_pel_pixels4_8_msa;
+            c->put_hevc_epel_bi_w[2][0][0] =
+                ff_hevc_put_hevc_bi_w_pel_pixels6_8_msa;
+            c->put_hevc_epel_bi_w[3][0][0] =
+                ff_hevc_put_hevc_bi_w_pel_pixels8_8_msa;
+            c->put_hevc_epel_bi_w[4][0][0] =
+                ff_hevc_put_hevc_bi_w_pel_pixels12_8_msa;
+            c->put_hevc_epel_bi_w[5][0][0] =
+                ff_hevc_put_hevc_bi_w_pel_pixels16_8_msa;
+            c->put_hevc_epel_bi_w[6][0][0] =
+                ff_hevc_put_hevc_bi_w_pel_pixels24_8_msa;
+            c->put_hevc_epel_bi_w[7][0][0] =
+                ff_hevc_put_hevc_bi_w_pel_pixels32_8_msa;
+
+            c->put_hevc_epel_bi_w[1][0][1] = ff_hevc_put_hevc_bi_w_epel_h4_8_msa;
+            c->put_hevc_epel_bi_w[2][0][1] = ff_hevc_put_hevc_bi_w_epel_h6_8_msa;
+            c->put_hevc_epel_bi_w[3][0][1] = ff_hevc_put_hevc_bi_w_epel_h8_8_msa;
+            c->put_hevc_epel_bi_w[4][0][1] = ff_hevc_put_hevc_bi_w_epel_h12_8_msa;
+            c->put_hevc_epel_bi_w[5][0][1] = ff_hevc_put_hevc_bi_w_epel_h16_8_msa;
+            c->put_hevc_epel_bi_w[6][0][1] = ff_hevc_put_hevc_bi_w_epel_h24_8_msa;
+            c->put_hevc_epel_bi_w[7][0][1] = ff_hevc_put_hevc_bi_w_epel_h32_8_msa;
+
+            c->put_hevc_epel_bi_w[1][1][0] = ff_hevc_put_hevc_bi_w_epel_v4_8_msa;
+            c->put_hevc_epel_bi_w[2][1][0] = ff_hevc_put_hevc_bi_w_epel_v6_8_msa;
+            c->put_hevc_epel_bi_w[3][1][0] = ff_hevc_put_hevc_bi_w_epel_v8_8_msa;
+            c->put_hevc_epel_bi_w[4][1][0] = ff_hevc_put_hevc_bi_w_epel_v12_8_msa;
+            c->put_hevc_epel_bi_w[5][1][0] = ff_hevc_put_hevc_bi_w_epel_v16_8_msa;
+            c->put_hevc_epel_bi_w[6][1][0] = ff_hevc_put_hevc_bi_w_epel_v24_8_msa;
+            c->put_hevc_epel_bi_w[7][1][0] = ff_hevc_put_hevc_bi_w_epel_v32_8_msa;
+
+            c->put_hevc_epel_bi_w[1][1][1] = ff_hevc_put_hevc_bi_w_epel_hv4_8_msa;
+            c->put_hevc_epel_bi_w[2][1][1] = ff_hevc_put_hevc_bi_w_epel_hv6_8_msa;
+            c->put_hevc_epel_bi_w[3][1][1] = ff_hevc_put_hevc_bi_w_epel_hv8_8_msa;
+            c->put_hevc_epel_bi_w[4][1][1] = ff_hevc_put_hevc_bi_w_epel_hv12_8_msa;
+            c->put_hevc_epel_bi_w[5][1][1] = ff_hevc_put_hevc_bi_w_epel_hv16_8_msa;
+            c->put_hevc_epel_bi_w[6][1][1] = ff_hevc_put_hevc_bi_w_epel_hv24_8_msa;
+            c->put_hevc_epel_bi_w[7][1][1] = ff_hevc_put_hevc_bi_w_epel_hv32_8_msa;
+
+            c->sao_band_filter[0] =
+            c->sao_band_filter[1] =
+            c->sao_band_filter[2] =
+            c->sao_band_filter[3] =
+            c->sao_band_filter[4] = ff_hevc_sao_band_filter_0_8_msa;
+
+            c->sao_edge_filter[0] =
+            c->sao_edge_filter[1] =
+            c->sao_edge_filter[2] =
+            c->sao_edge_filter[3] =
+            c->sao_edge_filter[4] = ff_hevc_sao_edge_filter_8_msa;
+
+            c->hevc_h_loop_filter_luma = ff_hevc_loop_filter_luma_h_8_msa;
+            c->hevc_v_loop_filter_luma = ff_hevc_loop_filter_luma_v_8_msa;
+
+            c->hevc_h_loop_filter_chroma = ff_hevc_loop_filter_chroma_h_8_msa;
+            c->hevc_v_loop_filter_chroma = ff_hevc_loop_filter_chroma_v_8_msa;
+
+            c->hevc_h_loop_filter_luma_c = ff_hevc_loop_filter_luma_h_8_msa;
+            c->hevc_v_loop_filter_luma_c = ff_hevc_loop_filter_luma_v_8_msa;
+
+            c->hevc_h_loop_filter_chroma_c =
+                ff_hevc_loop_filter_chroma_h_8_msa;
+            c->hevc_v_loop_filter_chroma_c =
+                ff_hevc_loop_filter_chroma_v_8_msa;
+
+            c->idct[0] = ff_hevc_idct_4x4_msa;
+            c->idct[1] = ff_hevc_idct_8x8_msa;
+            c->idct[2] = ff_hevc_idct_16x16_msa;
+            c->idct[3] = ff_hevc_idct_32x32_msa;
+            c->idct_dc[0] = ff_hevc_idct_dc_4x4_msa;
+            c->idct_dc[1] = ff_hevc_idct_dc_8x8_msa;
+            c->idct_dc[2] = ff_hevc_idct_dc_16x16_msa;
+            c->idct_dc[3] = ff_hevc_idct_dc_32x32_msa;
+            c->add_residual[0] = ff_hevc_addblk_4x4_msa;
+            c->add_residual[1] = ff_hevc_addblk_8x8_msa;
+            c->add_residual[2] = ff_hevc_addblk_16x16_msa;
+            c->add_residual[3] = ff_hevc_addblk_32x32_msa;
+            c->transform_4x4_luma = ff_hevc_idct_luma_4x4_msa;
+        }
     }
 }
-#endif  // #if HAVE_MSA
-
-void ff_hevc_dsp_init_mips(HEVCDSPContext *c, const int bit_depth)
-{
-#if HAVE_MMI
-    hevc_dsp_init_mmi(c, bit_depth);
-#endif  // #if HAVE_MMI
-#if HAVE_MSA
-    hevc_dsp_init_msa(c, bit_depth);
-#endif  // #if HAVE_MSA
-}
diff --git a/libavcodec/mips/hevcdsp_mmi.c b/libavcodec/mips/hevcdsp_mmi.c
index aa83e1f9ad..d473d692ab 100644
--- a/libavcodec/mips/hevcdsp_mmi.c
+++ b/libavcodec/mips/hevcdsp_mmi.c
@@ -32,9 +32,10 @@ void ff_hevc_put_hevc_qpel_h##w##_8_mmi(int16_t *dst, uint8_t *_src,     \
     int x, y;                                                            \
     pixel *src = (pixel*)_src - 3;                                       \
     ptrdiff_t srcstride = _srcstride / sizeof(pixel);                    \
-    uint64_t ftmp[15];                                                   \
+    double ftmp[15];                                                     \
     uint64_t rtmp[1];                                                    \
     const int8_t *filter = ff_hevc_qpel_filters[mx - 1];                 \
+    DECLARE_VAR_ALL64;                                                   \
                                                                          \
     x = x_step;                                                          \
     y = height;                                                          \
@@ -46,18 +47,14 @@ void ff_hevc_put_hevc_qpel_h##w##_8_mmi(int16_t *dst, uint8_t *_src,     \
         "punpcklbh    %[ftmp1],      %[ftmp0],      %[ftmp1]    \n\t"    \
         "psrah        %[ftmp1],      %[ftmp1],      %[ftmp0]    \n\t"    \
         "psrah        %[ftmp2],      %[ftmp2],      %[ftmp0]    \n\t"    \
-        "xor          %[ftmp0],      %[ftmp0],      %[ftmp0]    \n\t"    \
+        "pxor         %[ftmp0],      %[ftmp0],      %[ftmp0]    \n\t"    \
                                                                          \
         "1:                                                     \n\t"    \
         "2:                                                     \n\t"    \
-        "gsldlc1      %[ftmp3],      0x07(%[src])               \n\t"    \
-        "gsldrc1      %[ftmp3],      0x00(%[src])               \n\t"    \
-        "gsldlc1      %[ftmp4],      0x08(%[src])               \n\t"    \
-        "gsldrc1      %[ftmp4],      0x01(%[src])               \n\t"    \
-        "gsldlc1      %[ftmp5],      0x09(%[src])               \n\t"    \
-        "gsldrc1      %[ftmp5],      0x02(%[src])               \n\t"    \
-        "gsldlc1      %[ftmp6],      0x0a(%[src])               \n\t"    \
-        "gsldrc1      %[ftmp6],      0x03(%[src])               \n\t"    \
+        MMI_ULDC1(%[ftmp3], %[src], 0x00)                                \
+        MMI_ULDC1(%[ftmp4], %[src], 0x01)                                \
+        MMI_ULDC1(%[ftmp5], %[src], 0x02)                                \
+        MMI_ULDC1(%[ftmp6], %[src], 0x03)                                \
         "punpcklbh    %[ftmp7],      %[ftmp3],      %[ftmp0]    \n\t"    \
         "punpckhbh    %[ftmp8],      %[ftmp3],      %[ftmp0]    \n\t"    \
         "pmullh       %[ftmp7],      %[ftmp7],      %[ftmp1]    \n\t"    \
@@ -83,8 +80,7 @@ void ff_hevc_put_hevc_qpel_h##w##_8_mmi(int16_t *dst, uint8_t *_src,     \
         "paddh        %[ftmp3],      %[ftmp3],      %[ftmp4]    \n\t"    \
         "paddh        %[ftmp5],      %[ftmp5],      %[ftmp6]    \n\t"    \
         "paddh        %[ftmp3],      %[ftmp3],      %[ftmp5]    \n\t"    \
-        "gssdlc1      %[ftmp3],      0x07(%[dst])               \n\t"    \
-        "gssdrc1      %[ftmp3],      0x00(%[dst])               \n\t"    \
+        MMI_USDC1(%[ftmp3], %[dst], 0x00)                                \
                                                                          \
         "daddi        %[x],          %[x],         -0x01        \n\t"    \
         PTR_ADDIU    "%[src],        %[src],        0x04        \n\t"    \
@@ -98,7 +94,8 @@ void ff_hevc_put_hevc_qpel_h##w##_8_mmi(int16_t *dst, uint8_t *_src,     \
         PTR_ADDU     "%[src],        %[src],        %[stride]   \n\t"    \
         PTR_ADDIU    "%[dst],        %[dst],        0x80        \n\t"    \
         "bnez         %[y],          1b                         \n\t"    \
-        : [ftmp0]"=&f"(ftmp[0]), [ftmp1]"=&f"(ftmp[1]),                  \
+        : RESTRICT_ASM_ALL64                                             \
+          [ftmp0]"=&f"(ftmp[0]), [ftmp1]"=&f"(ftmp[1]),                  \
           [ftmp2]"=&f"(ftmp[2]), [ftmp3]"=&f"(ftmp[3]),                  \
           [ftmp4]"=&f"(ftmp[4]), [ftmp5]"=&f"(ftmp[5]),                  \
           [ftmp6]"=&f"(ftmp[6]), [ftmp7]"=&f"(ftmp[7]),                  \
@@ -132,8 +129,9 @@ void ff_hevc_put_hevc_qpel_hv##w##_8_mmi(int16_t *dst, uint8_t *_src,    \
     ptrdiff_t srcstride = _srcstride / sizeof(pixel);                    \
     int16_t tmp_array[(MAX_PB_SIZE + QPEL_EXTRA) * MAX_PB_SIZE];         \
     int16_t *tmp = tmp_array;                                            \
-    uint64_t ftmp[15];                                                   \
+    double ftmp[15];                                                     \
     uint64_t rtmp[1];                                                    \
+    DECLARE_VAR_ALL64;                                                   \
                                                                          \
     src   -= (QPEL_EXTRA_BEFORE * srcstride + 3);                        \
     filter = ff_hevc_qpel_filters[mx - 1];                               \
@@ -147,18 +145,14 @@ void ff_hevc_put_hevc_qpel_hv##w##_8_mmi(int16_t *dst, uint8_t *_src,    \
         "punpcklbh    %[ftmp1],      %[ftmp0],      %[ftmp1]    \n\t"    \
         "psrah        %[ftmp1],      %[ftmp1],      %[ftmp0]    \n\t"    \
         "psrah        %[ftmp2],      %[ftmp2],      %[ftmp0]    \n\t"    \
-        "xor          %[ftmp0],      %[ftmp0],      %[ftmp0]    \n\t"    \
+        "pxor         %[ftmp0],      %[ftmp0],      %[ftmp0]    \n\t"    \
                                                                          \
         "1:                                                     \n\t"    \
         "2:                                                     \n\t"    \
-        "gsldlc1      %[ftmp3],      0x07(%[src])               \n\t"    \
-        "gsldrc1      %[ftmp3],      0x00(%[src])               \n\t"    \
-        "gsldlc1      %[ftmp4],      0x08(%[src])               \n\t"    \
-        "gsldrc1      %[ftmp4],      0x01(%[src])               \n\t"    \
-        "gsldlc1      %[ftmp5],      0x09(%[src])               \n\t"    \
-        "gsldrc1      %[ftmp5],      0x02(%[src])               \n\t"    \
-        "gsldlc1      %[ftmp6],      0x0a(%[src])               \n\t"    \
-        "gsldrc1      %[ftmp6],      0x03(%[src])               \n\t"    \
+        MMI_ULDC1(%[ftmp3], %[src], 0x00)                                \
+        MMI_ULDC1(%[ftmp4], %[src], 0x01)                                \
+        MMI_ULDC1(%[ftmp5], %[src], 0x02)                                \
+        MMI_ULDC1(%[ftmp6], %[src], 0x03)                                \
         "punpcklbh    %[ftmp7],      %[ftmp3],      %[ftmp0]    \n\t"    \
         "punpckhbh    %[ftmp8],      %[ftmp3],      %[ftmp0]    \n\t"    \
         "pmullh       %[ftmp7],      %[ftmp7],      %[ftmp1]    \n\t"    \
@@ -184,8 +178,7 @@ void ff_hevc_put_hevc_qpel_hv##w##_8_mmi(int16_t *dst, uint8_t *_src,    \
         "paddh        %[ftmp3],      %[ftmp3],      %[ftmp4]    \n\t"    \
         "paddh        %[ftmp5],      %[ftmp5],      %[ftmp6]    \n\t"    \
         "paddh        %[ftmp3],      %[ftmp3],      %[ftmp5]    \n\t"    \
-        "gssdlc1      %[ftmp3],      0x07(%[tmp])               \n\t"    \
-        "gssdrc1      %[ftmp3],      0x00(%[tmp])               \n\t"    \
+        MMI_USDC1(%[ftmp3], %[tmp], 0x00)                                \
                                                                          \
         "daddi        %[x],          %[x],         -0x01        \n\t"    \
         PTR_ADDIU    "%[src],        %[src],        0x04        \n\t"    \
@@ -199,7 +192,8 @@ void ff_hevc_put_hevc_qpel_hv##w##_8_mmi(int16_t *dst, uint8_t *_src,    \
         PTR_ADDU     "%[src],        %[src],        %[stride]   \n\t"    \
         PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"    \
         "bnez         %[y],          1b                         \n\t"    \
-        : [ftmp0]"=&f"(ftmp[0]), [ftmp1]"=&f"(ftmp[1]),                  \
+        : RESTRICT_ASM_ALL64                                             \
+          [ftmp0]"=&f"(ftmp[0]), [ftmp1]"=&f"(ftmp[1]),                  \
           [ftmp2]"=&f"(ftmp[2]), [ftmp3]"=&f"(ftmp[3]),                  \
           [ftmp4]"=&f"(ftmp[4]), [ftmp5]"=&f"(ftmp[5]),                  \
           [ftmp6]"=&f"(ftmp[6]), [ftmp7]"=&f"(ftmp[7]),                  \
@@ -228,29 +222,21 @@ void ff_hevc_put_hevc_qpel_hv##w##_8_mmi(int16_t *dst, uint8_t *_src,    \
                                                                          \
         "1:                                                     \n\t"    \
         "2:                                                     \n\t"    \
-        "gsldlc1      %[ftmp3],      0x07(%[tmp])               \n\t"    \
-        "gsldrc1      %[ftmp3],      0x00(%[tmp])               \n\t"    \
+        MMI_ULDC1(%[ftmp3], %[tmp], 0x00)                                \
         PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"    \
-        "gsldlc1      %[ftmp4],      0x07(%[tmp])               \n\t"    \
-        "gsldrc1      %[ftmp4],      0x00(%[tmp])               \n\t"    \
+        MMI_ULDC1(%[ftmp4], %[tmp], 0x00)                                \
         PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"    \
-        "gsldlc1      %[ftmp5],      0x07(%[tmp])               \n\t"    \
-        "gsldrc1      %[ftmp5],      0x00(%[tmp])               \n\t"    \
+        MMI_ULDC1(%[ftmp5], %[tmp], 0x00)                                \
         PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"    \
-        "gsldlc1      %[ftmp6],      0x07(%[tmp])               \n\t"    \
-        "gsldrc1      %[ftmp6],      0x00(%[tmp])               \n\t"    \
+        MMI_ULDC1(%[ftmp6], %[tmp], 0x00)                                \
         PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"    \
-        "gsldlc1      %[ftmp7],      0x07(%[tmp])               \n\t"    \
-        "gsldrc1      %[ftmp7],      0x00(%[tmp])               \n\t"    \
+        MMI_ULDC1(%[ftmp7], %[tmp], 0x00)                                \
         PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"    \
-        "gsldlc1      %[ftmp8],      0x07(%[tmp])               \n\t"    \
-        "gsldrc1      %[ftmp8],      0x00(%[tmp])               \n\t"    \
+        MMI_ULDC1(%[ftmp8], %[tmp], 0x00)                                \
         PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"    \
-        "gsldlc1      %[ftmp9],      0x07(%[tmp])               \n\t"    \
-        "gsldrc1      %[ftmp9],      0x00(%[tmp])               \n\t"    \
+        MMI_ULDC1(%[ftmp9], %[tmp], 0x00)                                \
         PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"    \
-        "gsldlc1      %[ftmp10],     0x07(%[tmp])               \n\t"    \
-        "gsldrc1      %[ftmp10],     0x00(%[tmp])               \n\t"    \
+        MMI_ULDC1(%[ftmp10], %[tmp], 0x00)                               \
         PTR_ADDIU    "%[tmp],        %[tmp],        -0x380      \n\t"    \
         TRANSPOSE_4H(%[ftmp3], %[ftmp4], %[ftmp5], %[ftmp6],             \
                      %[ftmp11], %[ftmp12], %[ftmp13], %[ftmp14])         \
@@ -275,8 +261,7 @@ void ff_hevc_put_hevc_qpel_hv##w##_8_mmi(int16_t *dst, uint8_t *_src,    \
         "paddw        %[ftmp5],      %[ftmp5],      %[ftmp6]    \n\t"    \
         "psraw        %[ftmp5],      %[ftmp5],      %[ftmp0]    \n\t"    \
         "packsswh     %[ftmp3],      %[ftmp3],      %[ftmp5]    \n\t"    \
-        "gssdlc1      %[ftmp3],      0x07(%[dst])               \n\t"    \
-        "gssdrc1      %[ftmp3],      0x00(%[dst])               \n\t"    \
+        MMI_USDC1(%[ftmp3], %[dst], 0x00)                               \
                                                                          \
         "daddi        %[x],          %[x],         -0x01        \n\t"    \
         PTR_ADDIU    "%[dst],        %[dst],        0x08        \n\t"    \
@@ -290,7 +275,8 @@ void ff_hevc_put_hevc_qpel_hv##w##_8_mmi(int16_t *dst, uint8_t *_src,    \
         PTR_ADDIU    "%[dst],        %[dst],        0x80        \n\t"    \
         PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"    \
         "bnez         %[y],          1b                         \n\t"    \
-        : [ftmp0]"=&f"(ftmp[0]), [ftmp1]"=&f"(ftmp[1]),                  \
+        : RESTRICT_ASM_ALL64                                             \
+          [ftmp0]"=&f"(ftmp[0]), [ftmp1]"=&f"(ftmp[1]),                  \
           [ftmp2]"=&f"(ftmp[2]), [ftmp3]"=&f"(ftmp[3]),                  \
           [ftmp4]"=&f"(ftmp[4]), [ftmp5]"=&f"(ftmp[5]),                  \
           [ftmp6]"=&f"(ftmp[6]), [ftmp7]"=&f"(ftmp[7]),                  \
@@ -329,10 +315,14 @@ void ff_hevc_put_hevc_qpel_bi_h##w##_8_mmi(uint8_t *_dst,               \
     pixel *dst          = (pixel *)_dst;                                \
     ptrdiff_t dststride = _dststride / sizeof(pixel);                   \
     const int8_t *filter    = ff_hevc_qpel_filters[mx - 1];             \
-    uint64_t ftmp[20];                                                  \
+    double ftmp[20];                                                    \
     uint64_t rtmp[1];                                                   \
-    int shift = 7;                                                      \
-    int offset = 64;                                                    \
+    union av_intfloat64 shift;                                          \
+    union av_intfloat64 offset;                                         \
+    DECLARE_VAR_ALL64;                                                  \
+    DECLARE_VAR_LOW32;                                                  \
+    shift.i = 7;                                                        \
+    offset.i = 64;                                                      \
                                                                         \
     x = width >> 2;                                                     \
     y = height;                                                         \
@@ -344,21 +334,17 @@ void ff_hevc_put_hevc_qpel_bi_h##w##_8_mmi(uint8_t *_dst,               \
         "punpcklbh    %[ftmp1],      %[ftmp0],      %[ftmp1]    \n\t"   \
         "psrah        %[ftmp1],      %[ftmp1],      %[ftmp0]    \n\t"   \
         "psrah        %[ftmp2],      %[ftmp2],      %[ftmp0]    \n\t"   \
-        "xor          %[ftmp0],      %[ftmp0],      %[ftmp0]    \n\t"   \
+        "pxor         %[ftmp0],      %[ftmp0],      %[ftmp0]    \n\t"   \
         "punpcklhw    %[offset],     %[offset],     %[offset]   \n\t"   \
         "punpcklwd    %[offset],     %[offset],     %[offset]   \n\t"   \
                                                                         \
         "1:                                                     \n\t"   \
         "li           %[x],        " #x_step "                  \n\t"   \
         "2:                                                     \n\t"   \
-        "gsldlc1      %[ftmp3],      0x07(%[src])               \n\t"   \
-        "gsldrc1      %[ftmp3],      0x00(%[src])               \n\t"   \
-        "gsldlc1      %[ftmp4],      0x08(%[src])               \n\t"   \
-        "gsldrc1      %[ftmp4],      0x01(%[src])               \n\t"   \
-        "gsldlc1      %[ftmp5],      0x09(%[src])               \n\t"   \
-        "gsldrc1      %[ftmp5],      0x02(%[src])               \n\t"   \
-        "gsldlc1      %[ftmp6],      0x0a(%[src])               \n\t"   \
-        "gsldrc1      %[ftmp6],      0x03(%[src])               \n\t"   \
+        MMI_ULDC1(%[ftmp3], %[src], 0x00)                               \
+        MMI_ULDC1(%[ftmp4], %[src], 0x01)                               \
+        MMI_ULDC1(%[ftmp5], %[src], 0x02)                               \
+        MMI_ULDC1(%[ftmp6], %[src], 0x03)                               \
         "punpcklbh    %[ftmp7],      %[ftmp3],      %[ftmp0]    \n\t"   \
         "punpckhbh    %[ftmp8],      %[ftmp3],      %[ftmp0]    \n\t"   \
         "pmullh       %[ftmp7],      %[ftmp7],      %[ftmp1]    \n\t"   \
@@ -385,8 +371,7 @@ void ff_hevc_put_hevc_qpel_bi_h##w##_8_mmi(uint8_t *_dst,               \
         "paddh        %[ftmp5],      %[ftmp5],      %[ftmp6]    \n\t"   \
         "paddh        %[ftmp3],      %[ftmp3],      %[ftmp5]    \n\t"   \
         "paddh        %[ftmp3],      %[ftmp3],      %[offset]   \n\t"   \
-        "gsldlc1      %[ftmp4],      0x07(%[src2])              \n\t"   \
-        "gsldrc1      %[ftmp4],      0x00(%[src2])              \n\t"   \
+        MMI_ULDC1(%[ftmp4], %[src2], 0x00)                              \
         "li           %[rtmp0],      0x10                       \n\t"   \
         "dmtc1        %[rtmp0],      %[ftmp8]                   \n\t"   \
         "punpcklhw    %[ftmp5],      %[ftmp0],      %[ftmp3]    \n\t"   \
@@ -403,10 +388,9 @@ void ff_hevc_put_hevc_qpel_bi_h##w##_8_mmi(uint8_t *_dst,               \
         "psraw        %[ftmp6],      %[ftmp6],      %[shift]    \n\t"   \
         "packsswh     %[ftmp5],      %[ftmp5],      %[ftmp6]    \n\t"   \
         "pcmpgth      %[ftmp7],      %[ftmp5],      %[ftmp0]    \n\t"   \
-        "and          %[ftmp3],      %[ftmp5],      %[ftmp7]    \n\t"   \
+        "pand         %[ftmp3],      %[ftmp5],      %[ftmp7]    \n\t"   \
         "packushb     %[ftmp3],      %[ftmp3],      %[ftmp3]    \n\t"   \
-        "gsswlc1      %[ftmp3],      0x03(%[dst])               \n\t"   \
-        "gsswrc1      %[ftmp3],      0x00(%[dst])               \n\t"   \
+        MMI_USWC1(%[ftmp3], %[dst], 0x00)                               \
                                                                         \
         "daddi        %[x],          %[x],         -0x01        \n\t"   \
         PTR_ADDIU    "%[src],        %[src],        0x04        \n\t"   \
@@ -422,7 +406,8 @@ void ff_hevc_put_hevc_qpel_bi_h##w##_8_mmi(uint8_t *_dst,               \
         PTR_ADDU     "%[dst],        %[dst],    %[dst_stride]   \n\t"   \
         PTR_ADDIU    "%[src2],       %[src2],       0x80        \n\t"   \
         "bnez         %[y],          1b                         \n\t"   \
-        : [ftmp0]"=&f"(ftmp[0]), [ftmp1]"=&f"(ftmp[1]),                 \
+        : RESTRICT_ASM_ALL64 RESTRICT_ASM_LOW32                         \
+          [ftmp0]"=&f"(ftmp[0]), [ftmp1]"=&f"(ftmp[1]),                 \
           [ftmp2]"=&f"(ftmp[2]), [ftmp3]"=&f"(ftmp[3]),                 \
           [ftmp4]"=&f"(ftmp[4]), [ftmp5]"=&f"(ftmp[5]),                 \
           [ftmp6]"=&f"(ftmp[6]), [ftmp7]"=&f"(ftmp[7]),                 \
@@ -430,9 +415,9 @@ void ff_hevc_put_hevc_qpel_bi_h##w##_8_mmi(uint8_t *_dst,               \
           [ftmp10]"=&f"(ftmp[10]), [ftmp11]"=&f"(ftmp[11]),             \
           [ftmp12]"=&f"(ftmp[12]), [src2]"+&r"(src2),                   \
           [dst]"+&r"(dst), [src]"+&r"(src), [y]"+&r"(y), [x]"=&r"(x),   \
-          [offset]"+&f"(offset), [rtmp0]"=&r"(rtmp[0])                  \
+          [offset]"+&f"(offset.f), [rtmp0]"=&r"(rtmp[0])                \
         : [src_stride]"r"(srcstride), [dst_stride]"r"(dststride),       \
-          [filter]"r"(filter), [shift]"f"(shift)                        \
+          [filter]"r"(filter), [shift]"f"(shift.f)                      \
         : "memory"                                                      \
     );                                                                  \
 }
@@ -463,10 +448,14 @@ void ff_hevc_put_hevc_qpel_bi_hv##w##_8_mmi(uint8_t *_dst,              \
     ptrdiff_t dststride = _dststride / sizeof(pixel);                   \
     int16_t tmp_array[(MAX_PB_SIZE + QPEL_EXTRA) * MAX_PB_SIZE];        \
     int16_t *tmp = tmp_array;                                           \
-    uint64_t ftmp[20];                                                  \
+    double ftmp[20];                                                    \
     uint64_t rtmp[1];                                                   \
-    int shift = 7;                                                      \
-    int offset = 64;                                                    \
+    union av_intfloat64 shift;                                          \
+    union av_intfloat64 offset;                                         \
+    DECLARE_VAR_ALL64;                                                  \
+    DECLARE_VAR_LOW32;                                                  \
+    shift.i = 7;                                                        \
+    offset.i = 64;                                                      \
                                                                         \
     src   -= (QPEL_EXTRA_BEFORE * srcstride + 3);                       \
     filter = ff_hevc_qpel_filters[mx - 1];                              \
@@ -480,18 +469,14 @@ void ff_hevc_put_hevc_qpel_bi_hv##w##_8_mmi(uint8_t *_dst,              \
         "punpcklbh    %[ftmp1],      %[ftmp0],      %[ftmp1]    \n\t"   \
         "psrah        %[ftmp1],      %[ftmp1],      %[ftmp0]    \n\t"   \
         "psrah        %[ftmp2],      %[ftmp2],      %[ftmp0]    \n\t"   \
-        "xor          %[ftmp0],      %[ftmp0],      %[ftmp0]    \n\t"   \
+        "pxor         %[ftmp0],      %[ftmp0],      %[ftmp0]    \n\t"   \
                                                                         \
         "1:                                                     \n\t"   \
         "2:                                                     \n\t"   \
-        "gsldlc1      %[ftmp3],      0x07(%[src])               \n\t"   \
-        "gsldrc1      %[ftmp3],      0x00(%[src])               \n\t"   \
-        "gsldlc1      %[ftmp4],      0x08(%[src])               \n\t"   \
-        "gsldrc1      %[ftmp4],      0x01(%[src])               \n\t"   \
-        "gsldlc1      %[ftmp5],      0x09(%[src])               \n\t"   \
-        "gsldrc1      %[ftmp5],      0x02(%[src])               \n\t"   \
-        "gsldlc1      %[ftmp6],      0x0a(%[src])               \n\t"   \
-        "gsldrc1      %[ftmp6],      0x03(%[src])               \n\t"   \
+        MMI_ULDC1(%[ftmp3], %[src], 0x00)                               \
+        MMI_ULDC1(%[ftmp4], %[src], 0x01)                               \
+        MMI_ULDC1(%[ftmp5], %[src], 0x02)                               \
+        MMI_ULDC1(%[ftmp6], %[src], 0x03)                               \
         "punpcklbh    %[ftmp7],      %[ftmp3],      %[ftmp0]    \n\t"   \
         "punpckhbh    %[ftmp8],      %[ftmp3],      %[ftmp0]    \n\t"   \
         "pmullh       %[ftmp7],      %[ftmp7],      %[ftmp1]    \n\t"   \
@@ -517,8 +502,7 @@ void ff_hevc_put_hevc_qpel_bi_hv##w##_8_mmi(uint8_t *_dst,              \
         "paddh        %[ftmp3],      %[ftmp3],      %[ftmp4]    \n\t"   \
         "paddh        %[ftmp5],      %[ftmp5],      %[ftmp6]    \n\t"   \
         "paddh        %[ftmp3],      %[ftmp3],      %[ftmp5]    \n\t"   \
-        "gssdlc1      %[ftmp3],      0x07(%[tmp])               \n\t"   \
-        "gssdrc1      %[ftmp3],      0x00(%[tmp])               \n\t"   \
+        MMI_USDC1(%[ftmp3], %[tmp], 0x00)                               \
                                                                         \
         "daddi        %[x],          %[x],         -0x01        \n\t"   \
         PTR_ADDIU    "%[src],        %[src],        0x04        \n\t"   \
@@ -532,7 +516,8 @@ void ff_hevc_put_hevc_qpel_bi_hv##w##_8_mmi(uint8_t *_dst,              \
         PTR_ADDU     "%[src],        %[src],        %[stride]   \n\t"   \
         PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"   \
         "bnez         %[y],          1b                         \n\t"   \
-        : [ftmp0]"=&f"(ftmp[0]), [ftmp1]"=&f"(ftmp[1]),                 \
+        : RESTRICT_ASM_ALL64                                            \
+          [ftmp0]"=&f"(ftmp[0]), [ftmp1]"=&f"(ftmp[1]),                 \
           [ftmp2]"=&f"(ftmp[2]), [ftmp3]"=&f"(ftmp[3]),                 \
           [ftmp4]"=&f"(ftmp[4]), [ftmp5]"=&f"(ftmp[5]),                 \
           [ftmp6]"=&f"(ftmp[6]), [ftmp7]"=&f"(ftmp[7]),                 \
@@ -563,29 +548,21 @@ void ff_hevc_put_hevc_qpel_bi_hv##w##_8_mmi(uint8_t *_dst,              \
         "1:                                                     \n\t"   \
         "li           %[x],        " #x_step "                  \n\t"   \
         "2:                                                     \n\t"   \
-        "gsldlc1      %[ftmp3],      0x07(%[tmp])               \n\t"   \
-        "gsldrc1      %[ftmp3],      0x00(%[tmp])               \n\t"   \
+        MMI_ULDC1(%[ftmp3], %[tmp], 0x00)                               \
         PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"   \
-        "gsldlc1      %[ftmp4],      0x07(%[tmp])               \n\t"   \
-        "gsldrc1      %[ftmp4],      0x00(%[tmp])               \n\t"   \
+        MMI_ULDC1(%[ftmp4], %[tmp], 0x00)                               \
         PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"   \
-        "gsldlc1      %[ftmp5],      0x07(%[tmp])               \n\t"   \
-        "gsldrc1      %[ftmp5],      0x00(%[tmp])               \n\t"   \
+        MMI_ULDC1(%[ftmp5], %[tmp], 0x00)                               \
         PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"   \
-        "gsldlc1      %[ftmp6],      0x07(%[tmp])               \n\t"   \
-        "gsldrc1      %[ftmp6],      0x00(%[tmp])               \n\t"   \
+        MMI_ULDC1(%[ftmp6], %[tmp], 0x00)                               \
         PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"   \
-        "gsldlc1      %[ftmp7],      0x07(%[tmp])               \n\t"   \
-        "gsldrc1      %[ftmp7],      0x00(%[tmp])               \n\t"   \
+        MMI_ULDC1(%[ftmp7], %[tmp], 0x00)                               \
         PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"   \
-        "gsldlc1      %[ftmp8],      0x07(%[tmp])               \n\t"   \
-        "gsldrc1      %[ftmp8],      0x00(%[tmp])               \n\t"   \
+        MMI_ULDC1(%[ftmp8], %[tmp], 0x00)                               \
         PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"   \
-        "gsldlc1      %[ftmp9],      0x07(%[tmp])               \n\t"   \
-        "gsldrc1      %[ftmp9],      0x00(%[tmp])               \n\t"   \
+        MMI_ULDC1(%[ftmp9], %[tmp], 0x00)                               \
         PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"   \
-        "gsldlc1      %[ftmp10],     0x07(%[tmp])               \n\t"   \
-        "gsldrc1      %[ftmp10],     0x00(%[tmp])               \n\t"   \
+        MMI_ULDC1(%[ftmp10], %[tmp], 0x00)                              \
         PTR_ADDIU    "%[tmp],        %[tmp],        -0x380      \n\t"   \
         TRANSPOSE_4H(%[ftmp3], %[ftmp4], %[ftmp5], %[ftmp6],            \
                      %[ftmp11], %[ftmp12], %[ftmp13], %[ftmp14])        \
@@ -610,9 +587,8 @@ void ff_hevc_put_hevc_qpel_bi_hv##w##_8_mmi(uint8_t *_dst,              \
         "paddw        %[ftmp5],      %[ftmp5],      %[ftmp6]    \n\t"   \
         "psraw        %[ftmp5],      %[ftmp5],      %[ftmp0]    \n\t"   \
         "packsswh     %[ftmp3],      %[ftmp3],      %[ftmp5]    \n\t"   \
-        "gsldlc1      %[ftmp4],      0x07(%[src2])              \n\t"   \
-        "gsldrc1      %[ftmp4],      0x00(%[src2])              \n\t"   \
-        "xor          %[ftmp7],      %[ftmp7],      %[ftmp7]    \n\t"   \
+        MMI_ULDC1(%[ftmp4], %[src2], 0x00)                              \
+        "pxor         %[ftmp7],      %[ftmp7],      %[ftmp7]    \n\t"   \
         "li           %[rtmp0],      0x10                       \n\t"   \
         "dmtc1        %[rtmp0],      %[ftmp8]                   \n\t"   \
         "punpcklhw    %[ftmp5],      %[ftmp7],      %[ftmp3]    \n\t"   \
@@ -631,10 +607,9 @@ void ff_hevc_put_hevc_qpel_bi_hv##w##_8_mmi(uint8_t *_dst,              \
         "psraw        %[ftmp6],      %[ftmp6],      %[shift]    \n\t"   \
         "packsswh     %[ftmp5],      %[ftmp5],      %[ftmp6]    \n\t"   \
         "pcmpgth      %[ftmp7],      %[ftmp5],      %[ftmp7]    \n\t"   \
-        "and          %[ftmp3],      %[ftmp5],      %[ftmp7]    \n\t"   \
+        "pand         %[ftmp3],      %[ftmp5],      %[ftmp7]    \n\t"   \
         "packushb     %[ftmp3],      %[ftmp3],      %[ftmp3]    \n\t"   \
-        "gsswlc1      %[ftmp3],      0x03(%[dst])               \n\t"   \
-        "gsswrc1      %[ftmp3],      0x00(%[dst])               \n\t"   \
+        MMI_USWC1(%[ftmp3], %[dst], 0x00)                               \
                                                                         \
         "daddi        %[x],          %[x],         -0x01        \n\t"   \
         PTR_ADDIU    "%[src2],       %[src2],       0x08        \n\t"   \
@@ -650,7 +625,8 @@ void ff_hevc_put_hevc_qpel_bi_hv##w##_8_mmi(uint8_t *_dst,              \
         PTR_ADDU     "%[dst],        %[dst],        %[stride]   \n\t"   \
         PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"   \
         "bnez         %[y],          1b                         \n\t"   \
-        : [ftmp0]"=&f"(ftmp[0]), [ftmp1]"=&f"(ftmp[1]),                 \
+        : RESTRICT_ASM_ALL64 RESTRICT_ASM_LOW32                         \
+          [ftmp0]"=&f"(ftmp[0]), [ftmp1]"=&f"(ftmp[1]),                 \
           [ftmp2]"=&f"(ftmp[2]), [ftmp3]"=&f"(ftmp[3]),                 \
           [ftmp4]"=&f"(ftmp[4]), [ftmp5]"=&f"(ftmp[5]),                 \
           [ftmp6]"=&f"(ftmp[6]), [ftmp7]"=&f"(ftmp[7]),                 \
@@ -659,9 +635,9 @@ void ff_hevc_put_hevc_qpel_bi_hv##w##_8_mmi(uint8_t *_dst,              \
           [ftmp12]"=&f"(ftmp[12]), [ftmp13]"=&f"(ftmp[13]),             \
           [ftmp14]"=&f"(ftmp[14]), [src2]"+&r"(src2),                   \
           [dst]"+&r"(dst), [tmp]"+&r"(tmp), [y]"+&r"(y), [x]"=&r"(x),   \
-          [offset]"+&f"(offset), [rtmp0]"=&r"(rtmp[0])                  \
+          [offset]"+&f"(offset.f), [rtmp0]"=&r"(rtmp[0])                \
         : [filter]"r"(filter), [stride]"r"(dststride),                  \
-          [shift]"f"(shift)                                             \
+          [shift]"f"(shift.f)                                           \
         : "memory"                                                      \
     );                                                                  \
 }
@@ -692,10 +668,14 @@ void ff_hevc_put_hevc_epel_bi_hv##w##_8_mmi(uint8_t *_dst,              \
     const int8_t *filter = ff_hevc_epel_filters[mx - 1];                \
     int16_t tmp_array[(MAX_PB_SIZE + EPEL_EXTRA) * MAX_PB_SIZE];        \
     int16_t *tmp = tmp_array;                                           \
-    uint64_t ftmp[12];                                                  \
+    double  ftmp[12];                                                   \
     uint64_t rtmp[1];                                                   \
-    int shift = 7;                                                      \
-    int offset = 64;                                                    \
+    union av_intfloat64 shift;                                          \
+    union av_intfloat64 offset;                                         \
+    DECLARE_VAR_ALL64;                                                  \
+    DECLARE_VAR_LOW32;                                                  \
+    shift.i = 7;                                                        \
+    offset.i = 64;                                                      \
                                                                         \
     src -= (EPEL_EXTRA_BEFORE * srcstride + 1);                         \
     x = width >> 2;                                                     \
@@ -706,18 +686,14 @@ void ff_hevc_put_hevc_epel_bi_hv##w##_8_mmi(uint8_t *_dst,              \
         "dmtc1        %[rtmp0],      %[ftmp0]                   \n\t"   \
         "punpcklbh    %[ftmp1],      %[ftmp0],      %[ftmp1]    \n\t"   \
         "psrah        %[ftmp1],      %[ftmp1],      %[ftmp0]    \n\t"   \
-        "xor          %[ftmp0],      %[ftmp0],      %[ftmp0]    \n\t"   \
+        "pxor         %[ftmp0],      %[ftmp0],      %[ftmp0]    \n\t"   \
                                                                         \
         "1:                                                     \n\t"   \
         "2:                                                     \n\t"   \
-        "gslwlc1      %[ftmp2],      0x03(%[src])               \n\t"   \
-        "gslwrc1      %[ftmp2],      0x00(%[src])               \n\t"   \
-        "gslwlc1      %[ftmp3],      0x04(%[src])               \n\t"   \
-        "gslwrc1      %[ftmp3],      0x01(%[src])               \n\t"   \
-        "gslwlc1      %[ftmp4],      0x05(%[src])               \n\t"   \
-        "gslwrc1      %[ftmp4],      0x02(%[src])               \n\t"   \
-        "gslwlc1      %[ftmp5],      0x06(%[src])               \n\t"   \
-        "gslwrc1      %[ftmp5],      0x03(%[src])               \n\t"   \
+        MMI_ULWC1(%[ftmp2], %[src], 0x00)                               \
+        MMI_ULWC1(%[ftmp3], %[src], 0x01)                               \
+        MMI_ULWC1(%[ftmp4], %[src], 0x02)                               \
+        MMI_ULWC1(%[ftmp5], %[src], 0x03)                               \
         "punpcklbh    %[ftmp2],      %[ftmp2],      %[ftmp0]    \n\t"   \
         "pmullh       %[ftmp2],      %[ftmp2],      %[ftmp1]    \n\t"   \
         "punpcklbh    %[ftmp3],      %[ftmp3],      %[ftmp0]    \n\t"   \
@@ -731,8 +707,7 @@ void ff_hevc_put_hevc_epel_bi_hv##w##_8_mmi(uint8_t *_dst,              \
         "paddh        %[ftmp2],      %[ftmp2],      %[ftmp3]    \n\t"   \
         "paddh        %[ftmp4],      %[ftmp4],      %[ftmp5]    \n\t"   \
         "paddh        %[ftmp2],      %[ftmp2],      %[ftmp4]    \n\t"   \
-        "gssdlc1      %[ftmp2],      0x07(%[tmp])               \n\t"   \
-        "gssdrc1      %[ftmp2],      0x00(%[tmp])               \n\t"   \
+        MMI_USDC1(%[ftmp2], %[tmp], 0x00)                               \
                                                                         \
         "daddi        %[x],          %[x],         -0x01        \n\t"   \
         PTR_ADDIU    "%[src],        %[src],        0x04        \n\t"   \
@@ -746,7 +721,8 @@ void ff_hevc_put_hevc_epel_bi_hv##w##_8_mmi(uint8_t *_dst,              \
         PTR_ADDU     "%[src],        %[src],        %[stride]   \n\t"   \
         PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"   \
         "bnez         %[y],          1b                         \n\t"   \
-        : [ftmp0]"=&f"(ftmp[0]), [ftmp1]"=&f"(ftmp[1]),                 \
+        : RESTRICT_ASM_ALL64                                            \
+          [ftmp0]"=&f"(ftmp[0]), [ftmp1]"=&f"(ftmp[1]),                 \
           [ftmp2]"=&f"(ftmp[2]), [ftmp3]"=&f"(ftmp[3]),                 \
           [ftmp4]"=&f"(ftmp[4]), [ftmp5]"=&f"(ftmp[5]),                 \
           [ftmp6]"=&f"(ftmp[6]), [ftmp7]"=&f"(ftmp[7]),                 \
@@ -771,22 +747,18 @@ void ff_hevc_put_hevc_epel_bi_hv##w##_8_mmi(uint8_t *_dst,              \
         "li           %[rtmp0],      0x06                       \n\t"   \
         "dmtc1        %[rtmp0],      %[ftmp0]                   \n\t"   \
         "punpcklwd    %[offset],     %[offset],     %[offset]   \n\t"   \
-        "xor          %[ftmp2],      %[ftmp2],      %[ftmp2]    \n\t"   \
+        "pxor         %[ftmp2],      %[ftmp2],      %[ftmp2]    \n\t"   \
                                                                         \
         "1:                                                     \n\t"   \
         "li           %[x],        " #x_step "                  \n\t"   \
         "2:                                                     \n\t"   \
-        "gsldlc1      %[ftmp3],      0x07(%[tmp])               \n\t"   \
-        "gsldrc1      %[ftmp3],      0x00(%[tmp])               \n\t"   \
+        MMI_ULDC1(%[ftmp3], %[tmp], 0x00)                               \
         PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"   \
-        "gsldlc1      %[ftmp4],      0x07(%[tmp])               \n\t"   \
-        "gsldrc1      %[ftmp4],      0x00(%[tmp])               \n\t"   \
+        MMI_ULDC1(%[ftmp4], %[tmp], 0x00)                               \
         PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"   \
-        "gsldlc1      %[ftmp5],      0x07(%[tmp])               \n\t"   \
-        "gsldrc1      %[ftmp5],      0x00(%[tmp])               \n\t"   \
+        MMI_ULDC1(%[ftmp5], %[tmp], 0x00)                               \
         PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"   \
-        "gsldlc1      %[ftmp6],      0x07(%[tmp])               \n\t"   \
-        "gsldrc1      %[ftmp6],      0x00(%[tmp])               \n\t"   \
+        MMI_ULDC1(%[ftmp6], %[tmp], 0x00)                               \
         PTR_ADDIU    "%[tmp],        %[tmp],       -0x180       \n\t"   \
         TRANSPOSE_4H(%[ftmp3], %[ftmp4], %[ftmp5], %[ftmp6],            \
                      %[ftmp7], %[ftmp8], %[ftmp9], %[ftmp10])           \
@@ -801,8 +773,7 @@ void ff_hevc_put_hevc_epel_bi_hv##w##_8_mmi(uint8_t *_dst,              \
         "paddw        %[ftmp5],      %[ftmp5],      %[ftmp6]    \n\t"   \
         "psraw        %[ftmp5],      %[ftmp5],      %[ftmp0]    \n\t"   \
         "packsswh     %[ftmp3],      %[ftmp3],      %[ftmp5]    \n\t"   \
-        "gsldlc1      %[ftmp4],      0x07(%[src2])              \n\t"   \
-        "gsldrc1      %[ftmp4],      0x00(%[src2])              \n\t"   \
+        MMI_ULDC1(%[ftmp4], %[src2], 0x00)                               \
         "li           %[rtmp0],      0x10                       \n\t"   \
         "dmtc1        %[rtmp0],      %[ftmp8]                   \n\t"   \
         "punpcklhw    %[ftmp5],      %[ftmp2],      %[ftmp3]    \n\t"   \
@@ -821,10 +792,9 @@ void ff_hevc_put_hevc_epel_bi_hv##w##_8_mmi(uint8_t *_dst,              \
         "psraw        %[ftmp6],      %[ftmp6],      %[shift]    \n\t"   \
         "packsswh     %[ftmp5],      %[ftmp5],      %[ftmp6]    \n\t"   \
         "pcmpgth      %[ftmp7],      %[ftmp5],      %[ftmp2]    \n\t"   \
-        "and          %[ftmp3],      %[ftmp5],      %[ftmp7]    \n\t"   \
+        "pand         %[ftmp3],      %[ftmp5],      %[ftmp7]    \n\t"   \
         "packushb     %[ftmp3],      %[ftmp3],      %[ftmp3]    \n\t"   \
-        "gsswlc1      %[ftmp3],      0x03(%[dst])               \n\t"   \
-        "gsswrc1      %[ftmp3],      0x00(%[dst])               \n\t"   \
+        MMI_USWC1(%[ftmp3], %[dst], 0x0)                                \
                                                                         \
         "daddi        %[x],          %[x],         -0x01        \n\t"   \
         PTR_ADDIU    "%[src2],       %[src2],       0x08        \n\t"   \
@@ -840,16 +810,17 @@ void ff_hevc_put_hevc_epel_bi_hv##w##_8_mmi(uint8_t *_dst,              \
         PTR_ADDU     "%[dst],        %[dst],        %[stride]   \n\t"   \
         PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"   \
         "bnez         %[y],          1b                         \n\t"   \
-        : [ftmp0]"=&f"(ftmp[0]), [ftmp1]"=&f"(ftmp[1]),                 \
+        : RESTRICT_ASM_LOW32 RESTRICT_ASM_ALL64                         \
+          [ftmp0]"=&f"(ftmp[0]), [ftmp1]"=&f"(ftmp[1]),                 \
           [ftmp2]"=&f"(ftmp[2]), [ftmp3]"=&f"(ftmp[3]),                 \
           [ftmp4]"=&f"(ftmp[4]), [ftmp5]"=&f"(ftmp[5]),                 \
           [ftmp6]"=&f"(ftmp[6]), [ftmp7]"=&f"(ftmp[7]),                 \
           [ftmp8]"=&f"(ftmp[8]), [ftmp9]"=&f"(ftmp[9]),                 \
           [ftmp10]"=&f"(ftmp[10]), [src2]"+&r"(src2),                   \
           [dst]"+&r"(dst), [tmp]"+&r"(tmp), [y]"+&r"(y), [x]"=&r"(x),   \
-          [offset]"+&f"(offset), [rtmp0]"=&r"(rtmp[0])                  \
+          [offset]"+&f"(offset.f), [rtmp0]"=&r"(rtmp[0])                \
         : [filter]"r"(filter), [stride]"r"(dststride),                  \
-          [shift]"f"(shift)                                             \
+          [shift]"f"(shift.f)                                           \
         : "memory"                                                      \
     );                                                                  \
 }
@@ -875,14 +846,16 @@ void ff_hevc_put_hevc_pel_bi_pixels##w##_8_mmi(uint8_t *_dst,             \
     ptrdiff_t srcstride = _srcstride / sizeof(pixel);                     \
     pixel *dst          = (pixel *)_dst;                                  \
     ptrdiff_t dststride = _dststride / sizeof(pixel);                     \
-    uint64_t ftmp[12];                                                    \
+    double  ftmp[12];                                                     \
     uint64_t rtmp[1];                                                     \
-    int shift = 7;                                                        \
+    union av_intfloat64 shift;                                            \
+    DECLARE_VAR_ALL64;                                                    \
+    shift.i = 7;                                                          \
                                                                           \
     y = height;                                                           \
     x = width >> 3;                                                       \
     __asm__ volatile(                                                     \
-        "xor          %[ftmp0],      %[ftmp0],      %[ftmp0]    \n\t"     \
+        "pxor         %[ftmp0],      %[ftmp0],      %[ftmp0]    \n\t"     \
         "li           %[rtmp0],      0x06                       \n\t"     \
         "dmtc1        %[rtmp0],      %[ftmp1]                   \n\t"     \
         "li           %[rtmp0],      0x10                       \n\t"     \
@@ -894,12 +867,9 @@ void ff_hevc_put_hevc_pel_bi_pixels##w##_8_mmi(uint8_t *_dst,             \
                                                                           \
         "1:                                                     \n\t"     \
         "2:                                                     \n\t"     \
-        "gsldlc1      %[ftmp5],      0x07(%[src])               \n\t"     \
-        "gsldrc1      %[ftmp5],      0x00(%[src])               \n\t"     \
-        "gsldlc1      %[ftmp2],      0x07(%[src2])              \n\t"     \
-        "gsldrc1      %[ftmp2],      0x00(%[src2])              \n\t"     \
-        "gsldlc1      %[ftmp3],      0x0f(%[src2])              \n\t"     \
-        "gsldrc1      %[ftmp3],      0x08(%[src2])              \n\t"     \
+        MMI_ULDC1(%[ftmp5], %[src], 0x00)                                 \
+        MMI_ULDC1(%[ftmp2], %[src2], 0x00)                                \
+        MMI_ULDC1(%[ftmp3], %[src2], 0x08)                                \
         "punpcklbh    %[ftmp4],      %[ftmp5],      %[ftmp0]    \n\t"     \
         "punpckhbh    %[ftmp5],      %[ftmp5],      %[ftmp0]    \n\t"     \
         "psllh        %[ftmp4],      %[ftmp4],      %[ftmp1]    \n\t"     \
@@ -930,11 +900,10 @@ void ff_hevc_put_hevc_pel_bi_pixels##w##_8_mmi(uint8_t *_dst,             \
         "packsswh     %[ftmp4],      %[ftmp4],      %[ftmp5]    \n\t"     \
         "pcmpgth      %[ftmp3],      %[ftmp2],      %[ftmp0]    \n\t"     \
         "pcmpgth      %[ftmp5],      %[ftmp4],      %[ftmp0]    \n\t"     \
-        "and          %[ftmp2],      %[ftmp2],      %[ftmp3]    \n\t"     \
-        "and          %[ftmp4],      %[ftmp4],      %[ftmp5]    \n\t"     \
+        "pand         %[ftmp2],      %[ftmp2],      %[ftmp3]    \n\t"     \
+        "pand         %[ftmp4],      %[ftmp4],      %[ftmp5]    \n\t"     \
         "packushb     %[ftmp2],      %[ftmp2],      %[ftmp4]    \n\t"     \
-        "gssdlc1      %[ftmp2],      0x07(%[dst])               \n\t"     \
-        "gssdrc1      %[ftmp2],      0x00(%[dst])               \n\t"     \
+        MMI_USDC1(%[ftmp2], %[dst], 0x0)                                  \
                                                                           \
         "daddi        %[x],          %[x],         -0x01        \n\t"     \
         PTR_ADDIU    "%[src],        %[src],        0x08        \n\t"     \
@@ -951,7 +920,8 @@ void ff_hevc_put_hevc_pel_bi_pixels##w##_8_mmi(uint8_t *_dst,             \
         PTR_ADDU     "%[dst],        %[dst],       %[dststride] \n\t"     \
         PTR_ADDIU    "%[src2],       %[src2],       0x80        \n\t"     \
         "bnez         %[y],          1b                         \n\t"     \
-        : [ftmp0]"=&f"(ftmp[0]), [ftmp1]"=&f"(ftmp[1]),                   \
+        : RESTRICT_ASM_ALL64                                              \
+          [ftmp0]"=&f"(ftmp[0]), [ftmp1]"=&f"(ftmp[1]),                   \
           [ftmp2]"=&f"(ftmp[2]), [ftmp3]"=&f"(ftmp[3]),                   \
           [ftmp4]"=&f"(ftmp[4]), [ftmp5]"=&f"(ftmp[5]),                   \
           [ftmp6]"=&f"(ftmp[6]), [ftmp7]"=&f"(ftmp[7]),                   \
@@ -959,7 +929,7 @@ void ff_hevc_put_hevc_pel_bi_pixels##w##_8_mmi(uint8_t *_dst,             \
           [ftmp10]"=&f"(ftmp[10]), [offset]"=&f"(ftmp[11]),               \
           [src2]"+&r"(src2), [dst]"+&r"(dst), [src]"+&r"(src),            \
           [x]"+&r"(x), [y]"+&r"(y), [rtmp0]"=&r"(rtmp[0])                 \
-        : [dststride]"r"(dststride), [shift]"f"(shift),                   \
+        : [dststride]"r"(dststride), [shift]"f"(shift.f),                 \
           [srcstride]"r"(srcstride)                                       \
         : "memory"                                                        \
     );                                                                    \
@@ -989,10 +959,14 @@ void ff_hevc_put_hevc_qpel_uni_hv##w##_8_mmi(uint8_t *_dst,             \
     ptrdiff_t dststride = _dststride / sizeof(pixel);                   \
     int16_t tmp_array[(MAX_PB_SIZE + QPEL_EXTRA) * MAX_PB_SIZE];        \
     int16_t *tmp = tmp_array;                                           \
-    uint64_t ftmp[20];                                                  \
+    double ftmp[20];                                                    \
     uint64_t rtmp[1];                                                   \
-    int shift = 6;                                                      \
-    int offset = 32;                                                    \
+    union av_intfloat64 shift;                                          \
+    union av_intfloat64 offset;                                         \
+    DECLARE_VAR_ALL64;                                                  \
+    DECLARE_VAR_LOW32;                                                  \
+    shift.i = 6;                                                        \
+    offset.i = 32;                                                      \
                                                                         \
     src   -= (QPEL_EXTRA_BEFORE * srcstride + 3);                       \
     filter = ff_hevc_qpel_filters[mx - 1];                              \
@@ -1006,18 +980,14 @@ void ff_hevc_put_hevc_qpel_uni_hv##w##_8_mmi(uint8_t *_dst,             \
         "punpcklbh    %[ftmp1],      %[ftmp0],      %[ftmp1]    \n\t"   \
         "psrah        %[ftmp1],      %[ftmp1],      %[ftmp0]    \n\t"   \
         "psrah        %[ftmp2],      %[ftmp2],      %[ftmp0]    \n\t"   \
-        "xor          %[ftmp0],      %[ftmp0],      %[ftmp0]    \n\t"   \
+        "pxor         %[ftmp0],      %[ftmp0],      %[ftmp0]    \n\t"   \
                                                                         \
         "1:                                                     \n\t"   \
         "2:                                                     \n\t"   \
-        "gsldlc1      %[ftmp3],      0x07(%[src])               \n\t"   \
-        "gsldrc1      %[ftmp3],      0x00(%[src])               \n\t"   \
-        "gsldlc1      %[ftmp4],      0x08(%[src])               \n\t"   \
-        "gsldrc1      %[ftmp4],      0x01(%[src])               \n\t"   \
-        "gsldlc1      %[ftmp5],      0x09(%[src])               \n\t"   \
-        "gsldrc1      %[ftmp5],      0x02(%[src])               \n\t"   \
-        "gsldlc1      %[ftmp6],      0x0a(%[src])               \n\t"   \
-        "gsldrc1      %[ftmp6],      0x03(%[src])               \n\t"   \
+        MMI_ULDC1(%[ftmp3], %[src], 0x00)                               \
+        MMI_ULDC1(%[ftmp4], %[src], 0x01)                               \
+        MMI_ULDC1(%[ftmp5], %[src], 0x02)                               \
+        MMI_ULDC1(%[ftmp6], %[src], 0x03)                               \
         "punpcklbh    %[ftmp7],      %[ftmp3],      %[ftmp0]    \n\t"   \
         "punpckhbh    %[ftmp8],      %[ftmp3],      %[ftmp0]    \n\t"   \
         "pmullh       %[ftmp7],      %[ftmp7],      %[ftmp1]    \n\t"   \
@@ -1043,8 +1013,7 @@ void ff_hevc_put_hevc_qpel_uni_hv##w##_8_mmi(uint8_t *_dst,             \
         "paddh        %[ftmp3],      %[ftmp3],      %[ftmp4]    \n\t"   \
         "paddh        %[ftmp5],      %[ftmp5],      %[ftmp6]    \n\t"   \
         "paddh        %[ftmp3],      %[ftmp3],      %[ftmp5]    \n\t"   \
-        "gssdlc1      %[ftmp3],      0x07(%[tmp])               \n\t"   \
-        "gssdrc1      %[ftmp3],      0x00(%[tmp])               \n\t"   \
+        MMI_USDC1(%[ftmp3], %[tmp], 0x0)                                \
                                                                         \
         "daddi        %[x],          %[x],         -0x01        \n\t"   \
         PTR_ADDIU    "%[src],        %[src],        0x04        \n\t"   \
@@ -1058,7 +1027,8 @@ void ff_hevc_put_hevc_qpel_uni_hv##w##_8_mmi(uint8_t *_dst,             \
         PTR_ADDU     "%[src],        %[src],        %[stride]   \n\t"   \
         PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"   \
         "bnez         %[y],          1b                         \n\t"   \
-        : [ftmp0]"=&f"(ftmp[0]), [ftmp1]"=&f"(ftmp[1]),                 \
+        : RESTRICT_ASM_ALL64                                            \
+          [ftmp0]"=&f"(ftmp[0]), [ftmp1]"=&f"(ftmp[1]),                 \
           [ftmp2]"=&f"(ftmp[2]), [ftmp3]"=&f"(ftmp[3]),                 \
           [ftmp4]"=&f"(ftmp[4]), [ftmp5]"=&f"(ftmp[5]),                 \
           [ftmp6]"=&f"(ftmp[6]), [ftmp7]"=&f"(ftmp[7]),                 \
@@ -1090,29 +1060,21 @@ void ff_hevc_put_hevc_qpel_uni_hv##w##_8_mmi(uint8_t *_dst,             \
         "1:                                                     \n\t"   \
         "li           %[x],        " #x_step "                  \n\t"   \
         "2:                                                     \n\t"   \
-        "gsldlc1      %[ftmp3],      0x07(%[tmp])               \n\t"   \
-        "gsldrc1      %[ftmp3],      0x00(%[tmp])               \n\t"   \
+        MMI_ULDC1(%[ftmp3], %[tmp], 0x00)                               \
         PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"   \
-        "gsldlc1      %[ftmp4],      0x07(%[tmp])               \n\t"   \
-        "gsldrc1      %[ftmp4],      0x00(%[tmp])               \n\t"   \
+        MMI_ULDC1(%[ftmp4], %[tmp], 0x00)                               \
         PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"   \
-        "gsldlc1      %[ftmp5],      0x07(%[tmp])               \n\t"   \
-        "gsldrc1      %[ftmp5],      0x00(%[tmp])               \n\t"   \
+        MMI_ULDC1(%[ftmp5], %[tmp], 0x00)                               \
         PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"   \
-        "gsldlc1      %[ftmp6],      0x07(%[tmp])               \n\t"   \
-        "gsldrc1      %[ftmp6],      0x00(%[tmp])               \n\t"   \
+        MMI_ULDC1(%[ftmp6], %[tmp], 0x00)                               \
         PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"   \
-        "gsldlc1      %[ftmp7],      0x07(%[tmp])               \n\t"   \
-        "gsldrc1      %[ftmp7],      0x00(%[tmp])               \n\t"   \
+        MMI_ULDC1(%[ftmp7], %[tmp], 0x00)                               \
         PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"   \
-        "gsldlc1      %[ftmp8],      0x07(%[tmp])               \n\t"   \
-        "gsldrc1      %[ftmp8],      0x00(%[tmp])               \n\t"   \
+        MMI_ULDC1(%[ftmp8], %[tmp], 0x00)                               \
         PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"   \
-        "gsldlc1      %[ftmp9],      0x07(%[tmp])               \n\t"   \
-        "gsldrc1      %[ftmp9],      0x00(%[tmp])               \n\t"   \
+        MMI_ULDC1(%[ftmp9], %[tmp], 0x00)                               \
         PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"   \
-        "gsldlc1      %[ftmp10],     0x07(%[tmp])               \n\t"   \
-        "gsldrc1      %[ftmp10],     0x00(%[tmp])               \n\t"   \
+        MMI_ULDC1(%[ftmp10], %[tmp], 0x00)                              \
         PTR_ADDIU    "%[tmp],        %[tmp],        -0x380      \n\t"   \
         TRANSPOSE_4H(%[ftmp3], %[ftmp4], %[ftmp5], %[ftmp6],            \
                      %[ftmp11], %[ftmp12], %[ftmp13], %[ftmp14])        \
@@ -1139,12 +1101,11 @@ void ff_hevc_put_hevc_qpel_uni_hv##w##_8_mmi(uint8_t *_dst,             \
         "packsswh     %[ftmp3],      %[ftmp3],      %[ftmp5]    \n\t"   \
         "paddh        %[ftmp3],      %[ftmp3],      %[offset]   \n\t"   \
         "psrah        %[ftmp3],      %[ftmp3],      %[shift]    \n\t"   \
-        "xor          %[ftmp7],      %[ftmp7],      %[ftmp7]    \n\t"   \
+        "pxor         %[ftmp7],      %[ftmp7],      %[ftmp7]    \n\t"   \
         "pcmpgth      %[ftmp7],      %[ftmp3],      %[ftmp7]    \n\t"   \
-        "and          %[ftmp3],      %[ftmp3],      %[ftmp7]    \n\t"   \
+        "pand         %[ftmp3],      %[ftmp3],      %[ftmp7]    \n\t"   \
         "packushb     %[ftmp3],      %[ftmp3],      %[ftmp3]    \n\t"   \
-        "gsswlc1      %[ftmp3],      0x03(%[dst])               \n\t"   \
-        "gsswrc1      %[ftmp3],      0x00(%[dst])               \n\t"   \
+        MMI_USWC1(%[ftmp3], %[dst], 0x00)                               \
                                                                         \
         "daddi        %[x],          %[x],         -0x01        \n\t"   \
         PTR_ADDIU    "%[tmp],        %[tmp],        0x08        \n\t"   \
@@ -1157,7 +1118,8 @@ void ff_hevc_put_hevc_qpel_uni_hv##w##_8_mmi(uint8_t *_dst,             \
         PTR_ADDU     "%[dst],        %[dst],        %[stride]   \n\t"   \
         PTR_ADDIU    "%[tmp],        %[tmp],        0x80        \n\t"   \
         "bnez         %[y],          1b                         \n\t"   \
-        : [ftmp0]"=&f"(ftmp[0]), [ftmp1]"=&f"(ftmp[1]),                 \
+        : RESTRICT_ASM_ALL64 RESTRICT_ASM_LOW32                         \
+          [ftmp0]"=&f"(ftmp[0]), [ftmp1]"=&f"(ftmp[1]),                 \
           [ftmp2]"=&f"(ftmp[2]), [ftmp3]"=&f"(ftmp[3]),                 \
           [ftmp4]"=&f"(ftmp[4]), [ftmp5]"=&f"(ftmp[5]),                 \
           [ftmp6]"=&f"(ftmp[6]), [ftmp7]"=&f"(ftmp[7]),                 \
@@ -1166,9 +1128,9 @@ void ff_hevc_put_hevc_qpel_uni_hv##w##_8_mmi(uint8_t *_dst,             \
           [ftmp12]"=&f"(ftmp[12]), [ftmp13]"=&f"(ftmp[13]),             \
           [ftmp14]"=&f"(ftmp[14]),                                      \
           [dst]"+&r"(dst), [tmp]"+&r"(tmp), [y]"+&r"(y), [x]"=&r"(x),   \
-          [offset]"+&f"(offset), [rtmp0]"=&r"(rtmp[0])                  \
+          [offset]"+&f"(offset.f), [rtmp0]"=&r"(rtmp[0])                \
         : [filter]"r"(filter), [stride]"r"(dststride),                  \
-          [shift]"f"(shift)                                             \
+          [shift]"f"(shift.f)                                           \
         : "memory"                                                      \
     );                                                                  \
 }
diff --git a/libavcodec/mips/hevcdsp_msa.c b/libavcodec/mips/hevcdsp_msa.c
index 2c57ec857a..20f1cdfa63 100644
--- a/libavcodec/mips/hevcdsp_msa.c
+++ b/libavcodec/mips/hevcdsp_msa.c
@@ -81,12 +81,13 @@ static void hevc_copy_6w_msa(uint8_t *src, int32_t src_stride,
                              int16_t *dst, int32_t dst_stride,
                              int32_t height)
 {
-    uint32_t loop_cnt;
+    uint32_t loop_cnt = (height >> 3);
+    uint32_t res = height & 0x07;
     v16i8 zero = { 0 };
     v16i8 src0, src1, src2, src3, src4, src5, src6, src7;
     v8i16 in0, in1, in2, in3, in4, in5, in6, in7;
 
-    for (loop_cnt = (height >> 3); loop_cnt--;) {
+    for (; loop_cnt--; ) {
         LD_SB8(src, src_stride, src0, src1, src2, src3, src4, src5, src6, src7);
         src += (8 * src_stride);
 
@@ -99,6 +100,19 @@ static void hevc_copy_6w_msa(uint8_t *src, int32_t src_stride,
         ST12x8_UB(in0, in1, in2, in3, in4, in5, in6, in7, dst, 2 * dst_stride);
         dst += (8 * dst_stride);
     }
+    for (; res--; ) {
+        uint64_t out0;
+        uint32_t out1;
+        src0 = LD_SB(src);
+        src += src_stride;
+        in0  = (v8i16)__msa_ilvr_b((v16i8) zero, (v16i8) src0);
+        in0  = in0 << 6;
+        out0 = __msa_copy_u_d((v2i64) in0, 0);
+        out1 = __msa_copy_u_w((v4i32) in0, 2);
+        SD(out0, dst);
+        SW(out1, dst + 4);
+        dst += dst_stride;
+    }
 }
 
 static void hevc_copy_8w_msa(uint8_t *src, int32_t src_stride,
@@ -167,6 +181,7 @@ static void hevc_copy_12w_msa(uint8_t *src, int32_t src_stride,
                               int32_t height)
 {
     uint32_t loop_cnt;
+    uint32_t res = height & 0x07;
     v16i8 zero = { 0 };
     v16i8 src0, src1, src2, src3, src4, src5, src6, src7;
     v8i16 in0, in1, in0_r, in1_r, in2_r, in3_r;
@@ -197,6 +212,19 @@ static void hevc_copy_12w_msa(uint8_t *src, int32_t src_stride,
         ST_D4(in0, in1, 0, 1, 0, 1, dst + 8, dst_stride);
         dst += (4 * dst_stride);
     }
+    for (; res--; ) {
+        uint64_t out0;
+        src0 = LD_SB(src);
+        src += src_stride;
+        in0_r = (v8i16)__msa_ilvr_b((v16i8) zero, (v16i8) src0);
+        in0   = (v8i16)__msa_ilvl_b((v16i8) zero, (v16i8) src0);
+        in0_r = in0_r << 6;
+        in0   = in0 << 6;
+        ST_UH(in0_r, dst);
+        out0 = __msa_copy_u_d((v2i64) in0, 0);
+        SD(out0, dst + 8);
+        dst += dst_stride;
+    }
 }
 
 static void hevc_copy_16w_msa(uint8_t *src, int32_t src_stride,
@@ -450,6 +478,7 @@ static void hevc_hz_8t_4w_msa(uint8_t *src, int32_t src_stride,
                               const int8_t *filter, int32_t height)
 {
     uint32_t loop_cnt;
+    uint32_t res = (height & 0x07) >> 1;
     v16i8 src0, src1, src2, src3, src4, src5, src6, src7;
     v8i16 filt0, filt1, filt2, filt3;
     v16i8 mask1, mask2, mask3;
@@ -498,6 +527,18 @@ static void hevc_hz_8t_4w_msa(uint8_t *src, int32_t src_stride,
         ST_D8(dst0, dst1, dst2, dst3, 0, 1, 0, 1, 0, 1, 0, 1, dst, dst_stride);
         dst += (8 * dst_stride);
     }
+    for (; res--; ) {
+        LD_SB2(src, src_stride, src0, src1);
+        src += 2 * src_stride;
+        XORI_B2_128_SB(src0, src1);
+        VSHF_B4_SB(src0, src1, mask0, mask1, mask2, mask3,
+                   vec0, vec1, vec2, vec3);
+        dst0 = const_vec;
+        DPADD_SB4_SH(vec0, vec1, vec2, vec3, filt0, filt1, filt2, filt3,
+                     dst0, dst0, dst0, dst0);
+        ST_D2(dst0, 0, 1, dst, dst_stride);
+        dst += 2 * dst_stride;
+    }
 }
 
 static void hevc_hz_8t_8w_msa(uint8_t *src, int32_t src_stride,
@@ -992,6 +1033,7 @@ static void hevc_vt_8t_4w_msa(uint8_t *src, int32_t src_stride,
                               const int8_t *filter, int32_t height)
 {
     int32_t loop_cnt;
+    int32_t res = (height & 0x07) >> 1;
     v16i8 src0, src1, src2, src3, src4, src5, src6, src7, src8;
     v16i8 src9, src10, src11, src12, src13, src14;
     v16i8 src10_r, src32_r, src54_r, src76_r, src98_r;
@@ -1055,6 +1097,22 @@ static void hevc_vt_8t_4w_msa(uint8_t *src, int32_t src_stride,
         src6554 = src14131312;
         src6 = src14;
     }
+    for (; res--; ) {
+        LD_SB2(src, src_stride, src7, src8);
+        src += 2 * src_stride;
+        ILVR_B2_SB(src7, src6, src8, src7, src76_r, src87_r);
+        src8776 = (v16i8)__msa_ilvr_d((v2i64) src87_r, src76_r);
+        src8776 = (v16i8)__msa_xori_b((v16i8) src8776, 128);
+        dst10 = const_vec;
+        DPADD_SB4_SH(src2110, src4332, src6554, src8776,
+                     filt0, filt1, filt2, filt3, dst10, dst10, dst10, dst10);
+        ST_D2(dst10, 0, 1, dst, dst_stride);
+        dst += 2 * dst_stride;
+        src2110 = src4332;
+        src4332 = src6554;
+        src6554 = src8776;
+        src6    = src8;
+    }
 }
 
 static void hevc_vt_8t_8w_msa(uint8_t *src, int32_t src_stride,
@@ -2661,6 +2719,7 @@ static void hevc_vt_4t_6w_msa(uint8_t *src,
                               int32_t height)
 {
     int32_t loop_cnt;
+    int32_t res = height & 0x03;
     uint32_t dst_val_int0, dst_val_int1, dst_val_int2, dst_val_int3;
     uint64_t dst_val0, dst_val1, dst_val2, dst_val3;
     v16i8 src0, src1, src2, src3, src4;
@@ -2725,6 +2784,29 @@ static void hevc_vt_4t_6w_msa(uint8_t *src,
         SW(dst_val_int3, dst + 4);
         dst += dst_stride;
     }
+    if (res) {
+        LD_SB2(src, src_stride, src3, src4);
+        XORI_B2_128_SB(src3, src4);
+        ILVR_B2_SB(src3, src2, src4, src3, src32_r, src43_r);
+
+        dst0_r = const_vec;
+        DPADD_SB2_SH(src10_r, src32_r, filt0, filt1, dst0_r, dst0_r);
+        dst1_r = const_vec;
+        DPADD_SB2_SH(src21_r, src43_r, filt0, filt1, dst1_r, dst1_r);
+
+        dst_val0 = __msa_copy_u_d((v2i64) dst0_r, 0);
+        dst_val1 = __msa_copy_u_d((v2i64) dst1_r, 0);
+
+        dst_val_int0 = __msa_copy_u_w((v4i32) dst0_r, 2);
+        dst_val_int1 = __msa_copy_u_w((v4i32) dst1_r, 2);
+
+        SD(dst_val0, dst);
+        SW(dst_val_int0, dst + 4);
+        dst += dst_stride;
+        SD(dst_val1, dst);
+        SW(dst_val_int1, dst + 4);
+        dst += dst_stride;
+    }
 }
 
 static void hevc_vt_4t_8x2_msa(uint8_t *src,
diff --git a/libavcodec/mips/hevcpred_init_mips.c b/libavcodec/mips/hevcpred_init_mips.c
index e987698d66..f7ecb34dcc 100644
--- a/libavcodec/mips/hevcpred_init_mips.c
+++ b/libavcodec/mips/hevcpred_init_mips.c
@@ -18,32 +18,28 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
+#include "libavutil/mips/cpu.h"
 #include "config.h"
 #include "libavutil/attributes.h"
 #include "libavcodec/mips/hevcpred_mips.h"
 
-#if HAVE_MSA
-static av_cold void hevc_pred_init_msa(HEVCPredContext *c, const int bit_depth)
-{
-    if (8 == bit_depth) {
-        c->intra_pred[2] = ff_intra_pred_8_16x16_msa;
-        c->intra_pred[3] = ff_intra_pred_8_32x32_msa;
-        c->pred_planar[0] = ff_hevc_intra_pred_planar_0_msa;
-        c->pred_planar[1] = ff_hevc_intra_pred_planar_1_msa;
-        c->pred_planar[2] = ff_hevc_intra_pred_planar_2_msa;
-        c->pred_planar[3] = ff_hevc_intra_pred_planar_3_msa;
-        c->pred_dc = ff_hevc_intra_pred_dc_msa;
-        c->pred_angular[0] = ff_pred_intra_pred_angular_0_msa;
-        c->pred_angular[1] = ff_pred_intra_pred_angular_1_msa;
-        c->pred_angular[2] = ff_pred_intra_pred_angular_2_msa;
-        c->pred_angular[3] = ff_pred_intra_pred_angular_3_msa;
-    }
-}
-#endif  // #if HAVE_MSA
-
 void ff_hevc_pred_init_mips(HEVCPredContext *c, const int bit_depth)
 {
-#if HAVE_MSA
-    hevc_pred_init_msa(c, bit_depth);
-#endif  // #if HAVE_MSA
+    int cpu_flags = av_get_cpu_flags();
+
+    if (have_msa(cpu_flags)) {
+        if (bit_depth == 8) {
+            c->intra_pred[2] = ff_intra_pred_8_16x16_msa;
+            c->intra_pred[3] = ff_intra_pred_8_32x32_msa;
+            c->pred_planar[0] = ff_hevc_intra_pred_planar_0_msa;
+            c->pred_planar[1] = ff_hevc_intra_pred_planar_1_msa;
+            c->pred_planar[2] = ff_hevc_intra_pred_planar_2_msa;
+            c->pred_planar[3] = ff_hevc_intra_pred_planar_3_msa;
+            c->pred_dc = ff_hevc_intra_pred_dc_msa;
+            c->pred_angular[0] = ff_pred_intra_pred_angular_0_msa;
+            c->pred_angular[1] = ff_pred_intra_pred_angular_1_msa;
+            c->pred_angular[2] = ff_pred_intra_pred_angular_2_msa;
+            c->pred_angular[3] = ff_pred_intra_pred_angular_3_msa;
+        }
+    }
 }
diff --git a/libavcodec/mips/hevcpred_msa.c b/libavcodec/mips/hevcpred_msa.c
index b8df089e0c..f53276d34e 100644
--- a/libavcodec/mips/hevcpred_msa.c
+++ b/libavcodec/mips/hevcpred_msa.c
@@ -83,7 +83,7 @@ static void hevc_intra_pred_vert_4x4_msa(const uint8_t *src_top,
         vec2 -= vec0;
         vec2 >>= 1;
         vec2 += vec1;
-        vec2 = CLIP_SH_0_255(vec2);
+        CLIP_SH_0_255(vec2);
 
         for (col = 0; col < 4; col++) {
             dst[stride * col] = (uint8_t) vec2[col];
@@ -122,7 +122,7 @@ static void hevc_intra_pred_vert_8x8_msa(const uint8_t *src_top,
         vec2 -= vec0;
         vec2 >>= 1;
         vec2 += vec1;
-        vec2 = CLIP_SH_0_255(vec2);
+        CLIP_SH_0_255(vec2);
 
         val0 = vec2[0];
         val1 = vec2[1];
@@ -214,7 +214,7 @@ static void hevc_intra_pred_horiz_4x4_msa(const uint8_t *src_top,
         src0_r -= src_top_val;
         src0_r >>= 1;
         src0_r += src_left_val;
-        src0_r = CLIP_SH_0_255(src0_r);
+        CLIP_SH_0_255(src0_r);
         src0 = __msa_pckev_b((v16i8) src0_r, (v16i8) src0_r);
         val0 = __msa_copy_s_w((v4i32) src0, 0);
         SW(val0, dst);
@@ -254,7 +254,7 @@ static void hevc_intra_pred_horiz_8x8_msa(const uint8_t *src_top,
         src0_r -= src_top_val;
         src0_r >>= 1;
         src0_r += src_left_val;
-        src0_r = CLIP_SH_0_255(src0_r);
+        CLIP_SH_0_255(src0_r);
         src0 = __msa_pckev_b((v16i8) src0_r, (v16i8) src0_r);
         val0 = __msa_copy_s_d((v2i64) src0, 0);
         SD(val0, dst);
@@ -998,7 +998,8 @@ static void hevc_intra_pred_angular_upper_4width_msa(const uint8_t *src_top,
     ILVR_D2_SH(fact3, fact1, fact7, fact5, fact1, fact3);
     ILVR_B4_SH(zero, top0, zero, top1, zero, top2, zero, top3,
                diff0, diff2, diff4, diff6);
-    SLDI_B4_0_SH(diff0, diff2, diff4, diff6, diff1, diff3, diff5, diff7, 2);
+    SLDI_B4_SH(zero, diff0, zero, diff2, zero, diff4, zero, diff6, 2,
+               diff1, diff3, diff5, diff7);
     ILVR_D2_SH(diff2, diff0, diff6, diff4, diff0, diff2);
     ILVR_D2_SH(diff3, diff1, diff7, diff5, diff1, diff3);
     MUL2(diff1, fact0, diff3, fact2, diff1, diff3);
@@ -1093,8 +1094,8 @@ static void hevc_intra_pred_angular_upper_8width_msa(const uint8_t *src_top,
         UNPCK_UB_SH(top2, diff4, diff5);
         UNPCK_UB_SH(top3, diff6, diff7);
 
-        SLDI_B2_SH(diff1, diff3, diff0, diff2, diff1, diff3, 2);
-        SLDI_B2_SH(diff5, diff7, diff4, diff6, diff5, diff7, 2);
+        SLDI_B4_SH(diff1, diff0, diff3, diff2, diff5, diff4, diff7, diff6, 2,
+                   diff1, diff3, diff5, diff7);
         MUL4(diff1, fact0, diff3, fact2, diff5, fact4, diff7, fact6,
              diff1, diff3, diff5, diff7);
 
@@ -1186,8 +1187,8 @@ static void hevc_intra_pred_angular_upper_16width_msa(const uint8_t *src_top,
         fact6 = __msa_fill_h(fact_val3);
         fact7 = __msa_fill_h(32 - fact_val3);
 
-        SLDI_B2_UB(top1, top3, top0, top2, top1, top3, 1);
-        SLDI_B2_UB(top5, top7, top4, top6, top5, top7, 1);
+        SLDI_B4_UB(top1, top0, top3, top2, top5, top4, top7, top6, 1,
+                   top1, top3, top5, top7);
         UNPCK_UB_SH(top0, diff0, diff1);
         UNPCK_UB_SH(top1, diff2, diff3);
         UNPCK_UB_SH(top2, diff4, diff5);
@@ -1297,8 +1298,8 @@ static void hevc_intra_pred_angular_upper_32width_msa(const uint8_t *src_top,
         top2 = top1;
         top6 = top5;
 
-        SLDI_B2_UB(top1, top3, top0, top2, top1, top3, 1);
-        SLDI_B2_UB(top5, top7, top4, top6, top5, top7, 1);
+        SLDI_B4_UB(top1, top0, top3, top2, top5, top4, top7, top6, 1,
+                   top1, top3, top5, top7);
         UNPCK_UB_SH(top0, diff0, diff1);
         UNPCK_UB_SH(top1, diff2, diff3);
         UNPCK_UB_SH(top2, diff4, diff5);
@@ -1407,7 +1408,8 @@ static void hevc_intra_pred_angular_lower_4width_msa(const uint8_t *src_top,
     ILVR_D2_SH(fact3, fact1, fact7, fact5, fact1, fact3);
     ILVR_B4_SH(zero, top0, zero, top1, zero, top2, zero, top3,
                diff0, diff2, diff4, diff6);
-    SLDI_B4_0_SH(diff0, diff2, diff4, diff6, diff1, diff3, diff5, diff7, 2);
+    SLDI_B4_SH(zero, diff0, zero, diff2, zero, diff4, zero, diff6, 2,
+               diff1, diff3, diff5, diff7);
     ILVR_D2_SH(diff2, diff0, diff6, diff4, diff0, diff2);
     ILVR_D2_SH(diff3, diff1, diff7, diff5, diff1, diff3);
     MUL2(diff1, fact0, diff3, fact2, diff1, diff3);
@@ -1511,8 +1513,8 @@ static void hevc_intra_pred_angular_lower_8width_msa(const uint8_t *src_top,
         UNPCK_UB_SH(top1, diff2, diff3);
         UNPCK_UB_SH(top2, diff4, diff5);
         UNPCK_UB_SH(top3, diff6, diff7);
-        SLDI_B2_SH(diff1, diff3, diff0, diff2, diff1, diff3, 2);
-        SLDI_B2_SH(diff5, diff7, diff4, diff6, diff5, diff7, 2);
+        SLDI_B4_SH(diff1, diff0, diff3, diff2, diff5, diff4, diff7, diff6, 2,
+                   diff1, diff3, diff5, diff7);
         MUL4(diff1, fact0, diff3, fact2, diff5, fact4, diff7, fact6,
              diff1, diff3, diff5, diff7);
 
@@ -1606,8 +1608,8 @@ static void hevc_intra_pred_angular_lower_16width_msa(const uint8_t *src_top,
         fact6 = __msa_fill_h(fact_val3);
         fact7 = __msa_fill_h(32 - fact_val3);
 
-        SLDI_B2_SB(top1, top3, top0, top2, top1, top3, 1);
-        SLDI_B2_SB(top5, top7, top4, top6, top5, top7, 1);
+        SLDI_B4_SB(top1, top0, top3, top2, top5, top4, top7, top6, 1,
+                   top1, top3, top5, top7);
 
         UNPCK_UB_SH(top0, diff0, diff1);
         UNPCK_UB_SH(top1, diff2, diff3);
@@ -1713,8 +1715,8 @@ static void hevc_intra_pred_angular_lower_32width_msa(const uint8_t *src_top,
         top2 = top1;
         top6 = top5;
 
-        SLDI_B2_SB(top1, top3, top0, top2, top1, top3, 1);
-        SLDI_B2_SB(top5, top7, top4, top6, top5, top7, 1);
+        SLDI_B4_SB(top1, top0, top3, top2, top5, top4, top7, top6, 1,
+                   top1, top3, top5, top7);
 
         UNPCK_UB_SH(top0, diff0, diff1);
         UNPCK_UB_SH(top1, diff2, diff3);
diff --git a/libavcodec/mips/hpeldsp_init_mips.c b/libavcodec/mips/hpeldsp_init_mips.c
index d6f7a9793d..77cbe99fa4 100644
--- a/libavcodec/mips/hpeldsp_init_mips.c
+++ b/libavcodec/mips/hpeldsp_init_mips.c
@@ -19,104 +19,94 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
+#include "libavutil/mips/cpu.h"
 #include "../hpeldsp.h"
 #include "libavcodec/mips/hpeldsp_mips.h"
 
-#if HAVE_MSA
-static void ff_hpeldsp_init_msa(HpelDSPContext *c, int flags)
-{
-    c->put_pixels_tab[0][0] = ff_put_pixels16_msa;
-    c->put_pixels_tab[0][1] = ff_put_pixels16_x2_msa;
-    c->put_pixels_tab[0][2] = ff_put_pixels16_y2_msa;
-    c->put_pixels_tab[0][3] = ff_put_pixels16_xy2_msa;
-
-    c->put_pixels_tab[1][0] = ff_put_pixels8_msa;
-    c->put_pixels_tab[1][1] = ff_put_pixels8_x2_msa;
-    c->put_pixels_tab[1][2] = ff_put_pixels8_y2_msa;
-    c->put_pixels_tab[1][3] = ff_put_pixels8_xy2_msa;
-
-    c->put_pixels_tab[2][1] = ff_put_pixels4_x2_msa;
-    c->put_pixels_tab[2][2] = ff_put_pixels4_y2_msa;
-    c->put_pixels_tab[2][3] = ff_put_pixels4_xy2_msa;
-
-    c->put_no_rnd_pixels_tab[0][0] = ff_put_pixels16_msa;
-    c->put_no_rnd_pixels_tab[0][1] = ff_put_no_rnd_pixels16_x2_msa;
-    c->put_no_rnd_pixels_tab[0][2] = ff_put_no_rnd_pixels16_y2_msa;
-    c->put_no_rnd_pixels_tab[0][3] = ff_put_no_rnd_pixels16_xy2_msa;
-
-    c->put_no_rnd_pixels_tab[1][0] = ff_put_pixels8_msa;
-    c->put_no_rnd_pixels_tab[1][1] = ff_put_no_rnd_pixels8_x2_msa;
-    c->put_no_rnd_pixels_tab[1][2] = ff_put_no_rnd_pixels8_y2_msa;
-    c->put_no_rnd_pixels_tab[1][3] = ff_put_no_rnd_pixels8_xy2_msa;
-
-    c->avg_pixels_tab[0][0] = ff_avg_pixels16_msa;
-    c->avg_pixels_tab[0][1] = ff_avg_pixels16_x2_msa;
-    c->avg_pixels_tab[0][2] = ff_avg_pixels16_y2_msa;
-    c->avg_pixels_tab[0][3] = ff_avg_pixels16_xy2_msa;
-
-    c->avg_pixels_tab[1][0] = ff_avg_pixels8_msa;
-    c->avg_pixels_tab[1][1] = ff_avg_pixels8_x2_msa;
-    c->avg_pixels_tab[1][2] = ff_avg_pixels8_y2_msa;
-    c->avg_pixels_tab[1][3] = ff_avg_pixels8_xy2_msa;
-
-    c->avg_pixels_tab[2][0] = ff_avg_pixels4_msa;
-    c->avg_pixels_tab[2][1] = ff_avg_pixels4_x2_msa;
-    c->avg_pixels_tab[2][2] = ff_avg_pixels4_y2_msa;
-    c->avg_pixels_tab[2][3] = ff_avg_pixels4_xy2_msa;
-}
-#endif  // #if HAVE_MSA
-
-#if HAVE_MMI
-static void ff_hpeldsp_init_mmi(HpelDSPContext *c, int flags)
-{
-    c->put_pixels_tab[0][0] = ff_put_pixels16_8_mmi;
-    c->put_pixels_tab[0][1] = ff_put_pixels16_x2_8_mmi;
-    c->put_pixels_tab[0][2] = ff_put_pixels16_y2_8_mmi;
-    c->put_pixels_tab[0][3] = ff_put_pixels16_xy2_8_mmi;
-
-    c->put_pixels_tab[1][0] = ff_put_pixels8_8_mmi;
-    c->put_pixels_tab[1][1] = ff_put_pixels8_x2_8_mmi;
-    c->put_pixels_tab[1][2] = ff_put_pixels8_y2_8_mmi;
-    c->put_pixels_tab[1][3] = ff_put_pixels8_xy2_8_mmi;
-
-    c->put_pixels_tab[2][0] = ff_put_pixels4_8_mmi;
-    c->put_pixels_tab[2][1] = ff_put_pixels4_x2_8_mmi;
-    c->put_pixels_tab[2][2] = ff_put_pixels4_y2_8_mmi;
-    c->put_pixels_tab[2][3] = ff_put_pixels4_xy2_8_mmi;
-
-    c->put_no_rnd_pixels_tab[0][0] = ff_put_pixels16_8_mmi;
-    c->put_no_rnd_pixels_tab[0][1] = ff_put_no_rnd_pixels16_x2_8_mmi;
-    c->put_no_rnd_pixels_tab[0][2] = ff_put_no_rnd_pixels16_y2_8_mmi;
-    c->put_no_rnd_pixels_tab[0][3] = ff_put_no_rnd_pixels16_xy2_8_mmi;
-
-    c->put_no_rnd_pixels_tab[1][0] = ff_put_pixels8_8_mmi;
-    c->put_no_rnd_pixels_tab[1][1] = ff_put_no_rnd_pixels8_x2_8_mmi;
-    c->put_no_rnd_pixels_tab[1][2] = ff_put_no_rnd_pixels8_y2_8_mmi;
-    c->put_no_rnd_pixels_tab[1][3] = ff_put_no_rnd_pixels8_xy2_8_mmi;
-
-    c->avg_pixels_tab[0][0] = ff_avg_pixels16_8_mmi;
-    c->avg_pixels_tab[0][1] = ff_avg_pixels16_x2_8_mmi;
-    c->avg_pixels_tab[0][2] = ff_avg_pixels16_y2_8_mmi;
-    c->avg_pixels_tab[0][3] = ff_avg_pixels16_xy2_8_mmi;
-
-    c->avg_pixels_tab[1][0] = ff_avg_pixels8_8_mmi;
-    c->avg_pixels_tab[1][1] = ff_avg_pixels8_x2_8_mmi;
-    c->avg_pixels_tab[1][2] = ff_avg_pixels8_y2_8_mmi;
-    c->avg_pixels_tab[1][3] = ff_avg_pixels8_xy2_8_mmi;
-
-    c->avg_pixels_tab[2][0] = ff_avg_pixels4_8_mmi;
-    c->avg_pixels_tab[2][1] = ff_avg_pixels4_x2_8_mmi;
-    c->avg_pixels_tab[2][2] = ff_avg_pixels4_y2_8_mmi;
-    c->avg_pixels_tab[2][3] = ff_avg_pixels4_xy2_8_mmi;
-}
-#endif  // #if HAVE_MMI
-
 void ff_hpeldsp_init_mips(HpelDSPContext *c, int flags)
 {
-#if HAVE_MMI
-    ff_hpeldsp_init_mmi(c, flags);
-#endif  // #if HAVE_MMI
-#if HAVE_MSA
-    ff_hpeldsp_init_msa(c, flags);
-#endif  // #if HAVE_MSA
+    int cpu_flags = av_get_cpu_flags();
+
+    if (have_mmi(cpu_flags)) {
+        c->put_pixels_tab[0][0] = ff_put_pixels16_8_mmi;
+        c->put_pixels_tab[0][1] = ff_put_pixels16_x2_8_mmi;
+        c->put_pixels_tab[0][2] = ff_put_pixels16_y2_8_mmi;
+        c->put_pixels_tab[0][3] = ff_put_pixels16_xy2_8_mmi;
+
+        c->put_pixels_tab[1][0] = ff_put_pixels8_8_mmi;
+        c->put_pixels_tab[1][1] = ff_put_pixels8_x2_8_mmi;
+        c->put_pixels_tab[1][2] = ff_put_pixels8_y2_8_mmi;
+        c->put_pixels_tab[1][3] = ff_put_pixels8_xy2_8_mmi;
+
+        c->put_pixels_tab[2][0] = ff_put_pixels4_8_mmi;
+        c->put_pixels_tab[2][1] = ff_put_pixels4_x2_8_mmi;
+        c->put_pixels_tab[2][2] = ff_put_pixels4_y2_8_mmi;
+        c->put_pixels_tab[2][3] = ff_put_pixels4_xy2_8_mmi;
+
+        c->put_no_rnd_pixels_tab[0][0] = ff_put_pixels16_8_mmi;
+        c->put_no_rnd_pixels_tab[0][1] = ff_put_no_rnd_pixels16_x2_8_mmi;
+        c->put_no_rnd_pixels_tab[0][2] = ff_put_no_rnd_pixels16_y2_8_mmi;
+        c->put_no_rnd_pixels_tab[0][3] = ff_put_no_rnd_pixels16_xy2_8_mmi;
+
+        c->put_no_rnd_pixels_tab[1][0] = ff_put_pixels8_8_mmi;
+        c->put_no_rnd_pixels_tab[1][1] = ff_put_no_rnd_pixels8_x2_8_mmi;
+        c->put_no_rnd_pixels_tab[1][2] = ff_put_no_rnd_pixels8_y2_8_mmi;
+        c->put_no_rnd_pixels_tab[1][3] = ff_put_no_rnd_pixels8_xy2_8_mmi;
+
+        c->avg_pixels_tab[0][0] = ff_avg_pixels16_8_mmi;
+        c->avg_pixels_tab[0][1] = ff_avg_pixels16_x2_8_mmi;
+        c->avg_pixels_tab[0][2] = ff_avg_pixels16_y2_8_mmi;
+        c->avg_pixels_tab[0][3] = ff_avg_pixels16_xy2_8_mmi;
+
+        c->avg_pixels_tab[1][0] = ff_avg_pixels8_8_mmi;
+        c->avg_pixels_tab[1][1] = ff_avg_pixels8_x2_8_mmi;
+        c->avg_pixels_tab[1][2] = ff_avg_pixels8_y2_8_mmi;
+        c->avg_pixels_tab[1][3] = ff_avg_pixels8_xy2_8_mmi;
+
+        c->avg_pixels_tab[2][0] = ff_avg_pixels4_8_mmi;
+        c->avg_pixels_tab[2][1] = ff_avg_pixels4_x2_8_mmi;
+        c->avg_pixels_tab[2][2] = ff_avg_pixels4_y2_8_mmi;
+        c->avg_pixels_tab[2][3] = ff_avg_pixels4_xy2_8_mmi;
+    }
+
+    if (have_msa(cpu_flags)) {
+        c->put_pixels_tab[0][0] = ff_put_pixels16_msa;
+        c->put_pixels_tab[0][1] = ff_put_pixels16_x2_msa;
+        c->put_pixels_tab[0][2] = ff_put_pixels16_y2_msa;
+        c->put_pixels_tab[0][3] = ff_put_pixels16_xy2_msa;
+
+        c->put_pixels_tab[1][0] = ff_put_pixels8_msa;
+        c->put_pixels_tab[1][1] = ff_put_pixels8_x2_msa;
+        c->put_pixels_tab[1][2] = ff_put_pixels8_y2_msa;
+        c->put_pixels_tab[1][3] = ff_put_pixels8_xy2_msa;
+
+        c->put_pixels_tab[2][1] = ff_put_pixels4_x2_msa;
+        c->put_pixels_tab[2][2] = ff_put_pixels4_y2_msa;
+        c->put_pixels_tab[2][3] = ff_put_pixels4_xy2_msa;
+
+        c->put_no_rnd_pixels_tab[0][0] = ff_put_pixels16_msa;
+        c->put_no_rnd_pixels_tab[0][1] = ff_put_no_rnd_pixels16_x2_msa;
+        c->put_no_rnd_pixels_tab[0][2] = ff_put_no_rnd_pixels16_y2_msa;
+        c->put_no_rnd_pixels_tab[0][3] = ff_put_no_rnd_pixels16_xy2_msa;
+
+        c->put_no_rnd_pixels_tab[1][0] = ff_put_pixels8_msa;
+        c->put_no_rnd_pixels_tab[1][1] = ff_put_no_rnd_pixels8_x2_msa;
+        c->put_no_rnd_pixels_tab[1][2] = ff_put_no_rnd_pixels8_y2_msa;
+        c->put_no_rnd_pixels_tab[1][3] = ff_put_no_rnd_pixels8_xy2_msa;
+
+        c->avg_pixels_tab[0][0] = ff_avg_pixels16_msa;
+        c->avg_pixels_tab[0][1] = ff_avg_pixels16_x2_msa;
+        c->avg_pixels_tab[0][2] = ff_avg_pixels16_y2_msa;
+        c->avg_pixels_tab[0][3] = ff_avg_pixels16_xy2_msa;
+
+        c->avg_pixels_tab[1][0] = ff_avg_pixels8_msa;
+        c->avg_pixels_tab[1][1] = ff_avg_pixels8_x2_msa;
+        c->avg_pixels_tab[1][2] = ff_avg_pixels8_y2_msa;
+        c->avg_pixels_tab[1][3] = ff_avg_pixels8_xy2_msa;
+
+        c->avg_pixels_tab[2][0] = ff_avg_pixels4_msa;
+        c->avg_pixels_tab[2][1] = ff_avg_pixels4_x2_msa;
+        c->avg_pixels_tab[2][2] = ff_avg_pixels4_y2_msa;
+        c->avg_pixels_tab[2][3] = ff_avg_pixels4_xy2_msa;
+    }
 }
diff --git a/libavcodec/mips/hpeldsp_mmi.c b/libavcodec/mips/hpeldsp_mmi.c
index e69b2bd980..8e9c0fa821 100644
--- a/libavcodec/mips/hpeldsp_mmi.c
+++ b/libavcodec/mips/hpeldsp_mmi.c
@@ -307,6 +307,7 @@ inline void ff_put_pixels4_l2_8_mmi(uint8_t *dst, const uint8_t *src1,
     double ftmp[4];
     mips_reg addr[5];
     DECLARE_VAR_LOW32;
+    DECLARE_VAR_ADDRT;
 
     __asm__ volatile (
         "1:                                                             \n\t"
@@ -676,14 +677,14 @@ inline void ff_put_no_rnd_pixels8_l2_8_mmi(uint8_t *dst, const uint8_t *src1,
         PTR_ADDU   "%[addr1],   %[src2],        %[src_stride2]          \n\t"
         MMI_ULDC1(%[ftmp3], %[addr1], 0x00)
         PTR_ADDU   "%[src1],    %[src1],        %[addr2]                \n\t"
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp4]                \n\t"
-        "xor        %[ftmp1],   %[ftmp1],       %[ftmp4]                \n\t"
-        "xor        %[ftmp2],   %[ftmp2],       %[ftmp4]                \n\t"
-        "xor        %[ftmp3],   %[ftmp3],       %[ftmp4]                \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp4]                \n\t"
+        "pxor       %[ftmp1],   %[ftmp1],       %[ftmp4]                \n\t"
+        "pxor       %[ftmp2],   %[ftmp2],       %[ftmp4]                \n\t"
+        "pxor       %[ftmp3],   %[ftmp3],       %[ftmp4]                \n\t"
         "pavgb      %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
         "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp4]                \n\t"
-        "xor        %[ftmp1],   %[ftmp1],       %[ftmp4]                \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp4]                \n\t"
+        "pxor       %[ftmp1],   %[ftmp1],       %[ftmp4]                \n\t"
         MMI_SDC1(%[ftmp0], %[dst], 0x00)
         MMI_SDXC1(%[ftmp1], %[dst], %[dst_stride], 0x00)
         PTR_ADDU   "%[src2],    %[src2],        %[addr3]                \n\t"
@@ -696,14 +697,14 @@ inline void ff_put_no_rnd_pixels8_l2_8_mmi(uint8_t *dst, const uint8_t *src1,
         PTR_ADDU   "%[addr1],   %[src2],        %[src_stride2]          \n\t"
         MMI_ULDC1(%[ftmp3], %[addr1], 0x00)
         PTR_ADDU   "%[src1],    %[src1],        %[addr2]                \n\t"
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp4]                \n\t"
-        "xor        %[ftmp1],   %[ftmp1],       %[ftmp4]                \n\t"
-        "xor        %[ftmp2],   %[ftmp2],       %[ftmp4]                \n\t"
-        "xor        %[ftmp3],   %[ftmp3],       %[ftmp4]                \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp4]                \n\t"
+        "pxor       %[ftmp1],   %[ftmp1],       %[ftmp4]                \n\t"
+        "pxor       %[ftmp2],   %[ftmp2],       %[ftmp4]                \n\t"
+        "pxor       %[ftmp3],   %[ftmp3],       %[ftmp4]                \n\t"
         "pavgb      %[ftmp0],   %[ftmp0],       %[ftmp2]                \n\t"
         "pavgb      %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp4]                \n\t"
-        "xor        %[ftmp1],   %[ftmp1],       %[ftmp4]                \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp4]                \n\t"
+        "pxor       %[ftmp1],   %[ftmp1],       %[ftmp4]                \n\t"
         MMI_SDC1(%[ftmp0], %[dst], 0x00)
         MMI_SDXC1(%[ftmp1], %[dst], %[dst_stride], 0x00)
         PTR_ADDU   "%[src2],    %[src2],        %[addr3]                \n\t"
@@ -846,7 +847,7 @@ void ff_put_pixels8_xy2_8_mmi(uint8_t *block, const uint8_t *pixels,
     DECLARE_VAR_ADDRT;
 
     __asm__ volatile (
-        "xor        %[ftmp7],   %[ftmp7],       %[ftmp7]                \n\t"
+        "pxor       %[ftmp7],   %[ftmp7],       %[ftmp7]                \n\t"
         "dli        %[addr0],   0x0f                                    \n\t"
         "pcmpeqw    %[ftmp6],   %[ftmp6],       %[ftmp6]                \n\t"
         "dmtc1      %[addr0],   %[ftmp8]                                \n\t"
diff --git a/libavcodec/mips/hpeldsp_msa.c b/libavcodec/mips/hpeldsp_msa.c
index ad92f8f115..2bbe4771d4 100644
--- a/libavcodec/mips/hpeldsp_msa.c
+++ b/libavcodec/mips/hpeldsp_msa.c
@@ -59,12 +59,13 @@ static void common_hz_bil_4w_msa(const uint8_t *src, int32_t src_stride,
     uint8_t loop_cnt;
     uint32_t out0, out1;
     v16u8 src0, src1, src0_sld1, src1_sld1, res0, res1;
+    v16i8 zeros = { 0 };
 
     for (loop_cnt = (height >> 1); loop_cnt--;) {
         LD_UB2(src, src_stride, src0, src1);
         src += (2 * src_stride);
 
-        SLDI_B2_0_UB(src0, src1, src0_sld1, src1_sld1, 1);
+        SLDI_B2_UB(zeros, src0, zeros, src1, 1, src0_sld1, src1_sld1);
         AVER_UB2_UB(src0_sld1, src0, src1_sld1, src1, res0, res1);
 
         out0 = __msa_copy_u_w((v4i32) res0, 0);
@@ -82,13 +83,14 @@ static void common_hz_bil_8w_msa(const uint8_t *src, int32_t src_stride,
 {
     uint8_t loop_cnt;
     v16i8 src0, src1, src2, src3, src0_sld1, src1_sld1, src2_sld1, src3_sld1;
+    v16i8 zeros = { 0 };
 
     for (loop_cnt = (height >> 2); loop_cnt--;) {
         LD_SB4(src, src_stride, src0, src1, src2, src3);
         src += (4 * src_stride);
 
-        SLDI_B4_0_SB(src0, src1, src2, src3,
-                     src0_sld1, src1_sld1, src2_sld1, src3_sld1, 1);
+        SLDI_B4_SB(zeros, src0, zeros, src1, zeros, src2, zeros, src3, 1,
+                   src0_sld1, src1_sld1, src2_sld1, src3_sld1);
         AVER_ST8x4_UB(src0, src0_sld1, src1, src1_sld1,
                       src2, src2_sld1, src3, src3_sld1, dst, dst_stride);
         dst += (4 * dst_stride);
@@ -125,14 +127,15 @@ static void common_hz_bil_no_rnd_8x8_msa(const uint8_t *src, int32_t src_stride,
     v16i8 src0, src1, src2, src3, src4, src5, src6, src7;
     v16i8 src0_sld1, src1_sld1, src2_sld1, src3_sld1;
     v16i8 src4_sld1, src5_sld1, src6_sld1, src7_sld1;
+    v16i8 zeros = { 0 };
 
     LD_SB8(src, src_stride, src0, src1, src2, src3, src4, src5, src6, src7);
     src += (8 * src_stride);
 
-    SLDI_B4_0_SB(src0, src1, src2, src3,
-                 src0_sld1, src1_sld1, src2_sld1, src3_sld1, 1);
-    SLDI_B4_0_SB(src4, src5, src6, src7,
-                 src4_sld1, src5_sld1, src6_sld1, src7_sld1, 1);
+    SLDI_B4_SB(zeros, src0, zeros, src1, zeros, src2, zeros, src3, 1,
+               src0_sld1, src1_sld1, src2_sld1, src3_sld1);
+    SLDI_B4_SB(zeros, src4, zeros, src5, zeros, src6, zeros, src7, 1,
+               src4_sld1, src5_sld1, src6_sld1, src7_sld1);
 
     AVE_ST8x4_UB(src0, src0_sld1, src1, src1_sld1,
                  src2, src2_sld1, src3, src3_sld1, dst, dst_stride);
@@ -145,10 +148,11 @@ static void common_hz_bil_no_rnd_4x8_msa(const uint8_t *src, int32_t src_stride,
                                          uint8_t *dst, int32_t dst_stride)
 {
     v16i8 src0, src1, src2, src3, src0_sld1, src1_sld1, src2_sld1, src3_sld1;
+    v16i8 zeros = { 0 };
 
     LD_SB4(src, src_stride, src0, src1, src2, src3);
-    SLDI_B4_0_SB(src0, src1, src2, src3,
-                 src0_sld1, src1_sld1, src2_sld1, src3_sld1, 1);
+    SLDI_B4_SB(zeros, src0, zeros, src1, zeros, src2, zeros, src3, 1,
+               src0_sld1, src1_sld1, src2_sld1, src3_sld1);
     AVE_ST8x4_UB(src0, src0_sld1, src1, src1_sld1,
                  src2, src2_sld1, src3, src3_sld1, dst, dst_stride);
 }
@@ -216,12 +220,13 @@ static void common_hz_bil_and_aver_dst_4w_msa(const uint8_t *src,
     v16u8 src0, src1, src0_sld1, src1_sld1, res0, res1;
     v16u8 tmp0 = { 0 };
     v16u8 tmp1 = { 0 };
+    v16i8 zeros = { 0 };
 
     for (loop_cnt = (height >> 1); loop_cnt--;) {
         LD_UB2(src, src_stride, src0, src1);
         src += (2 * src_stride);
 
-        SLDI_B2_0_UB(src0, src1, src0_sld1, src1_sld1, 1);
+        SLDI_B2_UB(zeros, src0, zeros, src1, 1, src0_sld1, src1_sld1);
 
         dst0 = LW(dst);
         dst1 = LW(dst + dst_stride);
@@ -247,13 +252,14 @@ static void common_hz_bil_and_aver_dst_8w_msa(const uint8_t *src,
 {
     uint8_t loop_cnt;
     v16i8 src0, src1, src2, src3, src0_sld1, src1_sld1, src2_sld1, src3_sld1;
+    v16i8 zeros = { 0 };
 
     for (loop_cnt = (height >> 2); loop_cnt--;) {
         LD_SB4(src, src_stride, src0, src1, src2, src3);
         src += (4 * src_stride);
 
-        SLDI_B4_0_SB(src0, src1, src2, src3,
-                     src0_sld1, src1_sld1, src2_sld1, src3_sld1, 1);
+        SLDI_B4_SB(zeros, src0, zeros, src1, zeros, src2, zeros, src3, 1,
+                   src0_sld1, src1_sld1, src2_sld1, src3_sld1);
 
         AVER_DST_ST8x4_UB(src0, src0_sld1, src1, src1_sld1, src2, src2_sld1,
                           src3, src3_sld1, dst, dst_stride);
@@ -529,6 +535,7 @@ static void common_hv_bil_4w_msa(const uint8_t *src, int32_t src_stride,
     v16i8 src0, src1, src2, src0_sld1, src1_sld1, src2_sld1;
     v16u8 src0_r, src1_r, src2_r, res;
     v8u16 add0, add1, add2, sum0, sum1;
+    v16i8 zeros = { 0 };
 
     src0 = LD_SB(src);
     src += src_stride;
@@ -537,7 +544,8 @@ static void common_hv_bil_4w_msa(const uint8_t *src, int32_t src_stride,
         LD_SB2(src, src_stride, src1, src2);
         src += (2 * src_stride);
 
-        SLDI_B3_0_SB(src0, src1, src2, src0_sld1, src1_sld1, src2_sld1, 1);
+        SLDI_B3_SB(zeros, src0, zeros, src1, zeros, src2, 1, src0_sld1,
+                   src1_sld1, src2_sld1);
         ILVR_B3_UB(src0_sld1, src0, src1_sld1, src1, src2_sld1, src2,
                    src0_r, src1_r, src2_r);
         HADD_UB3_UH(src0_r, src1_r, src2_r, add0, add1, add2);
@@ -565,6 +573,7 @@ static void common_hv_bil_8w_msa(const uint8_t *src, int32_t src_stride,
     v16u8 src0_r, src1_r, src2_r, src3_r, src4_r;
     v8u16 add0, add1, add2, add3, add4;
     v8u16 sum0, sum1, sum2, sum3;
+    v16i8 zeros = { 0 };
 
     src0 = LD_SB(src);
     src += src_stride;
@@ -573,8 +582,9 @@ static void common_hv_bil_8w_msa(const uint8_t *src, int32_t src_stride,
         LD_SB4(src, src_stride, src1, src2, src3, src4);
         src += (4 * src_stride);
 
-        SLDI_B3_0_SB(src0, src1, src2, src0_sld1, src1_sld1, src2_sld1, 1);
-        SLDI_B2_0_SB(src3, src4, src3_sld1, src4_sld1, 1);
+        SLDI_B3_SB(zeros, src0, zeros, src1, zeros, src2, 1, src0_sld1,
+                   src1_sld1, src2_sld1);
+        SLDI_B2_SB(zeros, src3, zeros, src4, 1, src3_sld1, src4_sld1);
         ILVR_B3_UB(src0_sld1, src0, src1_sld1, src1, src2_sld1, src2, src0_r,
                    src1_r, src2_r);
         ILVR_B2_UB(src3_sld1, src3, src4_sld1, src4, src3_r, src4_r);
@@ -659,15 +669,17 @@ static void common_hv_bil_no_rnd_8x8_msa(const uint8_t *src, int32_t src_stride,
     v8u16 add0, add1, add2, add3, add4, add5, add6, add7, add8;
     v8u16 sum0, sum1, sum2, sum3, sum4, sum5, sum6, sum7;
     v16i8 out0, out1;
+    v16i8 zeros = { 0 };
 
     LD_UB8(src, src_stride, src0, src1, src2, src3, src4, src5, src6, src7);
     src += (8 * src_stride);
     src8 = LD_UB(src);
 
-    SLDI_B4_0_UB(src0, src1, src2, src3, src0_sld1, src1_sld1, src2_sld1,
-                 src3_sld1, 1);
-    SLDI_B3_0_UB(src4, src5, src6, src4_sld1, src5_sld1, src6_sld1, 1);
-    SLDI_B2_0_UB(src7, src8, src7_sld1, src8_sld1, 1);
+    SLDI_B4_UB(zeros, src0, zeros, src1, zeros, src2, zeros, src3, 1,
+               src0_sld1, src1_sld1, src2_sld1, src3_sld1);
+    SLDI_B3_UB(zeros, src4, zeros, src5, zeros, src6, 1, src4_sld1,
+               src5_sld1, src6_sld1);
+    SLDI_B2_UB(zeros, src7, zeros, src8, 1, src7_sld1, src8_sld1);
     ILVR_B4_UH(src0_sld1, src0, src1_sld1, src1, src2_sld1, src2, src3_sld1,
                src3, src0_r, src1_r, src2_r, src3_r);
     ILVR_B3_UH(src4_sld1, src4, src5_sld1, src5, src6_sld1, src6, src4_r,
@@ -703,13 +715,15 @@ static void common_hv_bil_no_rnd_4x8_msa(const uint8_t *src, int32_t src_stride,
     v8u16 add0, add1, add2, add3, add4;
     v8u16 sum0, sum1, sum2, sum3;
     v16i8 out0, out1;
+    v16i8 zeros = { 0 };
 
     LD_SB4(src, src_stride, src0, src1, src2, src3);
     src += (4 * src_stride);
     src4 = LD_SB(src);
 
-    SLDI_B3_0_SB(src0, src1, src2, src0_sld1, src1_sld1, src2_sld1, 1);
-    SLDI_B2_0_SB(src3, src4, src3_sld1, src4_sld1, 1);
+    SLDI_B3_SB(zeros, src0, zeros, src1, zeros, src2, 1, src0_sld1,
+               src1_sld1, src2_sld1);
+    SLDI_B2_SB(zeros, src3, zeros, src4, 1, src3_sld1, src4_sld1);
     ILVR_B3_UH(src0_sld1, src0, src1_sld1, src1, src2_sld1, src2, src0_r,
                src1_r, src2_r);
     ILVR_B2_UH(src3_sld1, src3, src4_sld1, src4, src3_r, src4_r);
@@ -918,6 +932,7 @@ static void common_hv_bil_and_aver_dst_4w_msa(const uint8_t *src,
     v16u8 src0_r, src1_r, src2_r;
     v8u16 add0, add1, add2, sum0, sum1;
     v16u8 dst0, dst1, res0, res1;
+    v16i8 zeros = { 0 };
 
     src0 = LD_SB(src);
     src += src_stride;
@@ -927,7 +942,8 @@ static void common_hv_bil_and_aver_dst_4w_msa(const uint8_t *src,
         src += (2 * src_stride);
 
         LD_UB2(dst, dst_stride, dst0, dst1);
-        SLDI_B3_0_SB(src0, src1, src2, src0_sld1, src1_sld1, src2_sld1, 1);
+        SLDI_B3_SB(zeros, src0, zeros, src1, zeros, src2, 1, src0_sld1,
+                   src1_sld1, src2_sld1);
         ILVR_B3_UB(src0_sld1, src0, src1_sld1, src1, src2_sld1, src2, src0_r,
                    src1_r, src2_r);
         HADD_UB3_UH(src0_r, src1_r, src2_r, add0, add1, add2);
@@ -959,6 +975,7 @@ static void common_hv_bil_and_aver_dst_8w_msa(const uint8_t *src,
     v16u8 src0_r, src1_r, src2_r, src3_r, src4_r;
     v8u16 add0, add1, add2, add3, add4;
     v8u16 sum0, sum1, sum2, sum3;
+    v16i8 zeros = { 0 };
 
     src0 = LD_SB(src);
     src += src_stride;
@@ -968,8 +985,9 @@ static void common_hv_bil_and_aver_dst_8w_msa(const uint8_t *src,
         src += (4 * src_stride);
 
         LD_UB4(dst, dst_stride, dst0, dst1, dst2, dst3);
-        SLDI_B3_0_SB(src0, src1, src2, src0_sld1, src1_sld1, src2_sld1, 1);
-        SLDI_B2_0_SB(src3, src4, src3_sld1, src4_sld1, 1);
+        SLDI_B3_SB(zeros, src0, zeros, src1, zeros, src2, 1, src0_sld1,
+                   src1_sld1, src2_sld1);
+        SLDI_B2_SB(zeros, src3, zeros, src4, 1, src3_sld1, src4_sld1);
         ILVR_B3_UB(src0_sld1, src0, src1_sld1, src1, src2_sld1, src2, src0_r,
                    src1_r, src2_r);
         ILVR_B2_UB(src3_sld1, src3, src4_sld1, src4, src3_r, src4_r);
diff --git a/libavcodec/mips/idctdsp_init_mips.c b/libavcodec/mips/idctdsp_init_mips.c
index 85b76ca478..23efd9ed58 100644
--- a/libavcodec/mips/idctdsp_init_mips.c
+++ b/libavcodec/mips/idctdsp_init_mips.c
@@ -19,56 +19,44 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
+#include "libavutil/mips/cpu.h"
 #include "idctdsp_mips.h"
 #include "xvididct_mips.h"
 
-#if HAVE_MSA
-static av_cold void idctdsp_init_msa(IDCTDSPContext *c, AVCodecContext *avctx,
-                                     unsigned high_bit_depth)
+av_cold void ff_idctdsp_init_mips(IDCTDSPContext *c, AVCodecContext *avctx,
+                          unsigned high_bit_depth)
 {
-    if ((avctx->lowres != 1) && (avctx->lowres != 2) && (avctx->lowres != 3) &&
-        (avctx->bits_per_raw_sample != 10) &&
-        (avctx->bits_per_raw_sample != 12) &&
-        (avctx->idct_algo == FF_IDCT_AUTO)) {
-                c->idct_put = ff_simple_idct_put_msa;
-                c->idct_add = ff_simple_idct_add_msa;
-                c->idct = ff_simple_idct_msa;
-                c->perm_type = FF_IDCT_PERM_NONE;
-    }
+    int cpu_flags = av_get_cpu_flags();
 
-    c->put_pixels_clamped = ff_put_pixels_clamped_msa;
-    c->put_signed_pixels_clamped = ff_put_signed_pixels_clamped_msa;
-    c->add_pixels_clamped = ff_add_pixels_clamped_msa;
-}
-#endif  // #if HAVE_MSA
+    if (have_mmi(cpu_flags)) {
+        if ((avctx->lowres != 1) && (avctx->lowres != 2) && (avctx->lowres != 3) &&
+            (avctx->bits_per_raw_sample != 10) &&
+            (avctx->bits_per_raw_sample != 12) &&
+            ((avctx->idct_algo == FF_IDCT_AUTO) || (avctx->idct_algo == FF_IDCT_SIMPLE))) {
+                    c->idct_put = ff_simple_idct_put_8_mmi;
+                    c->idct_add = ff_simple_idct_add_8_mmi;
+                    c->idct = ff_simple_idct_8_mmi;
+                    c->perm_type = FF_IDCT_PERM_NONE;
+        }
 
-#if HAVE_MMI
-static av_cold void idctdsp_init_mmi(IDCTDSPContext *c, AVCodecContext *avctx,
-        unsigned high_bit_depth)
-{
-    if ((avctx->lowres != 1) && (avctx->lowres != 2) && (avctx->lowres != 3) &&
-        (avctx->bits_per_raw_sample != 10) &&
-        (avctx->bits_per_raw_sample != 12) &&
-        ((avctx->idct_algo == FF_IDCT_AUTO) || (avctx->idct_algo == FF_IDCT_SIMPLE))) {
-                c->idct_put = ff_simple_idct_put_8_mmi;
-                c->idct_add = ff_simple_idct_add_8_mmi;
-                c->idct = ff_simple_idct_8_mmi;
-                c->perm_type = FF_IDCT_PERM_NONE;
+        c->put_pixels_clamped = ff_put_pixels_clamped_mmi;
+        c->add_pixels_clamped = ff_add_pixels_clamped_mmi;
+        c->put_signed_pixels_clamped = ff_put_signed_pixels_clamped_mmi;
     }
 
-    c->put_pixels_clamped = ff_put_pixels_clamped_mmi;
-    c->add_pixels_clamped = ff_add_pixels_clamped_mmi;
-    c->put_signed_pixels_clamped = ff_put_signed_pixels_clamped_mmi;
-}
-#endif /* HAVE_MMI */
+    if (have_msa(cpu_flags)) {
+        if ((avctx->lowres != 1) && (avctx->lowres != 2) && (avctx->lowres != 3) &&
+            (avctx->bits_per_raw_sample != 10) &&
+            (avctx->bits_per_raw_sample != 12) &&
+            (avctx->idct_algo == FF_IDCT_AUTO)) {
+                    c->idct_put = ff_simple_idct_put_msa;
+                    c->idct_add = ff_simple_idct_add_msa;
+                    c->idct = ff_simple_idct_msa;
+                    c->perm_type = FF_IDCT_PERM_NONE;
+        }
 
-av_cold void ff_idctdsp_init_mips(IDCTDSPContext *c, AVCodecContext *avctx,
-                          unsigned high_bit_depth)
-{
-#if HAVE_MMI
-    idctdsp_init_mmi(c, avctx, high_bit_depth);
-#endif /* HAVE_MMI */
-#if HAVE_MSA
-    idctdsp_init_msa(c, avctx, high_bit_depth);
-#endif  // #if HAVE_MSA
+        c->put_pixels_clamped = ff_put_pixels_clamped_msa;
+        c->put_signed_pixels_clamped = ff_put_signed_pixels_clamped_msa;
+        c->add_pixels_clamped = ff_add_pixels_clamped_msa;
+    }
 }
diff --git a/libavcodec/mips/idctdsp_mmi.c b/libavcodec/mips/idctdsp_mmi.c
index a96dac4704..d22e5eedd7 100644
--- a/libavcodec/mips/idctdsp_mmi.c
+++ b/libavcodec/mips/idctdsp_mmi.c
@@ -142,7 +142,7 @@ void ff_put_signed_pixels_clamped_mmi(const int16_t *block,
           [pixels]"+&r"(pixels)
         : [block]"r"(block),
           [line_size]"r"((mips_reg)line_size),
-          [ff_pb_80]"f"(ff_pb_80)
+          [ff_pb_80]"f"(ff_pb_80.f)
         : "memory"
     );
 }
@@ -154,7 +154,7 @@ void ff_add_pixels_clamped_mmi(const int16_t *block,
     uint64_t tmp[1];
     __asm__ volatile (
         "li         %[tmp0],    0x04                           \n\t"
-        "xor        %[ftmp0],   %[ftmp0],   %[ftmp0]           \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],   %[ftmp0]           \n\t"
         "1:                                                    \n\t"
         MMI_LDC1(%[ftmp5], %[pixels], 0x00)
         PTR_ADDU   "%[pixels],  %[pixels],  %[line_size]       \n\t"
diff --git a/libavcodec/mips/idctdsp_msa.c b/libavcodec/mips/idctdsp_msa.c
index b29e420556..b6b98dc7fc 100644
--- a/libavcodec/mips/idctdsp_msa.c
+++ b/libavcodec/mips/idctdsp_msa.c
@@ -28,8 +28,7 @@ static void put_pixels_clamped_msa(const int16_t *block, uint8_t *pixels,
     v8i16 in0, in1, in2, in3, in4, in5, in6, in7;
 
     LD_SH8(block, 8, in0, in1, in2, in3, in4, in5, in6, in7);
-    CLIP_SH4_0_255(in0, in1, in2, in3);
-    CLIP_SH4_0_255(in4, in5, in6, in7);
+    CLIP_SH8_0_255(in0, in1, in2, in3, in4, in5, in6, in7);
     PCKEV_B4_SH(in0, in0, in1, in1, in2, in2, in3, in3, in0, in1, in2, in3);
     PCKEV_B4_SH(in4, in4, in5, in5, in6, in6, in7, in7, in4, in5, in6, in7);
 
@@ -63,8 +62,7 @@ static void put_signed_pixels_clamped_msa(const int16_t *block, uint8_t *pixels,
     in6 += 128;
     in7 += 128;
 
-    CLIP_SH4_0_255(in0, in1, in2, in3);
-    CLIP_SH4_0_255(in4, in5, in6, in7);
+    CLIP_SH8_0_255(in0, in1, in2, in3, in4, in5, in6, in7);
     PCKEV_B4_SH(in0, in0, in1, in1, in2, in2, in3, in3, in0, in1, in2, in3);
     PCKEV_B4_SH(in4, in4, in5, in5, in6, in6, in7, in7, in4, in5, in6, in7);
 
@@ -109,8 +107,7 @@ static void add_pixels_clamped_msa(const int16_t *block, uint8_t *pixels,
     in6 += (v8i16) pix6;
     in7 += (v8i16) pix7;
 
-    CLIP_SH4_0_255(in0, in1, in2, in3);
-    CLIP_SH4_0_255(in4, in5, in6, in7);
+    CLIP_SH8_0_255(in0, in1, in2, in3, in4, in5, in6, in7);
     PCKEV_B4_SH(in0, in0, in1, in1, in2, in2, in3, in3, in0, in1, in2, in3);
     PCKEV_B4_SH(in4, in4, in5, in5, in6, in6, in7, in7, in4, in5, in6, in7);
 
diff --git a/libavcodec/mips/me_cmp_init_mips.c b/libavcodec/mips/me_cmp_init_mips.c
index 219a0dc00c..e3e33b8e5e 100644
--- a/libavcodec/mips/me_cmp_init_mips.c
+++ b/libavcodec/mips/me_cmp_init_mips.c
@@ -18,39 +18,35 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
+#include "libavutil/mips/cpu.h"
 #include "me_cmp_mips.h"
 
-#if HAVE_MSA
-static av_cold void me_cmp_msa(MECmpContext *c, AVCodecContext *avctx)
+av_cold void ff_me_cmp_init_mips(MECmpContext *c, AVCodecContext *avctx)
 {
+    int cpu_flags = av_get_cpu_flags();
+
+    if (have_msa(cpu_flags)) {
 #if BIT_DEPTH == 8
-    c->pix_abs[0][0] = ff_pix_abs16_msa;
-    c->pix_abs[0][1] = ff_pix_abs16_x2_msa;
-    c->pix_abs[0][2] = ff_pix_abs16_y2_msa;
-    c->pix_abs[0][3] = ff_pix_abs16_xy2_msa;
-    c->pix_abs[1][0] = ff_pix_abs8_msa;
-    c->pix_abs[1][1] = ff_pix_abs8_x2_msa;
-    c->pix_abs[1][2] = ff_pix_abs8_y2_msa;
-    c->pix_abs[1][3] = ff_pix_abs8_xy2_msa;
+        c->pix_abs[0][0] = ff_pix_abs16_msa;
+        c->pix_abs[0][1] = ff_pix_abs16_x2_msa;
+        c->pix_abs[0][2] = ff_pix_abs16_y2_msa;
+        c->pix_abs[0][3] = ff_pix_abs16_xy2_msa;
+        c->pix_abs[1][0] = ff_pix_abs8_msa;
+        c->pix_abs[1][1] = ff_pix_abs8_x2_msa;
+        c->pix_abs[1][2] = ff_pix_abs8_y2_msa;
+        c->pix_abs[1][3] = ff_pix_abs8_xy2_msa;
 
-    c->hadamard8_diff[0] = ff_hadamard8_diff16_msa;
-    c->hadamard8_diff[1] = ff_hadamard8_diff8x8_msa;
+        c->hadamard8_diff[0] = ff_hadamard8_diff16_msa;
+        c->hadamard8_diff[1] = ff_hadamard8_diff8x8_msa;
 
-    c->hadamard8_diff[4] = ff_hadamard8_intra16_msa;
-    c->hadamard8_diff[5] = ff_hadamard8_intra8x8_msa;
+        c->hadamard8_diff[4] = ff_hadamard8_intra16_msa;
+        c->hadamard8_diff[5] = ff_hadamard8_intra8x8_msa;
 
-    c->sad[0] = ff_pix_abs16_msa;
-    c->sad[1] = ff_pix_abs8_msa;
-    c->sse[0] = ff_sse16_msa;
-    c->sse[1] = ff_sse8_msa;
-    c->sse[2] = ff_sse4_msa;
+        c->sad[0] = ff_pix_abs16_msa;
+        c->sad[1] = ff_pix_abs8_msa;
+        c->sse[0] = ff_sse16_msa;
+        c->sse[1] = ff_sse8_msa;
+        c->sse[2] = ff_sse4_msa;
 #endif
-}
-#endif  // #if HAVE_MSA
-
-av_cold void ff_me_cmp_init_mips(MECmpContext *c, AVCodecContext *avctx)
-{
-#if HAVE_MSA
-    me_cmp_msa(c, avctx);
-#endif  // #if HAVE_MSA
+    }
 }
diff --git a/libavcodec/mips/me_cmp_msa.c b/libavcodec/mips/me_cmp_msa.c
index 0e3165cd8f..e9b0164b4d 100644
--- a/libavcodec/mips/me_cmp_msa.c
+++ b/libavcodec/mips/me_cmp_msa.c
@@ -25,11 +25,13 @@ static uint32_t sad_8width_msa(uint8_t *src, int32_t src_stride,
                                uint8_t *ref, int32_t ref_stride,
                                int32_t height)
 {
-    int32_t ht_cnt;
+    int32_t ht_cnt = height >> 2;
+    int res = (height & 0x03);
     v16u8 src0, src1, src2, src3, ref0, ref1, ref2, ref3;
+    v8u16 zero = { 0 };
     v8u16 sad = { 0 };
 
-    for (ht_cnt = (height >> 2); ht_cnt--;) {
+    for (; ht_cnt--; ) {
         LD_UB4(src, src_stride, src0, src1, src2, src3);
         src += (4 * src_stride);
         LD_UB4(ref, ref_stride, ref0, ref1, ref2, ref3);
@@ -39,6 +41,16 @@ static uint32_t sad_8width_msa(uint8_t *src, int32_t src_stride,
                     src0, src1, ref0, ref1);
         sad += SAD_UB2_UH(src0, src1, ref0, ref1);
     }
+    for (; res--; ) {
+        v16u8 diff;
+        src0 = LD_UB(src);
+        ref0 = LD_UB(ref);
+        src += src_stride;
+        ref += ref_stride;
+        diff = __msa_asub_u_b((v16u8) src0, (v16u8) ref0);
+        diff = (v16u8)__msa_ilvr_d((v2i64)zero, (v2i64)diff);
+        sad += __msa_hadd_u_h((v16u8) diff, (v16u8) diff);
+    }
 
     return (HADD_UH_U32(sad));
 }
@@ -47,11 +59,12 @@ static uint32_t sad_16width_msa(uint8_t *src, int32_t src_stride,
                                 uint8_t *ref, int32_t ref_stride,
                                 int32_t height)
 {
-    int32_t ht_cnt;
+    int32_t ht_cnt = height >> 2;
+    int res = (height & 0x03);
     v16u8 src0, src1, ref0, ref1;
     v8u16 sad = { 0 };
 
-    for (ht_cnt = (height >> 2); ht_cnt--;) {
+    for (; ht_cnt--; ) {
         LD_UB2(src, src_stride, src0, src1);
         src += (2 * src_stride);
         LD_UB2(ref, ref_stride, ref0, ref1);
@@ -64,7 +77,15 @@ static uint32_t sad_16width_msa(uint8_t *src, int32_t src_stride,
         ref += (2 * ref_stride);
         sad += SAD_UB2_UH(src0, src1, ref0, ref1);
     }
-
+    for (; res > 0; res--) {
+        v16u8 diff;
+        src0 = LD_UB(src);
+        ref0 = LD_UB(ref);
+        src += src_stride;
+        ref += ref_stride;
+        diff = __msa_asub_u_b((v16u8) src0, (v16u8) ref0);
+        sad += __msa_hadd_u_h((v16u8) diff, (v16u8) diff);
+    }
     return (HADD_UH_U32(sad));
 }
 
@@ -74,12 +95,14 @@ static uint32_t sad_horiz_bilinear_filter_8width_msa(uint8_t *src,
                                                      int32_t ref_stride,
                                                      int32_t height)
 {
-    int32_t ht_cnt;
+    int32_t ht_cnt = height >> 3;
+    int32_t res = height & 0x07;
     v16u8 src0, src1, src2, src3, comp0, comp1;
     v16u8 ref0, ref1, ref2, ref3, ref4, ref5;
+    v8u16 zero = { 0 };
     v8u16 sad = { 0 };
 
-    for (ht_cnt = (height >> 3); ht_cnt--;) {
+    for (; ht_cnt--; ) {
         LD_UB4(src, src_stride, src0, src1, src2, src3);
         src += (4 * src_stride);
         LD_UB4(ref, ref_stride, ref0, ref1, ref2, ref3);
@@ -87,8 +110,8 @@ static uint32_t sad_horiz_bilinear_filter_8width_msa(uint8_t *src,
 
         PCKEV_D2_UB(src1, src0, src3, src2, src0, src1);
         PCKEV_D2_UB(ref1, ref0, ref3, ref2, ref4, ref5);
-        SLDI_B2_UB(ref0, ref1, ref0, ref1, ref0, ref1, 1);
-        SLDI_B2_UB(ref2, ref3, ref2, ref3, ref2, ref3, 1);
+        SLDI_B4_UB(ref0, ref0, ref1, ref1, ref2, ref2, ref3, ref3, 1,
+                   ref0, ref1, ref2, ref3);
         PCKEV_D2_UB(ref1, ref0, ref3, ref2, ref0, ref1);
         AVER_UB2_UB(ref4, ref0, ref5, ref1, comp0, comp1);
         sad += SAD_UB2_UH(src0, src1, comp0, comp1);
@@ -100,13 +123,25 @@ static uint32_t sad_horiz_bilinear_filter_8width_msa(uint8_t *src,
 
         PCKEV_D2_UB(src1, src0, src3, src2, src0, src1);
         PCKEV_D2_UB(ref1, ref0, ref3, ref2, ref4, ref5);
-        SLDI_B2_UB(ref0, ref1, ref0, ref1, ref0, ref1, 1);
-        SLDI_B2_UB(ref2, ref3, ref2, ref3, ref2, ref3, 1);
+        SLDI_B4_UB(ref0, ref0, ref1, ref1, ref2, ref2, ref3, ref3, 1,
+                   ref0, ref1, ref2, ref3);
         PCKEV_D2_UB(ref1, ref0, ref3, ref2, ref0, ref1);
         AVER_UB2_UB(ref4, ref0, ref5, ref1, comp0, comp1);
         sad += SAD_UB2_UH(src0, src1, comp0, comp1);
     }
 
+    for (; res--; ) {
+        v16u8 diff;
+        src0 = LD_UB(src);
+        ref0 = LD_UB(ref);
+        ref1 = LD_UB(ref + 1);
+        src += src_stride;
+        ref += ref_stride;
+        comp0 = (v16u8)__msa_aver_u_b((v16u8) ref0, (v16u8) ref1);
+        diff = __msa_asub_u_b((v16u8) src0, (v16u8) comp0);
+        diff = (v16u8)__msa_ilvr_d((v2i64) zero, (v2i64) diff);
+        sad += __msa_hadd_u_h((v16u8) diff, (v16u8) diff);
+    }
     return (HADD_UH_U32(sad));
 }
 
@@ -116,12 +151,13 @@ static uint32_t sad_horiz_bilinear_filter_16width_msa(uint8_t *src,
                                                       int32_t ref_stride,
                                                       int32_t height)
 {
-    int32_t ht_cnt;
+    int32_t ht_cnt = height >> 3;
+    int32_t res = height & 0x07;
     v16u8 src0, src1, src2, src3, comp0, comp1;
     v16u8 ref00, ref10, ref20, ref30, ref01, ref11, ref21, ref31;
     v8u16 sad = { 0 };
 
-    for (ht_cnt = (height >> 3); ht_cnt--;) {
+    for (; ht_cnt--; ) {
         LD_UB4(src, src_stride, src0, src1, src2, src3);
         src += (4 * src_stride);
         LD_UB4(ref, ref_stride, ref00, ref10, ref20, ref30);
@@ -145,6 +181,17 @@ static uint32_t sad_horiz_bilinear_filter_16width_msa(uint8_t *src,
         sad += SAD_UB2_UH(src2, src3, comp0, comp1);
     }
 
+    for (; res--; ) {
+        v16u8 diff;
+        src0  = LD_UB(src);
+        ref00 = LD_UB(ref);
+        ref01 = LD_UB(ref + 1);
+        src += src_stride;
+        ref += ref_stride;
+        comp0 = (v16u8)__msa_aver_u_b((v16u8) ref00, (v16u8) ref01);
+        diff = __msa_asub_u_b((v16u8) src0, (v16u8) comp0);
+        sad += __msa_hadd_u_h((v16u8) diff, (v16u8) diff);
+    }
     return (HADD_UH_U32(sad));
 }
 
@@ -154,12 +201,14 @@ static uint32_t sad_vert_bilinear_filter_8width_msa(uint8_t *src,
                                                     int32_t ref_stride,
                                                     int32_t height)
 {
-    int32_t ht_cnt;
+    int32_t ht_cnt = height >> 3;
+    int32_t res = height & 0x07;
     v16u8 src0, src1, src2, src3, comp0, comp1;
     v16u8 ref0, ref1, ref2, ref3, ref4;
+    v8u16 zero = { 0 };
     v8u16 sad = { 0 };
 
-    for (ht_cnt = (height >> 3); ht_cnt--;) {
+    for (; ht_cnt--; ) {
         LD_UB4(src, src_stride, src0, src1, src2, src3);
         src += (4 * src_stride);
         LD_UB5(ref, ref_stride, ref0, ref1, ref2, ref3, ref4);
@@ -183,6 +232,17 @@ static uint32_t sad_vert_bilinear_filter_8width_msa(uint8_t *src,
         sad += SAD_UB2_UH(src0, src1, comp0, comp1);
     }
 
+    for (; res--; ) {
+        v16u8 diff;
+        src0 = LD_UB(src);
+        LD_UB2(ref, ref_stride, ref0, ref1);
+        src += src_stride;
+        ref += ref_stride;
+        comp0 = (v16u8)__msa_aver_u_b((v16u8) ref0, (v16u8) ref1);
+        diff = __msa_asub_u_b((v16u8) src0, (v16u8) comp0);
+        diff = (v16u8)__msa_ilvr_d((v2i64) zero, (v2i64) diff);
+        sad += __msa_hadd_u_h((v16u8) diff, (v16u8) diff);
+    }
     return (HADD_UH_U32(sad));
 }
 
@@ -192,12 +252,13 @@ static uint32_t sad_vert_bilinear_filter_16width_msa(uint8_t *src,
                                                      int32_t ref_stride,
                                                      int32_t height)
 {
-    int32_t ht_cnt;
+    int32_t ht_cnt = height >> 3;
+    int32_t res = height & 0x07;
     v16u8 src0, src1, src2, src3, comp0, comp1;
     v16u8 ref0, ref1, ref2, ref3, ref4;
     v8u16 sad = { 0 };
 
-    for (ht_cnt = (height >> 3); ht_cnt--;) {
+    for (; ht_cnt--; ) {
         LD_UB5(ref, ref_stride, ref4, ref0, ref1, ref2, ref3);
         ref += (5 * ref_stride);
         LD_UB4(src, src_stride, src0, src1, src2, src3);
@@ -221,6 +282,16 @@ static uint32_t sad_vert_bilinear_filter_16width_msa(uint8_t *src,
         sad += SAD_UB2_UH(src2, src3, comp0, comp1);
     }
 
+    for (; res--; ) {
+        v16u8 diff;
+        src0 = LD_UB(src);
+        LD_UB2(ref, ref_stride, ref0, ref1);
+        src += src_stride;
+        ref += ref_stride;
+        comp0 = (v16u8)__msa_aver_u_b((v16u8) ref0, (v16u8) ref1);
+        diff = __msa_asub_u_b((v16u8) src0, (v16u8) comp0);
+        sad += __msa_hadd_u_h((v16u8) diff, (v16u8) diff);
+    }
     return (HADD_UH_U32(sad));
 }
 
@@ -230,11 +301,13 @@ static uint32_t sad_hv_bilinear_filter_8width_msa(uint8_t *src,
                                                   int32_t ref_stride,
                                                   int32_t height)
 {
-    int32_t ht_cnt;
+    int32_t ht_cnt = height >> 2;
+    int32_t res = height & 0x03;
     v16u8 src0, src1, src2, src3, temp0, temp1, diff;
     v16u8 ref0, ref1, ref2, ref3, ref4;
     v16i8 mask = { 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8 };
     v8u16 comp0, comp1, comp2, comp3;
+    v8u16 zero = { 0 };
     v8u16 sad = { 0 };
 
     for (ht_cnt = (height >> 2); ht_cnt--;) {
@@ -277,6 +350,22 @@ static uint32_t sad_hv_bilinear_filter_8width_msa(uint8_t *src,
         sad += __msa_hadd_u_h(diff, diff);
     }
 
+    for (; res--; ) {
+        src0 = LD_UB(src);
+        LD_UB2(ref, ref_stride, ref0, ref1);
+        temp0 = (v16u8) __msa_vshf_b(mask, (v16i8) ref0, (v16i8) ref0);
+        temp1 = (v16u8) __msa_vshf_b(mask, (v16i8) ref1, (v16i8) ref1);
+        src += src_stride;
+        ref += ref_stride;
+        comp0 = __msa_hadd_u_h(temp0, temp0);
+        comp2 = __msa_hadd_u_h(temp1, temp1);
+        comp2 += comp0;
+        comp2 = (v8u16)__msa_srari_h((v8i16) comp2, 2);
+        comp0 = (v16u8) __msa_pckev_b((v16i8) zero, (v16i8) comp2);
+        diff = __msa_asub_u_b(src0, comp0);
+        diff = (v16u8)__msa_ilvr_d((v2i64) zero, (v2i64) diff);
+        sad += __msa_hadd_u_h(diff, diff);
+    }
     return (HADD_UH_U32(sad));
 }
 
@@ -286,14 +375,15 @@ static uint32_t sad_hv_bilinear_filter_16width_msa(uint8_t *src,
                                                    int32_t ref_stride,
                                                    int32_t height)
 {
-    int32_t ht_cnt;
+    int32_t ht_cnt = height >> 3;
+    int32_t res = height & 0x07;
     v16u8 src0, src1, src2, src3, comp, diff;
     v16u8 temp0, temp1, temp2, temp3;
     v16u8 ref00, ref01, ref02, ref03, ref04, ref10, ref11, ref12, ref13, ref14;
     v8u16 comp0, comp1, comp2, comp3;
     v8u16 sad = { 0 };
 
-    for (ht_cnt = (height >> 3); ht_cnt--;) {
+    for (; ht_cnt--; ) {
         LD_UB4(src, src_stride, src0, src1, src2, src3);
         src += (4 * src_stride);
         LD_UB5(ref, ref_stride, ref04, ref00, ref01, ref02, ref03);
@@ -389,6 +479,25 @@ static uint32_t sad_hv_bilinear_filter_16width_msa(uint8_t *src,
         diff = __msa_asub_u_b(src3, comp);
         sad += __msa_hadd_u_h(diff, diff);
     }
+    for (; res--; ) {
+        src0 = LD_UB(src);
+        LD_UB2(ref, ref_stride, ref00, ref10);
+        LD_UB2(ref + 1, ref_stride, ref01, ref11);
+        src += src_stride;
+        ref += ref_stride;
+        ILVRL_B2_UB(ref10, ref00, temp0, temp1);
+        ILVRL_B2_UB(ref11, ref01, temp2, temp3);
+        comp0 = __msa_hadd_u_h(temp0, temp0);
+        comp1 = __msa_hadd_u_h(temp1, temp1);
+        comp2 = __msa_hadd_u_h(temp2, temp2);
+        comp3 = __msa_hadd_u_h(temp3, temp3);
+        comp2 += comp0;
+        comp3 += comp1;
+        SRARI_H2_UH(comp2, comp3, 2);
+        comp = (v16u8) __msa_pckev_b((v16i8) comp3, (v16i8) comp2);
+        diff = __msa_asub_u_b(src0, comp);
+        sad += __msa_hadd_u_h(diff, diff);
+    }
 
     return (HADD_UH_U32(sad));
 }
@@ -407,15 +516,17 @@ static uint32_t sse_4width_msa(uint8_t *src_ptr, int32_t src_stride,
                                uint8_t *ref_ptr, int32_t ref_stride,
                                int32_t height)
 {
-    int32_t ht_cnt;
+    int32_t ht_cnt = height >> 2;
+    int32_t res = height & 0x03;
     uint32_t sse;
     uint32_t src0, src1, src2, src3;
     uint32_t ref0, ref1, ref2, ref3;
-    v16u8 src = { 0 };
-    v16u8 ref = { 0 };
-    v4i32 var = { 0 };
+    v16u8 src  = { 0 };
+    v16u8 ref  = { 0 };
+    v16u8 zero = { 0 };
+    v4i32 var  = { 0 };
 
-    for (ht_cnt = (height >> 2); ht_cnt--;) {
+    for (; ht_cnt--; ) {
         LW4(src_ptr, src_stride, src0, src1, src2, src3);
         src_ptr += (4 * src_stride);
         LW4(ref_ptr, ref_stride, ref0, ref1, ref2, ref3);
@@ -426,6 +537,20 @@ static uint32_t sse_4width_msa(uint8_t *src_ptr, int32_t src_stride,
         CALC_MSE_B(src, ref, var);
     }
 
+    for (; res--; ) {
+        v16u8 reg0;
+        v8i16 tmp0;
+        src0 = LW(src_ptr);
+        ref0 = LW(ref_ptr);
+        src_ptr += src_stride;
+        ref_ptr += ref_stride;
+        src  = (v16u8)__msa_insert_w((v4i32) src, 0, src0);
+        ref  = (v16u8)__msa_insert_w((v4i32) ref, 0, ref0);
+        reg0 = (v16u8)__msa_ilvr_b(src, ref);
+        reg0 = (v16u8)__msa_ilvr_d((v2i64) zero, (v2i64) reg0);
+        tmp0 = (v8i16)__msa_hsub_u_h((v16u8) reg0, (v16u8) reg0);
+        var  = (v4i32)__msa_dpadd_s_w((v4i32) var, (v8i16) tmp0, (v8i16) tmp0);
+    }
     sse = HADD_SW_S32(var);
 
     return sse;
@@ -435,13 +560,14 @@ static uint32_t sse_8width_msa(uint8_t *src_ptr, int32_t src_stride,
                                uint8_t *ref_ptr, int32_t ref_stride,
                                int32_t height)
 {
-    int32_t ht_cnt;
+    int32_t ht_cnt = height >> 2;
+    int32_t res = height & 0x03;
     uint32_t sse;
     v16u8 src0, src1, src2, src3;
     v16u8 ref0, ref1, ref2, ref3;
     v4i32 var = { 0 };
 
-    for (ht_cnt = (height >> 2); ht_cnt--;) {
+    for (; ht_cnt--; ) {
         LD_UB4(src_ptr, src_stride, src0, src1, src2, src3);
         src_ptr += (4 * src_stride);
         LD_UB4(ref_ptr, ref_stride, ref0, ref1, ref2, ref3);
@@ -453,6 +579,16 @@ static uint32_t sse_8width_msa(uint8_t *src_ptr, int32_t src_stride,
         CALC_MSE_B(src1, ref1, var);
     }
 
+    for (; res--; ) {
+        v8i16 tmp0;
+        src0 = LD_UB(src_ptr);
+        ref0 = LD_UB(ref_ptr);
+        src_ptr += src_stride;
+        ref_ptr += ref_stride;
+        ref1 = (v16u8)__msa_ilvr_b(src0, ref0);
+        tmp0 = (v8i16)__msa_hsub_u_h((v16u8) ref1, (v16u8) ref1);
+        var  = (v4i32)__msa_dpadd_s_w((v4i32) var, (v8i16) tmp0, (v8i16) tmp0);
+    }
     sse = HADD_SW_S32(var);
 
     return sse;
@@ -462,12 +598,13 @@ static uint32_t sse_16width_msa(uint8_t *src_ptr, int32_t src_stride,
                                 uint8_t *ref_ptr, int32_t ref_stride,
                                 int32_t height)
 {
-    int32_t ht_cnt;
+    int32_t ht_cnt = height >> 2;
+    int32_t res = height & 0x03;
     uint32_t sse;
     v16u8 src, ref;
     v4i32 var = { 0 };
 
-    for (ht_cnt = (height >> 2); ht_cnt--;) {
+    for (; ht_cnt--; ) {
         src = LD_UB(src_ptr);
         src_ptr += src_stride;
         ref = LD_UB(ref_ptr);
@@ -493,6 +630,14 @@ static uint32_t sse_16width_msa(uint8_t *src_ptr, int32_t src_stride,
         CALC_MSE_B(src, ref, var);
     }
 
+    for (; res--; ) {
+        src = LD_UB(src_ptr);
+        src_ptr += src_stride;
+        ref = LD_UB(ref_ptr);
+        ref_ptr += ref_stride;
+        CALC_MSE_B(src, ref, var);
+    }
+
     sse = HADD_SW_S32(var);
 
     return sse;
@@ -544,7 +689,7 @@ static int32_t hadamard_diff_8x8_msa(uint8_t *src, int32_t src_stride,
 }
 
 static int32_t hadamard_intra_8x8_msa(uint8_t *src, int32_t src_stride,
-                                      uint8_t *ref, int32_t ref_stride)
+                                      uint8_t *dumy, int32_t ref_stride)
 {
     int32_t sum_res = 0;
     v16u8 src0, src1, src2, src3, src4, src5, src6, src7;
@@ -659,10 +804,10 @@ int ff_hadamard8_diff8x8_msa(MpegEncContext *s, uint8_t *dst, uint8_t *src,
     return hadamard_diff_8x8_msa(src, stride, dst, stride);
 }
 
-int ff_hadamard8_intra8x8_msa(MpegEncContext *s, uint8_t *dst, uint8_t *src,
+int ff_hadamard8_intra8x8_msa(MpegEncContext *s, uint8_t *src, uint8_t *dummy,
                               ptrdiff_t stride, int h)
 {
-    return hadamard_intra_8x8_msa(src, stride, dst, stride);
+    return hadamard_intra_8x8_msa(src, stride, dummy, stride);
 }
 
 /* Hadamard Transform functions */
diff --git a/libavcodec/mips/mpegaudiodsp_mips_float.c b/libavcodec/mips/mpegaudiodsp_mips_float.c
index 481b69c10e..ae130c752e 100644
--- a/libavcodec/mips/mpegaudiodsp_mips_float.c
+++ b/libavcodec/mips/mpegaudiodsp_mips_float.c
@@ -287,9 +287,16 @@ static void ff_dct32_mips_float(float *out, const float *tab)
           val8 , val9 , val10, val11, val12, val13, val14, val15,
           val16, val17, val18, val19, val20, val21, val22, val23,
           val24, val25, val26, val27, val28, val29, val30, val31;
-    float fTmp1, fTmp2, fTmp3, fTmp4, fTmp5, fTmp6, fTmp7, fTmp8,
-          fTmp9, fTmp10, fTmp11;
+    float fTmp1, fTmp2, fTmp3, fTmp4, fTmp5, fTmp6, fTmp8, fTmp9;
+    float f1, f2, f3, f4, f5, f6, f7;
 
+    f1 = 0.50241928618815570551;
+    f2 = 0.50060299823519630134;
+    f3 = 10.19000812354805681150;
+    f4 = 5.10114861868916385802;
+    f5 = 0.67480834145500574602;
+    f6 = 0.74453627100229844977;
+    f7 = 0.50979557910415916894;
     /**
     * instructions are scheduled to minimize pipeline stall.
     */
@@ -298,149 +305,142 @@ static void ff_dct32_mips_float(float *out, const float *tab)
         "lwc1       %[fTmp2],       31*4(%[tab])                            \n\t"
         "lwc1       %[fTmp3],       15*4(%[tab])                            \n\t"
         "lwc1       %[fTmp4],       16*4(%[tab])                            \n\t"
-        "li.s       %[fTmp7],       0.50241928618815570551                  \n\t"
         "add.s      %[fTmp5],       %[fTmp1],       %[fTmp2]                \n\t"
         "sub.s      %[fTmp8],       %[fTmp1],       %[fTmp2]                \n\t"
         "add.s      %[fTmp6],       %[fTmp3],       %[fTmp4]                \n\t"
         "sub.s      %[fTmp9],       %[fTmp3],       %[fTmp4]                \n\t"
-        "li.s       %[fTmp10],      0.50060299823519630134                  \n\t"
-        "li.s       %[fTmp11],      10.19000812354805681150                 \n\t"
-        "mul.s      %[fTmp8],       %[fTmp8],       %[fTmp10]               \n\t"
+        "mul.s      %[fTmp8],       %[fTmp8],       %[f2]                   \n\t"
         "add.s      %[val0],        %[fTmp5],       %[fTmp6]                \n\t"
         "sub.s      %[val15],       %[fTmp5],       %[fTmp6]                \n\t"
         "lwc1       %[fTmp1],       7*4(%[tab])                             \n\t"
         "lwc1       %[fTmp2],       24*4(%[tab])                            \n\t"
-        "madd.s     %[val16],       %[fTmp8],       %[fTmp9],   %[fTmp11]   \n\t"
-        "nmsub.s    %[val31],       %[fTmp8],       %[fTmp9],   %[fTmp11]   \n\t"
-        "mul.s      %[val15],       %[val15],       %[fTmp7]                \n\t"
+        "madd.s     %[val16],       %[fTmp8],       %[fTmp9],   %[f3]       \n\t"
+        "nmsub.s    %[val31],       %[fTmp8],       %[fTmp9],   %[f3]       \n\t"
+        "mul.s      %[val15],       %[val15],       %[f1]                   \n\t"
         "lwc1       %[fTmp3],       8*4(%[tab])                             \n\t"
         "lwc1       %[fTmp4],       23*4(%[tab])                            \n\t"
         "add.s      %[fTmp5],       %[fTmp1],       %[fTmp2]                \n\t"
-        "mul.s      %[val31],       %[val31],       %[fTmp7]                \n\t"
+        "mul.s      %[val31],       %[val31],       %[f1]                   \n\t"
         "sub.s      %[fTmp8],       %[fTmp1],       %[fTmp2]                \n\t"
         "add.s      %[fTmp6],       %[fTmp3],       %[fTmp4]                \n\t"
         "sub.s      %[fTmp9],       %[fTmp3],       %[fTmp4]                \n\t"
-        "li.s       %[fTmp7],       5.10114861868916385802                  \n\t"
-        "li.s       %[fTmp10],      0.67480834145500574602                  \n\t"
-        "li.s       %[fTmp11],      0.74453627100229844977                  \n\t"
         "add.s      %[val7],        %[fTmp5],       %[fTmp6]                \n\t"
         "sub.s      %[val8],        %[fTmp5],       %[fTmp6]                \n\t"
-        "mul.s      %[fTmp8],       %[fTmp8],       %[fTmp10]               \n\t"
-        "li.s       %[fTmp1],       0.50979557910415916894                  \n\t"
+        "mul.s      %[fTmp8],       %[fTmp8],       %[f5]                   \n\t"
         "sub.s      %[fTmp2],       %[val0],        %[val7]                 \n\t"
-        "mul.s      %[val8],        %[val8],        %[fTmp7]                \n\t"
-        "madd.s     %[val23],       %[fTmp8],       %[fTmp9],   %[fTmp11]   \n\t"
-        "nmsub.s    %[val24],       %[fTmp8],       %[fTmp9],   %[fTmp11]   \n\t"
+        "mul.s      %[val8],        %[val8],        %[f4]                   \n\t"
+        "madd.s     %[val23],       %[fTmp8],       %[fTmp9],   %[f6]       \n\t"
+        "nmsub.s    %[val24],       %[fTmp8],       %[fTmp9],   %[f6]       \n\t"
         "add.s      %[val0],        %[val0],        %[val7]                 \n\t"
-        "mul.s      %[val7],        %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val7],        %[f7],          %[fTmp2]                \n\t"
         "sub.s      %[fTmp2],       %[val15],       %[val8]                 \n\t"
         "add.s      %[val8],        %[val15],       %[val8]                 \n\t"
-        "mul.s      %[val24],       %[val24],       %[fTmp7]                \n\t"
+        "mul.s      %[val24],       %[val24],       %[f4]                   \n\t"
         "sub.s      %[fTmp3],       %[val16],       %[val23]                \n\t"
         "add.s      %[val16],       %[val16],       %[val23]                \n\t"
-        "mul.s      %[val15],       %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val15],       %[f7],          %[fTmp2]                \n\t"
         "sub.s      %[fTmp4],       %[val31],       %[val24]                \n\t"
-        "mul.s      %[val23],       %[fTmp1],       %[fTmp3]                \n\t"
+        "mul.s      %[val23],       %[f7],          %[fTmp3]                \n\t"
         "add.s      %[val24],       %[val31],       %[val24]                \n\t"
-        "mul.s      %[val31],       %[fTmp1],       %[fTmp4]                \n\t"
+        "mul.s      %[val31],       %[f7],          %[fTmp4]                \n\t"
 
         : [fTmp1]  "=&f" (fTmp1),  [fTmp2] "=&f" (fTmp2), [fTmp3] "=&f" (fTmp3),
           [fTmp4]  "=&f" (fTmp4),  [fTmp5] "=&f" (fTmp5), [fTmp6] "=&f" (fTmp6),
-          [fTmp7]  "=&f" (fTmp7),  [fTmp8] "=&f" (fTmp8), [fTmp9] "=&f" (fTmp9),
-          [fTmp10] "=&f" (fTmp10), [fTmp11] "=&f" (fTmp11),
-          [val0]  "=f" (val0),  [val7]  "=f" (val7),
-          [val8]  "=f" (val8),  [val15] "=f" (val15),
-          [val16] "=f" (val16), [val23] "=f" (val23),
-          [val24] "=f" (val24), [val31] "=f" (val31)
-        : [tab] "r" (tab)
+          [fTmp8] "=&f" (fTmp8), [fTmp9] "=&f" (fTmp9),
+          [val0]  "=&f" (val0),  [val7]  "=&f" (val7),
+          [val8]  "=&f" (val8),  [val15] "=&f" (val15),
+          [val16] "=&f" (val16), [val23] "=&f" (val23),
+          [val24] "=&f" (val24), [val31] "=&f" (val31)
+        : [tab] "r" (tab), [f1]"f"(f1), [f2]"f"(f2), [f3]"f"(f3),
+          [f4]"f"(f4), [f5]"f"(f5), [f6]"f"(f6), [f7]"f"(f7)
         : "memory"
     );
 
+    f1 = 0.64682178335999012954;
+    f2 = 0.53104259108978417447;
+    f3 = 1.48416461631416627724;
+    f4 = 0.78815462345125022473;
+    f5 = 0.55310389603444452782;
+    f6 = 1.16943993343288495515;
+    f7 = 2.56291544774150617881;
     __asm__ volatile (
         "lwc1       %[fTmp1],       3*4(%[tab])                             \n\t"
         "lwc1       %[fTmp2],       28*4(%[tab])                            \n\t"
         "lwc1       %[fTmp3],       12*4(%[tab])                            \n\t"
         "lwc1       %[fTmp4],       19*4(%[tab])                            \n\t"
-        "li.s       %[fTmp7],       0.64682178335999012954                  \n\t"
         "add.s      %[fTmp5],       %[fTmp1],       %[fTmp2]                \n\t"
         "sub.s      %[fTmp8],       %[fTmp1],       %[fTmp2]                \n\t"
         "add.s      %[fTmp6],       %[fTmp3],       %[fTmp4]                \n\t"
         "sub.s      %[fTmp9],       %[fTmp3],       %[fTmp4]                \n\t"
-        "li.s       %[fTmp10],      0.53104259108978417447                  \n\t"
-        "li.s       %[fTmp11],      1.48416461631416627724                  \n\t"
-        "mul.s      %[fTmp8],       %[fTmp8],       %[fTmp10]               \n\t"
+        "mul.s      %[fTmp8],       %[fTmp8],       %[f2]                   \n\t"
         "add.s      %[val3],        %[fTmp5],       %[fTmp6]                \n\t"
         "sub.s      %[val12],       %[fTmp5],       %[fTmp6]                \n\t"
         "lwc1       %[fTmp1],       4*4(%[tab])                             \n\t"
         "lwc1       %[fTmp2],       27*4(%[tab])                            \n\t"
-        "madd.s     %[val19],       %[fTmp8],       %[fTmp9],   %[fTmp11]   \n\t"
-        "nmsub.s    %[val28],       %[fTmp8],       %[fTmp9],   %[fTmp11]   \n\t"
-        "mul.s      %[val12],       %[val12],       %[fTmp7]                \n\t"
+        "madd.s     %[val19],       %[fTmp8],       %[fTmp9],   %[f3]       \n\t"
+        "nmsub.s    %[val28],       %[fTmp8],       %[fTmp9],   %[f3]       \n\t"
+        "mul.s      %[val12],       %[val12],       %[f1]                   \n\t"
         "lwc1       %[fTmp3],       11*4(%[tab])                            \n\t"
         "lwc1       %[fTmp4],       20*4(%[tab])                            \n\t"
         "add.s      %[fTmp5],       %[fTmp1],       %[fTmp2]                \n\t"
-        "mul.s      %[val28],       %[val28],       %[fTmp7]                \n\t"
+        "mul.s      %[val28],       %[val28],       %[f1]                   \n\t"
         "sub.s      %[fTmp8],       %[fTmp1],       %[fTmp2]                \n\t"
-        "li.s       %[fTmp7],       0.78815462345125022473                  \n\t"
         "add.s      %[fTmp6],       %[fTmp3],       %[fTmp4]                \n\t"
         "sub.s      %[fTmp9],       %[fTmp3],       %[fTmp4]                \n\t"
-        "li.s       %[fTmp10],      0.55310389603444452782                  \n\t"
-        "li.s       %[fTmp11],      1.16943993343288495515                  \n\t"
-        "mul.s      %[fTmp8],       %[fTmp8],       %[fTmp10]               \n\t"
+        "mul.s      %[fTmp8],       %[fTmp8],       %[f5]                   \n\t"
         "add.s      %[val4],        %[fTmp5],       %[fTmp6]                \n\t"
         "sub.s      %[val11],       %[fTmp5],       %[fTmp6]                \n\t"
-        "li.s       %[fTmp1],       2.56291544774150617881                  \n\t"
-        "madd.s     %[val20],       %[fTmp8],       %[fTmp9],   %[fTmp11]   \n\t"
-        "nmsub.s    %[val27],       %[fTmp8],       %[fTmp9],   %[fTmp11]   \n\t"
-        "mul.s      %[val11],       %[val11],       %[fTmp7]                \n\t"
+        "madd.s     %[val20],       %[fTmp8],       %[fTmp9],   %[f6]       \n\t"
+        "nmsub.s    %[val27],       %[fTmp8],       %[fTmp9],   %[f6]       \n\t"
+        "mul.s      %[val11],       %[val11],       %[f4]                   \n\t"
         "sub.s      %[fTmp2],       %[val3],        %[val4]                 \n\t"
         "add.s      %[val3],        %[val3],        %[val4]                 \n\t"
         "sub.s      %[fTmp4],       %[val19],       %[val20]                \n\t"
-        "mul.s      %[val27],       %[val27],       %[fTmp7]                \n\t"
+        "mul.s      %[val27],       %[val27],       %[f4]                   \n\t"
         "sub.s      %[fTmp3],       %[val12],       %[val11]                \n\t"
-        "mul.s      %[val4],        %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val4],        %[f7],          %[fTmp2]                \n\t"
         "add.s      %[val11],       %[val12],       %[val11]                \n\t"
         "add.s      %[val19],       %[val19],       %[val20]                \n\t"
-        "mul.s      %[val20],       %[fTmp1],       %[fTmp4]                \n\t"
-        "mul.s      %[val12],       %[fTmp1],       %[fTmp3]                \n\t"
+        "mul.s      %[val20],       %[f7],          %[fTmp4]                \n\t"
+        "mul.s      %[val12],       %[f7],          %[fTmp3]                \n\t"
         "sub.s      %[fTmp2],       %[val28],       %[val27]                \n\t"
         "add.s      %[val27],       %[val28],       %[val27]                \n\t"
-        "mul.s      %[val28],       %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val28],       %[f7],          %[fTmp2]                \n\t"
 
         : [fTmp1]  "=&f" (fTmp1),  [fTmp2]  "=&f" (fTmp2), [fTmp3] "=&f" (fTmp3),
           [fTmp4]  "=&f" (fTmp4),  [fTmp5]  "=&f" (fTmp5), [fTmp6] "=&f" (fTmp6),
-          [fTmp7]  "=&f" (fTmp7),  [fTmp8]  "=&f" (fTmp8), [fTmp9] "=&f" (fTmp9),
-          [fTmp10] "=&f" (fTmp10), [fTmp11] "=&f" (fTmp11),
-          [val3]  "=f" (val3),  [val4]  "=f" (val4),
-          [val11] "=f" (val11), [val12] "=f" (val12),
-          [val19] "=f" (val19), [val20] "=f" (val20),
-          [val27] "=f" (val27), [val28] "=f" (val28)
-        : [tab] "r" (tab)
+          [fTmp8]  "=&f" (fTmp8), [fTmp9] "=&f" (fTmp9),
+          [val3]  "=&f" (val3),  [val4]  "=&f" (val4),
+          [val11] "=&f" (val11), [val12] "=&f" (val12),
+          [val19] "=&f" (val19), [val20] "=&f" (val20),
+          [val27] "=&f" (val27), [val28] "=&f" (val28)
+        : [tab] "r" (tab), [f1]"f"(f1), [f2]"f"(f2), [f3]"f"(f3),
+          [f4]"f"(f4), [f5]"f"(f5), [f6]"f"(f6), [f7]"f"(f7)
         : "memory"
     );
 
+    f1 = 0.54119610014619698439;
     __asm__ volatile (
-        "li.s       %[fTmp1],       0.54119610014619698439                  \n\t"
         "sub.s      %[fTmp2],       %[val0],        %[val3]                 \n\t"
         "add.s      %[val0],        %[val0],        %[val3]                 \n\t"
         "sub.s      %[fTmp3],       %[val7],        %[val4]                 \n\t"
         "add.s      %[val4],        %[val7],        %[val4]                 \n\t"
         "sub.s      %[fTmp4],       %[val8],        %[val11]                \n\t"
-        "mul.s      %[val3],        %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val3],        %[f1],          %[fTmp2]                \n\t"
         "add.s      %[val8],        %[val8],        %[val11]                \n\t"
-        "mul.s      %[val7],        %[fTmp1],       %[fTmp3]                \n\t"
+        "mul.s      %[val7],        %[f1],          %[fTmp3]                \n\t"
         "sub.s      %[fTmp2],       %[val15],       %[val12]                \n\t"
-        "mul.s      %[val11],       %[fTmp1],       %[fTmp4]                \n\t"
+        "mul.s      %[val11],       %[f1],          %[fTmp4]                \n\t"
         "add.s      %[val12],       %[val15],       %[val12]                \n\t"
-        "mul.s      %[val15],       %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val15],       %[f1],          %[fTmp2]                \n\t"
 
-        : [val0]  "+f" (val0),   [val3] "+f" (val3),
-          [val4]  "+f" (val4),   [val7] "+f" (val7),
-          [val8]  "+f" (val8),   [val11] "+f" (val11),
-          [val12] "+f" (val12),  [val15] "+f" (val15),
-          [fTmp1] "=f"  (fTmp1), [fTmp2] "=&f" (fTmp2),
+        : [val0]  "+&f" (val0),   [val3] "+&f" (val3),
+          [val4]  "+&f" (val4),   [val7] "+&f" (val7),
+          [val8]  "+&f" (val8),   [val11] "+&f" (val11),
+          [val12] "+&f" (val12),  [val15] "+&f" (val15),
+          [fTmp2] "=&f" (fTmp2),
           [fTmp3] "=&f" (fTmp3), [fTmp4] "=&f" (fTmp4)
-        :
+        : [f1] "f" (f1)
     );
 
     __asm__ volatile (
@@ -449,169 +449,169 @@ static void ff_dct32_mips_float(float *out, const float *tab)
         "sub.s      %[fTmp3],       %[val23],       %[val20]                \n\t"
         "add.s      %[val20],       %[val23],       %[val20]                \n\t"
         "sub.s      %[fTmp4],       %[val24],       %[val27]                \n\t"
-        "mul.s      %[val19],       %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val19],       %[f1],          %[fTmp2]                \n\t"
         "add.s      %[val24],       %[val24],       %[val27]                \n\t"
-        "mul.s      %[val23],       %[fTmp1],       %[fTmp3]                \n\t"
+        "mul.s      %[val23],       %[f1],          %[fTmp3]                \n\t"
         "sub.s      %[fTmp2],       %[val31],       %[val28]                \n\t"
-        "mul.s      %[val27],       %[fTmp1],       %[fTmp4]                \n\t"
+        "mul.s      %[val27],       %[f1],          %[fTmp4]                \n\t"
         "add.s      %[val28],       %[val31],       %[val28]                \n\t"
-        "mul.s      %[val31],       %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val31],       %[f1],          %[fTmp2]                \n\t"
 
         : [fTmp2] "=&f" (fTmp2), [fTmp3] "=&f" (fTmp3), [fTmp4] "=&f" (fTmp4),
-          [val16] "+f" (val16), [val19] "+f" (val19), [val20] "+f" (val20),
-          [val23] "+f" (val23), [val24] "+f" (val24), [val27] "+f" (val27),
-          [val28] "+f" (val28), [val31] "+f" (val31)
-        : [fTmp1] "f" (fTmp1)
+          [val16] "+&f" (val16), [val19] "+&f" (val19), [val20] "+&f" (val20),
+          [val23] "+&f" (val23), [val24] "+&f" (val24), [val27] "+&f" (val27),
+          [val28] "+&f" (val28), [val31] "+&f" (val31)
+        : [f1] "f" (f1)
     );
 
+    f1 = 0.52249861493968888062;
+    f2 = 0.50547095989754365998;
+    f3 = 3.40760841846871878570;
+    f4 = 1.72244709823833392782;
+    f5 = 0.62250412303566481615;
+    f6 = 0.83934964541552703873;
+    f7 = 0.60134488693504528054;
     __asm__ volatile (
         "lwc1       %[fTmp1],       1*4(%[tab])                             \n\t"
         "lwc1       %[fTmp2],       30*4(%[tab])                            \n\t"
         "lwc1       %[fTmp3],       14*4(%[tab])                            \n\t"
         "lwc1       %[fTmp4],       17*4(%[tab])                            \n\t"
-        "li.s       %[fTmp7],       0.52249861493968888062                  \n\t"
         "add.s      %[fTmp5],       %[fTmp1],       %[fTmp2]                \n\t"
         "sub.s      %[fTmp8],       %[fTmp1],       %[fTmp2]                \n\t"
         "add.s      %[fTmp6],       %[fTmp3],       %[fTmp4]                \n\t"
         "sub.s      %[fTmp9],       %[fTmp3],       %[fTmp4]                \n\t"
-        "li.s       %[fTmp10],      0.50547095989754365998                  \n\t"
-        "li.s       %[fTmp11],      3.40760841846871878570                  \n\t"
-        "mul.s      %[fTmp8],       %[fTmp8],       %[fTmp10]               \n\t"
+        "mul.s      %[fTmp8],       %[fTmp8],       %[f2]                   \n\t"
         "add.s      %[val1],        %[fTmp5],       %[fTmp6]                \n\t"
         "sub.s      %[val14],       %[fTmp5],       %[fTmp6]                \n\t"
         "lwc1       %[fTmp1],       6*4(%[tab])                             \n\t"
         "lwc1       %[fTmp2],       25*4(%[tab])                            \n\t"
-        "madd.s     %[val17],       %[fTmp8],       %[fTmp9],   %[fTmp11]   \n\t"
-        "nmsub.s    %[val30],       %[fTmp8],       %[fTmp9],   %[fTmp11]   \n\t"
-        "mul.s      %[val14],       %[val14],       %[fTmp7]                \n\t"
+        "madd.s     %[val17],       %[fTmp8],       %[fTmp9],   %[f3]       \n\t"
+        "nmsub.s    %[val30],       %[fTmp8],       %[fTmp9],   %[f3]       \n\t"
+        "mul.s      %[val14],       %[val14],       %[f1]                   \n\t"
         "lwc1       %[fTmp3],       9*4(%[tab])                             \n\t"
         "lwc1       %[fTmp4],       22*4(%[tab])                            \n\t"
         "add.s      %[fTmp5],       %[fTmp1],       %[fTmp2]                \n\t"
-        "mul.s      %[val30],       %[val30],       %[fTmp7]                \n\t"
+        "mul.s      %[val30],       %[val30],       %[f1]                   \n\t"
         "sub.s      %[fTmp8],       %[fTmp1],       %[fTmp2]                \n\t"
         "add.s      %[fTmp6],       %[fTmp3],       %[fTmp4]                \n\t"
         "sub.s      %[fTmp9],       %[fTmp3],       %[fTmp4]                \n\t"
-        "li.s       %[fTmp7],       1.72244709823833392782                  \n\t"
-        "li.s       %[fTmp10],      0.62250412303566481615                  \n\t"
-        "li.s       %[fTmp11],      0.83934964541552703873                  \n\t"
         "add.s      %[val6],        %[fTmp5],       %[fTmp6]                \n\t"
         "sub.s      %[val9],        %[fTmp5],       %[fTmp6]                \n\t"
-        "mul.s      %[fTmp8],       %[fTmp8],       %[fTmp10]               \n\t"
-        "li.s       %[fTmp1],       0.60134488693504528054                  \n\t"
+        "mul.s      %[fTmp8],       %[fTmp8],       %[f5]                   \n\t"
         "sub.s      %[fTmp2],       %[val1],        %[val6]                 \n\t"
         "add.s      %[val1],        %[val1],        %[val6]                 \n\t"
-        "mul.s      %[val9],        %[val9],        %[fTmp7]                \n\t"
-        "madd.s     %[val22],       %[fTmp8],       %[fTmp9],   %[fTmp11]   \n\t"
-        "nmsub.s    %[val25],       %[fTmp8],       %[fTmp9],   %[fTmp11]   \n\t"
-        "mul.s      %[val6],        %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val9],        %[val9],        %[f4]                   \n\t"
+        "madd.s     %[val22],       %[fTmp8],       %[fTmp9],   %[f6]       \n\t"
+        "nmsub.s    %[val25],       %[fTmp8],       %[fTmp9],   %[f6]       \n\t"
+        "mul.s      %[val6],        %[f7],          %[fTmp2]                \n\t"
         "sub.s      %[fTmp2],       %[val14],       %[val9]                 \n\t"
         "add.s      %[val9],        %[val14],       %[val9]                 \n\t"
-        "mul.s      %[val25],       %[val25],       %[fTmp7]                \n\t"
+        "mul.s      %[val25],       %[val25],       %[f4]                   \n\t"
         "sub.s      %[fTmp3],       %[val17],       %[val22]                \n\t"
         "add.s      %[val17],       %[val17],       %[val22]                \n\t"
-        "mul.s      %[val14],       %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val14],       %[f7],          %[fTmp2]                \n\t"
         "sub.s      %[fTmp2],       %[val30],       %[val25]                \n\t"
-        "mul.s      %[val22],       %[fTmp1],       %[fTmp3]                \n\t"
+        "mul.s      %[val22],       %[f7],          %[fTmp3]                \n\t"
         "add.s      %[val25],       %[val30],       %[val25]                \n\t"
-        "mul.s      %[val30],       %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val30],       %[f7],          %[fTmp2]                \n\t"
 
         : [fTmp1]  "=&f" (fTmp1),  [fTmp2]  "=&f" (fTmp2), [fTmp3] "=&f" (fTmp3),
           [fTmp4]  "=&f" (fTmp4),  [fTmp5]  "=&f" (fTmp5), [fTmp6] "=&f" (fTmp6),
-          [fTmp7]  "=&f" (fTmp7),  [fTmp8]  "=&f" (fTmp8), [fTmp9] "=&f" (fTmp9),
-          [fTmp10] "=&f" (fTmp10), [fTmp11] "=&f" (fTmp11),
-          [val1]  "=f" (val1),  [val6]  "=f" (val6),
-          [val9]  "=f" (val9),  [val14] "=f" (val14),
-          [val17] "=f" (val17), [val22] "=f" (val22),
-          [val25] "=f" (val25), [val30] "=f" (val30)
-        : [tab] "r" (tab)
+          [fTmp8]  "=&f" (fTmp8), [fTmp9] "=&f" (fTmp9),
+          [val1]  "=&f" (val1),  [val6]  "=&f" (val6),
+          [val9]  "=&f" (val9),  [val14] "=&f" (val14),
+          [val17] "=&f" (val17), [val22] "=&f" (val22),
+          [val25] "=&f" (val25), [val30] "=&f" (val30)
+        : [tab] "r" (tab), [f1]"f"(f1), [f2]"f"(f2), [f3]"f"(f3),
+          [f4]"f"(f4), [f5]"f"(f5), [f6]"f"(f6), [f7]"f"(f7)
         : "memory"
     );
 
+    f1 = 0.56694403481635770368;
+    f2 = 0.51544730992262454697;
+    f3 = 2.05778100995341155085;
+    f4 = 1.06067768599034747134;
+    f5 = 0.58293496820613387367;
+    f6 = 0.97256823786196069369;
+    f7 = 0.89997622313641570463;
     __asm__ volatile (
         "lwc1       %[fTmp1],       2*4(%[tab])                             \n\t"
         "lwc1       %[fTmp2],       29*4(%[tab])                            \n\t"
         "lwc1       %[fTmp3],       13*4(%[tab])                            \n\t"
         "lwc1       %[fTmp4],       18*4(%[tab])                            \n\t"
-        "li.s       %[fTmp7],       0.56694403481635770368                  \n\t"
         "add.s      %[fTmp5],       %[fTmp1],       %[fTmp2]                \n\t"
         "sub.s      %[fTmp8],       %[fTmp1],       %[fTmp2]                \n\t"
         "add.s      %[fTmp6],       %[fTmp3],       %[fTmp4]                \n\t"
         "sub.s      %[fTmp9],       %[fTmp3],       %[fTmp4]                \n\t"
-        "li.s       %[fTmp10],      0.51544730992262454697                  \n\t"
-        "li.s       %[fTmp11],      2.05778100995341155085                  \n\t"
-        "mul.s      %[fTmp8],       %[fTmp8],       %[fTmp10]               \n\t"
+        "mul.s      %[fTmp8],       %[fTmp8],       %[f2]                   \n\t"
         "add.s      %[val2],        %[fTmp5],       %[fTmp6]                \n\t"
         "sub.s      %[val13],       %[fTmp5],       %[fTmp6]                \n\t"
         "lwc1       %[fTmp1],       5*4(%[tab])                             \n\t"
         "lwc1       %[fTmp2],       26*4(%[tab])                            \n\t"
-        "madd.s     %[val18],       %[fTmp8],       %[fTmp9],   %[fTmp11]   \n\t"
-        "nmsub.s    %[val29],       %[fTmp8],       %[fTmp9],   %[fTmp11]   \n\t"
-        "mul.s      %[val13],       %[val13],       %[fTmp7]                \n\t"
+        "madd.s     %[val18],       %[fTmp8],       %[fTmp9],   %[f3]       \n\t"
+        "nmsub.s    %[val29],       %[fTmp8],       %[fTmp9],   %[f3]       \n\t"
+        "mul.s      %[val13],       %[val13],       %[f1]                   \n\t"
         "lwc1       %[fTmp3],       10*4(%[tab])                            \n\t"
         "lwc1       %[fTmp4],       21*4(%[tab])                            \n\t"
-        "mul.s      %[val29],       %[val29],       %[fTmp7]                \n\t"
+        "mul.s      %[val29],       %[val29],       %[f1]                   \n\t"
         "add.s      %[fTmp5],       %[fTmp1],       %[fTmp2]                \n\t"
         "sub.s      %[fTmp8],       %[fTmp1],       %[fTmp2]                \n\t"
         "add.s      %[fTmp6],       %[fTmp3],       %[fTmp4]                \n\t"
         "sub.s      %[fTmp9],       %[fTmp3],       %[fTmp4]                \n\t"
-        "li.s       %[fTmp7],       1.06067768599034747134                  \n\t"
-        "li.s       %[fTmp10],      0.58293496820613387367                  \n\t"
-        "li.s       %[fTmp11],      0.97256823786196069369                  \n\t"
         "add.s      %[val5],        %[fTmp5],       %[fTmp6]                \n\t"
         "sub.s      %[val10],       %[fTmp5],       %[fTmp6]                \n\t"
-        "mul.s      %[fTmp8],       %[fTmp8],       %[fTmp10]               \n\t"
-        "li.s       %[fTmp1],       0.89997622313641570463                  \n\t"
+        "mul.s      %[fTmp8],       %[fTmp8],       %[f5]                   \n\t"
         "sub.s      %[fTmp2],       %[val2],        %[val5]                 \n\t"
-        "mul.s      %[val10],       %[val10],       %[fTmp7]                \n\t"
-        "madd.s     %[val21],       %[fTmp8],       %[fTmp9],   %[fTmp11]   \n\t"
-        "nmsub.s    %[val26],       %[fTmp8],       %[fTmp9],   %[fTmp11]   \n\t"
+        "mul.s      %[val10],       %[val10],       %[f4]                   \n\t"
+        "madd.s     %[val21],       %[fTmp8],       %[fTmp9],   %[f6]       \n\t"
+        "nmsub.s    %[val26],       %[fTmp8],       %[fTmp9],   %[f6]       \n\t"
         "add.s      %[val2],        %[val2],        %[val5]                 \n\t"
-        "mul.s      %[val5],        %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val5],        %[f7],          %[fTmp2]                \n\t"
         "sub.s      %[fTmp3],       %[val13],       %[val10]                \n\t"
         "add.s      %[val10],       %[val13],       %[val10]                \n\t"
-        "mul.s      %[val26],       %[val26],       %[fTmp7]                \n\t"
+        "mul.s      %[val26],       %[val26],       %[f4]                   \n\t"
         "sub.s      %[fTmp4],       %[val18],       %[val21]                \n\t"
         "add.s      %[val18],       %[val18],       %[val21]                \n\t"
-        "mul.s      %[val13],       %[fTmp1],       %[fTmp3]                \n\t"
+        "mul.s      %[val13],       %[f7],          %[fTmp3]                \n\t"
         "sub.s      %[fTmp2],       %[val29],       %[val26]                \n\t"
         "add.s      %[val26],       %[val29],       %[val26]                \n\t"
-        "mul.s      %[val21],       %[fTmp1],       %[fTmp4]                \n\t"
-        "mul.s      %[val29],       %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val21],       %[f7],          %[fTmp4]                \n\t"
+        "mul.s      %[val29],       %[f7],          %[fTmp2]                \n\t"
 
         : [fTmp1]  "=&f" (fTmp1),  [fTmp2]  "=&f" (fTmp2), [fTmp3] "=&f" (fTmp3),
           [fTmp4]  "=&f" (fTmp4),  [fTmp5]  "=&f" (fTmp5), [fTmp6] "=&f" (fTmp6),
-          [fTmp7]  "=&f" (fTmp7),  [fTmp8]  "=&f" (fTmp8), [fTmp9] "=&f" (fTmp9),
-          [fTmp10] "=&f" (fTmp10), [fTmp11] "=&f" (fTmp11),
-          [val2]  "=f" (val2),  [val5]  "=f" (val5),
-          [val10] "=f" (val10), [val13] "=f" (val13),
-          [val18] "=f" (val18), [val21] "=f" (val21),
-          [val26] "=f" (val26), [val29] "=f" (val29)
-        : [tab] "r" (tab)
+          [fTmp8]  "=&f" (fTmp8), [fTmp9] "=&f" (fTmp9),
+          [val2]  "=&f" (val2),  [val5]  "=&f" (val5),
+          [val10] "=&f" (val10), [val13] "=&f" (val13),
+          [val18] "=&f" (val18), [val21] "=&f" (val21),
+          [val26] "=&f" (val26), [val29] "=&f" (val29)
+        : [tab] "r" (tab), [f1]"f"(f1), [f2]"f"(f2), [f3]"f"(f3),
+          [f4]"f"(f4), [f5]"f"(f5), [f6]"f"(f6), [f7]"f"(f7)
         : "memory"
     );
 
+    f1 = 1.30656296487637652785;
     __asm__ volatile (
-        "li.s       %[fTmp1],       1.30656296487637652785                  \n\t"
         "sub.s      %[fTmp2],       %[val1],        %[val2]                 \n\t"
         "add.s      %[val1],        %[val1],        %[val2]                 \n\t"
         "sub.s      %[fTmp3],       %[val6],        %[val5]                 \n\t"
         "add.s      %[val5],        %[val6],        %[val5]                 \n\t"
         "sub.s      %[fTmp4],       %[val9],        %[val10]                \n\t"
-        "mul.s      %[val2],        %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val2],        %[f1],          %[fTmp2]                \n\t"
         "add.s      %[val9],        %[val9],        %[val10]                \n\t"
-        "mul.s      %[val6],        %[fTmp1],       %[fTmp3]                \n\t"
+        "mul.s      %[val6],        %[f1],          %[fTmp3]                \n\t"
         "sub.s      %[fTmp2],       %[val14],       %[val13]                \n\t"
-        "mul.s      %[val10],       %[fTmp1],       %[fTmp4]                \n\t"
+        "mul.s      %[val10],       %[f1],          %[fTmp4]                \n\t"
         "add.s      %[val13],       %[val14],       %[val13]                \n\t"
-        "mul.s      %[val14],       %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val14],       %[f1],          %[fTmp2]                \n\t"
 
-        : [fTmp1] "=f"  (fTmp1), [fTmp2] "=&f" (fTmp2),
+        : [fTmp2] "=&f" (fTmp2),
           [fTmp3] "=&f" (fTmp3), [fTmp4] "=&f" (fTmp4),
-          [val1]  "+f" (val1),  [val2]  "+f" (val2),
-          [val5]  "+f" (val5),  [val6]  "+f" (val6),
-          [val9]  "+f" (val9),  [val10] "+f" (val10),
-          [val13] "+f" (val13), [val14] "+f" (val14)
-        :
+          [val1]  "+&f" (val1),  [val2]  "+&f" (val2),
+          [val5]  "+&f" (val5),  [val6]  "+&f" (val6),
+          [val9]  "+&f" (val9),  [val10] "+&f" (val10),
+          [val13] "+&f" (val13), [val14] "+&f" (val14)
+        : [f1]"f"(f1)
     );
 
     __asm__ volatile (
@@ -620,39 +620,39 @@ static void ff_dct32_mips_float(float *out, const float *tab)
         "sub.s      %[fTmp3],       %[val22],       %[val21]                \n\t"
         "add.s      %[val21],       %[val22],       %[val21]                \n\t"
         "sub.s      %[fTmp4],       %[val25],       %[val26]                \n\t"
-        "mul.s      %[val18],       %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val18],       %[f1],          %[fTmp2]                \n\t"
         "add.s      %[val25],       %[val25],       %[val26]                \n\t"
-        "mul.s      %[val22],       %[fTmp1],       %[fTmp3]                \n\t"
+        "mul.s      %[val22],       %[f1],          %[fTmp3]                \n\t"
         "sub.s      %[fTmp2],       %[val30],       %[val29]                \n\t"
-        "mul.s      %[val26],       %[fTmp1],       %[fTmp4]                \n\t"
+        "mul.s      %[val26],       %[f1],          %[fTmp4]                \n\t"
         "add.s      %[val29],       %[val30],       %[val29]                \n\t"
-        "mul.s      %[val30],       %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val30],       %[f1],          %[fTmp2]                \n\t"
 
         : [fTmp2] "=&f" (fTmp2), [fTmp3] "=&f" (fTmp3), [fTmp4] "=&f" (fTmp4),
-          [val17] "+f" (val17), [val18] "+f" (val18), [val21] "+f" (val21),
-          [val22] "+f" (val22), [val25] "+f" (val25), [val26] "+f" (val26),
-          [val29] "+f" (val29), [val30] "+f" (val30)
-        : [fTmp1] "f" (fTmp1)
+          [val17] "+&f" (val17), [val18] "+&f" (val18), [val21] "+&f" (val21),
+          [val22] "+&f" (val22), [val25] "+&f" (val25), [val26] "+&f" (val26),
+          [val29] "+&f" (val29), [val30] "+&f" (val30)
+        : [f1] "f" (f1)
     );
 
+    f1 = 0.70710678118654752439;
     __asm__ volatile (
-        "li.s       %[fTmp1],       0.70710678118654752439                  \n\t"
         "sub.s      %[fTmp2],       %[val0],        %[val1]                 \n\t"
         "add.s      %[val0],        %[val0],        %[val1]                 \n\t"
         "sub.s      %[fTmp3],       %[val3],        %[val2]                 \n\t"
         "add.s      %[val2],        %[val3],        %[val2]                 \n\t"
         "sub.s      %[fTmp4],       %[val4],        %[val5]                 \n\t"
-        "mul.s      %[val1],        %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val1],        %[f1],          %[fTmp2]                \n\t"
         "swc1       %[val0],        0(%[out])                               \n\t"
-        "mul.s      %[val3],        %[fTmp3],       %[fTmp1]                \n\t"
+        "mul.s      %[val3],        %[fTmp3],       %[f1]                   \n\t"
         "add.s      %[val4],        %[val4],        %[val5]                 \n\t"
-        "mul.s      %[val5],        %[fTmp1],       %[fTmp4]                \n\t"
+        "mul.s      %[val5],        %[f1],          %[fTmp4]                \n\t"
         "swc1       %[val1],        16*4(%[out])                            \n\t"
         "sub.s      %[fTmp2],       %[val7],        %[val6]                 \n\t"
         "add.s      %[val2],        %[val2],        %[val3]                 \n\t"
         "swc1       %[val3],        24*4(%[out])                            \n\t"
         "add.s      %[val6],        %[val7],        %[val6]                 \n\t"
-        "mul.s      %[val7],        %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val7],        %[f1],          %[fTmp2]                \n\t"
         "swc1       %[val2],        8*4(%[out])                             \n\t"
         "add.s      %[val6],        %[val6],        %[val7]                 \n\t"
         "swc1       %[val7],        28*4(%[out])                            \n\t"
@@ -663,13 +663,13 @@ static void ff_dct32_mips_float(float *out, const float *tab)
         "swc1       %[val5],        20*4(%[out])                            \n\t"
         "swc1       %[val6],        12*4(%[out])                            \n\t"
 
-        : [fTmp1] "=f"  (fTmp1), [fTmp2] "=&f" (fTmp2),
+        : [fTmp2] "=&f" (fTmp2),
           [fTmp3] "=&f" (fTmp3), [fTmp4] "=&f" (fTmp4),
-          [val0] "+f" (val0), [val1] "+f" (val1),
-          [val2] "+f" (val2), [val3] "+f" (val3),
-          [val4] "+f" (val4), [val5] "+f" (val5),
-          [val6] "+f" (val6), [val7] "+f" (val7)
-        : [out] "r" (out)
+          [val0] "+&f" (val0), [val1] "+&f" (val1),
+          [val2] "+&f" (val2), [val3] "+&f" (val3),
+          [val4] "+&f" (val4), [val5] "+&f" (val5),
+          [val6] "+&f" (val6), [val7] "+&f" (val7)
+        : [out] "r" (out), [f1]"f"(f1)
     );
 
     __asm__ volatile (
@@ -678,14 +678,14 @@ static void ff_dct32_mips_float(float *out, const float *tab)
         "sub.s      %[fTmp3],       %[val11],       %[val10]                \n\t"
         "add.s      %[val10],       %[val11],       %[val10]                \n\t"
         "sub.s      %[fTmp4],       %[val12],       %[val13]                \n\t"
-        "mul.s      %[val9],        %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val9],        %[f1],          %[fTmp2]                \n\t"
         "add.s      %[val12],       %[val12],       %[val13]                \n\t"
-        "mul.s      %[val11],       %[fTmp1],       %[fTmp3]                \n\t"
+        "mul.s      %[val11],       %[f1],          %[fTmp3]                \n\t"
         "sub.s      %[fTmp2],       %[val15],       %[val14]                \n\t"
-        "mul.s      %[val13],       %[fTmp1],       %[fTmp4]                \n\t"
+        "mul.s      %[val13],       %[f1],          %[fTmp4]                \n\t"
         "add.s      %[val14],       %[val15],       %[val14]                \n\t"
         "add.s      %[val10],       %[val10],       %[val11]                \n\t"
-        "mul.s      %[val15],       %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val15],       %[f1],          %[fTmp2]                \n\t"
         "add.s      %[val14],       %[val14],       %[val15]                \n\t"
         "add.s      %[val12],       %[val12],       %[val14]                \n\t"
         "add.s      %[val14],       %[val14],       %[val13]                \n\t"
@@ -707,10 +707,10 @@ static void ff_dct32_mips_float(float *out, const float *tab)
         "swc1       %[val15],       30*4(%[out])                            \n\t"
 
         : [fTmp2] "=&f" (fTmp2), [fTmp3] "=&f" (fTmp3), [fTmp4] "=&f" (fTmp4),
-          [val8]  "+f" (val8),  [val9]  "+f" (val9),  [val10] "+f" (val10),
-          [val11] "+f" (val11), [val12] "+f" (val12), [val13] "+f" (val13),
-          [val14] "+f" (val14), [val15] "+f" (val15)
-        : [fTmp1] "f" (fTmp1), [out] "r" (out)
+          [val8]  "+&f" (val8),  [val9]  "+&f" (val9),  [val10] "+&f" (val10),
+          [val11] "+&f" (val11), [val12] "+&f" (val12), [val13] "+&f" (val13),
+          [val14] "+&f" (val14), [val15] "+&f" (val15)
+        : [f1] "f" (f1), [out] "r" (out)
     );
 
     __asm__ volatile (
@@ -719,24 +719,24 @@ static void ff_dct32_mips_float(float *out, const float *tab)
         "sub.s      %[fTmp3],       %[val19],       %[val18]                \n\t"
         "add.s      %[val18],       %[val19],       %[val18]                \n\t"
         "sub.s      %[fTmp4],       %[val20],       %[val21]                \n\t"
-        "mul.s      %[val17],       %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val17],       %[f1],          %[fTmp2]                \n\t"
         "add.s      %[val20],       %[val20],       %[val21]                \n\t"
-        "mul.s      %[val19],       %[fTmp1],       %[fTmp3]                \n\t"
+        "mul.s      %[val19],       %[f1],          %[fTmp3]                \n\t"
         "sub.s      %[fTmp2],       %[val23],       %[val22]                \n\t"
-        "mul.s      %[val21],       %[fTmp1],       %[fTmp4]                \n\t"
+        "mul.s      %[val21],       %[f1],          %[fTmp4]                \n\t"
         "add.s      %[val22],       %[val23],       %[val22]                \n\t"
         "add.s      %[val18],       %[val18],       %[val19]                \n\t"
-        "mul.s      %[val23],       %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val23],       %[f1],          %[fTmp2]                \n\t"
         "add.s      %[val22],       %[val22],       %[val23]                \n\t"
         "add.s      %[val20],       %[val20],       %[val22]                \n\t"
         "add.s      %[val22],       %[val22],       %[val21]                \n\t"
         "add.s      %[val21],       %[val21],       %[val23]                \n\t"
 
         : [fTmp2] "=&f" (fTmp2), [fTmp3] "=&f" (fTmp3), [fTmp4] "=&f" (fTmp4),
-          [val16] "+f" (val16), [val17] "+f" (val17), [val18] "+f" (val18),
-          [val19] "+f" (val19), [val20] "+f" (val20), [val21] "+f" (val21),
-          [val22] "+f" (val22), [val23] "+f" (val23)
-        : [fTmp1] "f" (fTmp1)
+          [val16] "+&f" (val16), [val17] "+&f" (val17), [val18] "+&f" (val18),
+          [val19] "+&f" (val19), [val20] "+&f" (val20), [val21] "+&f" (val21),
+          [val22] "+&f" (val22), [val23] "+&f" (val23)
+        : [f1] "f" (f1)
     );
 
     __asm__ volatile (
@@ -745,14 +745,14 @@ static void ff_dct32_mips_float(float *out, const float *tab)
         "sub.s      %[fTmp3],       %[val27],       %[val26]                \n\t"
         "add.s      %[val26],       %[val27],       %[val26]                \n\t"
         "sub.s      %[fTmp4],       %[val28],       %[val29]                \n\t"
-        "mul.s      %[val25],       %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val25],       %[f1],          %[fTmp2]                \n\t"
         "add.s      %[val28],       %[val28],       %[val29]                \n\t"
-        "mul.s      %[val27],       %[fTmp1],       %[fTmp3]                \n\t"
+        "mul.s      %[val27],       %[f1],          %[fTmp3]                \n\t"
         "sub.s      %[fTmp2],       %[val31],       %[val30]                \n\t"
-        "mul.s      %[val29],       %[fTmp1],       %[fTmp4]                \n\t"
+        "mul.s      %[val29],       %[f1],          %[fTmp4]                \n\t"
         "add.s      %[val30],       %[val31],       %[val30]                \n\t"
         "add.s      %[val26],       %[val26],       %[val27]                \n\t"
-        "mul.s      %[val31],       %[fTmp1],       %[fTmp2]                \n\t"
+        "mul.s      %[val31],       %[f1],          %[fTmp2]                \n\t"
         "add.s      %[val30],       %[val30],       %[val31]                \n\t"
         "add.s      %[val28],       %[val28],       %[val30]                \n\t"
         "add.s      %[val30],       %[val30],       %[val29]                \n\t"
@@ -766,10 +766,10 @@ static void ff_dct32_mips_float(float *out, const float *tab)
         "add.s      %[val27],       %[val27],       %[val31]                \n\t"
 
         : [fTmp2] "=&f" (fTmp2), [fTmp3] "=&f" (fTmp3), [fTmp4] "=&f" (fTmp4),
-          [val24] "+f" (val24), [val25] "+f" (val25), [val26] "+f" (val26),
-          [val27] "+f" (val27), [val28] "+f" (val28), [val29] "+f" (val29),
-          [val30] "+f" (val30), [val31] "+f" (val31)
-        : [fTmp1] "f" (fTmp1)
+          [val24] "+&f" (val24), [val25] "+&f" (val25), [val26] "+&f" (val26),
+          [val27] "+&f" (val27), [val28] "+&f" (val28), [val29] "+&f" (val29),
+          [val30] "+&f" (val30), [val31] "+&f" (val31)
+        : [f1] "f" (f1)
     );
 
     out[ 1] = val16 + val24;
@@ -797,7 +797,7 @@ static void imdct36_mips_float(float *out, float *buf, float *in, float *win)
     /* temporary variables */
     float in1, in2, in3, in4, in5, in6;
     float out1, out2, out3, out4, out5;
-    float c1, c2, c3, c4, c5, c6, c7, c8, c9;
+    float f1, f2, f3, f4, f5, f6, f7, f8, f9;
 
     /**
     * all loops are unrolled totally, and instructions are scheduled to
@@ -881,33 +881,36 @@ static void imdct36_mips_float(float *out, float *buf, float *in, float *win)
     );
 
     /* loop 3 */
+    f1 = 0.5;
+    f2 = 0.93969262078590838405;
+    f3 = -0.76604444311897803520;
+    f4 = -0.17364817766693034885;
+    f5 = -0.86602540378443864676;
+    f6 = 0.98480775301220805936;
+    f7 = -0.34202014332566873304;
+    f8 = 0.86602540378443864676;
+    f9 = -0.64278760968653932632;
     __asm__ volatile (
-        "li.s    %[c1],   0.5                                           \t\n"
         "lwc1    %[in1],  8*4(%[in])                                    \t\n"
         "lwc1    %[in2],  16*4(%[in])                                   \t\n"
         "lwc1    %[in3],  4*4(%[in])                                    \t\n"
         "lwc1    %[in4],  0(%[in])                                      \t\n"
         "lwc1    %[in5],  12*4(%[in])                                   \t\n"
-        "li.s    %[c2],   0.93969262078590838405                        \t\n"
         "add.s   %[t2],   %[in1],  %[in2]                               \t\n"
         "add.s   %[t0],   %[in1],  %[in3]                               \t\n"
-        "li.s    %[c3],   -0.76604444311897803520                       \t\n"
-        "madd.s  %[t3],   %[in4],  %[in5], %[c1]                        \t\n"
+        "madd.s  %[t3],   %[in4],  %[in5], %[f1]                        \t\n"
         "sub.s   %[t1],   %[in4],  %[in5]                               \t\n"
         "sub.s   %[t2],   %[t2],   %[in3]                               \t\n"
-        "mul.s   %[t0],   %[t0],   %[c2]                                \t\n"
-        "li.s    %[c4],   -0.17364817766693034885                       \t\n"
-        "li.s    %[c5],   -0.86602540378443864676                       \t\n"
-        "li.s    %[c6],   0.98480775301220805936                        \t\n"
-        "nmsub.s %[out1], %[t1],   %[t2],  %[c1]                        \t\n"
+        "mul.s   %[t0],   %[t0],   %[f2]                                \t\n"
+        "nmsub.s %[out1], %[t1],   %[t2],  %[f1]                        \t\n"
         "add.s   %[out2], %[t1],   %[t2]                                \t\n"
         "add.s   %[t2],   %[in2],  %[in3]                               \t\n"
         "sub.s   %[t1],   %[in1],  %[in2]                               \t\n"
         "sub.s   %[out3], %[t3],   %[t0]                                \t\n"
         "swc1    %[out1], 6*4(%[tmp])                                   \t\n"
         "swc1    %[out2], 16*4(%[tmp])                                  \t\n"
-        "mul.s   %[t2],   %[t2],   %[c3]                                \t\n"
-        "mul.s   %[t1],   %[t1],   %[c4]                                \t\n"
+        "mul.s   %[t2],   %[t2],   %[f3]                                \t\n"
+        "mul.s   %[t1],   %[t1],   %[f4]                                \t\n"
         "add.s   %[out1], %[t3],   %[t0]                                \t\n"
         "lwc1    %[in1],  10*4(%[in])                                   \t\n"
         "lwc1    %[in2],  14*4(%[in])                                   \t\n"
@@ -923,19 +926,16 @@ static void imdct36_mips_float(float *out, float *buf, float *in, float *win)
         "add.s   %[t2],   %[in1],  %[in3]                               \t\n"
         "sub.s   %[t3],   %[in1],  %[in2]                               \t\n"
         "swc1    %[out2], 14*4(%[tmp])                                  \t\n"
-        "li.s    %[c7],   -0.34202014332566873304                       \t\n"
         "sub.s   %[out1], %[out1], %[in3]                               \t\n"
-        "mul.s   %[t2],   %[t2],   %[c6]                                \t\n"
-        "mul.s   %[t3],   %[t3],   %[c7]                                \t\n"
-        "li.s    %[c8],   0.86602540378443864676                        \t\n"
-        "mul.s   %[t0],   %[in4],  %[c8]                                \t\n"
-        "mul.s   %[out1], %[out1], %[c5]                                \t\n"
+        "mul.s   %[t2],   %[t2],   %[f6]                                \t\n"
+        "mul.s   %[t3],   %[t3],   %[f7]                                \t\n"
+        "mul.s   %[t0],   %[in4],  %[f8]                                \t\n"
+        "mul.s   %[out1], %[out1], %[f5]                                \t\n"
         "add.s   %[t1],   %[in2],  %[in3]                               \t\n"
-        "li.s    %[c9],   -0.64278760968653932632                       \t\n"
         "add.s   %[out2], %[t2],   %[t3]                                \t\n"
         "lwc1    %[in1],  9*4(%[in])                                    \t\n"
         "swc1    %[out1], 4*4(%[tmp])                                   \t\n"
-        "mul.s   %[t1],   %[t1],   %[c9]                                \t\n"
+        "mul.s   %[t1],   %[t1],   %[f9]                                \t\n"
         "lwc1    %[in2],  17*4(%[in])                                   \t\n"
         "add.s   %[out2], %[out2], %[t0]                                \t\n"
         "lwc1    %[in3],  5*4(%[in])                                    \t\n"
@@ -948,21 +948,21 @@ static void imdct36_mips_float(float *out, float *buf, float *in, float *win)
         "sub.s   %[out3], %[out3], %[t0]                                \t\n"
         "sub.s   %[out1], %[out1], %[t0]                                \t\n"
         "add.s   %[t0],   %[in1],  %[in3]                               \t\n"
-        "madd.s  %[t3],   %[in4],  %[in5], %[c1]                        \t\n"
+        "madd.s  %[t3],   %[in4],  %[in5], %[f1]                        \t\n"
         "sub.s   %[t2],   %[t2],   %[in3]                               \t\n"
         "swc1    %[out3], 12*4(%[tmp])                                  \t\n"
         "swc1    %[out1], 8*4(%[tmp])                                   \t\n"
         "sub.s   %[t1],   %[in4],  %[in5]                               \t\n"
-        "mul.s   %[t0],   %[t0],   %[c2]                                \t\n"
-        "nmsub.s %[out1], %[t1],   %[t2],  %[c1]                        \t\n"
+        "mul.s   %[t0],   %[t0],   %[f2]                                \t\n"
+        "nmsub.s %[out1], %[t1],   %[t2],  %[f1]                        \t\n"
         "add.s   %[out2], %[t1],   %[t2]                                \t\n"
         "add.s   %[t2],   %[in2],  %[in3]                               \t\n"
         "sub.s   %[t1],   %[in1],  %[in2]                               \t\n"
         "sub.s   %[out3], %[t3],   %[t0]                                \t\n"
         "swc1    %[out1], 7*4(%[tmp])                                   \t\n"
         "swc1    %[out2], 17*4(%[tmp])                                  \t\n"
-        "mul.s   %[t2],   %[t2],   %[c3]                                \t\n"
-        "mul.s   %[t1],   %[t1],   %[c4]                                \t\n"
+        "mul.s   %[t2],   %[t2],   %[f3]                                \t\n"
+        "mul.s   %[t1],   %[t1],   %[f4]                                \t\n"
         "add.s   %[out1], %[t3],   %[t0]                                \t\n"
         "lwc1    %[in1],  11*4(%[in])                                   \t\n"
         "lwc1    %[in2],  15*4(%[in])                                   \t\n"
@@ -978,14 +978,14 @@ static void imdct36_mips_float(float *out, float *buf, float *in, float *win)
         "add.s   %[t2],   %[in1],  %[in3]                               \t\n"
         "sub.s   %[t3],   %[in1],  %[in2]                               \t\n"
         "swc1    %[out2], 15*4(%[tmp])                                  \t\n"
-        "mul.s   %[t0],   %[in4],  %[c8]                                \t\n"
+        "mul.s   %[t0],   %[in4],  %[f8]                                \t\n"
         "sub.s   %[out3], %[out3], %[in3]                               \t\n"
-        "mul.s   %[t2],   %[t2],   %[c6]                                \t\n"
-        "mul.s   %[t3],   %[t3],   %[c7]                                \t\n"
+        "mul.s   %[t2],   %[t2],   %[f6]                                \t\n"
+        "mul.s   %[t3],   %[t3],   %[f7]                                \t\n"
         "add.s   %[t1],   %[in2],  %[in3]                               \t\n"
-        "mul.s   %[out3], %[out3], %[c5]                                \t\n"
+        "mul.s   %[out3], %[out3], %[f5]                                \t\n"
         "add.s   %[out1], %[t2],   %[t3]                                \t\n"
-        "mul.s   %[t1],   %[t1],   %[c9]                                \t\n"
+        "mul.s   %[t1],   %[t1],   %[f9]                                \t\n"
         "swc1    %[out3], 5*4(%[tmp])                                   \t\n"
         "add.s   %[out1], %[out1], %[t0]                                \t\n"
         "add.s   %[out2], %[t2],   %[t1]                                \t\n"
@@ -1000,26 +1000,29 @@ static void imdct36_mips_float(float *out, float *buf, float *in, float *win)
           [t2] "=&f" (t2), [t3] "=&f" (t3),
           [in1] "=&f" (in1), [in2] "=&f" (in2),
           [in3] "=&f" (in3), [in4] "=&f" (in4),
-          [in5] "=&f" (in5),
-          [out1] "=&f" (out1), [out2] "=&f" (out2),
-          [out3] "=&f" (out3),
-          [c1] "=&f" (c1), [c2] "=&f" (c2),
-          [c3] "=&f" (c3), [c4] "=&f" (c4),
-          [c5] "=&f" (c5), [c6] "=&f" (c6),
-          [c7] "=&f" (c7), [c8] "=&f" (c8),
-          [c9] "=&f" (c9)
-        : [in] "r" (in), [tmp] "r" (tmp)
+          [in5] "=&f" (in5), [out1] "=&f" (out1),
+          [out2] "=&f" (out2), [out3] "=&f" (out3)
+        : [in] "r" (in), [tmp] "r" (tmp), [f1]"f"(f1), [f2]"f"(f2),
+          [f3]"f"(f3), [f4]"f"(f4), [f5]"f"(f5), [f6]"f"(f6),
+          [f7]"f"(f7), [f8]"f"(f8), [f9]"f"(f9)
         : "memory"
     );
 
     /* loop 4 */
+    f1 = 0.50190991877167369479;
+    f2 = 5.73685662283492756461;
+    f3 = 0.51763809020504152469;
+    f4 = 1.93185165257813657349;
+    f5 = 0.55168895948124587824;
+    f6 = 1.18310079157624925896;
+    f7 = 0.61038729438072803416;
+    f8 = 0.87172339781054900991;
+    f9 = 0.70710678118654752439;
     __asm__ volatile (
         "lwc1   %[in1],  2*4(%[tmp])                                    \t\n"
         "lwc1   %[in2],  0(%[tmp])                                      \t\n"
         "lwc1   %[in3],  3*4(%[tmp])                                    \t\n"
         "lwc1   %[in4],  1*4(%[tmp])                                    \t\n"
-        "li.s   %[c1],   0.50190991877167369479                         \t\n"
-        "li.s   %[c2],   5.73685662283492756461                         \t\n"
         "add.s  %[s0],   %[in1], %[in2]                                 \t\n"
         "sub.s  %[s2],   %[in1], %[in2]                                 \t\n"
         "add.s  %[s1],   %[in3], %[in4]                                 \t\n"
@@ -1027,15 +1030,13 @@ static void imdct36_mips_float(float *out, float *buf, float *in, float *win)
         "lwc1   %[in1],  9*4(%[win])                                    \t\n"
         "lwc1   %[in2],  4*9*4(%[buf])                                  \t\n"
         "lwc1   %[in3],  8*4(%[win])                                    \t\n"
-        "mul.s  %[s1],   %[s1],  %[c1]                                  \t\n"
-        "mul.s  %[s3],   %[s3],  %[c2]                                  \t\n"
+        "mul.s  %[s1],   %[s1],  %[f1]                                  \t\n"
+        "mul.s  %[s3],   %[s3],  %[f2]                                  \t\n"
         "lwc1   %[in4],  4*8*4(%[buf])                                  \t\n"
         "lwc1   %[in5],  29*4(%[win])                                   \t\n"
         "lwc1   %[in6],  28*4(%[win])                                   \t\n"
         "add.s  %[t0],   %[s0],  %[s1]                                  \t\n"
         "sub.s  %[t1],   %[s0],  %[s1]                                  \t\n"
-        "li.s   %[c1],   0.51763809020504152469                         \t\n"
-        "li.s   %[c2],   1.93185165257813657349                         \t\n"
         "mul.s  %[out3], %[in5], %[t0]                                  \t\n"
         "madd.s %[out1], %[in2], %[in1], %[t1]                          \t\n"
         "madd.s %[out2], %[in4], %[in3], %[t1]                          \t\n"
@@ -1071,14 +1072,13 @@ static void imdct36_mips_float(float *out, float *buf, float *in, float *win)
         "lwc1   %[in1],  10*4(%[win])                                   \t\n"
         "lwc1   %[in2],  4*10*4(%[buf])                                 \t\n"
         "lwc1   %[in3],  7*4(%[win])                                    \t\n"
-        "mul.s  %[s1],   %[s1],  %[c1]                                  \t\n"
-        "mul.s  %[s3],   %[s3],  %[c2]                                  \t\n"
+        "mul.s  %[s1],   %[s1],  %[f3]                                  \t\n"
+        "mul.s  %[s3],   %[s3],  %[f4]                                  \t\n"
         "add.s  %[t0],   %[s0],  %[s1]                                  \t\n"
         "sub.s  %[t1],   %[s0],  %[s1]                                  \t\n"
         "lwc1   %[in4],  4*7*4(%[buf])                                  \t\n"
         "lwc1   %[in5],  30*4(%[win])                                   \t\n"
         "lwc1   %[in6],  27*4(%[win])                                   \t\n"
-        "li.s   %[c1],   0.55168895948124587824                         \t\n"
         "madd.s %[out1], %[in2], %[in1], %[t1]                          \t\n"
         "madd.s %[out2], %[in4], %[in3], %[t1]                          \t\n"
         "mul.s  %[out3], %[t0],  %[in5]                                 \t\n"
@@ -1105,7 +1105,6 @@ static void imdct36_mips_float(float *out, float *buf, float *in, float *win)
         "swc1   %[out2], 32*4(%[out])                                   \t\n"
         "swc1   %[out3], 4*16*4(%[buf])                                 \t\n"
         "swc1   %[out4], 4*1*4(%[buf])                                  \t\n"
-        "li.s   %[c2],   1.18310079157624925896                         \t\n"
         "add.s  %[s0],   %[in1], %[in2]                                 \t\n"
         "sub.s  %[s2],   %[in1], %[in2]                                 \t\n"
         "lwc1   %[in3],  11*4(%[tmp])                                   \t\n"
@@ -1115,8 +1114,8 @@ static void imdct36_mips_float(float *out, float *buf, float *in, float *win)
         "lwc1   %[in1],  11*4(%[win])                                   \t\n"
         "lwc1   %[in2],  4*11*4(%[buf])                                 \t\n"
         "lwc1   %[in3],  6*4(%[win])                                    \t\n"
-        "mul.s  %[s1],   %[s1],  %[c1]                                  \t\n"
-        "mul.s  %[s3],   %[s3],  %[c2]                                  \t\n"
+        "mul.s  %[s1],   %[s1],  %[f5]                                  \t\n"
+        "mul.s  %[s3],   %[s3],  %[f6]                                  \t\n"
         "lwc1   %[in4],  4*6*4(%[buf])                                  \t\n"
         "lwc1   %[in5],  31*4(%[win])                                   \t\n"
         "lwc1   %[in6],  26*4(%[win])                                   \t\n"
@@ -1152,15 +1151,13 @@ static void imdct36_mips_float(float *out, float *buf, float *in, float *win)
         "add.s  %[s0],   %[in1], %[in2]                                 \t\n"
         "sub.s  %[s2],   %[in1], %[in2]                                 \t\n"
         "lwc1   %[in4],  13*4(%[tmp])                                   \t\n"
-        "li.s   %[c1],   0.61038729438072803416                         \t\n"
-        "li.s   %[c2],   0.87172339781054900991                         \t\n"
         "add.s  %[s1],   %[in3], %[in4]                                 \t\n"
         "sub.s  %[s3],   %[in3], %[in4]                                 \t\n"
         "lwc1   %[in1],  12*4(%[win])                                   \t\n"
         "lwc1   %[in2],  4*12*4(%[buf])                                 \t\n"
         "lwc1   %[in3],  5*4(%[win])                                    \t\n"
-        "mul.s  %[s1],   %[s1],  %[c1]                                  \t\n"
-        "mul.s  %[s3],   %[s3],  %[c2]                                  \t\n"
+        "mul.s  %[s1],   %[s1],  %[f7]                                  \t\n"
+        "mul.s  %[s3],   %[s3],  %[f8]                                  \t\n"
         "lwc1   %[in4],  4*5*4(%[buf])                                  \t\n"
         "lwc1   %[in5],  32*4(%[win])                                   \t\n"
         "lwc1   %[in6],  25*4(%[win])                                   \t\n"
@@ -1168,7 +1165,6 @@ static void imdct36_mips_float(float *out, float *buf, float *in, float *win)
         "sub.s  %[t1],   %[s0],  %[s1]                                  \t\n"
         "lwc1   %[s0],   16*4(%[tmp])                                   \t\n"
         "lwc1   %[s1],   17*4(%[tmp])                                   \t\n"
-        "li.s   %[c1],   0.70710678118654752439                         \t\n"
         "mul.s  %[out3], %[t0],  %[in5]                                 \t\n"
         "madd.s %[out1], %[in2], %[in1], %[t1]                          \t\n"
         "madd.s %[out2], %[in4], %[in3], %[t1]                          \t\n"
@@ -1186,7 +1182,7 @@ static void imdct36_mips_float(float *out, float *buf, float *in, float *win)
         "lwc1   %[in5],  34*4(%[win])                                   \t\n"
         "lwc1   %[in6],  23*4(%[win])                                   \t\n"
         "madd.s %[out1], %[in2], %[in1], %[t1]                          \t\n"
-        "mul.s  %[s1],   %[s1],  %[c1]                                  \t\n"
+        "mul.s  %[s1],   %[s1],  %[f9]                                  \t\n"
         "madd.s %[out2], %[in4], %[in3], %[t1]                          \t\n"
         "mul.s  %[out3], %[in5], %[t0]                                  \t\n"
         "mul.s  %[out4], %[in6], %[t0]                                  \t\n"
@@ -1211,18 +1207,18 @@ static void imdct36_mips_float(float *out, float *buf, float *in, float *win)
         "swc1   %[out3], 4*13*4(%[buf])                                 \t\n"
         "swc1   %[out4], 4*4*4(%[buf])                                  \t\n"
 
-        : [c1] "=&f" (c1), [c2] "=&f" (c2),
-          [in1] "=&f" (in1), [in2] "=&f" (in2),
+        : [in1] "=&f" (in1), [in2] "=&f" (in2),
           [in3] "=&f" (in3), [in4] "=&f" (in4),
           [in5] "=&f" (in5), [in6] "=&f" (in6),
           [out1] "=&f" (out1), [out2] "=&f" (out2),
           [out3] "=&f" (out3), [out4] "=&f" (out4),
           [t0] "=&f" (t0), [t1] "=&f" (t1),
-          [t2] "=&f" (t2), [t3] "=&f" (t3),
           [s0] "=&f" (s0), [s1] "=&f" (s1),
           [s2] "=&f" (s2), [s3] "=&f" (s3)
         : [tmp] "r" (tmp), [win] "r" (win),
-          [buf] "r" (buf), [out] "r" (out)
+          [buf] "r" (buf), [out] "r" (out),
+          [f1]"f"(f1), [f2]"f"(f2), [f3]"f"(f3), [f4]"f"(f4),
+          [f5]"f"(f5), [f6]"f"(f6), [f7]"f"(f7), [f8]"f"(f8), [f9]"f"(f9)
         : "memory"
     );
 }
diff --git a/libavcodec/mips/mpegvideo_init_mips.c b/libavcodec/mips/mpegvideo_init_mips.c
index be77308140..bfda90bbcc 100644
--- a/libavcodec/mips/mpegvideo_init_mips.c
+++ b/libavcodec/mips/mpegvideo_init_mips.c
@@ -18,41 +18,31 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
+#include "libavutil/mips/cpu.h"
 #include "h263dsp_mips.h"
 #include "mpegvideo_mips.h"
 
-#if HAVE_MSA
-static av_cold void dct_unquantize_init_msa(MpegEncContext *s)
+av_cold void ff_mpv_common_init_mips(MpegEncContext *s)
 {
-    s->dct_unquantize_h263_intra = ff_dct_unquantize_h263_intra_msa;
-    s->dct_unquantize_h263_inter = ff_dct_unquantize_h263_inter_msa;
-    if (!s->q_scale_type)
-        s->dct_unquantize_mpeg2_inter = ff_dct_unquantize_mpeg2_inter_msa;
-}
-#endif  // #if HAVE_MSA
+    int cpu_flags = av_get_cpu_flags();
 
-#if HAVE_MMI
-static av_cold void dct_unquantize_init_mmi(MpegEncContext *s)
-{
-    s->dct_unquantize_h263_intra = ff_dct_unquantize_h263_intra_mmi;
-    s->dct_unquantize_h263_inter = ff_dct_unquantize_h263_inter_mmi;
-    s->dct_unquantize_mpeg1_intra = ff_dct_unquantize_mpeg1_intra_mmi;
-    s->dct_unquantize_mpeg1_inter = ff_dct_unquantize_mpeg1_inter_mmi;
+    if (have_mmi(cpu_flags)) {
+        s->dct_unquantize_h263_intra = ff_dct_unquantize_h263_intra_mmi;
+        s->dct_unquantize_h263_inter = ff_dct_unquantize_h263_inter_mmi;
+        s->dct_unquantize_mpeg1_intra = ff_dct_unquantize_mpeg1_intra_mmi;
+        s->dct_unquantize_mpeg1_inter = ff_dct_unquantize_mpeg1_inter_mmi;
 
-    if (!(s->avctx->flags & AV_CODEC_FLAG_BITEXACT))
-        if (!s->q_scale_type)
-            s->dct_unquantize_mpeg2_intra = ff_dct_unquantize_mpeg2_intra_mmi;
+        if (!(s->avctx->flags & AV_CODEC_FLAG_BITEXACT))
+            if (!s->q_scale_type)
+                s->dct_unquantize_mpeg2_intra = ff_dct_unquantize_mpeg2_intra_mmi;
 
-    s->denoise_dct= ff_denoise_dct_mmi;
-}
-#endif /* HAVE_MMI */
+        s->denoise_dct= ff_denoise_dct_mmi;
+    }
 
-av_cold void ff_mpv_common_init_mips(MpegEncContext *s)
-{
-#if HAVE_MMI
-    dct_unquantize_init_mmi(s);
-#endif /* HAVE_MMI */
-#if HAVE_MSA
-    dct_unquantize_init_msa(s);
-#endif  // #if HAVE_MSA
+    if (have_msa(cpu_flags)) {
+        s->dct_unquantize_h263_intra = ff_dct_unquantize_h263_intra_msa;
+        s->dct_unquantize_h263_inter = ff_dct_unquantize_h263_inter_msa;
+        if (!s->q_scale_type)
+            s->dct_unquantize_mpeg2_inter = ff_dct_unquantize_mpeg2_inter_msa;
+    }
 }
diff --git a/libavcodec/mips/mpegvideo_mmi.c b/libavcodec/mips/mpegvideo_mmi.c
index e4aba08661..3d5b5e20ab 100644
--- a/libavcodec/mips/mpegvideo_mmi.c
+++ b/libavcodec/mips/mpegvideo_mmi.c
@@ -28,12 +28,13 @@
 void ff_dct_unquantize_h263_intra_mmi(MpegEncContext *s, int16_t *block,
         int n, int qscale)
 {
-    int64_t level, qmul, qadd, nCoeffs;
+    int64_t level, nCoeffs;
     double ftmp[6];
     mips_reg addr[1];
+    union mmi_intfloat64 qmul_u, qadd_u;
     DECLARE_VAR_ALL64;
 
-    qmul = qscale << 1;
+    qmul_u.i = qscale << 1;
     av_assert2(s->block_last_index[n]>=0 || s->h263_aic);
 
     if (!s->h263_aic) {
@@ -41,9 +42,9 @@ void ff_dct_unquantize_h263_intra_mmi(MpegEncContext *s, int16_t *block,
             level = block[0] * s->y_dc_scale;
         else
             level = block[0] * s->c_dc_scale;
-        qadd = (qscale-1) | 1;
+        qadd_u.i = (qscale-1) | 1;
     } else {
-        qadd = 0;
+        qadd_u.i = 0;
         level = block[0];
     }
 
@@ -53,13 +54,13 @@ void ff_dct_unquantize_h263_intra_mmi(MpegEncContext *s, int16_t *block,
         nCoeffs = s->inter_scantable.raster_end[s->block_last_index[n]];
 
     __asm__ volatile (
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
         "packsswh   %[qmul],    %[qmul],        %[qmul]                 \n\t"
         "packsswh   %[qmul],    %[qmul],        %[qmul]                 \n\t"
         "packsswh   %[qadd],    %[qadd],        %[qadd]                 \n\t"
         "packsswh   %[qadd],    %[qadd],        %[qadd]                 \n\t"
         "psubh      %[ftmp0],   %[ftmp0],       %[qadd]                 \n\t"
-        "xor        %[ftmp5],   %[ftmp5],       %[ftmp5]                \n\t"
+        "pxor       %[ftmp5],   %[ftmp5],       %[ftmp5]                \n\t"
         ".p2align   4                                                   \n\t"
 
         "1:                                                             \n\t"
@@ -72,12 +73,12 @@ void ff_dct_unquantize_h263_intra_mmi(MpegEncContext *s, int16_t *block,
         "pmullh     %[ftmp2],   %[ftmp2],       %[qmul]                 \n\t"
         "pcmpgth    %[ftmp3],   %[ftmp3],       %[ftmp5]                \n\t"
         "pcmpgth    %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
-        "xor        %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
-        "xor        %[ftmp2],   %[ftmp2],       %[ftmp4]                \n\t"
+        "pxor       %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
+        "pxor       %[ftmp2],   %[ftmp2],       %[ftmp4]                \n\t"
         "paddh      %[ftmp1],   %[ftmp1],       %[ftmp0]                \n\t"
         "paddh      %[ftmp2],   %[ftmp2],       %[ftmp0]                \n\t"
-        "xor        %[ftmp3],   %[ftmp3],       %[ftmp1]                \n\t"
-        "xor        %[ftmp4],   %[ftmp4],       %[ftmp2]                \n\t"
+        "pxor       %[ftmp3],   %[ftmp3],       %[ftmp1]                \n\t"
+        "pxor       %[ftmp4],   %[ftmp4],       %[ftmp2]                \n\t"
         "pcmpeqh    %[ftmp1],   %[ftmp1],       %[ftmp0]                \n\t"
         "pcmpeqh    %[ftmp2],   %[ftmp2],       %[ftmp0]                \n\t"
         "pandn      %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
@@ -93,7 +94,7 @@ void ff_dct_unquantize_h263_intra_mmi(MpegEncContext *s, int16_t *block,
           [addr0]"=&r"(addr[0])
         : [block]"r"((mips_reg)(block+nCoeffs)),
           [nCoeffs]"r"((mips_reg)(2*(-nCoeffs))),
-          [qmul]"f"(qmul),                  [qadd]"f"(qadd)
+          [qmul]"f"(qmul_u.f),              [qadd]"f"(qadd_u.f)
         : "memory"
     );
 
@@ -103,24 +104,25 @@ void ff_dct_unquantize_h263_intra_mmi(MpegEncContext *s, int16_t *block,
 void ff_dct_unquantize_h263_inter_mmi(MpegEncContext *s, int16_t *block,
         int n, int qscale)
 {
-    int64_t qmul, qadd, nCoeffs;
+    int64_t nCoeffs;
     double ftmp[6];
     mips_reg addr[1];
+    union mmi_intfloat64 qmul_u, qadd_u;
     DECLARE_VAR_ALL64;
 
-    qmul = qscale << 1;
-    qadd = (qscale - 1) | 1;
+    qmul_u.i = qscale << 1;
+    qadd_u.i = (qscale - 1) | 1;
     av_assert2(s->block_last_index[n]>=0 || s->h263_aic);
     nCoeffs = s->inter_scantable.raster_end[s->block_last_index[n]];
 
     __asm__ volatile (
         "packsswh   %[qmul],    %[qmul],        %[qmul]                 \n\t"
         "packsswh   %[qmul],    %[qmul],        %[qmul]                 \n\t"
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
         "packsswh   %[qadd],    %[qadd],        %[qadd]                 \n\t"
         "packsswh   %[qadd],    %[qadd],        %[qadd]                 \n\t"
         "psubh      %[ftmp0],   %[ftmp0],       %[qadd]                 \n\t"
-        "xor        %[ftmp5],   %[ftmp5],       %[ftmp5]                \n\t"
+        "pxor       %[ftmp5],   %[ftmp5],       %[ftmp5]                \n\t"
         ".p2align   4                                                   \n\t"
         "1:                                                             \n\t"
         PTR_ADDU   "%[addr0],   %[block],       %[nCoeffs]              \n\t"
@@ -132,12 +134,12 @@ void ff_dct_unquantize_h263_inter_mmi(MpegEncContext *s, int16_t *block,
         "pmullh     %[ftmp2],   %[ftmp2],       %[qmul]                 \n\t"
         "pcmpgth    %[ftmp3],   %[ftmp3],       %[ftmp5]                \n\t"
         "pcmpgth    %[ftmp4],   %[ftmp4],       %[ftmp5]                \n\t"
-        "xor        %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
-        "xor        %[ftmp2],   %[ftmp2],       %[ftmp4]                \n\t"
+        "pxor       %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
+        "pxor       %[ftmp2],   %[ftmp2],       %[ftmp4]                \n\t"
         "paddh      %[ftmp1],   %[ftmp1],       %[ftmp0]                \n\t"
         "paddh      %[ftmp2],   %[ftmp2],       %[ftmp0]                \n\t"
-        "xor        %[ftmp3],   %[ftmp3],       %[ftmp1]                \n\t"
-        "xor        %[ftmp4],   %[ftmp4],       %[ftmp2]                \n\t"
+        "pxor       %[ftmp3],   %[ftmp3],       %[ftmp1]                \n\t"
+        "pxor       %[ftmp4],   %[ftmp4],       %[ftmp2]                \n\t"
         "pcmpeqh    %[ftmp1],   %[ftmp1],       %[ftmp0]                \n\t"
         "pcmpeqh    %[ftmp2],   %[ftmp2],       %[ftmp0]                \n\t"
         "pandn      %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
@@ -153,7 +155,7 @@ void ff_dct_unquantize_h263_inter_mmi(MpegEncContext *s, int16_t *block,
           [addr0]"=&r"(addr[0])
         : [block]"r"((mips_reg)(block+nCoeffs)),
           [nCoeffs]"r"((mips_reg)(2*(-nCoeffs))),
-          [qmul]"f"(qmul),                  [qadd]"f"(qadd)
+          [qmul]"f"(qmul_u.f),              [qadd]"f"(qadd_u.f)
         : "memory"
     );
 }
@@ -201,18 +203,18 @@ void ff_dct_unquantize_mpeg1_intra_mmi(MpegEncContext *s, int16_t *block,
         MMI_LDXC1(%[ftmp7], %[addr0], %[quant], 0x08)
         "pmullh     %[ftmp6],   %[ftmp6],       %[ftmp1]                \n\t"
         "pmullh     %[ftmp7],   %[ftmp7],       %[ftmp1]                \n\t"
-        "xor        %[ftmp8],   %[ftmp8],       %[ftmp8]                \n\t"
-        "xor        %[ftmp9],   %[ftmp9],       %[ftmp9]                \n\t"
+        "pxor       %[ftmp8],   %[ftmp8],       %[ftmp8]                \n\t"
+        "pxor       %[ftmp9],   %[ftmp9],       %[ftmp9]                \n\t"
         "pcmpgth    %[ftmp8],   %[ftmp8],       %[ftmp2]                \n\t"
         "pcmpgth    %[ftmp9],   %[ftmp9],       %[ftmp3]                \n\t"
-        "xor        %[ftmp2],   %[ftmp2],       %[ftmp8]                \n\t"
-        "xor        %[ftmp3],   %[ftmp3],       %[ftmp9]                \n\t"
+        "pxor       %[ftmp2],   %[ftmp2],       %[ftmp8]                \n\t"
+        "pxor       %[ftmp3],   %[ftmp3],       %[ftmp9]                \n\t"
         "psubh      %[ftmp2],   %[ftmp2],       %[ftmp8]                \n\t"
         "psubh      %[ftmp3],   %[ftmp3],       %[ftmp9]                \n\t"
         "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp6]                \n\t"
         "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp7]                \n\t"
-        "xor        %[ftmp6],   %[ftmp6],       %[ftmp6]                \n\t"
-        "xor        %[ftmp7],   %[ftmp7],       %[ftmp7]                \n\t"
+        "pxor       %[ftmp6],   %[ftmp6],       %[ftmp6]                \n\t"
+        "pxor       %[ftmp7],   %[ftmp7],       %[ftmp7]                \n\t"
         "pcmpeqh    %[ftmp6],   %[ftmp6],       %[ftmp4]                \n\t"
         "dli        %[tmp0],    0x03                                    \n\t"
         "pcmpeqh    %[ftmp7],   %[ftmp7],       %[ftmp5]                \n\t"
@@ -221,10 +223,10 @@ void ff_dct_unquantize_mpeg1_intra_mmi(MpegEncContext *s, int16_t *block,
         "psrah      %[ftmp3],   %[ftmp3],       %[ftmp4]                \n\t"
         "psubh      %[ftmp2],   %[ftmp2],       %[ftmp0]                \n\t"
         "psubh      %[ftmp3],   %[ftmp3],       %[ftmp0]                \n\t"
-        "or         %[ftmp2],   %[ftmp2],       %[ftmp0]                \n\t"
-        "or         %[ftmp3],   %[ftmp3],       %[ftmp0]                \n\t"
-        "xor        %[ftmp2],   %[ftmp2],       %[ftmp8]                \n\t"
-        "xor        %[ftmp3],   %[ftmp3],       %[ftmp9]                \n\t"
+        "por        %[ftmp2],   %[ftmp2],       %[ftmp0]                \n\t"
+        "por        %[ftmp3],   %[ftmp3],       %[ftmp0]                \n\t"
+        "pxor       %[ftmp2],   %[ftmp2],       %[ftmp8]                \n\t"
+        "pxor       %[ftmp3],   %[ftmp3],       %[ftmp9]                \n\t"
         "psubh      %[ftmp2],   %[ftmp2],       %[ftmp8]                \n\t"
         "psubh      %[ftmp3],   %[ftmp3],       %[ftmp9]                \n\t"
         "pandn      %[ftmp6],   %[ftmp6],       %[ftmp2]                \n\t"
@@ -287,12 +289,12 @@ void ff_dct_unquantize_mpeg1_inter_mmi(MpegEncContext *s, int16_t *block,
         MMI_LDXC1(%[ftmp7], %[addr0], %[quant], 0x08)
         "pmullh     %[ftmp6],   %[ftmp6],       %[ftmp1]                \n\t"
         "pmullh     %[ftmp7],   %[ftmp7],       %[ftmp1]                \n\t"
-        "xor        %[ftmp8],   %[ftmp8],       %[ftmp8]                \n\t"
-        "xor        %[ftmp9],   %[ftmp9],       %[ftmp9]                \n\t"
+        "pxor       %[ftmp8],   %[ftmp8],       %[ftmp8]                \n\t"
+        "pxor       %[ftmp9],   %[ftmp9],       %[ftmp9]                \n\t"
         "pcmpgth    %[ftmp8],   %[ftmp8],       %[ftmp2]                \n\t"
         "pcmpgth    %[ftmp9],   %[ftmp9],       %[ftmp3]                \n\t"
-        "xor        %[ftmp2],   %[ftmp2],       %[ftmp8]                \n\t"
-        "xor        %[ftmp3],   %[ftmp3],       %[ftmp9]                \n\t"
+        "pxor       %[ftmp2],   %[ftmp2],       %[ftmp8]                \n\t"
+        "pxor       %[ftmp3],   %[ftmp3],       %[ftmp9]                \n\t"
         "psubh      %[ftmp2],   %[ftmp2],       %[ftmp8]                \n\t"
         "psubh      %[ftmp3],   %[ftmp3],       %[ftmp9]                \n\t"
         "paddh      %[ftmp2],   %[ftmp2],       %[ftmp2]                \n\t"
@@ -301,8 +303,8 @@ void ff_dct_unquantize_mpeg1_inter_mmi(MpegEncContext *s, int16_t *block,
         "paddh      %[ftmp3],   %[ftmp3],       %[ftmp0]                \n\t"
         "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp6]                \n\t"
         "pmullh     %[ftmp3],   %[ftmp3],       %[ftmp7]                \n\t"
-        "xor        %[ftmp6],   %[ftmp6],       %[ftmp6]                \n\t"
-        "xor        %[ftmp7],   %[ftmp7],       %[ftmp7]                \n\t"
+        "pxor       %[ftmp6],   %[ftmp6],       %[ftmp6]                \n\t"
+        "pxor       %[ftmp7],   %[ftmp7],       %[ftmp7]                \n\t"
         "pcmpeqh    %[ftmp6],   %[ftmp6],       %[ftmp4]                \n\t"
         "dli        %[tmp0],    0x04                                    \n\t"
         "pcmpeqh    %[ftmp7],   %[ftmp7],       %[ftmp5]                \n\t"
@@ -311,10 +313,10 @@ void ff_dct_unquantize_mpeg1_inter_mmi(MpegEncContext *s, int16_t *block,
         "psrah      %[ftmp3],   %[ftmp3],       %[ftmp4]                \n\t"
         "psubh      %[ftmp2],   %[ftmp2],       %[ftmp0]                \n\t"
         "psubh      %[ftmp3],   %[ftmp3],       %[ftmp0]                \n\t"
-        "or         %[ftmp2],   %[ftmp2],       %[ftmp0]                \n\t"
-        "or         %[ftmp3],   %[ftmp3],       %[ftmp0]                \n\t"
-        "xor        %[ftmp2],   %[ftmp2],       %[ftmp8]                \n\t"
-        "xor        %[ftmp3],   %[ftmp3],       %[ftmp9]                \n\t"
+        "por        %[ftmp2],   %[ftmp2],       %[ftmp0]                \n\t"
+        "por        %[ftmp3],   %[ftmp3],       %[ftmp0]                \n\t"
+        "pxor       %[ftmp2],   %[ftmp2],       %[ftmp8]                \n\t"
+        "pxor       %[ftmp3],   %[ftmp3],       %[ftmp9]                \n\t"
         "psubh      %[ftmp2],   %[ftmp2],       %[ftmp8]                \n\t"
         "psubh      %[ftmp3],   %[ftmp3],       %[ftmp9]                \n\t"
         "pandn      %[ftmp6],   %[ftmp6],       %[ftmp2]                \n\t"
@@ -386,26 +388,26 @@ void ff_dct_unquantize_mpeg2_intra_mmi(MpegEncContext *s, int16_t *block,
         MMI_LDXC1(%[ftmp6], %[addr0], %[quant], 0x08)
         "pmullh     %[ftmp5],   %[ftmp5],       %[ftmp9]                \n\t"
         "pmullh     %[ftmp6],   %[ftmp6],       %[ftmp9]                \n\t"
-        "xor        %[ftmp7],   %[ftmp7],       %[ftmp7]                \n\t"
-        "xor        %[ftmp8],   %[ftmp8],       %[ftmp8]                \n\t"
+        "pxor       %[ftmp7],   %[ftmp7],       %[ftmp7]                \n\t"
+        "pxor       %[ftmp8],   %[ftmp8],       %[ftmp8]                \n\t"
         "pcmpgth    %[ftmp7],   %[ftmp7],       %[ftmp1]                \n\t"
         "pcmpgth    %[ftmp8],   %[ftmp8],       %[ftmp2]                \n\t"
-        "xor        %[ftmp1],   %[ftmp1],       %[ftmp7]                \n\t"
-        "xor        %[ftmp2],   %[ftmp2],       %[ftmp8]                \n\t"
+        "pxor       %[ftmp1],   %[ftmp1],       %[ftmp7]                \n\t"
+        "pxor       %[ftmp2],   %[ftmp2],       %[ftmp8]                \n\t"
         "psubh      %[ftmp1],   %[ftmp1],       %[ftmp7]                \n\t"
         "psubh      %[ftmp2],   %[ftmp2],       %[ftmp8]                \n\t"
         "pmullh     %[ftmp1],   %[ftmp1],       %[ftmp5]                \n\t"
         "pmullh     %[ftmp2],   %[ftmp2],       %[ftmp6]                \n\t"
-        "xor        %[ftmp5],   %[ftmp5],       %[ftmp5]                \n\t"
-        "xor        %[ftmp6],   %[ftmp6],       %[ftmp6]                \n\t"
+        "pxor       %[ftmp5],   %[ftmp5],       %[ftmp5]                \n\t"
+        "pxor       %[ftmp6],   %[ftmp6],       %[ftmp6]                \n\t"
         "pcmpeqh    %[ftmp5],   %[ftmp5],       %[ftmp3]                \n\t"
         "dli        %[tmp0],    0x03                                    \n\t"
         "pcmpeqh    %[ftmp6] ,  %[ftmp6],       %[ftmp4]                \n\t"
         "mtc1       %[tmp0],    %[ftmp3]                                \n\t"
         "psrah      %[ftmp1],   %[ftmp1],       %[ftmp3]                \n\t"
         "psrah      %[ftmp2],   %[ftmp2],       %[ftmp3]                \n\t"
-        "xor        %[ftmp1],   %[ftmp1],       %[ftmp7]                \n\t"
-        "xor        %[ftmp2],   %[ftmp2],       %[ftmp8]                \n\t"
+        "pxor       %[ftmp1],   %[ftmp1],       %[ftmp7]                \n\t"
+        "pxor       %[ftmp2],   %[ftmp2],       %[ftmp8]                \n\t"
         "psubh      %[ftmp1],   %[ftmp1],       %[ftmp7]                \n\t"
         "psubh      %[ftmp2],   %[ftmp2],       %[ftmp8]                \n\t"
         "pandn      %[ftmp5],   %[ftmp5],       %[ftmp1]                \n\t"
@@ -445,16 +447,16 @@ void ff_denoise_dct_mmi(MpegEncContext *s, int16_t *block)
     s->dct_count[intra]++;
 
     __asm__ volatile(
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
         "1:                                                             \n\t"
         MMI_LDC1(%[ftmp1], %[block], 0x00)
-        "xor        %[ftmp2],   %[ftmp2],       %[ftmp2]                \n\t"
+        "pxor       %[ftmp2],   %[ftmp2],       %[ftmp2]                \n\t"
         MMI_LDC1(%[ftmp3], %[block], 0x08)
-        "xor        %[ftmp4],   %[ftmp4],       %[ftmp4]                \n\t"
+        "pxor       %[ftmp4],   %[ftmp4],       %[ftmp4]                \n\t"
         "pcmpgth    %[ftmp2],   %[ftmp2],       %[ftmp1]                \n\t"
         "pcmpgth    %[ftmp4],   %[ftmp4],       %[ftmp3]                \n\t"
-        "xor        %[ftmp1],   %[ftmp1],       %[ftmp2]                \n\t"
-        "xor        %[ftmp3],   %[ftmp3],       %[ftmp4]                \n\t"
+        "pxor       %[ftmp1],   %[ftmp1],       %[ftmp2]                \n\t"
+        "pxor       %[ftmp3],   %[ftmp3],       %[ftmp4]                \n\t"
         "psubh      %[ftmp1],   %[ftmp1],       %[ftmp2]                \n\t"
         "psubh      %[ftmp3],   %[ftmp3],       %[ftmp4]                \n\t"
         MMI_LDC1(%[ftmp6], %[offset], 0x00)
@@ -463,8 +465,8 @@ void ff_denoise_dct_mmi(MpegEncContext *s, int16_t *block)
         MMI_LDC1(%[ftmp6], %[offset], 0x08)
         "mov.d      %[ftmp7],   %[ftmp3]                                \n\t"
         "psubush    %[ftmp3],   %[ftmp3],       %[ftmp6]                \n\t"
-        "xor        %[ftmp1],   %[ftmp1],       %[ftmp2]                \n\t"
-        "xor        %[ftmp3],   %[ftmp3],       %[ftmp4]                \n\t"
+        "pxor       %[ftmp1],   %[ftmp1],       %[ftmp2]                \n\t"
+        "pxor       %[ftmp3],   %[ftmp3],       %[ftmp4]                \n\t"
         "psubh      %[ftmp1],   %[ftmp1],       %[ftmp2]                \n\t"
         "psubh      %[ftmp3],   %[ftmp3],       %[ftmp4]                \n\t"
         MMI_SDC1(%[ftmp1], %[block], 0x00)
diff --git a/libavcodec/mips/mpegvideoencdsp_init_mips.c b/libavcodec/mips/mpegvideoencdsp_init_mips.c
index 9bfe94e4cd..71831a61ac 100644
--- a/libavcodec/mips/mpegvideoencdsp_init_mips.c
+++ b/libavcodec/mips/mpegvideoencdsp_init_mips.c
@@ -18,23 +18,18 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
+#include "libavutil/mips/cpu.h"
 #include "libavcodec/bit_depth_template.c"
 #include "h263dsp_mips.h"
 
-#if HAVE_MSA
-static av_cold void mpegvideoencdsp_init_msa(MpegvideoEncDSPContext *c,
-                                             AVCodecContext *avctx)
-{
-#if BIT_DEPTH == 8
-    c->pix_sum = ff_pix_sum_msa;
-#endif
-}
-#endif  // #if HAVE_MSA
-
 av_cold void ff_mpegvideoencdsp_init_mips(MpegvideoEncDSPContext *c,
                                           AVCodecContext *avctx)
 {
-#if HAVE_MSA
-    mpegvideoencdsp_init_msa(c, avctx);
-#endif  // #if HAVE_MSA
+    int cpu_flags = av_get_cpu_flags();
+
+    if (have_msa(cpu_flags)) {
+#if BIT_DEPTH == 8
+        c->pix_sum = ff_pix_sum_msa;
+#endif
+    }
 }
diff --git a/libavcodec/mips/pixblockdsp_init_mips.c b/libavcodec/mips/pixblockdsp_init_mips.c
index fd0238d79b..2e2d70953b 100644
--- a/libavcodec/mips/pixblockdsp_init_mips.c
+++ b/libavcodec/mips/pixblockdsp_init_mips.c
@@ -19,51 +19,38 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
+#include "libavutil/mips/cpu.h"
 #include "pixblockdsp_mips.h"
 
-#if HAVE_MSA
-static av_cold void pixblockdsp_init_msa(PixblockDSPContext *c,
-                                         AVCodecContext *avctx,
-                                         unsigned high_bit_depth)
+void ff_pixblockdsp_init_mips(PixblockDSPContext *c, AVCodecContext *avctx,
+                              unsigned high_bit_depth)
 {
-    c->diff_pixels = ff_diff_pixels_msa;
+    int cpu_flags = av_get_cpu_flags();
+
+    if (have_mmi(cpu_flags)) {
+        c->diff_pixels = ff_diff_pixels_mmi;
 
-    switch (avctx->bits_per_raw_sample) {
-    case 9:
-    case 10:
-    case 12:
-    case 14:
-        c->get_pixels = ff_get_pixels_16_msa;
-        break;
-    default:
-        if (avctx->bits_per_raw_sample <= 8 || avctx->codec_type !=
-            AVMEDIA_TYPE_VIDEO) {
-            c->get_pixels = ff_get_pixels_8_msa;
+        if (!high_bit_depth || avctx->codec_type != AVMEDIA_TYPE_VIDEO) {
+            c->get_pixels = ff_get_pixels_8_mmi;
         }
-        break;
     }
-}
-#endif  // #if HAVE_MSA
 
-#if HAVE_MMI
-static av_cold void pixblockdsp_init_mmi(PixblockDSPContext *c,
-        AVCodecContext *avctx, unsigned high_bit_depth)
-{
-    c->diff_pixels = ff_diff_pixels_mmi;
+    if (have_msa(cpu_flags)) {
+        c->diff_pixels = ff_diff_pixels_msa;
 
-    if (!high_bit_depth || avctx->codec_type != AVMEDIA_TYPE_VIDEO) {
-        c->get_pixels = ff_get_pixels_8_mmi;
+        switch (avctx->bits_per_raw_sample) {
+        case 9:
+        case 10:
+        case 12:
+        case 14:
+            c->get_pixels = ff_get_pixels_16_msa;
+            break;
+        default:
+            if (avctx->bits_per_raw_sample <= 8 || avctx->codec_type !=
+                AVMEDIA_TYPE_VIDEO) {
+                c->get_pixels = ff_get_pixels_8_msa;
+            }
+            break;
+        }
     }
 }
-#endif /* HAVE_MMI */
-
-void ff_pixblockdsp_init_mips(PixblockDSPContext *c, AVCodecContext *avctx,
-                              unsigned high_bit_depth)
-{
-#if HAVE_MMI
-    pixblockdsp_init_mmi(c, avctx, high_bit_depth);
-#endif /* HAVE_MMI */
-#if HAVE_MSA
-    pixblockdsp_init_msa(c, avctx, high_bit_depth);
-#endif  // #if HAVE_MSA
-}
diff --git a/libavcodec/mips/pixblockdsp_mmi.c b/libavcodec/mips/pixblockdsp_mmi.c
index a915a3c28b..1230f5de88 100644
--- a/libavcodec/mips/pixblockdsp_mmi.c
+++ b/libavcodec/mips/pixblockdsp_mmi.c
@@ -33,7 +33,7 @@ void ff_get_pixels_8_mmi(int16_t *av_restrict block, const uint8_t *pixels,
     DECLARE_VAR_ADDRT;
 
     __asm__ volatile (
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
 
         MMI_LDC1(%[ftmp1], %[pixels], 0x00)
         MMI_LDXC1(%[ftmp2], %[pixels], %[stride], 0x00)
@@ -103,12 +103,12 @@ void ff_diff_pixels_mmi(int16_t *av_restrict block, const uint8_t *src1,
 
     __asm__ volatile (
         "li         %[tmp0],    0x08                                    \n\t"
-        "xor        %[ftmp4],   %[ftmp4],       %[ftmp4]                \n\t"
+        "pxor       %[ftmp4],   %[ftmp4],       %[ftmp4]                \n\t"
         "1:                                                             \n\t"
         MMI_LDC1(%[ftmp0], %[src1], 0x00)
-        "or         %[ftmp1],   %[ftmp0],       %[ftmp0]                \n\t"
+        "por        %[ftmp1],   %[ftmp0],       %[ftmp0]                \n\t"
         MMI_LDC1(%[ftmp2], %[src2], 0x00)
-        "or         %[ftmp3],   %[ftmp2],       %[ftmp2]                \n\t"
+        "por        %[ftmp3],   %[ftmp2],       %[ftmp2]                \n\t"
         "punpcklbh  %[ftmp0],   %[ftmp0],       %[ftmp4]                \n\t"
         "punpckhbh  %[ftmp1],   %[ftmp1],       %[ftmp4]                \n\t"
         "punpcklbh  %[ftmp2],   %[ftmp2],       %[ftmp4]                \n\t"
diff --git a/libavcodec/mips/qpeldsp_init_mips.c b/libavcodec/mips/qpeldsp_init_mips.c
index 140e8f89c9..cccf9d4429 100644
--- a/libavcodec/mips/qpeldsp_init_mips.c
+++ b/libavcodec/mips/qpeldsp_init_mips.c
@@ -18,150 +18,146 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
+#include "libavutil/mips/cpu.h"
 #include "qpeldsp_mips.h"
 
-#if HAVE_MSA
-static av_cold void qpeldsp_init_msa(QpelDSPContext *c)
+void ff_qpeldsp_init_mips(QpelDSPContext *c)
 {
-    c->put_qpel_pixels_tab[0][0] = ff_copy_16x16_msa;
-    c->put_qpel_pixels_tab[0][1] = ff_horiz_mc_qpel_aver_src0_16width_msa;
-    c->put_qpel_pixels_tab[0][2] = ff_horiz_mc_qpel_16width_msa;
-    c->put_qpel_pixels_tab[0][3] = ff_horiz_mc_qpel_aver_src1_16width_msa;
-    c->put_qpel_pixels_tab[0][4] = ff_vert_mc_qpel_aver_src0_16x16_msa;
-    c->put_qpel_pixels_tab[0][5] = ff_hv_mc_qpel_aver_hv_src00_16x16_msa;
-    c->put_qpel_pixels_tab[0][6] = ff_hv_mc_qpel_aver_v_src0_16x16_msa;
-    c->put_qpel_pixels_tab[0][7] = ff_hv_mc_qpel_aver_hv_src10_16x16_msa;
-    c->put_qpel_pixels_tab[0][8] = ff_vert_mc_qpel_16x16_msa;
-    c->put_qpel_pixels_tab[0][9] = ff_hv_mc_qpel_aver_h_src0_16x16_msa;
-    c->put_qpel_pixels_tab[0][10] = ff_hv_mc_qpel_16x16_msa;
-    c->put_qpel_pixels_tab[0][11] = ff_hv_mc_qpel_aver_h_src1_16x16_msa;
-    c->put_qpel_pixels_tab[0][12] = ff_vert_mc_qpel_aver_src1_16x16_msa;
-    c->put_qpel_pixels_tab[0][13] = ff_hv_mc_qpel_aver_hv_src01_16x16_msa;
-    c->put_qpel_pixels_tab[0][14] = ff_hv_mc_qpel_aver_v_src1_16x16_msa;
-    c->put_qpel_pixels_tab[0][15] = ff_hv_mc_qpel_aver_hv_src11_16x16_msa;
+    int cpu_flags = av_get_cpu_flags();
 
-    c->put_qpel_pixels_tab[1][0] = ff_copy_8x8_msa;
-    c->put_qpel_pixels_tab[1][1] = ff_horiz_mc_qpel_aver_src0_8width_msa;
-    c->put_qpel_pixels_tab[1][2] = ff_horiz_mc_qpel_8width_msa;
-    c->put_qpel_pixels_tab[1][3] = ff_horiz_mc_qpel_aver_src1_8width_msa;
-    c->put_qpel_pixels_tab[1][4] = ff_vert_mc_qpel_aver_src0_8x8_msa;
-    c->put_qpel_pixels_tab[1][5] = ff_hv_mc_qpel_aver_hv_src00_8x8_msa;
-    c->put_qpel_pixels_tab[1][6] = ff_hv_mc_qpel_aver_v_src0_8x8_msa;
-    c->put_qpel_pixels_tab[1][7] = ff_hv_mc_qpel_aver_hv_src10_8x8_msa;
-    c->put_qpel_pixels_tab[1][8] = ff_vert_mc_qpel_8x8_msa;
-    c->put_qpel_pixels_tab[1][9] = ff_hv_mc_qpel_aver_h_src0_8x8_msa;
-    c->put_qpel_pixels_tab[1][10] = ff_hv_mc_qpel_8x8_msa;
-    c->put_qpel_pixels_tab[1][11] = ff_hv_mc_qpel_aver_h_src1_8x8_msa;
-    c->put_qpel_pixels_tab[1][12] = ff_vert_mc_qpel_aver_src1_8x8_msa;
-    c->put_qpel_pixels_tab[1][13] = ff_hv_mc_qpel_aver_hv_src01_8x8_msa;
-    c->put_qpel_pixels_tab[1][14] = ff_hv_mc_qpel_aver_v_src1_8x8_msa;
-    c->put_qpel_pixels_tab[1][15] = ff_hv_mc_qpel_aver_hv_src11_8x8_msa;
+    if (have_msa(cpu_flags)) {
+        c->put_qpel_pixels_tab[0][0] = ff_copy_16x16_msa;
+        c->put_qpel_pixels_tab[0][1] = ff_horiz_mc_qpel_aver_src0_16width_msa;
+        c->put_qpel_pixels_tab[0][2] = ff_horiz_mc_qpel_16width_msa;
+        c->put_qpel_pixels_tab[0][3] = ff_horiz_mc_qpel_aver_src1_16width_msa;
+        c->put_qpel_pixels_tab[0][4] = ff_vert_mc_qpel_aver_src0_16x16_msa;
+        c->put_qpel_pixels_tab[0][5] = ff_hv_mc_qpel_aver_hv_src00_16x16_msa;
+        c->put_qpel_pixels_tab[0][6] = ff_hv_mc_qpel_aver_v_src0_16x16_msa;
+        c->put_qpel_pixels_tab[0][7] = ff_hv_mc_qpel_aver_hv_src10_16x16_msa;
+        c->put_qpel_pixels_tab[0][8] = ff_vert_mc_qpel_16x16_msa;
+        c->put_qpel_pixels_tab[0][9] = ff_hv_mc_qpel_aver_h_src0_16x16_msa;
+        c->put_qpel_pixels_tab[0][10] = ff_hv_mc_qpel_16x16_msa;
+        c->put_qpel_pixels_tab[0][11] = ff_hv_mc_qpel_aver_h_src1_16x16_msa;
+        c->put_qpel_pixels_tab[0][12] = ff_vert_mc_qpel_aver_src1_16x16_msa;
+        c->put_qpel_pixels_tab[0][13] = ff_hv_mc_qpel_aver_hv_src01_16x16_msa;
+        c->put_qpel_pixels_tab[0][14] = ff_hv_mc_qpel_aver_v_src1_16x16_msa;
+        c->put_qpel_pixels_tab[0][15] = ff_hv_mc_qpel_aver_hv_src11_16x16_msa;
 
-    c->put_no_rnd_qpel_pixels_tab[0][0] = ff_copy_16x16_msa;
-    c->put_no_rnd_qpel_pixels_tab[0][1] =
-        ff_horiz_mc_qpel_no_rnd_aver_src0_16width_msa;
-    c->put_no_rnd_qpel_pixels_tab[0][2] = ff_horiz_mc_qpel_no_rnd_16width_msa;
-    c->put_no_rnd_qpel_pixels_tab[0][3] =
-        ff_horiz_mc_qpel_no_rnd_aver_src1_16width_msa;
-    c->put_no_rnd_qpel_pixels_tab[0][4] =
-        ff_vert_mc_qpel_no_rnd_aver_src0_16x16_msa;
-    c->put_no_rnd_qpel_pixels_tab[0][5] =
-        ff_hv_mc_qpel_no_rnd_aver_hv_src00_16x16_msa;
-    c->put_no_rnd_qpel_pixels_tab[0][6] =
-        ff_hv_mc_qpel_no_rnd_aver_v_src0_16x16_msa;
-    c->put_no_rnd_qpel_pixels_tab[0][7] =
-        ff_hv_mc_qpel_no_rnd_aver_hv_src10_16x16_msa;
-    c->put_no_rnd_qpel_pixels_tab[0][8] = ff_vert_mc_qpel_no_rnd_16x16_msa;
-    c->put_no_rnd_qpel_pixels_tab[0][9] =
-        ff_hv_mc_qpel_no_rnd_aver_h_src0_16x16_msa;
-    c->put_no_rnd_qpel_pixels_tab[0][10] = ff_hv_mc_qpel_no_rnd_16x16_msa;
-    c->put_no_rnd_qpel_pixels_tab[0][11] =
-        ff_hv_mc_qpel_no_rnd_aver_h_src1_16x16_msa;
-    c->put_no_rnd_qpel_pixels_tab[0][12] =
-        ff_vert_mc_qpel_no_rnd_aver_src1_16x16_msa;
-    c->put_no_rnd_qpel_pixels_tab[0][13] =
-        ff_hv_mc_qpel_no_rnd_aver_hv_src01_16x16_msa;
-    c->put_no_rnd_qpel_pixels_tab[0][14] =
-        ff_hv_mc_qpel_no_rnd_aver_v_src1_16x16_msa;
-    c->put_no_rnd_qpel_pixels_tab[0][15] =
-        ff_hv_mc_qpel_no_rnd_aver_hv_src11_16x16_msa;
+        c->put_qpel_pixels_tab[1][0] = ff_copy_8x8_msa;
+        c->put_qpel_pixels_tab[1][1] = ff_horiz_mc_qpel_aver_src0_8width_msa;
+        c->put_qpel_pixels_tab[1][2] = ff_horiz_mc_qpel_8width_msa;
+        c->put_qpel_pixels_tab[1][3] = ff_horiz_mc_qpel_aver_src1_8width_msa;
+        c->put_qpel_pixels_tab[1][4] = ff_vert_mc_qpel_aver_src0_8x8_msa;
+        c->put_qpel_pixels_tab[1][5] = ff_hv_mc_qpel_aver_hv_src00_8x8_msa;
+        c->put_qpel_pixels_tab[1][6] = ff_hv_mc_qpel_aver_v_src0_8x8_msa;
+        c->put_qpel_pixels_tab[1][7] = ff_hv_mc_qpel_aver_hv_src10_8x8_msa;
+        c->put_qpel_pixels_tab[1][8] = ff_vert_mc_qpel_8x8_msa;
+        c->put_qpel_pixels_tab[1][9] = ff_hv_mc_qpel_aver_h_src0_8x8_msa;
+        c->put_qpel_pixels_tab[1][10] = ff_hv_mc_qpel_8x8_msa;
+        c->put_qpel_pixels_tab[1][11] = ff_hv_mc_qpel_aver_h_src1_8x8_msa;
+        c->put_qpel_pixels_tab[1][12] = ff_vert_mc_qpel_aver_src1_8x8_msa;
+        c->put_qpel_pixels_tab[1][13] = ff_hv_mc_qpel_aver_hv_src01_8x8_msa;
+        c->put_qpel_pixels_tab[1][14] = ff_hv_mc_qpel_aver_v_src1_8x8_msa;
+        c->put_qpel_pixels_tab[1][15] = ff_hv_mc_qpel_aver_hv_src11_8x8_msa;
 
-    c->put_no_rnd_qpel_pixels_tab[1][0] = ff_copy_8x8_msa;
-    c->put_no_rnd_qpel_pixels_tab[1][1] =
-        ff_horiz_mc_qpel_no_rnd_aver_src0_8width_msa;
-    c->put_no_rnd_qpel_pixels_tab[1][2] = ff_horiz_mc_qpel_no_rnd_8width_msa;
-    c->put_no_rnd_qpel_pixels_tab[1][3] =
-        ff_horiz_mc_qpel_no_rnd_aver_src1_8width_msa;
-    c->put_no_rnd_qpel_pixels_tab[1][4] =
-        ff_vert_mc_qpel_no_rnd_aver_src0_8x8_msa;
-    c->put_no_rnd_qpel_pixels_tab[1][5] =
-        ff_hv_mc_qpel_no_rnd_aver_hv_src00_8x8_msa;
-    c->put_no_rnd_qpel_pixels_tab[1][6] =
-        ff_hv_mc_qpel_no_rnd_aver_v_src0_8x8_msa;
-    c->put_no_rnd_qpel_pixels_tab[1][7] =
-        ff_hv_mc_qpel_no_rnd_aver_hv_src10_8x8_msa;
-    c->put_no_rnd_qpel_pixels_tab[1][8] = ff_vert_mc_qpel_no_rnd_8x8_msa;
-    c->put_no_rnd_qpel_pixels_tab[1][9] =
-        ff_hv_mc_qpel_no_rnd_aver_h_src0_8x8_msa;
-    c->put_no_rnd_qpel_pixels_tab[1][10] = ff_hv_mc_qpel_no_rnd_8x8_msa;
-    c->put_no_rnd_qpel_pixels_tab[1][11] =
-        ff_hv_mc_qpel_no_rnd_aver_h_src1_8x8_msa;
-    c->put_no_rnd_qpel_pixels_tab[1][12] =
-        ff_vert_mc_qpel_no_rnd_aver_src1_8x8_msa;
-    c->put_no_rnd_qpel_pixels_tab[1][13] =
-        ff_hv_mc_qpel_no_rnd_aver_hv_src01_8x8_msa;
-    c->put_no_rnd_qpel_pixels_tab[1][14] =
-        ff_hv_mc_qpel_no_rnd_aver_v_src1_8x8_msa;
-    c->put_no_rnd_qpel_pixels_tab[1][15] =
-        ff_hv_mc_qpel_no_rnd_aver_hv_src11_8x8_msa;
+        c->put_no_rnd_qpel_pixels_tab[0][0] = ff_copy_16x16_msa;
+        c->put_no_rnd_qpel_pixels_tab[0][1] =
+            ff_horiz_mc_qpel_no_rnd_aver_src0_16width_msa;
+        c->put_no_rnd_qpel_pixels_tab[0][2] = ff_horiz_mc_qpel_no_rnd_16width_msa;
+        c->put_no_rnd_qpel_pixels_tab[0][3] =
+            ff_horiz_mc_qpel_no_rnd_aver_src1_16width_msa;
+        c->put_no_rnd_qpel_pixels_tab[0][4] =
+            ff_vert_mc_qpel_no_rnd_aver_src0_16x16_msa;
+        c->put_no_rnd_qpel_pixels_tab[0][5] =
+            ff_hv_mc_qpel_no_rnd_aver_hv_src00_16x16_msa;
+        c->put_no_rnd_qpel_pixels_tab[0][6] =
+            ff_hv_mc_qpel_no_rnd_aver_v_src0_16x16_msa;
+        c->put_no_rnd_qpel_pixels_tab[0][7] =
+            ff_hv_mc_qpel_no_rnd_aver_hv_src10_16x16_msa;
+        c->put_no_rnd_qpel_pixels_tab[0][8] = ff_vert_mc_qpel_no_rnd_16x16_msa;
+        c->put_no_rnd_qpel_pixels_tab[0][9] =
+            ff_hv_mc_qpel_no_rnd_aver_h_src0_16x16_msa;
+        c->put_no_rnd_qpel_pixels_tab[0][10] = ff_hv_mc_qpel_no_rnd_16x16_msa;
+        c->put_no_rnd_qpel_pixels_tab[0][11] =
+            ff_hv_mc_qpel_no_rnd_aver_h_src1_16x16_msa;
+        c->put_no_rnd_qpel_pixels_tab[0][12] =
+            ff_vert_mc_qpel_no_rnd_aver_src1_16x16_msa;
+        c->put_no_rnd_qpel_pixels_tab[0][13] =
+            ff_hv_mc_qpel_no_rnd_aver_hv_src01_16x16_msa;
+        c->put_no_rnd_qpel_pixels_tab[0][14] =
+            ff_hv_mc_qpel_no_rnd_aver_v_src1_16x16_msa;
+        c->put_no_rnd_qpel_pixels_tab[0][15] =
+            ff_hv_mc_qpel_no_rnd_aver_hv_src11_16x16_msa;
 
-    c->avg_qpel_pixels_tab[0][0] = ff_avg_width16_msa;
-    c->avg_qpel_pixels_tab[0][1] =
-        ff_horiz_mc_qpel_avg_dst_aver_src0_16width_msa;
-    c->avg_qpel_pixels_tab[0][2] = ff_horiz_mc_qpel_avg_dst_16width_msa;
-    c->avg_qpel_pixels_tab[0][3] =
-        ff_horiz_mc_qpel_avg_dst_aver_src1_16width_msa;
-    c->avg_qpel_pixels_tab[0][4] = ff_vert_mc_qpel_avg_dst_aver_src0_16x16_msa;
-    c->avg_qpel_pixels_tab[0][5] =
-        ff_hv_mc_qpel_avg_dst_aver_hv_src00_16x16_msa;
-    c->avg_qpel_pixels_tab[0][6] = ff_hv_mc_qpel_avg_dst_aver_v_src0_16x16_msa;
-    c->avg_qpel_pixels_tab[0][7] =
-        ff_hv_mc_qpel_avg_dst_aver_hv_src10_16x16_msa;
-    c->avg_qpel_pixels_tab[0][8] = ff_vert_mc_qpel_avg_dst_16x16_msa;
-    c->avg_qpel_pixels_tab[0][9] = ff_hv_mc_qpel_avg_dst_aver_h_src0_16x16_msa;
-    c->avg_qpel_pixels_tab[0][10] = ff_hv_mc_qpel_avg_dst_16x16_msa;
-    c->avg_qpel_pixels_tab[0][11] = ff_hv_mc_qpel_avg_dst_aver_h_src1_16x16_msa;
-    c->avg_qpel_pixels_tab[0][12] = ff_vert_mc_qpel_avg_dst_aver_src1_16x16_msa;
-    c->avg_qpel_pixels_tab[0][13] =
-        ff_hv_mc_qpel_avg_dst_aver_hv_src01_16x16_msa;
-    c->avg_qpel_pixels_tab[0][14] = ff_hv_mc_qpel_avg_dst_aver_v_src1_16x16_msa;
-    c->avg_qpel_pixels_tab[0][15] =
-        ff_hv_mc_qpel_avg_dst_aver_hv_src11_16x16_msa;
+        c->put_no_rnd_qpel_pixels_tab[1][0] = ff_copy_8x8_msa;
+        c->put_no_rnd_qpel_pixels_tab[1][1] =
+            ff_horiz_mc_qpel_no_rnd_aver_src0_8width_msa;
+        c->put_no_rnd_qpel_pixels_tab[1][2] = ff_horiz_mc_qpel_no_rnd_8width_msa;
+        c->put_no_rnd_qpel_pixels_tab[1][3] =
+            ff_horiz_mc_qpel_no_rnd_aver_src1_8width_msa;
+        c->put_no_rnd_qpel_pixels_tab[1][4] =
+            ff_vert_mc_qpel_no_rnd_aver_src0_8x8_msa;
+        c->put_no_rnd_qpel_pixels_tab[1][5] =
+            ff_hv_mc_qpel_no_rnd_aver_hv_src00_8x8_msa;
+        c->put_no_rnd_qpel_pixels_tab[1][6] =
+            ff_hv_mc_qpel_no_rnd_aver_v_src0_8x8_msa;
+        c->put_no_rnd_qpel_pixels_tab[1][7] =
+            ff_hv_mc_qpel_no_rnd_aver_hv_src10_8x8_msa;
+        c->put_no_rnd_qpel_pixels_tab[1][8] = ff_vert_mc_qpel_no_rnd_8x8_msa;
+        c->put_no_rnd_qpel_pixels_tab[1][9] =
+            ff_hv_mc_qpel_no_rnd_aver_h_src0_8x8_msa;
+        c->put_no_rnd_qpel_pixels_tab[1][10] = ff_hv_mc_qpel_no_rnd_8x8_msa;
+        c->put_no_rnd_qpel_pixels_tab[1][11] =
+            ff_hv_mc_qpel_no_rnd_aver_h_src1_8x8_msa;
+        c->put_no_rnd_qpel_pixels_tab[1][12] =
+            ff_vert_mc_qpel_no_rnd_aver_src1_8x8_msa;
+        c->put_no_rnd_qpel_pixels_tab[1][13] =
+            ff_hv_mc_qpel_no_rnd_aver_hv_src01_8x8_msa;
+        c->put_no_rnd_qpel_pixels_tab[1][14] =
+            ff_hv_mc_qpel_no_rnd_aver_v_src1_8x8_msa;
+        c->put_no_rnd_qpel_pixels_tab[1][15] =
+            ff_hv_mc_qpel_no_rnd_aver_hv_src11_8x8_msa;
 
-    c->avg_qpel_pixels_tab[1][0] = ff_avg_width8_msa;
-    c->avg_qpel_pixels_tab[1][1] =
-        ff_horiz_mc_qpel_avg_dst_aver_src0_8width_msa;
-    c->avg_qpel_pixels_tab[1][2] = ff_horiz_mc_qpel_avg_dst_8width_msa;
-    c->avg_qpel_pixels_tab[1][3] =
-        ff_horiz_mc_qpel_avg_dst_aver_src1_8width_msa;
-    c->avg_qpel_pixels_tab[1][4] = ff_vert_mc_qpel_avg_dst_aver_src0_8x8_msa;
-    c->avg_qpel_pixels_tab[1][5] = ff_hv_mc_qpel_avg_dst_aver_hv_src00_8x8_msa;
-    c->avg_qpel_pixels_tab[1][6] = ff_hv_mc_qpel_avg_dst_aver_v_src0_8x8_msa;
-    c->avg_qpel_pixels_tab[1][7] = ff_hv_mc_qpel_avg_dst_aver_hv_src10_8x8_msa;
-    c->avg_qpel_pixels_tab[1][8] = ff_vert_mc_qpel_avg_dst_8x8_msa;
-    c->avg_qpel_pixels_tab[1][9] = ff_hv_mc_qpel_avg_dst_aver_h_src0_8x8_msa;
-    c->avg_qpel_pixels_tab[1][10] = ff_hv_mc_qpel_avg_dst_8x8_msa;
-    c->avg_qpel_pixels_tab[1][11] = ff_hv_mc_qpel_avg_dst_aver_h_src1_8x8_msa;
-    c->avg_qpel_pixels_tab[1][12] = ff_vert_mc_qpel_avg_dst_aver_src1_8x8_msa;
-    c->avg_qpel_pixels_tab[1][13] = ff_hv_mc_qpel_avg_dst_aver_hv_src01_8x8_msa;
-    c->avg_qpel_pixels_tab[1][14] = ff_hv_mc_qpel_avg_dst_aver_v_src1_8x8_msa;
-    c->avg_qpel_pixels_tab[1][15] = ff_hv_mc_qpel_avg_dst_aver_hv_src11_8x8_msa;
-}
-#endif  // #if HAVE_MSA
+        c->avg_qpel_pixels_tab[0][0] = ff_avg_width16_msa;
+        c->avg_qpel_pixels_tab[0][1] =
+            ff_horiz_mc_qpel_avg_dst_aver_src0_16width_msa;
+        c->avg_qpel_pixels_tab[0][2] = ff_horiz_mc_qpel_avg_dst_16width_msa;
+        c->avg_qpel_pixels_tab[0][3] =
+            ff_horiz_mc_qpel_avg_dst_aver_src1_16width_msa;
+        c->avg_qpel_pixels_tab[0][4] = ff_vert_mc_qpel_avg_dst_aver_src0_16x16_msa;
+        c->avg_qpel_pixels_tab[0][5] =
+            ff_hv_mc_qpel_avg_dst_aver_hv_src00_16x16_msa;
+        c->avg_qpel_pixels_tab[0][6] = ff_hv_mc_qpel_avg_dst_aver_v_src0_16x16_msa;
+        c->avg_qpel_pixels_tab[0][7] =
+            ff_hv_mc_qpel_avg_dst_aver_hv_src10_16x16_msa;
+        c->avg_qpel_pixels_tab[0][8] = ff_vert_mc_qpel_avg_dst_16x16_msa;
+        c->avg_qpel_pixels_tab[0][9] = ff_hv_mc_qpel_avg_dst_aver_h_src0_16x16_msa;
+        c->avg_qpel_pixels_tab[0][10] = ff_hv_mc_qpel_avg_dst_16x16_msa;
+        c->avg_qpel_pixels_tab[0][11] = ff_hv_mc_qpel_avg_dst_aver_h_src1_16x16_msa;
+        c->avg_qpel_pixels_tab[0][12] = ff_vert_mc_qpel_avg_dst_aver_src1_16x16_msa;
+        c->avg_qpel_pixels_tab[0][13] =
+            ff_hv_mc_qpel_avg_dst_aver_hv_src01_16x16_msa;
+        c->avg_qpel_pixels_tab[0][14] = ff_hv_mc_qpel_avg_dst_aver_v_src1_16x16_msa;
+        c->avg_qpel_pixels_tab[0][15] =
+            ff_hv_mc_qpel_avg_dst_aver_hv_src11_16x16_msa;
 
-void ff_qpeldsp_init_mips(QpelDSPContext *c)
-{
-#if HAVE_MSA
-    qpeldsp_init_msa(c);
-#endif  // #if HAVE_MSA
+        c->avg_qpel_pixels_tab[1][0] = ff_avg_width8_msa;
+        c->avg_qpel_pixels_tab[1][1] =
+            ff_horiz_mc_qpel_avg_dst_aver_src0_8width_msa;
+        c->avg_qpel_pixels_tab[1][2] = ff_horiz_mc_qpel_avg_dst_8width_msa;
+        c->avg_qpel_pixels_tab[1][3] =
+            ff_horiz_mc_qpel_avg_dst_aver_src1_8width_msa;
+        c->avg_qpel_pixels_tab[1][4] = ff_vert_mc_qpel_avg_dst_aver_src0_8x8_msa;
+        c->avg_qpel_pixels_tab[1][5] = ff_hv_mc_qpel_avg_dst_aver_hv_src00_8x8_msa;
+        c->avg_qpel_pixels_tab[1][6] = ff_hv_mc_qpel_avg_dst_aver_v_src0_8x8_msa;
+        c->avg_qpel_pixels_tab[1][7] = ff_hv_mc_qpel_avg_dst_aver_hv_src10_8x8_msa;
+        c->avg_qpel_pixels_tab[1][8] = ff_vert_mc_qpel_avg_dst_8x8_msa;
+        c->avg_qpel_pixels_tab[1][9] = ff_hv_mc_qpel_avg_dst_aver_h_src0_8x8_msa;
+        c->avg_qpel_pixels_tab[1][10] = ff_hv_mc_qpel_avg_dst_8x8_msa;
+        c->avg_qpel_pixels_tab[1][11] = ff_hv_mc_qpel_avg_dst_aver_h_src1_8x8_msa;
+        c->avg_qpel_pixels_tab[1][12] = ff_vert_mc_qpel_avg_dst_aver_src1_8x8_msa;
+        c->avg_qpel_pixels_tab[1][13] = ff_hv_mc_qpel_avg_dst_aver_hv_src01_8x8_msa;
+        c->avg_qpel_pixels_tab[1][14] = ff_hv_mc_qpel_avg_dst_aver_v_src1_8x8_msa;
+        c->avg_qpel_pixels_tab[1][15] = ff_hv_mc_qpel_avg_dst_aver_hv_src11_8x8_msa;
+    }
 }
diff --git a/libavcodec/mips/qpeldsp_msa.c b/libavcodec/mips/qpeldsp_msa.c
index fba42b3003..c7675f112e 100644
--- a/libavcodec/mips/qpeldsp_msa.c
+++ b/libavcodec/mips/qpeldsp_msa.c
@@ -96,7 +96,7 @@
     DPADD_UB2_UH(sum2_r, sum1_r, coef2, coef1, sum0_r, sum3_r);         \
     res0_r = (v8i16) (sum0_r - sum3_r);                                 \
     res0_r = __msa_srari_h(res0_r, 5);                                  \
-    res0_r = CLIP_SH_0_255(res0_r);                                     \
+    CLIP_SH_0_255(res0_r);                                              \
     out = (v16u8) __msa_pckev_b((v16i8) res0_r, (v16i8) res0_r);        \
                                                                         \
     out;                                                                \
@@ -118,7 +118,7 @@
     res0_r = (v8i16) (sum0_r - sum3_r);                                   \
     res0_r += 15;                                                         \
     res0_r >>= 5;                                                         \
-    res0_r = CLIP_SH_0_255(res0_r);                                       \
+    CLIP_SH_0_255(res0_r);                                                \
     out = (v16u8) __msa_pckev_b((v16i8) res0_r, (v16i8) res0_r);          \
                                                                           \
     out;                                                                  \
@@ -480,8 +480,8 @@ static void horiz_mc_qpel_aver_src1_8width_msa(const uint8_t *src,
         res1 = APPLY_HORIZ_QPEL_FILTER_8BYTE(inp2, inp3,
                                              mask0, mask1, mask2, mask3,
                                              const20, const6, const3);
-        SLDI_B2_UB(inp0, inp1, inp0, inp1, inp0, inp1, 1);
-        SLDI_B2_UB(inp2, inp3, inp2, inp3, inp2, inp3, 1);
+        SLDI_B4_UB(inp0, inp0, inp1, inp1, inp2, inp2, inp3, inp3, 1,
+                   inp0, inp1, inp2, inp3);
         inp0 = (v16u8) __msa_insve_d((v2i64) inp0, 1, (v2i64) inp1);
         inp2 = (v16u8) __msa_insve_d((v2i64) inp2, 1, (v2i64) inp3);
         AVER_UB2_UB(inp0, res0, inp2, res1, res0, res1);
@@ -710,8 +710,8 @@ static void horiz_mc_qpel_no_rnd_aver_src1_8width_msa(const uint8_t *src,
         res1 = APPLY_HORIZ_QPEL_NO_ROUND_FILTER_8BYTE(inp2, inp3, mask0, mask1,
                                                       mask2, mask3, const20,
                                                       const6, const3);
-        SLDI_B2_UB(inp0, inp1, inp0, inp1, inp0, inp1, 1);
-        SLDI_B2_UB(inp2, inp3, inp2, inp3, inp2, inp3, 1);
+        SLDI_B4_UB(inp0, inp0, inp1, inp1, inp2, inp2, inp3, inp3, 1,
+                   inp0, inp1, inp2, inp3);
         inp0 = (v16u8) __msa_insve_d((v2i64) inp0, 1, (v2i64) inp1);
         inp2 = (v16u8) __msa_insve_d((v2i64) inp2, 1, (v2i64) inp3);
         res0 = __msa_ave_u_b(inp0, res0);
@@ -948,8 +948,8 @@ static void horiz_mc_qpel_avg_dst_aver_src1_8width_msa(const uint8_t *src,
                                              mask0, mask1, mask2, mask3,
                                              const20, const6, const3);
         LD_UB4(dst, dst_stride, dst0, dst1, dst2, dst3);
-        SLDI_B2_UB(inp0, inp1, inp0, inp1, inp0, inp1, 1);
-        SLDI_B2_UB(inp2, inp3, inp2, inp3, inp2, inp3, 1);
+        SLDI_B4_UB(inp0, inp0, inp1, inp1, inp2, inp2, inp3, inp3, 1,
+                   inp0, inp1, inp2, inp3);
         inp0 = (v16u8) __msa_insve_d((v2i64) inp0, 1, (v2i64) inp1);
         inp2 = (v16u8) __msa_insve_d((v2i64) inp2, 1, (v2i64) inp3);
         dst0 = (v16u8) __msa_insve_d((v2i64) dst0, 1, (v2i64) dst1);
@@ -3094,7 +3094,7 @@ static void hv_mc_qpel_no_rnd_aver_hv_src10_8x8_msa(const uint8_t *src,
     res0 = APPLY_HORIZ_QPEL_NO_ROUND_FILTER_8BYTE(inp0, inp1, mask0, mask1,
                                                   mask2, mask3, const20,
                                                   const6, const3);
-    SLDI_B2_UB(inp0, inp1, inp0, inp1, inp0, inp1, 1);
+    SLDI_B2_UB(inp0, inp0, inp1, inp1, 1, inp0, inp1);
 
     inp0 = (v16u8) __msa_insve_d((v2i64) inp0, 1, (v2i64) inp1);
     horiz0 = __msa_ave_u_b(inp0, res0);
@@ -3104,7 +3104,7 @@ static void hv_mc_qpel_no_rnd_aver_hv_src10_8x8_msa(const uint8_t *src,
     res1 = APPLY_HORIZ_QPEL_NO_ROUND_FILTER_8BYTE(inp2, inp3, mask0, mask1,
                                                   mask2, mask3, const20,
                                                   const6, const3);
-    SLDI_B2_UB(inp2, inp3, inp2, inp3, inp2, inp3, 1);
+    SLDI_B2_UB(inp2, inp2, inp3, inp3, 1, inp2, inp3);
 
     inp2 = (v16u8) __msa_insve_d((v2i64) inp2, 1, (v2i64) inp3);
     horiz2 = __msa_ave_u_b(inp2, res1);
@@ -3114,7 +3114,7 @@ static void hv_mc_qpel_no_rnd_aver_hv_src10_8x8_msa(const uint8_t *src,
     res0 = APPLY_HORIZ_QPEL_NO_ROUND_FILTER_8BYTE(inp0, inp1, mask0, mask1,
                                                   mask2, mask3, const20,
                                                   const6, const3);
-    SLDI_B2_UB(inp0, inp1, inp0, inp1, inp0, inp1, 1);
+    SLDI_B2_UB(inp0, inp0, inp1, inp1, 1, inp0, inp1);
 
     inp0 = (v16u8) __msa_insve_d((v2i64) inp0, 1, (v2i64) inp1);
     horiz4 = __msa_ave_u_b(inp0, res0);
@@ -3134,7 +3134,7 @@ static void hv_mc_qpel_no_rnd_aver_hv_src10_8x8_msa(const uint8_t *src,
     res1 = APPLY_HORIZ_QPEL_NO_ROUND_FILTER_8BYTE(inp2, inp3, mask0, mask1,
                                                   mask2, mask3, const20,
                                                   const6, const3);
-    SLDI_B2_UB(inp2, inp3, inp2, inp3, inp2, inp3, 1);
+    SLDI_B2_UB(inp2, inp2, inp3, inp3, 1, inp2, inp3);
 
     inp2 = (v16u8) __msa_insve_d((v2i64) inp2, 1, (v2i64) inp3);
     horiz6 = __msa_ave_u_b(inp2, res1);
@@ -3389,7 +3389,7 @@ static void hv_mc_qpel_no_rnd_aver_h_src1_8x8_msa(const uint8_t *src,
     res0 = APPLY_HORIZ_QPEL_NO_ROUND_FILTER_8BYTE(inp0, inp1, mask0, mask1,
                                                   mask2, mask3, const20,
                                                   const6, const3);
-    SLDI_B2_UB(inp0, inp1, inp0, inp1, inp0, inp1, 1);
+    SLDI_B2_UB(inp0, inp0, inp1, inp1, 1, inp0, inp1);
 
     inp0 = (v16u8) __msa_insve_d((v2i64) inp0, 1, (v2i64) inp1);
     horiz0 = __msa_ave_u_b(inp0, res0);
@@ -3399,7 +3399,7 @@ static void hv_mc_qpel_no_rnd_aver_h_src1_8x8_msa(const uint8_t *src,
     res1 = APPLY_HORIZ_QPEL_NO_ROUND_FILTER_8BYTE(inp2, inp3, mask0, mask1,
                                                   mask2, mask3, const20,
                                                   const6, const3);
-    SLDI_B2_UB(inp2, inp3, inp2, inp3, inp2, inp3, 1);
+    SLDI_B2_UB(inp2, inp2, inp3, inp3, 1, inp2, inp3);
 
     inp2 = (v16u8) __msa_insve_d((v2i64) inp2, 1, (v2i64) inp3);
     horiz2 = __msa_ave_u_b(inp2, res1);
@@ -3409,7 +3409,7 @@ static void hv_mc_qpel_no_rnd_aver_h_src1_8x8_msa(const uint8_t *src,
     res0 = APPLY_HORIZ_QPEL_NO_ROUND_FILTER_8BYTE(inp0, inp1, mask0, mask1,
                                                   mask2, mask3, const20,
                                                   const6, const3);
-    SLDI_B2_UB(inp0, inp1, inp0, inp1, inp0, inp1, 1);
+    SLDI_B2_UB(inp0, inp0, inp1, inp1, 1, inp0, inp1);
 
     inp0 = (v16u8) __msa_insve_d((v2i64) inp0, 1, (v2i64) inp1);
     horiz4 = __msa_ave_u_b(inp0, res0);
@@ -3427,7 +3427,7 @@ static void hv_mc_qpel_no_rnd_aver_h_src1_8x8_msa(const uint8_t *src,
     res1 = APPLY_HORIZ_QPEL_NO_ROUND_FILTER_8BYTE(inp2, inp3, mask0, mask1,
                                                   mask2, mask3, const20,
                                                   const6, const3);
-    SLDI_B2_UB(inp2, inp3, inp2, inp3, inp2, inp3, 1);
+    SLDI_B2_UB(inp2, inp2, inp3, inp3, 1, inp2, inp3);
 
     inp2 = (v16u8) __msa_insve_d((v2i64) inp2, 1, (v2i64) inp3);
     horiz6 = __msa_ave_u_b(inp2, res1);
@@ -3691,7 +3691,7 @@ static void hv_mc_qpel_no_rnd_aver_hv_src11_8x8_msa(const uint8_t *src,
     res0 = APPLY_HORIZ_QPEL_NO_ROUND_FILTER_8BYTE(inp0, inp1, mask0, mask1,
                                                   mask2, mask3, const20,
                                                   const6, const3);
-    SLDI_B2_UB(inp0, inp1, inp0, inp1, inp0, inp1, 1);
+    SLDI_B2_UB(inp0, inp0, inp1, inp1, 1, inp0, inp1);
 
     inp0 = (v16u8) __msa_insve_d((v2i64) inp0, 1, (v2i64) inp1);
     horiz0 = __msa_ave_u_b(inp0, res0);
@@ -3701,7 +3701,7 @@ static void hv_mc_qpel_no_rnd_aver_hv_src11_8x8_msa(const uint8_t *src,
     res1 = APPLY_HORIZ_QPEL_NO_ROUND_FILTER_8BYTE(inp2, inp3, mask0, mask1,
                                                   mask2, mask3, const20,
                                                   const6, const3);
-    SLDI_B2_UB(inp2, inp3, inp2, inp3, inp2, inp3, 1);
+    SLDI_B2_UB(inp2, inp2, inp3, inp3, 1, inp2, inp3);
 
     inp2 = (v16u8) __msa_insve_d((v2i64) inp2, 1, (v2i64) inp3);
     horiz2 = __msa_ave_u_b(inp2, res1);
@@ -3712,7 +3712,7 @@ static void hv_mc_qpel_no_rnd_aver_hv_src11_8x8_msa(const uint8_t *src,
                                                   mask2, mask3, const20,
                                                   const6, const3);
 
-    SLDI_B2_UB(inp0, inp1, inp0, inp1, inp0, inp1, 1);
+    SLDI_B2_UB(inp0, inp0, inp1, inp1, 1, inp0, inp1);
     inp0 = (v16u8) __msa_insve_d((v2i64) inp0, 1, (v2i64) inp1);
     horiz4 = __msa_ave_u_b(inp0, res0);
     horiz5 = (v16u8) __msa_splati_d((v2i64) horiz4, 1);
@@ -3731,7 +3731,7 @@ static void hv_mc_qpel_no_rnd_aver_hv_src11_8x8_msa(const uint8_t *src,
     res1 = APPLY_HORIZ_QPEL_NO_ROUND_FILTER_8BYTE(inp2, inp3, mask0, mask1,
                                                   mask2, mask3, const20,
                                                   const6, const3);
-    SLDI_B2_UB(inp2, inp3, inp2, inp3, inp2, inp3, 1);
+    SLDI_B2_UB(inp2, inp2, inp3, inp3, 1, inp2, inp3);
 
     inp2 = (v16u8) __msa_insve_d((v2i64) inp2, 1, (v2i64) inp3);
     horiz6 = __msa_ave_u_b(inp2, res1);
@@ -4134,12 +4134,12 @@ static void hv_mc_qpel_aver_hv_src10_8x8_msa(const uint8_t *src,
                                          const20, const6, const3);
     res1 = APPLY_HORIZ_QPEL_FILTER_8BYTE(inp2, inp3, mask0, mask1, mask2, mask3,
                                          const20, const6, const3);
-    SLDI_B2_UB(inp0, inp1, inp0, inp1, inp0, inp1, 1);
+    SLDI_B2_UB(inp0, inp0, inp1, inp1, 1, inp0, inp1);
 
     inp0 = (v16u8) __msa_insve_d((v2i64) inp0, 1, (v2i64) inp1);
     horiz0 = __msa_aver_u_b(inp0, res0);
     horiz1 = (v16u8) __msa_splati_d((v2i64) horiz0, 1);
-    SLDI_B2_UB(inp2, inp3, inp2, inp3, inp2, inp3, 1);
+    SLDI_B2_UB(inp2, inp2, inp3, inp3, 1, inp2, inp3);
 
     inp2 = (v16u8) __msa_insve_d((v2i64) inp2, 1, (v2i64) inp3);
     horiz2 = __msa_aver_u_b(inp2, res1);
@@ -4150,12 +4150,12 @@ static void hv_mc_qpel_aver_hv_src10_8x8_msa(const uint8_t *src,
                                          const20, const6, const3);
     res1 = APPLY_HORIZ_QPEL_FILTER_8BYTE(inp2, inp3, mask0, mask1, mask2, mask3,
                                          const20, const6, const3);
-    SLDI_B2_UB(inp0, inp1, inp0, inp1, inp0, inp1, 1);
+    SLDI_B2_UB(inp0, inp0, inp1, inp1, 1, inp0, inp1);
 
     inp0 = (v16u8) __msa_insve_d((v2i64) inp0, 1, (v2i64) inp1);
     horiz4 = __msa_aver_u_b(inp0, res0);
     horiz5 = (v16u8) __msa_splati_d((v2i64) horiz4, 1);
-    SLDI_B2_UB(inp2, inp3, inp2, inp3, inp2, inp3, 1);
+    SLDI_B2_UB(inp2, inp2, inp3, inp3, 1, inp2, inp3);
 
     inp2 = (v16u8) __msa_insve_d((v2i64) inp2, 1, (v2i64) inp3);
     horiz6 = __msa_aver_u_b(inp2, res1);
@@ -4410,12 +4410,12 @@ static void hv_mc_qpel_aver_h_src1_8x8_msa(const uint8_t *src,
                                          const20, const6, const3);
     res1 = APPLY_HORIZ_QPEL_FILTER_8BYTE(inp2, inp3, mask0, mask1, mask2, mask3,
                                          const20, const6, const3);
-    SLDI_B2_UB(inp0, inp1, inp0, inp1, inp0, inp1, 1);
+    SLDI_B2_UB(inp0, inp0, inp1, inp1, 1, inp0, inp1);
 
     inp0 = (v16u8) __msa_insve_d((v2i64) inp0, 1, (v2i64) inp1);
     horiz0 = __msa_aver_u_b(inp0, res0);
     horiz1 = (v16u8) __msa_splati_d((v2i64) horiz0, 1);
-    SLDI_B2_UB(inp2, inp3, inp2, inp3, inp2, inp3, 1);
+    SLDI_B2_UB(inp2, inp2, inp3, inp3, 1, inp2, inp3);
 
     inp2 = (v16u8) __msa_insve_d((v2i64) inp2, 1, (v2i64) inp3);
     horiz2 = __msa_aver_u_b(inp2, res1);
@@ -4426,12 +4426,12 @@ static void hv_mc_qpel_aver_h_src1_8x8_msa(const uint8_t *src,
                                          const20, const6, const3);
     res1 = APPLY_HORIZ_QPEL_FILTER_8BYTE(inp2, inp3, mask0, mask1, mask2, mask3,
                                          const20, const6, const3);
-    SLDI_B2_UB(inp0, inp1, inp0, inp1, inp0, inp1, 1);
+    SLDI_B2_UB(inp0, inp0, inp1, inp1, 1, inp0, inp1);
 
     inp0 = (v16u8) __msa_insve_d((v2i64) inp0, 1, (v2i64) inp1);
     horiz4 = __msa_aver_u_b(inp0, res0);
     horiz5 = (v16u8) __msa_splati_d((v2i64) horiz4, 1);
-    SLDI_B2_UB(inp2, inp3, inp2, inp3, inp2, inp3, 1);
+    SLDI_B2_UB(inp2, inp2, inp3, inp3, 1, inp2, inp3);
 
     inp2 = (v16u8) __msa_insve_d((v2i64) inp2, 1, (v2i64) inp3);
     horiz6 = __msa_aver_u_b(inp2, res1);
@@ -4690,14 +4690,14 @@ static void hv_mc_qpel_aver_hv_src11_8x8_msa(const uint8_t *src,
     res0 = APPLY_HORIZ_QPEL_FILTER_8BYTE(inp0, inp1,
                                          mask0, mask1, mask2, mask3,
                                          const20, const6, const3);
-    SLDI_B2_UB(inp0, inp1, inp0, inp1, inp0, inp1, 1);
+    SLDI_B2_UB(inp0, inp0, inp1, inp1, 1, inp0, inp1);
 
     inp0 = (v16u8) __msa_insve_d((v2i64) inp0, 1, (v2i64) inp1);
     horiz0 = __msa_aver_u_b(inp0, res0);
     horiz1 = (v16u8) __msa_splati_d((v2i64) horiz0, 1);
     res1 = APPLY_HORIZ_QPEL_FILTER_8BYTE(inp2, inp3, mask0, mask1, mask2, mask3,
                                          const20, const6, const3);
-    SLDI_B2_UB(inp2, inp3, inp2, inp3, inp2, inp3, 1);
+    SLDI_B2_UB(inp2, inp2, inp3, inp3, 1, inp2, inp3);
 
     inp2 = (v16u8) __msa_insve_d((v2i64) inp2, 1, (v2i64) inp3);
     horiz2 = __msa_aver_u_b(inp2, res1);
@@ -4706,7 +4706,7 @@ static void hv_mc_qpel_aver_hv_src11_8x8_msa(const uint8_t *src,
     src += (2 * src_stride);
     res0 = APPLY_HORIZ_QPEL_FILTER_8BYTE(inp0, inp1, mask0, mask1, mask2, mask3,
                                          const20, const6, const3);
-    SLDI_B2_UB(inp0, inp1, inp0, inp1, inp0, inp1, 1);
+    SLDI_B2_UB(inp0, inp0, inp1, inp1, 1, inp0, inp1);
 
     inp0 = (v16u8) __msa_insve_d((v2i64) inp0, 1, (v2i64) inp1);
     horiz4 = __msa_aver_u_b(inp0, res0);
@@ -4725,7 +4725,7 @@ static void hv_mc_qpel_aver_hv_src11_8x8_msa(const uint8_t *src,
 
     res1 = APPLY_HORIZ_QPEL_FILTER_8BYTE(inp2, inp3, mask0, mask1, mask2, mask3,
                                          const20, const6, const3);
-    SLDI_B2_UB(inp2, inp3, inp2, inp3, inp2, inp3, 1);
+    SLDI_B2_UB(inp2, inp2, inp3, inp3, 1, inp2, inp3);
 
     inp2 = (v16u8) __msa_insve_d((v2i64) inp2, 1, (v2i64) inp3);
     horiz6 = __msa_aver_u_b(inp2, res1);
@@ -5020,7 +5020,7 @@ static void hv_mc_qpel_avg_dst_aver_hv_src10_8x8_msa(const uint8_t *src,
 
     LD_UB2(src, src_stride, inp2, inp3);
     src += (2 * src_stride);
-    SLDI_B2_UB(inp0, inp1, inp0, inp1, inp0, inp1, 1);
+    SLDI_B2_UB(inp0, inp0, inp1, inp1, 1, inp0, inp1);
 
     inp0 = (v16u8) __msa_ilvr_d((v2i64) inp1, (v2i64) inp0);
     horiz0 = __msa_aver_u_b(inp0, res0);
@@ -5029,7 +5029,7 @@ static void hv_mc_qpel_avg_dst_aver_hv_src10_8x8_msa(const uint8_t *src,
                                          const20, const6, const3);
     LD_UB2(src, src_stride, inp0, inp1);
     src += (2 * src_stride);
-    SLDI_B2_UB(inp2, inp3, inp2, inp3, inp2, inp3, 1);
+    SLDI_B2_UB(inp2, inp2, inp3, inp3, 1, inp2, inp3);
 
     inp2 = (v16u8) __msa_ilvr_d((v2i64) inp3, (v2i64) inp2);
     horiz2 = __msa_aver_u_b(inp2, res1);
@@ -5037,7 +5037,7 @@ static void hv_mc_qpel_avg_dst_aver_hv_src10_8x8_msa(const uint8_t *src,
     res0 = APPLY_HORIZ_QPEL_FILTER_8BYTE(inp0, inp1, mask0, mask1, mask2, mask3,
                                          const20, const6, const3);
 
-    SLDI_B2_UB(inp0, inp1, inp0, inp1, inp0, inp1, 1);
+    SLDI_B2_UB(inp0, inp0, inp1, inp1, 1, inp0, inp1);
 
     inp0 = (v16u8) __msa_ilvr_d((v2i64) inp1, (v2i64) inp0);
     horiz4 = __msa_aver_u_b(inp0, res0);
@@ -5060,7 +5060,7 @@ static void hv_mc_qpel_avg_dst_aver_hv_src10_8x8_msa(const uint8_t *src,
     res1 = APPLY_HORIZ_QPEL_FILTER_8BYTE(inp2, inp3, mask0, mask1, mask2, mask3,
                                          const20, const6, const3);
 
-    SLDI_B2_UB(inp2, inp3, inp2, inp3, inp2, inp3, 1);
+    SLDI_B2_UB(inp2, inp2, inp3, inp3, 1, inp2, inp3);
 
     inp2 = (v16u8) __msa_ilvr_d((v2i64) inp3, (v2i64) inp2);
     horiz6 = __msa_aver_u_b(inp2, res1);
@@ -5347,7 +5347,7 @@ static void hv_mc_qpel_avg_dst_aver_h_src1_8x8_msa(const uint8_t *src,
                                          const20, const6, const3);
     LD_UB2(src, src_stride, inp2, inp3);
     src += (2 * src_stride);
-    SLDI_B2_UB(inp0, inp1, inp0, inp1, inp0, inp1, 1);
+    SLDI_B2_UB(inp0, inp0, inp1, inp1, 1, inp0, inp1);
 
     inp0 = (v16u8) __msa_ilvr_d((v2i64) inp1, (v2i64) inp0);
     horiz0 = __msa_aver_u_b(inp0, res0);
@@ -5356,7 +5356,7 @@ static void hv_mc_qpel_avg_dst_aver_h_src1_8x8_msa(const uint8_t *src,
                                          const20, const6, const3);
     LD_UB2(src, src_stride, inp0, inp1);
     src += (2 * src_stride);
-    SLDI_B2_UB(inp2, inp3, inp2, inp3, inp2, inp3, 1);
+    SLDI_B2_UB(inp2, inp2, inp3, inp3, 1, inp2, inp3);
 
     inp2 = (v16u8) __msa_ilvr_d((v2i64) inp3, (v2i64) inp2);
     horiz2 = __msa_aver_u_b(inp2, res1);
@@ -5364,7 +5364,7 @@ static void hv_mc_qpel_avg_dst_aver_h_src1_8x8_msa(const uint8_t *src,
     res0 = APPLY_HORIZ_QPEL_FILTER_8BYTE(inp0, inp1, mask0, mask1, mask2, mask3,
                                          const20, const6, const3);
 
-    SLDI_B2_UB(inp0, inp1, inp0, inp1, inp0, inp1, 1);
+    SLDI_B2_UB(inp0, inp0, inp1, inp1, 1, inp0, inp1);
 
     inp0 = (v16u8) __msa_ilvr_d((v2i64) inp1, (v2i64) inp0);
     horiz4 = __msa_aver_u_b(inp0, res0);
@@ -5385,7 +5385,7 @@ static void hv_mc_qpel_avg_dst_aver_h_src1_8x8_msa(const uint8_t *src,
     res1 = APPLY_HORIZ_QPEL_FILTER_8BYTE(inp2, inp3, mask0, mask1, mask2, mask3,
                                          const20, const6, const3);
 
-    SLDI_B2_UB(inp2, inp3, inp2, inp3, inp2, inp3, 1);
+    SLDI_B2_UB(inp2, inp2, inp3, inp3, 1, inp2, inp3);
 
     inp2 = (v16u8) __msa_ilvr_d((v2i64) inp3, (v2i64) inp2);
     horiz6 = __msa_aver_u_b(inp2, res1);
@@ -5684,7 +5684,7 @@ static void hv_mc_qpel_avg_dst_aver_hv_src11_8x8_msa(const uint8_t *src,
                                          const20, const6, const3);
     LD_UB2(src, src_stride, inp2, inp3);
     src += (2 * src_stride);
-    SLDI_B2_UB(inp0, inp1, inp0, inp1, inp0, inp1, 1);
+    SLDI_B2_UB(inp0, inp0, inp1, inp1, 1, inp0, inp1);
 
     inp0 = (v16u8) __msa_ilvr_d((v2i64) inp1, (v2i64) inp0);
     horiz0 = __msa_aver_u_b(inp0, res0);
@@ -5693,14 +5693,14 @@ static void hv_mc_qpel_avg_dst_aver_hv_src11_8x8_msa(const uint8_t *src,
                                          const20, const6, const3);
     LD_UB2(src, src_stride, inp0, inp1);
     src += (2 * src_stride);
-    SLDI_B2_UB(inp2, inp3, inp2, inp3, inp2, inp3, 1);
+    SLDI_B2_UB(inp2, inp2, inp3, inp3, 1, inp2, inp3);
 
     inp2 = (v16u8) __msa_ilvr_d((v2i64) inp3, (v2i64) inp2);
     horiz2 = __msa_aver_u_b(inp2, res1);
     horiz3 = (v16u8) __msa_splati_d((v2i64) horiz2, 1);
     res0 = APPLY_HORIZ_QPEL_FILTER_8BYTE(inp0, inp1, mask0, mask1, mask2, mask3,
                                          const20, const6, const3);
-    SLDI_B2_UB(inp0, inp1, inp0, inp1, inp0, inp1, 1);
+    SLDI_B2_UB(inp0, inp0, inp1, inp1, 1, inp0, inp1);
 
     inp0 = (v16u8) __msa_ilvr_d((v2i64) inp1, (v2i64) inp0);
     horiz4 = __msa_aver_u_b(inp0, res0);
@@ -5721,7 +5721,7 @@ static void hv_mc_qpel_avg_dst_aver_hv_src11_8x8_msa(const uint8_t *src,
     src += (2 * src_stride);
     res1 = APPLY_HORIZ_QPEL_FILTER_8BYTE(inp2, inp3, mask0, mask1, mask2, mask3,
                                          const20, const6, const3);
-    SLDI_B2_UB(inp2, inp3, inp2, inp3, inp2, inp3, 1);
+    SLDI_B2_UB(inp2, inp2, inp3, inp3, 1, inp2, inp3);
 
     inp2 = (v16u8) __msa_ilvr_d((v2i64) inp3, (v2i64) inp2);
     horiz6 = __msa_aver_u_b(inp2, res1);
diff --git a/libavcodec/mips/sbrdsp_mips.c b/libavcodec/mips/sbrdsp_mips.c
index 1b0a10608d..6c57f2661b 100644
--- a/libavcodec/mips/sbrdsp_mips.c
+++ b/libavcodec/mips/sbrdsp_mips.c
@@ -796,9 +796,9 @@ static void sbr_hf_apply_noise_2_mips(float (*Y)[2], const float *s_m,
                                  const float *q_filt, int noise,
                                  int kx, int m_max)
 {
-    int m;
+    int m, temp0, temp1;
     float *ff_table;
-    float y0,y1, temp0, temp1, temp2, temp3, temp4, temp5;
+    float y0, y1, temp2, temp3, temp4, temp5;
 
     for (m = 0; m < m_max; m++) {
 
@@ -808,14 +808,14 @@ static void sbr_hf_apply_noise_2_mips(float (*Y)[2], const float *s_m,
 
         __asm__ volatile(
             "lwc1   %[y0],       0(%[Y1])                                  \n\t"
-            "lwc1   %[temp1],    0(%[s_m1])                                \n\t"
+            "lwc1   %[temp3],    0(%[s_m1])                                \n\t"
             "addiu  %[noise],    %[noise],              1                  \n\t"
             "andi   %[noise],    %[noise],              0x1ff              \n\t"
             "sll    %[temp0],    %[noise],              3                  \n\t"
             PTR_ADDU "%[ff_table],%[ff_sbr_noise_table],%[temp0]           \n\t"
-            "sub.s  %[y0],       %[y0],                 %[temp1]           \n\t"
-            "mfc1   %[temp3],    %[temp1]                                  \n\t"
-            "bne    %[temp3],    $0,                    1f                 \n\t"
+            "sub.s  %[y0],       %[y0],                 %[temp3]           \n\t"
+            "mfc1   %[temp1],    %[temp3]                                  \n\t"
+            "bne    %[temp1],    $0,                    1f                 \n\t"
             "lwc1   %[y1],       4(%[Y1])                                  \n\t"
             "lwc1   %[temp2],    0(%[q_filt1])                             \n\t"
             "lwc1   %[temp4],    0(%[ff_table])                            \n\t"
@@ -826,9 +826,10 @@ static void sbr_hf_apply_noise_2_mips(float (*Y)[2], const float *s_m,
         "1:                                                                \n\t"
             "swc1   %[y0],       0(%[Y1])                                  \n\t"
 
-            : [temp0]"=&r"(temp0), [ff_table]"=&r"(ff_table), [y0]"=&f"(y0),
-              [y1]"=&f"(y1), [temp1]"=&f"(temp1), [temp2]"=&f"(temp2),
-              [temp3]"=&r"(temp3), [temp4]"=&f"(temp4), [temp5]"=&f"(temp5)
+            : [temp0]"=&r"(temp0), [temp1]"=&r"(temp1), [y0]"=&f"(y0),
+              [y1]"=&f"(y1), [ff_table]"=&r"(ff_table),
+              [temp2]"=&f"(temp2), [temp3]"=&f"(temp3),
+              [temp4]"=&f"(temp4), [temp5]"=&f"(temp5)
             : [ff_sbr_noise_table]"r"(ff_sbr_noise_table), [noise]"r"(noise),
               [Y1]"r"(Y1), [s_m1]"r"(s_m1), [q_filt1]"r"(q_filt1)
             : "memory"
diff --git a/libavcodec/mips/simple_idct_mmi.c b/libavcodec/mips/simple_idct_mmi.c
index 7f4bb74fd2..f331954e75 100644
--- a/libavcodec/mips/simple_idct_mmi.c
+++ b/libavcodec/mips/simple_idct_mmi.c
@@ -39,7 +39,7 @@
 #define COL_SHIFT 20
 #define DC_SHIFT 3
 
-DECLARE_ALIGNED(8, const int16_t, W_arr)[46] = {
+DECLARE_ALIGNED(16, const int16_t, W_arr)[46] = {
     W4,  W2,  W4,  W6,
     W1,  W3,  W5,  W7,
     W4,  W6, -W4, -W2,
@@ -55,6 +55,8 @@ DECLARE_ALIGNED(8, const int16_t, W_arr)[46] = {
 
 void ff_simple_idct_8_mmi(int16_t *block)
 {
+    DECLARE_VAR_ALL64;
+
     BACKUP_REG
     __asm__ volatile (
 
@@ -132,7 +134,7 @@ void ff_simple_idct_8_mmi(int16_t *block)
         "psllh        $f28,     "#src1",    $f30                \n\t" \
         "dmtc1        $9,        $f31                           \n\t" \
         "punpcklhw    $f29,      $f28,      $f28                \n\t" \
-        "and          $f29,      $f29,      $f31                \n\t" \
+        "pand         $f29,      $f29,      $f31                \n\t" \
         "paddw        $f28,      $f28,      $f29                \n\t" \
         "punpcklwd   "#src1",    $f28,      $f28                \n\t" \
         "punpcklwd   "#src2",    $f28,      $f28                \n\t" \
@@ -141,20 +143,20 @@ void ff_simple_idct_8_mmi(int16_t *block)
         /* idctRowCondDC row0~8 */
 
         /* load W */
-        "gslqc1       $f19,      $f18,      0x00(%[w_arr])      \n\t"
-        "gslqc1       $f21,      $f20,      0x10(%[w_arr])      \n\t"
-        "gslqc1       $f23,      $f22,      0x20(%[w_arr])      \n\t"
-        "gslqc1       $f25,      $f24,      0x30(%[w_arr])      \n\t"
-        "gslqc1       $f17,      $f16,      0x40(%[w_arr])      \n\t"
+        MMI_LQC1($f19, $f18, %[w_arr], 0x00)
+        MMI_LQC1($f21, $f20, %[w_arr], 0x10)
+        MMI_LQC1($f23, $f22, %[w_arr], 0x20)
+        MMI_LQC1($f25, $f24, %[w_arr], 0x30)
+        MMI_LQC1($f17, $f16, %[w_arr], 0x40)
         /* load source in block */
-        "gslqc1       $f1,       $f0,       0x00(%[block])      \n\t"
-        "gslqc1       $f3,       $f2,       0x10(%[block])      \n\t"
-        "gslqc1       $f5,       $f4,       0x20(%[block])      \n\t"
-        "gslqc1       $f7,       $f6,       0x30(%[block])      \n\t"
-        "gslqc1       $f9,       $f8,       0x40(%[block])      \n\t"
-        "gslqc1       $f11,      $f10,      0x50(%[block])      \n\t"
-        "gslqc1       $f13,      $f12,      0x60(%[block])      \n\t"
-        "gslqc1       $f15,      $f14,      0x70(%[block])      \n\t"
+        MMI_LQC1($f1, $f0, %[block], 0x00)
+        MMI_LQC1($f3, $f2, %[block], 0x10)
+        MMI_LQC1($f5, $f4, %[block], 0x20)
+        MMI_LQC1($f7, $f6, %[block], 0x30)
+        MMI_LQC1($f9, $f8, %[block], 0x40)
+        MMI_LQC1($f11, $f10, %[block], 0x50)
+        MMI_LQC1($f13, $f12, %[block], 0x60)
+        MMI_LQC1($f15, $f14, %[block], 0x70)
 
         /* $9: mask ; $f17: ROW_SHIFT */
         "dmfc1        $9,        $f17                           \n\t"
@@ -252,8 +254,7 @@ void ff_simple_idct_8_mmi(int16_t *block)
         /* idctSparseCol col0~3 */
 
         /* $f17: ff_p16_32; $f16: COL_SHIFT-16 */
-        "gsldlc1      $f17,      0x57(%[w_arr])                 \n\t"
-        "gsldrc1      $f17,      0x50(%[w_arr])                 \n\t"
+        MMI_ULDC1($f17, %[w_arr], 0x50)
         "li           $10,       4                              \n\t"
         "dmtc1        $10,       $f16                           \n\t"
         "paddh        $f0,       $f0,       $f17                \n\t"
@@ -267,9 +268,9 @@ void ff_simple_idct_8_mmi(int16_t *block)
         "punpcklwd    $f8,       $f27,      $f29                \n\t"
         "punpckhwd    $f12,      $f27,      $f29                \n\t"
 
-        "or           $f26,      $f2,       $f6                 \n\t"
-        "or           $f26,      $f26,      $f10                \n\t"
-        "or           $f26,      $f26,      $f14                \n\t"
+        "por          $f26,      $f2,       $f6                 \n\t"
+        "por          $f26,      $f26,      $f10                \n\t"
+        "por          $f26,      $f26,      $f14                \n\t"
         "dmfc1        $10,       $f26                           \n\t"
         "bnez         $10,       1f                             \n\t"
         /* case1: In this case, row[1,3,5,7] are all zero */
@@ -337,9 +338,9 @@ void ff_simple_idct_8_mmi(int16_t *block)
         "punpcklwd    $f9,       $f27,      $f29                \n\t"
         "punpckhwd    $f13,      $f27,      $f29                \n\t"
 
-        "or           $f26,      $f3,       $f7                 \n\t"
-        "or           $f26,      $f26,      $f11                \n\t"
-        "or           $f26,      $f26,      $f15                \n\t"
+        "por          $f26,      $f3,       $f7                 \n\t"
+        "por          $f26,      $f26,      $f11                \n\t"
+        "por          $f26,      $f26,      $f15                \n\t"
         "dmfc1        $10,       $f26                           \n\t"
         "bnez         $10,       1f                             \n\t"
         /* case1: In this case, row[1,3,5,7] are all zero */
@@ -394,16 +395,16 @@ void ff_simple_idct_8_mmi(int16_t *block)
         "punpcklwd    $f11,      $f27,      $f29                \n\t"
         "punpckhwd    $f15,      $f27,      $f29                \n\t"
         /* Store */
-        "gssqc1       $f1,       $f0,       0x00(%[block])      \n\t"
-        "gssqc1       $f5,       $f4,       0x10(%[block])      \n\t"
-        "gssqc1       $f9,       $f8,       0x20(%[block])      \n\t"
-        "gssqc1       $f13,      $f12,      0x30(%[block])      \n\t"
-        "gssqc1       $f3,       $f2,       0x40(%[block])      \n\t"
-        "gssqc1       $f7,       $f6,       0x50(%[block])      \n\t"
-        "gssqc1       $f11,      $f10,      0x60(%[block])      \n\t"
-        "gssqc1       $f15,      $f14,      0x70(%[block])      \n\t"
+        MMI_SQC1($f1, $f0, %[block], 0x00)
+        MMI_SQC1($f5, $f4, %[block], 0x10)
+        MMI_SQC1($f9, $f8, %[block], 0x20)
+        MMI_SQC1($f13, $f12, %[block], 0x30)
+        MMI_SQC1($f3, $f2, %[block], 0x40)
+        MMI_SQC1($f7, $f6, %[block], 0x50)
+        MMI_SQC1($f11, $f10, %[block], 0x60)
+        MMI_SQC1($f15, $f14, %[block], 0x70)
 
-        : [block]"+&r"(block)
+        : RESTRICT_ASM_ALL64 [block]"+&r"(block)
         : [w_arr]"r"(W_arr)
         : "memory"
     );
diff --git a/libavcodec/mips/simple_idct_msa.c b/libavcodec/mips/simple_idct_msa.c
index 8a7235927e..4bd3dd8a25 100644
--- a/libavcodec/mips/simple_idct_msa.c
+++ b/libavcodec/mips/simple_idct_msa.c
@@ -336,35 +336,26 @@ static void simple_idct_put_msa(uint8_t *dst, int32_t dst_stride,
     SRA_4V(temp2_r, temp2_l, temp3_r, temp3_l, 20);
     SRA_4V(a3_r, a3_l, a2_r, a2_l, 20);
     SRA_4V(a1_r, a1_l, a0_r, a0_l, 20);
-    PCKEV_H4_SW(temp0_l, temp0_r, temp1_l, temp1_r, temp2_l, temp2_r,
-                temp3_l, temp3_r, temp0_r, temp1_r, temp2_r, temp3_r);
-    PCKEV_H4_SW(a0_l, a0_r, a1_l, a1_r, a2_l, a2_r, a3_l, a3_r,
-                a0_r, a1_r, a2_r, a3_r);
-    temp0_r = (v4i32) CLIP_SH_0_255(temp0_r);
-    temp1_r = (v4i32) CLIP_SH_0_255(temp1_r);
-    temp2_r = (v4i32) CLIP_SH_0_255(temp2_r);
-    temp3_r = (v4i32) CLIP_SH_0_255(temp3_r);
-    PCKEV_B4_SW(temp0_r, temp0_r, temp1_r, temp1_r,
-                temp2_r, temp2_r, temp3_r, temp3_r,
-                temp0_r, temp1_r, temp2_r, temp3_r);
-    tmp0 = __msa_copy_u_d((v2i64) temp0_r, 1);
-    tmp1 = __msa_copy_u_d((v2i64) temp1_r, 1);
-    tmp2 = __msa_copy_u_d((v2i64) temp2_r, 1);
-    tmp3 = __msa_copy_u_d((v2i64) temp3_r, 1);
-    SD4(tmp0, tmp1, tmp2, tmp3, dst, dst_stride);
-    dst += 4 * dst_stride;
-    a0_r = (v4i32) CLIP_SH_0_255(a0_r);
-    a1_r = (v4i32) CLIP_SH_0_255(a1_r);
-    a2_r = (v4i32) CLIP_SH_0_255(a2_r);
-    a3_r = (v4i32) CLIP_SH_0_255(a3_r);
-    PCKEV_B4_SW(a0_r, a0_r, a1_r, a1_r,
-                a2_r, a2_r, a3_r, a3_r, a0_r, a1_r, a2_r, a3_r);
-    tmp3 = __msa_copy_u_d((v2i64) a0_r, 1);
-    tmp2 = __msa_copy_u_d((v2i64) a1_r, 1);
-    tmp1 = __msa_copy_u_d((v2i64) a2_r, 1);
-    tmp0 = __msa_copy_u_d((v2i64) a3_r, 1);
+    PCKEV_H4_SH(temp0_l, temp0_r, temp1_l, temp1_r, temp2_l, temp2_r,
+                temp3_l, temp3_r, in0, in1, in2, in3);
+    PCKEV_H4_SH(a0_l, a0_r, a1_l, a1_r, a2_l, a2_r, a3_l, a3_r,
+                in4, in5, in6, in7);
+    CLIP_SH4_0_255(in0, in1, in2, in3);
+    PCKEV_B4_SH(in0, in0, in1, in1, in2, in2, in3, in3,
+                in0, in1, in2, in3);
+    tmp0 = __msa_copy_u_d((v2i64) in0, 1);
+    tmp1 = __msa_copy_u_d((v2i64) in1, 1);
+    tmp2 = __msa_copy_u_d((v2i64) in2, 1);
+    tmp3 = __msa_copy_u_d((v2i64) in3, 1);
     SD4(tmp0, tmp1, tmp2, tmp3, dst, dst_stride);
-    dst += 4 * dst_stride;
+    CLIP_SH4_0_255(in4, in5, in6, in7);
+    PCKEV_B4_SH(in4, in4, in5, in5, in6, in6, in7, in7,
+                in4, in5, in6, in7);
+    tmp3 = __msa_copy_u_d((v2i64) in4, 1);
+    tmp2 = __msa_copy_u_d((v2i64) in5, 1);
+    tmp1 = __msa_copy_u_d((v2i64) in6, 1);
+    tmp0 = __msa_copy_u_d((v2i64) in7, 1);
+    SD4(tmp0, tmp1, tmp2, tmp3, dst + 4 * dst_stride, dst_stride);
 }
 
 static void simple_idct_add_msa(uint8_t *dst, int32_t dst_stride,
@@ -516,21 +507,17 @@ static void simple_idct_add_msa(uint8_t *dst, int32_t dst_stride,
                 temp3_l, temp3_r, temp0_r, temp1_r, temp2_r, temp3_r);
     ILVR_B4_SW(zero, in0, zero, in1, zero, in2, zero, in3,
                temp0_l, temp1_l, temp2_l, temp3_l);
-    temp0_r = (v4i32) ((v8i16) (temp0_r) + (v8i16) (temp0_l));
-    temp1_r = (v4i32) ((v8i16) (temp1_r) + (v8i16) (temp1_l));
-    temp2_r = (v4i32) ((v8i16) (temp2_r) + (v8i16) (temp2_l));
-    temp3_r = (v4i32) ((v8i16) (temp3_r) + (v8i16) (temp3_l));
-    temp0_r = (v4i32) CLIP_SH_0_255(temp0_r);
-    temp1_r = (v4i32) CLIP_SH_0_255(temp1_r);
-    temp2_r = (v4i32) CLIP_SH_0_255(temp2_r);
-    temp3_r = (v4i32) CLIP_SH_0_255(temp3_r);
-    PCKEV_B4_SW(temp0_r, temp0_r, temp1_r, temp1_r,
-                temp2_r, temp2_r, temp3_r, temp3_r,
-                temp0_r, temp1_r, temp2_r, temp3_r);
-    tmp0 = __msa_copy_u_d((v2i64) temp0_r, 1);
-    tmp1 = __msa_copy_u_d((v2i64) temp1_r, 1);
-    tmp2 = __msa_copy_u_d((v2i64) temp2_r, 1);
-    tmp3 = __msa_copy_u_d((v2i64) temp3_r, 1);
+    in0 = (v8i16) (temp0_r) + (v8i16) (temp0_l);
+    in1 = (v8i16) (temp1_r) + (v8i16) (temp1_l);
+    in2 = (v8i16) (temp2_r) + (v8i16) (temp2_l);
+    in3 = (v8i16) (temp3_r) + (v8i16) (temp3_l);
+    CLIP_SH4_0_255(in0, in1, in2, in3);
+    PCKEV_B4_SH(in0, in0, in1, in1, in2, in2, in3, in3,
+                in0, in1, in2, in3);
+    tmp0 = __msa_copy_u_d((v2i64) in0, 1);
+    tmp1 = __msa_copy_u_d((v2i64) in1, 1);
+    tmp2 = __msa_copy_u_d((v2i64) in2, 1);
+    tmp3 = __msa_copy_u_d((v2i64) in3, 1);
     SD4(tmp0, tmp1, tmp2, tmp3, dst, dst_stride);
 
     SRA_4V(a3_r, a3_l, a2_r, a2_l, 20);
@@ -540,20 +527,17 @@ static void simple_idct_add_msa(uint8_t *dst, int32_t dst_stride,
                 a0_r, a1_r, a2_r, a3_r);
     ILVR_B4_SW(zero, in4, zero, in5, zero, in6, zero, in7,
                a3_l, a2_l, a1_l, a0_l);
-    a3_r = (v4i32) ((v8i16) (a3_r) + (v8i16) (a3_l));
-    a2_r = (v4i32) ((v8i16) (a2_r) + (v8i16) (a2_l));
-    a1_r = (v4i32) ((v8i16) (a1_r) + (v8i16) (a1_l));
-    a0_r = (v4i32) ((v8i16) (a0_r) + (v8i16) (a0_l));
-    a3_r = (v4i32) CLIP_SH_0_255(a3_r);
-    a2_r = (v4i32) CLIP_SH_0_255(a2_r);
-    a1_r = (v4i32) CLIP_SH_0_255(a1_r);
-    a0_r = (v4i32) CLIP_SH_0_255(a0_r);
-    PCKEV_B4_SW(a0_r, a0_r, a1_r, a1_r,
-                a2_r, a2_r, a3_r, a3_r, a0_r, a1_r, a2_r, a3_r);
-    tmp0 = __msa_copy_u_d((v2i64) a3_r, 1);
-    tmp1 = __msa_copy_u_d((v2i64) a2_r, 1);
-    tmp2 = __msa_copy_u_d((v2i64) a1_r, 1);
-    tmp3 = __msa_copy_u_d((v2i64) a0_r, 1);
+    in4 = (v8i16) (a3_r) + (v8i16) (a3_l);
+    in5 = (v8i16) (a2_r) + (v8i16) (a2_l);
+    in6 = (v8i16) (a1_r) + (v8i16) (a1_l);
+    in7 = (v8i16) (a0_r) + (v8i16) (a0_l);
+    CLIP_SH4_0_255(in4, in5, in6, in7);
+    PCKEV_B4_SH(in4, in4, in5, in5, in6, in6, in7, in7,
+                in4, in5, in6, in7);
+    tmp0 = __msa_copy_u_d((v2i64) in4, 1);
+    tmp1 = __msa_copy_u_d((v2i64) in5, 1);
+    tmp2 = __msa_copy_u_d((v2i64) in6, 1);
+    tmp3 = __msa_copy_u_d((v2i64) in7, 1);
     SD4(tmp0, tmp1, tmp2, tmp3, dst + 4 * dst_stride, dst_stride);
 }
 
diff --git a/libavcodec/mips/vc1dsp_init_mips.c b/libavcodec/mips/vc1dsp_init_mips.c
index 4adc9e1d4e..94126f3a9d 100644
--- a/libavcodec/mips/vc1dsp_init_mips.c
+++ b/libavcodec/mips/vc1dsp_init_mips.c
@@ -18,91 +18,103 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
+#include "libavutil/mips/cpu.h"
 #include "libavutil/attributes.h"
 #include "libavcodec/vc1dsp.h"
 #include "vc1dsp_mips.h"
 #include "config.h"
 
-#if HAVE_MMI
-static av_cold void vc1dsp_init_mmi(VC1DSPContext *dsp)
-{
-#if _MIPS_SIM != _ABIO32
-    dsp->vc1_inv_trans_8x8    = ff_vc1_inv_trans_8x8_mmi;
-    dsp->vc1_inv_trans_4x8    = ff_vc1_inv_trans_4x8_mmi;
-    dsp->vc1_inv_trans_8x4    = ff_vc1_inv_trans_8x4_mmi;
-#endif
-    dsp->vc1_inv_trans_4x4    = ff_vc1_inv_trans_4x4_mmi;
-    dsp->vc1_inv_trans_8x8_dc = ff_vc1_inv_trans_8x8_dc_mmi;
-    dsp->vc1_inv_trans_4x8_dc = ff_vc1_inv_trans_4x8_dc_mmi;
-    dsp->vc1_inv_trans_8x4_dc = ff_vc1_inv_trans_8x4_dc_mmi;
-    dsp->vc1_inv_trans_4x4_dc = ff_vc1_inv_trans_4x4_dc_mmi;
-
-    dsp->vc1_h_overlap        = ff_vc1_h_overlap_mmi;
-    dsp->vc1_v_overlap        = ff_vc1_v_overlap_mmi;
-    dsp->vc1_h_s_overlap      = ff_vc1_h_s_overlap_mmi;
-    dsp->vc1_v_s_overlap      = ff_vc1_v_s_overlap_mmi;
-
-    dsp->vc1_v_loop_filter4  = ff_vc1_v_loop_filter4_mmi;
-    dsp->vc1_h_loop_filter4  = ff_vc1_h_loop_filter4_mmi;
-    dsp->vc1_v_loop_filter8  = ff_vc1_v_loop_filter8_mmi;
-    dsp->vc1_h_loop_filter8  = ff_vc1_h_loop_filter8_mmi;
-    dsp->vc1_v_loop_filter16 = ff_vc1_v_loop_filter16_mmi;
-    dsp->vc1_h_loop_filter16 = ff_vc1_h_loop_filter16_mmi;
-
 #define FN_ASSIGN(OP, X, Y, INSN) \
     dsp->OP##vc1_mspel_pixels_tab[1][X+4*Y] = ff_##OP##vc1_mspel_mc##X##Y##INSN; \
     dsp->OP##vc1_mspel_pixels_tab[0][X+4*Y] = ff_##OP##vc1_mspel_mc##X##Y##_16##INSN
 
-    FN_ASSIGN(put_, 0, 0, _mmi);
-    FN_ASSIGN(put_, 0, 1, _mmi);
-    FN_ASSIGN(put_, 0, 2, _mmi);
-    FN_ASSIGN(put_, 0, 3, _mmi);
-
-    FN_ASSIGN(put_, 1, 0, _mmi);
-    //FN_ASSIGN(put_, 1, 1, _mmi);//FIXME
-    //FN_ASSIGN(put_, 1, 2, _mmi);//FIXME
-    //FN_ASSIGN(put_, 1, 3, _mmi);//FIXME
-
-    FN_ASSIGN(put_, 2, 0, _mmi);
-    //FN_ASSIGN(put_, 2, 1, _mmi);//FIXME
-    //FN_ASSIGN(put_, 2, 2, _mmi);//FIXME
-    //FN_ASSIGN(put_, 2, 3, _mmi);//FIXME
-
-    FN_ASSIGN(put_, 3, 0, _mmi);
-    //FN_ASSIGN(put_, 3, 1, _mmi);//FIXME
-    //FN_ASSIGN(put_, 3, 2, _mmi);//FIXME
-    //FN_ASSIGN(put_, 3, 3, _mmi);//FIXME
-
-    FN_ASSIGN(avg_, 0, 0, _mmi);
-    FN_ASSIGN(avg_, 0, 1, _mmi);
-    FN_ASSIGN(avg_, 0, 2, _mmi);
-    FN_ASSIGN(avg_, 0, 3, _mmi);
-
-    FN_ASSIGN(avg_, 1, 0, _mmi);
-    //FN_ASSIGN(avg_, 1, 1, _mmi);//FIXME
-    //FN_ASSIGN(avg_, 1, 2, _mmi);//FIXME
-    //FN_ASSIGN(avg_, 1, 3, _mmi);//FIXME
-
-    FN_ASSIGN(avg_, 2, 0, _mmi);
-    //FN_ASSIGN(avg_, 2, 1, _mmi);//FIXME
-    //FN_ASSIGN(avg_, 2, 2, _mmi);//FIXME
-    //FN_ASSIGN(avg_, 2, 3, _mmi);//FIXME
-
-    FN_ASSIGN(avg_, 3, 0, _mmi);
-    //FN_ASSIGN(avg_, 3, 1, _mmi);//FIXME
-    //FN_ASSIGN(avg_, 3, 2, _mmi);//FIXME
-    //FN_ASSIGN(avg_, 3, 3, _mmi);//FIXME
-
-    dsp->put_no_rnd_vc1_chroma_pixels_tab[0] = ff_put_no_rnd_vc1_chroma_mc8_mmi;
-    dsp->avg_no_rnd_vc1_chroma_pixels_tab[0] = ff_avg_no_rnd_vc1_chroma_mc8_mmi;
-    dsp->put_no_rnd_vc1_chroma_pixels_tab[1] = ff_put_no_rnd_vc1_chroma_mc4_mmi;
-    dsp->avg_no_rnd_vc1_chroma_pixels_tab[1] = ff_avg_no_rnd_vc1_chroma_mc4_mmi;
-}
-#endif /* HAVE_MMI */
-
 av_cold void ff_vc1dsp_init_mips(VC1DSPContext *dsp)
 {
-#if HAVE_MMI
-    vc1dsp_init_mmi(dsp);
-#endif /* HAVE_MMI */
+    int cpu_flags = av_get_cpu_flags();
+
+    if (have_mmi(cpu_flags)) {
+ #if _MIPS_SIM != _ABIO32
+        dsp->vc1_inv_trans_8x8    = ff_vc1_inv_trans_8x8_mmi;
+        dsp->vc1_inv_trans_4x8    = ff_vc1_inv_trans_4x8_mmi;
+        dsp->vc1_inv_trans_8x4    = ff_vc1_inv_trans_8x4_mmi;
+#endif
+        dsp->vc1_inv_trans_4x4    = ff_vc1_inv_trans_4x4_mmi;
+        dsp->vc1_inv_trans_8x8_dc = ff_vc1_inv_trans_8x8_dc_mmi;
+        dsp->vc1_inv_trans_4x8_dc = ff_vc1_inv_trans_4x8_dc_mmi;
+        dsp->vc1_inv_trans_8x4_dc = ff_vc1_inv_trans_8x4_dc_mmi;
+        dsp->vc1_inv_trans_4x4_dc = ff_vc1_inv_trans_4x4_dc_mmi;
+
+        dsp->vc1_h_overlap        = ff_vc1_h_overlap_mmi;
+        dsp->vc1_v_overlap        = ff_vc1_v_overlap_mmi;
+        dsp->vc1_h_s_overlap      = ff_vc1_h_s_overlap_mmi;
+        dsp->vc1_v_s_overlap      = ff_vc1_v_s_overlap_mmi;
+
+        dsp->vc1_v_loop_filter4  = ff_vc1_v_loop_filter4_mmi;
+        dsp->vc1_h_loop_filter4  = ff_vc1_h_loop_filter4_mmi;
+        dsp->vc1_v_loop_filter8  = ff_vc1_v_loop_filter8_mmi;
+        dsp->vc1_h_loop_filter8  = ff_vc1_h_loop_filter8_mmi;
+        dsp->vc1_v_loop_filter16 = ff_vc1_v_loop_filter16_mmi;
+        dsp->vc1_h_loop_filter16 = ff_vc1_h_loop_filter16_mmi;
+
+        FN_ASSIGN(put_, 0, 0, _mmi);
+        FN_ASSIGN(put_, 0, 1, _mmi);
+        FN_ASSIGN(put_, 0, 2, _mmi);
+        FN_ASSIGN(put_, 0, 3, _mmi);
+
+        FN_ASSIGN(put_, 1, 0, _mmi);
+        //FN_ASSIGN(put_, 1, 1, _mmi);//FIXME
+        //FN_ASSIGN(put_, 1, 2, _mmi);//FIXME
+        //FN_ASSIGN(put_, 1, 3, _mmi);//FIXME
+
+        FN_ASSIGN(put_, 2, 0, _mmi);
+        //FN_ASSIGN(put_, 2, 1, _mmi);//FIXME
+        //FN_ASSIGN(put_, 2, 2, _mmi);//FIXME
+        //FN_ASSIGN(put_, 2, 3, _mmi);//FIXME
+
+        FN_ASSIGN(put_, 3, 0, _mmi);
+        //FN_ASSIGN(put_, 3, 1, _mmi);//FIXME
+        //FN_ASSIGN(put_, 3, 2, _mmi);//FIXME
+        //FN_ASSIGN(put_, 3, 3, _mmi);//FIXME
+
+        FN_ASSIGN(avg_, 0, 0, _mmi);
+        FN_ASSIGN(avg_, 0, 1, _mmi);
+        FN_ASSIGN(avg_, 0, 2, _mmi);
+        FN_ASSIGN(avg_, 0, 3, _mmi);
+
+        FN_ASSIGN(avg_, 1, 0, _mmi);
+        //FN_ASSIGN(avg_, 1, 1, _mmi);//FIXME
+        //FN_ASSIGN(avg_, 1, 2, _mmi);//FIXME
+        //FN_ASSIGN(avg_, 1, 3, _mmi);//FIXME
+
+        FN_ASSIGN(avg_, 2, 0, _mmi);
+        //FN_ASSIGN(avg_, 2, 1, _mmi);//FIXME
+        //FN_ASSIGN(avg_, 2, 2, _mmi);//FIXME
+        //FN_ASSIGN(avg_, 2, 3, _mmi);//FIXME
+
+        FN_ASSIGN(avg_, 3, 0, _mmi);
+        //FN_ASSIGN(avg_, 3, 1, _mmi);//FIXME
+        //FN_ASSIGN(avg_, 3, 2, _mmi);//FIXME
+        //FN_ASSIGN(avg_, 3, 3, _mmi);//FIXME
+
+        dsp->put_no_rnd_vc1_chroma_pixels_tab[0] = ff_put_no_rnd_vc1_chroma_mc8_mmi;
+        dsp->avg_no_rnd_vc1_chroma_pixels_tab[0] = ff_avg_no_rnd_vc1_chroma_mc8_mmi;
+        dsp->put_no_rnd_vc1_chroma_pixels_tab[1] = ff_put_no_rnd_vc1_chroma_mc4_mmi;
+        dsp->avg_no_rnd_vc1_chroma_pixels_tab[1] = ff_avg_no_rnd_vc1_chroma_mc4_mmi;
+    }
+
+    if (have_msa(cpu_flags)) {
+        dsp->vc1_inv_trans_8x8 = ff_vc1_inv_trans_8x8_msa;
+        dsp->vc1_inv_trans_4x8 = ff_vc1_inv_trans_4x8_msa;
+        dsp->vc1_inv_trans_8x4 = ff_vc1_inv_trans_8x4_msa;
+
+        FN_ASSIGN(put_, 1, 1, _msa);
+        FN_ASSIGN(put_, 1, 2, _msa);
+        FN_ASSIGN(put_, 1, 3, _msa);
+        FN_ASSIGN(put_, 2, 1, _msa);
+        FN_ASSIGN(put_, 2, 2, _msa);
+        FN_ASSIGN(put_, 2, 3, _msa);
+        FN_ASSIGN(put_, 3, 1, _msa);
+        FN_ASSIGN(put_, 3, 2, _msa);
+        FN_ASSIGN(put_, 3, 3, _msa);
+    }
 }
diff --git a/libavcodec/mips/vc1dsp_mips.h b/libavcodec/mips/vc1dsp_mips.h
index 0db85fac94..5897daea8c 100644
--- a/libavcodec/mips/vc1dsp_mips.h
+++ b/libavcodec/mips/vc1dsp_mips.h
@@ -180,15 +180,38 @@ void ff_vc1_h_loop_filter16_mmi(uint8_t *src, int stride, int pq);
 
 void ff_put_no_rnd_vc1_chroma_mc8_mmi(uint8_t *dst /* align 8 */,
                                       uint8_t *src /* align 1 */,
-                                      int stride, int h, int x, int y);
+                                      ptrdiff_t stride, int h, int x, int y);
 void ff_put_no_rnd_vc1_chroma_mc4_mmi(uint8_t *dst /* align 8 */,
                                       uint8_t *src /* align 1 */,
-                                      int stride, int h, int x, int y);
+                                      ptrdiff_t stride, int h, int x, int y);
 void ff_avg_no_rnd_vc1_chroma_mc8_mmi(uint8_t *dst /* align 8 */,
                                       uint8_t *src /* align 1 */,
-                                      int stride, int h, int x, int y);
+                                      ptrdiff_t stride, int h, int x, int y);
 void ff_avg_no_rnd_vc1_chroma_mc4_mmi(uint8_t *dst /* align 8 */,
                                       uint8_t *src /* align 1 */,
-                                      int stride, int h, int x, int y);
+                                      ptrdiff_t stride, int h, int x, int y);
 
+void ff_vc1_inv_trans_8x8_msa(int16_t block[64]);
+void ff_vc1_inv_trans_8x4_msa(uint8_t *dest, ptrdiff_t linesize, int16_t *block);
+void ff_vc1_inv_trans_4x8_msa(uint8_t *dest, ptrdiff_t linesize, int16_t *block);
+
+#define FF_PUT_VC1_MSPEL_MC_MSA(hmode, vmode)                                 \
+void ff_put_vc1_mspel_mc ## hmode ## vmode ## _msa(uint8_t *dst,              \
+                                                  const uint8_t *src,         \
+                                                  ptrdiff_t stride, int rnd); \
+void ff_put_vc1_mspel_mc ## hmode ## vmode ## _16_msa(uint8_t *dst,           \
+                                                  const uint8_t *src,         \
+                                                  ptrdiff_t stride, int rnd);
+
+FF_PUT_VC1_MSPEL_MC_MSA(1, 1);
+FF_PUT_VC1_MSPEL_MC_MSA(1, 2);
+FF_PUT_VC1_MSPEL_MC_MSA(1, 3);
+
+FF_PUT_VC1_MSPEL_MC_MSA(2, 1);
+FF_PUT_VC1_MSPEL_MC_MSA(2, 2);
+FF_PUT_VC1_MSPEL_MC_MSA(2, 3);
+
+FF_PUT_VC1_MSPEL_MC_MSA(3, 1);
+FF_PUT_VC1_MSPEL_MC_MSA(3, 2);
+FF_PUT_VC1_MSPEL_MC_MSA(3, 3);
 #endif /* AVCODEC_MIPS_VC1DSP_MIPS_H */
diff --git a/libavcodec/mips/vc1dsp_mmi.c b/libavcodec/mips/vc1dsp_mmi.c
index db314de496..693f1013fe 100644
--- a/libavcodec/mips/vc1dsp_mmi.c
+++ b/libavcodec/mips/vc1dsp_mmi.c
@@ -126,12 +126,14 @@ void ff_vc1_inv_trans_8x8_dc_mmi(uint8_t *dest, ptrdiff_t linesize, int16_t *blo
     double ftmp[9];
     mips_reg addr[1];
     int count;
+    union mmi_intfloat64 dc_u;
 
     dc = (3 * dc +  1) >> 1;
     dc = (3 * dc + 16) >> 5;
+    dc_u.i = dc;
 
     __asm__ volatile(
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
         "pshufh     %[dc],      %[dc],          %[ftmp0]                \n\t"
         "li         %[count],   0x02                                    \n\t"
 
@@ -186,7 +188,7 @@ void ff_vc1_inv_trans_8x8_dc_mmi(uint8_t *dest, ptrdiff_t linesize, int16_t *blo
           [addr0]"=&r"(addr[0]),
           [count]"=&r"(count),          [dest]"+&r"(dest)
         : [linesize]"r"((mips_reg)linesize),
-          [dc]"f"(dc)
+          [dc]"f"(dc_u.f)
         : "memory"
     );
 }
@@ -195,9 +197,6 @@ void ff_vc1_inv_trans_8x8_dc_mmi(uint8_t *dest, ptrdiff_t linesize, int16_t *blo
 void ff_vc1_inv_trans_8x8_mmi(int16_t block[64])
 {
     DECLARE_ALIGNED(16, int16_t, temp[64]);
-    DECLARE_ALIGNED(8, const uint64_t, ff_pw_1_local) = {0x0000000100000001ULL};
-    DECLARE_ALIGNED(8, const uint64_t, ff_pw_4_local) = {0x0000000400000004ULL};
-    DECLARE_ALIGNED(8, const uint64_t, ff_pw_64_local)= {0x0000004000000040ULL};
     double ftmp[23];
     uint64_t tmp[1];
 
@@ -404,8 +403,8 @@ void ff_vc1_inv_trans_8x8_mmi(int16_t block[64])
           [ftmp20]"=&f"(ftmp[20]),      [ftmp21]"=&f"(ftmp[21]),
           [ftmp22]"=&f"(ftmp[22]),
           [tmp0]"=&r"(tmp[0])
-        : [ff_pw_1]"f"(ff_pw_1_local),  [ff_pw_64]"f"(ff_pw_64_local),
-          [ff_pw_4]"f"(ff_pw_4_local), [block]"r"(block),
+        : [ff_pw_1]"f"(ff_pw_32_1.f),   [ff_pw_64]"f"(ff_pw_32_64.f),
+          [ff_pw_4]"f"(ff_pw_32_4.f),   [block]"r"(block),
           [temp]"r"(temp)
         : "memory"
     );
@@ -417,12 +416,14 @@ void ff_vc1_inv_trans_8x4_dc_mmi(uint8_t *dest, ptrdiff_t linesize, int16_t *blo
 {
     int dc = block[0];
     double ftmp[9];
+    union mmi_intfloat64 dc_u;
 
     dc = ( 3 * dc +  1) >> 1;
     dc = (17 * dc + 64) >> 7;
+    dc_u.i = dc;
 
     __asm__ volatile(
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
         "pshufh     %[dc],      %[dc],          %[ftmp0]                \n\t"
 
         MMI_LDC1(%[ftmp1], %[dest0], 0x00)
@@ -464,7 +465,7 @@ void ff_vc1_inv_trans_8x4_dc_mmi(uint8_t *dest, ptrdiff_t linesize, int16_t *blo
           [ftmp8]"=&f"(ftmp[8])
         : [dest0]"r"(dest+0*linesize),  [dest1]"r"(dest+1*linesize),
           [dest2]"r"(dest+2*linesize),  [dest3]"r"(dest+3*linesize),
-          [dc]"f"(dc)
+          [dc]"f"(dc_u.f)
         : "memory"
     );
 }
@@ -477,8 +478,6 @@ void ff_vc1_inv_trans_8x4_mmi(uint8_t *dest, ptrdiff_t linesize, int16_t *block)
     double ftmp[16];
     uint32_t tmp[1];
     int16_t count = 4;
-    DECLARE_ALIGNED(16, const uint64_t, ff_pw_4_local) = {0x0000000400000004ULL};
-    DECLARE_ALIGNED(16, const uint64_t, ff_pw_64_local)= {0x0000004000000040ULL};
     int16_t coeff[64] = {12, 16,  16,  15,  12,   9,   6,   4,
                          12, 15,   6,  -4, -12, -16, -16,  -9,
                          12,  9,  -6, -16, -12,   4,  16,  15,
@@ -588,7 +587,7 @@ void ff_vc1_inv_trans_8x4_mmi(uint8_t *dest, ptrdiff_t linesize, int16_t *block)
           [ftmp12]"=&f"(ftmp[12]),      [ftmp13]"=&f"(ftmp[13]),
           [ftmp14]"=&f"(ftmp[14]),      [tmp0]"=&r"(tmp[0]),
           [src]"+&r"(src), [dst]"+&r"(dst), [count]"+&r"(count)
-        : [ff_pw_4]"f"(ff_pw_4_local),  [coeff]"r"(coeff)
+        : [ff_pw_4]"f"(ff_pw_32_4.f),   [coeff]"r"(coeff)
         : "memory"
     );
 
@@ -702,7 +701,7 @@ void ff_vc1_inv_trans_8x4_mmi(uint8_t *dest, ptrdiff_t linesize, int16_t *block)
         MMI_LWC1(%[ftmp3], %[tmp0], 0x00)
         PTR_ADDU    "%[tmp0],   %[tmp0],    %[linesize]                 \n\t"
         MMI_LWC1(%[ftmp4], %[tmp0], 0x00)
-        "xor        %[ftmp0],   %[ftmp0],   %[ftmp0]                    \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],   %[ftmp0]                    \n\t"
         "punpcklbh  %[ftmp1],   %[ftmp1],   %[ftmp0]                    \n\t"
         "punpcklbh  %[ftmp2],   %[ftmp2],   %[ftmp0]                    \n\t"
         "punpcklbh  %[ftmp3],   %[ftmp3],   %[ftmp0]                    \n\t"
@@ -826,7 +825,7 @@ void ff_vc1_inv_trans_8x4_mmi(uint8_t *dest, ptrdiff_t linesize, int16_t *block)
         MMI_LWC1(%[ftmp3], %[tmp0], 0x04)
         PTR_ADDU    "%[tmp0],   %[tmp0],    %[linesize]                 \n\t"
         MMI_LWC1(%[ftmp4], %[tmp0], 0x04)
-        "xor        %[ftmp0],   %[ftmp0],   %[ftmp0]                    \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],   %[ftmp0]                    \n\t"
         "punpcklbh  %[ftmp1],   %[ftmp1],   %[ftmp0]                    \n\t"
         "punpcklbh  %[ftmp2],   %[ftmp2],   %[ftmp0]                    \n\t"
         "punpcklbh  %[ftmp3],   %[ftmp3],   %[ftmp0]                    \n\t"
@@ -856,7 +855,7 @@ void ff_vc1_inv_trans_8x4_mmi(uint8_t *dest, ptrdiff_t linesize, int16_t *block)
           [ftmp12]"=&f"(ftmp[12]),      [ftmp13]"=&f"(ftmp[13]),
           [ftmp14]"=&f"(ftmp[14]),      [ftmp15]"=&f"(ftmp[15]),
           [tmp0]"=&r"(tmp[0])
-        : [ff_pw_64]"f"(ff_pw_64_local),
+        : [ff_pw_64]"f"(ff_pw_32_64.f),
           [src]"r"(src), [dest]"r"(dest), [linesize]"r"(linesize)
         :"memory"
     );
@@ -868,13 +867,15 @@ void ff_vc1_inv_trans_4x8_dc_mmi(uint8_t *dest, ptrdiff_t linesize, int16_t *blo
 {
     int dc = block[0];
     double ftmp[9];
+    union mmi_intfloat64 dc_u;
     DECLARE_VAR_LOW32;
 
     dc = (17 * dc +  4) >> 3;
     dc = (12 * dc + 64) >> 7;
+    dc_u.i = dc;
 
     __asm__ volatile(
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
         "pshufh     %[dc],      %[dc],          %[ftmp0]                \n\t"
 
         MMI_LWC1(%[ftmp1], %[dest0], 0x00)
@@ -931,7 +932,7 @@ void ff_vc1_inv_trans_4x8_dc_mmi(uint8_t *dest, ptrdiff_t linesize, int16_t *blo
           [dest2]"r"(dest+2*linesize),  [dest3]"r"(dest+3*linesize),
           [dest4]"r"(dest+4*linesize),  [dest5]"r"(dest+5*linesize),
           [dest6]"r"(dest+6*linesize),  [dest7]"r"(dest+7*linesize),
-          [dc]"f"(dc)
+          [dc]"f"(dc_u.f)
         : "memory"
     );
 }
@@ -942,14 +943,11 @@ void ff_vc1_inv_trans_4x8_mmi(uint8_t *dest, ptrdiff_t linesize, int16_t *block)
     int16_t *src = block;
     int16_t *dst = block;
     double ftmp[23];
-    uint32_t count = 8, tmp[1];
+    uint64_t count = 8, tmp[1];
     int16_t coeff[16] = {17, 22, 17, 10,
                          17, 10,-17,-22,
                          17,-10,-17, 22,
                          17,-22, 17,-10};
-    DECLARE_ALIGNED(8, const uint64_t, ff_pw_1_local) = {0x0000000100000001ULL};
-    DECLARE_ALIGNED(8, const uint64_t, ff_pw_4_local) = {0x0000000400000004ULL};
-    DECLARE_ALIGNED(8, const uint64_t, ff_pw_64_local)= {0x0000004000000040ULL};
 
     // 1st loop
     __asm__ volatile (
@@ -995,7 +993,7 @@ void ff_vc1_inv_trans_4x8_mmi(uint8_t *dest, ptrdiff_t linesize, int16_t *block)
           [ftmp10]"=&f"(ftmp[10]),      [ftmp11]"=&f"(ftmp[11]),
           [tmp0]"=&r"(tmp[0]),          [count]"+&r"(count),
           [src]"+&r"(src),              [dst]"+&r"(dst)
-        : [ff_pw_4]"f"(ff_pw_4_local),  [coeff]"r"(coeff)
+        : [ff_pw_4]"f"(ff_pw_32_4.f),   [coeff]"r"(coeff)
         : "memory"
     );
 
@@ -1055,7 +1053,7 @@ void ff_vc1_inv_trans_4x8_mmi(uint8_t *dest, ptrdiff_t linesize, int16_t *block)
         MMI_LWC1(%[ftmp7], %[tmp0], 0x00)
         PTR_ADDU  "%[tmp0],   %[tmp0],    %[linesize]                 \n\t"
         MMI_LWC1(%[ftmp8], %[tmp0], 0x00)
-        "xor        %[ftmp0],   %[ftmp0],   %[ftmp0]                    \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],   %[ftmp0]                    \n\t"
         "punpcklbh  %[ftmp1],   %[ftmp1],   %[ftmp0]                    \n\t"
         "punpcklbh  %[ftmp2],   %[ftmp2],   %[ftmp0]                    \n\t"
         "punpcklbh  %[ftmp3],   %[ftmp3],   %[ftmp0]                    \n\t"
@@ -1112,7 +1110,7 @@ void ff_vc1_inv_trans_4x8_mmi(uint8_t *dest, ptrdiff_t linesize, int16_t *block)
           [ftmp20]"=&f"(ftmp[20]),      [ftmp21]"=&f"(ftmp[21]),
           [ftmp22]"=&f"(ftmp[22]),
           [tmp0]"=&r"(tmp[0])
-        : [ff_pw_1]"f"(ff_pw_1_local),  [ff_pw_64]"f"(ff_pw_64_local),
+        : [ff_pw_1]"f"(ff_pw_32_1.f),   [ff_pw_64]"f"(ff_pw_32_64.f),
           [src]"r"(src), [dest]"r"(dest), [linesize]"r"(linesize)
         : "memory"
     );
@@ -1124,13 +1122,15 @@ void ff_vc1_inv_trans_4x4_dc_mmi(uint8_t *dest, ptrdiff_t linesize, int16_t *blo
 {
     int dc = block[0];
     double ftmp[5];
+    union mmi_intfloat64 dc_u;
     DECLARE_VAR_LOW32;
 
     dc = (17 * dc +  4) >> 3;
     dc = (17 * dc + 64) >> 7;
+    dc_u.i = dc;
 
     __asm__ volatile(
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]                \n\t"
         "pshufh     %[dc],      %[dc],          %[ftmp0]                \n\t"
 
         MMI_LWC1(%[ftmp1], %[dest0], 0x00)
@@ -1163,7 +1163,7 @@ void ff_vc1_inv_trans_4x4_dc_mmi(uint8_t *dest, ptrdiff_t linesize, int16_t *blo
           [ftmp4]"=&f"(ftmp[4])
         : [dest0]"r"(dest+0*linesize),  [dest1]"r"(dest+1*linesize),
           [dest2]"r"(dest+2*linesize),  [dest3]"r"(dest+3*linesize),
-          [dc]"f"(dc)
+          [dc]"f"(dc_u.f)
         : "memory"
     );
 }
@@ -1178,8 +1178,6 @@ void ff_vc1_inv_trans_4x4_mmi(uint8_t *dest, ptrdiff_t linesize, int16_t *block)
                          17, 10,-17,-22,
                          17,-10,-17, 22,
                          17,-22, 17,-10};
-    DECLARE_ALIGNED(8, const uint64_t, ff_pw_4_local) = {0x0000000400000004ULL};
-    DECLARE_ALIGNED(8, const uint64_t, ff_pw_64_local)= {0x0000004000000040ULL};
     // 1st loop
     __asm__ volatile (
 
@@ -1223,7 +1221,7 @@ void ff_vc1_inv_trans_4x4_mmi(uint8_t *dest, ptrdiff_t linesize, int16_t *block)
           [ftmp10]"=&f"(ftmp[10]),      [ftmp11]"=&f"(ftmp[11]),
           [tmp0]"=&r"(tmp[0]),          [count]"+&r"(count),
           [src]"+&r"(src),              [dst]"+&r"(dst)
-        : [ff_pw_4]"f"(ff_pw_4_local),  [coeff]"r"(coeff)
+        : [ff_pw_4]"f"(ff_pw_32_4.f),   [coeff]"r"(coeff)
         : "memory"
     );
 
@@ -1336,7 +1334,7 @@ void ff_vc1_inv_trans_4x4_mmi(uint8_t *dest, ptrdiff_t linesize, int16_t *block)
         MMI_LWC1(%[ftmp3], %[tmp0], 0x00)
         PTR_ADDU    "%[tmp0],   %[tmp0],    %[linesize]                 \n\t"
         MMI_LWC1(%[ftmp4], %[tmp0], 0x00)
-        "xor        %[ftmp0],   %[ftmp0],   %[ftmp0]                    \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],   %[ftmp0]                    \n\t"
         "punpcklbh  %[ftmp1],   %[ftmp1],   %[ftmp0]                    \n\t"
         "punpcklbh  %[ftmp2],   %[ftmp2],   %[ftmp0]                    \n\t"
         "punpcklbh  %[ftmp3],   %[ftmp3],   %[ftmp0]                    \n\t"
@@ -1367,7 +1365,7 @@ void ff_vc1_inv_trans_4x4_mmi(uint8_t *dest, ptrdiff_t linesize, int16_t *block)
           [ftmp12]"=&f"(ftmp[12]),      [ftmp13]"=&f"(ftmp[13]),
           [ftmp14]"=&f"(ftmp[14]),      [ftmp15]"=&f"(ftmp[15]),
           [tmp0]"=&r"(tmp[0])
-        : [ff_pw_64]"f"(ff_pw_64_local),
+        : [ff_pw_64]"f"(ff_pw_32_64.f),
           [src]"r"(src), [dest]"r"(dest), [linesize]"r"(linesize)
         :"memory"
     );
@@ -1657,14 +1655,15 @@ static void vc1_put_ver_16b_shift2_mmi(int16_t *dst,
                                        const uint8_t *src, mips_reg stride,
                                        int rnd, int64_t shift)
 {
+    union mmi_intfloat64 shift_u;
     DECLARE_VAR_LOW32;
     DECLARE_VAR_ADDRT;
+    shift_u.i = shift;
 
     __asm__ volatile(
-        "xor        $f0,    $f0,    $f0             \n\t"
+        "pxor       $f0,    $f0,    $f0             \n\t"
         "li         $8,     0x03                    \n\t"
         LOAD_ROUNDER_MMI("%[rnd]")
-        "ldc1       $f12,   %[ff_pw_9]              \n\t"
         "1:                                         \n\t"
         MMI_ULWC1($f4, %[src], 0x00)
         PTR_ADDU   "%[src], %[src], %[stride]       \n\t"
@@ -1686,9 +1685,9 @@ static void vc1_put_ver_16b_shift2_mmi(int16_t *dst,
         : RESTRICT_ASM_LOW32            RESTRICT_ASM_ADDRT
           [src]"+r"(src),               [dst]"+r"(dst)
         : [stride]"r"(stride),          [stride1]"r"(-2*stride),
-          [shift]"f"(shift),            [rnd]"m"(rnd),
-          [stride2]"r"(9*stride-4),     [ff_pw_9]"m"(ff_pw_9)
-        : "$8", "$9", "$f0", "$f2", "$f4", "$f6", "$f8", "$f10", "$f12",
+          [shift]"f"(shift_u.f),        [rnd]"m"(rnd),
+          [stride2]"r"(9*stride-4)
+        : "$8", "$9", "$f0", "$f2", "$f4", "$f6", "$f8", "$f10",
           "$f14", "$f16", "memory"
     );
 }
@@ -1710,8 +1709,6 @@ static void OPNAME ## vc1_hor_16b_shift2_mmi(uint8_t *dst, mips_reg stride, \
                                                                             \
     __asm__ volatile(                                                       \
         LOAD_ROUNDER_MMI("%[rnd]")                                          \
-        "ldc1       $f12,   %[ff_pw_128]            \n\t"                   \
-        "ldc1       $f10,   %[ff_pw_9]              \n\t"                   \
         "1:                                         \n\t"                   \
         MMI_ULDC1($f2, %[src], 0x00)                                        \
         MMI_ULDC1($f4, %[src], 0x08)                                        \
@@ -1725,16 +1722,16 @@ static void OPNAME ## vc1_hor_16b_shift2_mmi(uint8_t *dst, mips_reg stride, \
         "paddh      $f6,    $f6,    $f0             \n\t"                   \
         MMI_ULDC1($f0, %[src], 0x0b)                                        \
         "paddh      $f8,    $f8,    $f0             \n\t"                   \
-        "pmullh     $f6,    $f6,    $f10            \n\t"                   \
-        "pmullh     $f8,    $f8,    $f10            \n\t"                   \
+        "pmullh     $f6,    $f6,    %[ff_pw_9]      \n\t"                   \
+        "pmullh     $f8,    $f8,    %[ff_pw_9]      \n\t"                   \
         "psubh      $f6,    $f6,    $f2             \n\t"                   \
         "psubh      $f8,    $f8,    $f4             \n\t"                   \
         "li         $8,     0x07                    \n\t"                   \
         "mtc1       $8,     $f16                    \n\t"                   \
         NORMALIZE_MMI("$f16")                                               \
         /* Remove bias */                                                   \
-        "paddh      $f6,    $f6,    $f12            \n\t"                   \
-        "paddh      $f8,    $f8,    $f12            \n\t"                   \
+        "paddh      $f6,    $f6,    %[ff_pw_128]    \n\t"                   \
+        "paddh      $f8,    $f8,    %[ff_pw_128]    \n\t"                   \
         TRANSFER_DO_PACK(OP)                                                \
         "addiu      %[h],   %[h],  -0x01            \n\t"                   \
         PTR_ADDIU  "%[src], %[src], 0x18            \n\t"                   \
@@ -1744,8 +1741,8 @@ static void OPNAME ## vc1_hor_16b_shift2_mmi(uint8_t *dst, mips_reg stride, \
           [h]"+r"(h),                                                       \
           [src]"+r"(src),               [dst]"+r"(dst)                      \
         : [stride]"r"(stride),          [rnd]"m"(rnd),                      \
-          [ff_pw_9]"m"(ff_pw_9),        [ff_pw_128]"m"(ff_pw_128)           \
-        : "$8", "$f0", "$f2", "$f4", "$f6", "$f8", "$f10", "$f12", "$f14",  \
+          [ff_pw_9]"f"(ff_pw_9.f),      [ff_pw_128]"f"(ff_pw_128.f)         \
+        : "$8", "$f0", "$f2", "$f4", "$f6", "$f8", "$f14",                  \
           "$f16", "memory"                                                  \
     );                                                                      \
 }
@@ -1768,10 +1765,9 @@ static void OPNAME ## vc1_shift2_mmi(uint8_t *dst, const uint8_t *src,      \
     rnd = 8 - rnd;                                                          \
                                                                             \
     __asm__ volatile(                                                       \
-        "xor        $f0,    $f0,    $f0             \n\t"                   \
+        "pxor       $f0,    $f0,    $f0             \n\t"                   \
         "li         $10,    0x08                    \n\t"                   \
         LOAD_ROUNDER_MMI("%[rnd]")                                          \
-        "ldc1       $f12,   %[ff_pw_9]              \n\t"                   \
         "1:                                         \n\t"                   \
         MMI_ULWC1($f6, %[src], 0x00)                                        \
         MMI_ULWC1($f8, %[src], 0x04)                                        \
@@ -1788,8 +1784,8 @@ static void OPNAME ## vc1_shift2_mmi(uint8_t *dst, const uint8_t *src,      \
         PTR_ADDU   "$9,     %[src], %[offset_x2n]   \n\t"                   \
         MMI_ULWC1($f2, $9, 0x00)                                            \
         MMI_ULWC1($f4, $9, 0x04)                                            \
-        "pmullh     $f6,    $f6,    $f12            \n\t" /* 0,9,9,0*/      \
-        "pmullh     $f8,    $f8,    $f12            \n\t" /* 0,9,9,0*/      \
+        "pmullh     $f6,    $f6,    %[ff_pw_9]      \n\t" /* 0,9,9,0*/      \
+        "pmullh     $f8,    $f8,    %[ff_pw_9]      \n\t" /* 0,9,9,0*/      \
         "punpcklbh  $f2,    $f2,    $f0             \n\t"                   \
         "punpcklbh  $f4,    $f4,    $f0             \n\t"                   \
         "psubh      $f6,    $f6,    $f2             \n\t" /*-1,9,9,0*/      \
@@ -1816,9 +1812,9 @@ static void OPNAME ## vc1_shift2_mmi(uint8_t *dst, const uint8_t *src,      \
         : [offset]"r"(offset),          [offset_x2n]"r"(-2*offset),         \
           [stride]"r"(stride),          [rnd]"m"(rnd),                      \
           [stride1]"r"(stride-offset),                                      \
-          [ff_pw_9]"m"(ff_pw_9)                                             \
+          [ff_pw_9]"f"(ff_pw_9.f)                                           \
         : "$8", "$9", "$10", "$f0", "$f2", "$f4", "$f6", "$f8", "$f10",     \
-          "$f12", "$f14", "$f16", "memory"                                  \
+          "$f14", "$f16", "memory"                                          \
     );                                                                      \
 }
 
@@ -1849,8 +1845,8 @@ VC1_SHIFT2(OP_AVG, avg_)
     LOAD($f8, $9, M*4)                                                      \
     UNPACK("$f6")                                                           \
     UNPACK("$f8")                                                           \
-    "pmullh     $f6,    $f6,    $f12            \n\t" /* *18 */             \
-    "pmullh     $f8,    $f8,    $f12            \n\t" /* *18 */             \
+    "pmullh     $f6,    $f6,    %[ff_pw_18]     \n\t" /* *18 */             \
+    "pmullh     $f8,    $f8,    %[ff_pw_18]     \n\t" /* *18 */             \
     "psubh      $f6,    $f6,    $f2             \n\t" /* *18, -3 */         \
     "psubh      $f8,    $f8,    $f4             \n\t" /* *18, -3 */         \
     PTR_ADDU   "$9,     %[src], "#A4"           \n\t"                       \
@@ -1869,8 +1865,8 @@ VC1_SHIFT2(OP_AVG, avg_)
     LOAD($f4, $9, M*4)                                                      \
     UNPACK("$f2")                                                           \
     UNPACK("$f4")                                                           \
-    "pmullh     $f2,    $f2,    $f10            \n\t" /* *53 */             \
-    "pmullh     $f4,    $f4,    $f10            \n\t" /* *53 */             \
+    "pmullh     $f2,    $f2,    %[ff_pw_53]     \n\t" /* *53 */             \
+    "pmullh     $f4,    $f4,    %[ff_pw_53]     \n\t" /* *53 */             \
     "paddh      $f6,    $f6,    $f2             \n\t" /* 4,53,18,-3 */      \
     "paddh      $f8,    $f8,    $f4             \n\t" /* 4,53,18,-3 */
 
@@ -1889,16 +1885,16 @@ vc1_put_ver_16b_ ## NAME ## _mmi(int16_t *dst, const uint8_t *src,          \
                                  int rnd, int64_t shift)                    \
 {                                                                           \
     int h = 8;                                                              \
+    union mmi_intfloat64 shift_u;                                           \
     DECLARE_VAR_LOW32;                                                      \
     DECLARE_VAR_ADDRT;                                                      \
+    shift_u.i = shift;                                                      \
                                                                             \
     src -= src_stride;                                                      \
                                                                             \
     __asm__ volatile(                                                       \
-        "xor        $f0,    $f0,    $f0             \n\t"                   \
+        "pxor       $f0,    $f0,    $f0             \n\t"                   \
         LOAD_ROUNDER_MMI("%[rnd]")                                          \
-        "ldc1       $f10,   %[ff_pw_53]             \n\t"                   \
-        "ldc1       $f12,   %[ff_pw_18]             \n\t"                   \
         ".p2align 3                                 \n\t"                   \
         "1:                                         \n\t"                   \
         MSPEL_FILTER13_CORE(DO_UNPACK, MMI_ULWC1, 1, A1, A2, A3, A4)        \
@@ -1914,12 +1910,12 @@ vc1_put_ver_16b_ ## NAME ## _mmi(int16_t *dst, const uint8_t *src,          \
         PTR_ADDU   "$9,     %[src], "#A2"           \n\t"                   \
         MMI_ULWC1($f6, $9, 0x08)                                            \
         DO_UNPACK("$f6")                                                    \
-        "pmullh     $f6,    $f6,    $f12            \n\t" /* *18 */         \
+        "pmullh     $f6,    $f6,    %[ff_pw_18]     \n\t" /* *18 */         \
         "psubh      $f6,    $f6,    $f2             \n\t" /* *18,-3 */      \
         PTR_ADDU   "$9,     %[src], "#A3"           \n\t"                   \
         MMI_ULWC1($f2, $9, 0x08)                                            \
         DO_UNPACK("$f2")                                                    \
-        "pmullh     $f2,    $f2,    $f10            \n\t" /* *53 */         \
+        "pmullh     $f2,    $f2,    %[ff_pw_53]     \n\t" /* *53 */         \
         "paddh      $f6,    $f6,    $f2             \n\t" /* *53,18,-3 */   \
         PTR_ADDU   "$9,     %[src], "#A4"           \n\t"                   \
         MMI_ULWC1($f2, $9, 0x08)                                            \
@@ -1942,10 +1938,10 @@ vc1_put_ver_16b_ ## NAME ## _mmi(int16_t *dst, const uint8_t *src,          \
           [src]"+r"(src),               [dst]"+r"(dst)                      \
         : [stride_x1]"r"(src_stride),   [stride_x2]"r"(2*src_stride),       \
           [stride_x3]"r"(3*src_stride),                                     \
-          [rnd]"m"(rnd),                [shift]"f"(shift),                  \
-          [ff_pw_53]"m"(ff_pw_53),      [ff_pw_18]"m"(ff_pw_18),            \
-          [ff_pw_3]"f"(ff_pw_3)                                             \
-        : "$8", "$9", "$f0", "$f2", "$f4", "$f6", "$f8", "$f10", "$f12",    \
+          [rnd]"m"(rnd),                [shift]"f"(shift_u.f),              \
+          [ff_pw_53]"f"(ff_pw_53.f),    [ff_pw_18]"f"(ff_pw_18.f),          \
+          [ff_pw_3]"f"(ff_pw_3.f)                                           \
+        : "$8", "$9", "$f0", "$f2", "$f4", "$f6", "$f8",                    \
           "$f14", "$f16", "memory"                                          \
     );                                                                      \
 }
@@ -1970,10 +1966,8 @@ OPNAME ## vc1_hor_16b_ ## NAME ## _mmi(uint8_t *dst, mips_reg stride,       \
     rnd -= (-4+58+13-3)*256; /* Add -256 bias */                            \
                                                                             \
     __asm__ volatile(                                                       \
-        "xor        $f0,    $f0,    $f0             \n\t"                   \
+        "pxor       $f0,    $f0,    $f0             \n\t"                   \
         LOAD_ROUNDER_MMI("%[rnd]")                                          \
-        "ldc1       $f10,   %[ff_pw_53]             \n\t"                   \
-        "ldc1       $f12,   %[ff_pw_18]             \n\t"                   \
         ".p2align 3                                 \n\t"                   \
         "1:                                         \n\t"                   \
         MSPEL_FILTER13_CORE(DONT_UNPACK, MMI_ULDC1, 2, A1, A2, A3, A4)      \
@@ -1992,9 +1986,9 @@ OPNAME ## vc1_hor_16b_ ## NAME ## _mmi(uint8_t *dst, mips_reg stride,       \
           [h]"+r"(h),                                                       \
           [src]"+r"(src),               [dst]"+r"(dst)                      \
         : [stride]"r"(stride),          [rnd]"m"(rnd),                      \
-          [ff_pw_53]"m"(ff_pw_53),      [ff_pw_18]"m"(ff_pw_18),            \
-          [ff_pw_3]"f"(ff_pw_3),        [ff_pw_128]"f"(ff_pw_128)           \
-        : "$8", "$9", "$f0", "$f2", "$f4", "$f6", "$f8", "$f10", "$f12",    \
+          [ff_pw_53]"f"(ff_pw_53.f),    [ff_pw_18]"f"(ff_pw_18.f),          \
+          [ff_pw_3]"f"(ff_pw_3.f),      [ff_pw_128]"f"(ff_pw_128.f)         \
+        : "$8", "$9", "$f0", "$f2", "$f4", "$f6", "$f8",                    \
           "$f14", "$f16", "memory"                                          \
     );                                                                      \
 }
@@ -2020,10 +2014,8 @@ OPNAME ## vc1_## NAME ## _mmi(uint8_t *dst, const uint8_t *src,             \
     rnd = 32-rnd;                                                           \
                                                                             \
     __asm__ volatile (                                                      \
-        "xor        $f0,    $f0,    $f0             \n\t"                   \
+        "pxor       $f0,    $f0,    $f0             \n\t"                   \
         LOAD_ROUNDER_MMI("%[rnd]")                                          \
-        "ldc1       $f10,   %[ff_pw_53]             \n\t"                   \
-        "ldc1       $f12,   %[ff_pw_18]             \n\t"                   \
         ".p2align 3                                 \n\t"                   \
         "1:                                         \n\t"                   \
         MSPEL_FILTER13_CORE(DO_UNPACK, MMI_ULWC1, 1, A1, A2, A3, A4)        \
@@ -2041,9 +2033,9 @@ OPNAME ## vc1_## NAME ## _mmi(uint8_t *dst, const uint8_t *src,             \
         : [offset_x1]"r"(offset),       [offset_x2]"r"(2*offset),           \
           [offset_x3]"r"(3*offset),     [stride]"r"(stride),                \
           [rnd]"m"(rnd),                                                    \
-          [ff_pw_53]"m"(ff_pw_53),      [ff_pw_18]"m"(ff_pw_18),            \
-          [ff_pw_3]"f"(ff_pw_3)                                             \
-        : "$8", "$9", "$f0", "$f2", "$f4", "$f6", "$f8", "$f10", "$f12",    \
+          [ff_pw_53]"f"(ff_pw_53.f),    [ff_pw_18]"f"(ff_pw_18.f),          \
+          [ff_pw_3]"f"(ff_pw_3.f)                                           \
+        : "$8", "$9", "$f0", "$f2", "$f4", "$f6", "$f8",                    \
           "$f14", "$f16", "memory"                                          \
     );                                                                      \
 }
@@ -2241,22 +2233,23 @@ DECLARE_FUNCTION(3, 3)
 
 void ff_put_no_rnd_vc1_chroma_mc8_mmi(uint8_t *dst /* align 8 */,
                                       uint8_t *src /* align 1 */,
-                                      int stride, int h, int x, int y)
+                                      ptrdiff_t stride, int h, int x, int y)
 {
-    const int A = (8 - x) * (8 - y);
-    const int B =     (x) * (8 - y);
-    const int C = (8 - x) *     (y);
-    const int D =     (x) *     (y);
+    union mmi_intfloat64 A, B, C, D;
     double ftmp[10];
     uint32_t tmp[1];
     DECLARE_VAR_ALL64;
     DECLARE_VAR_ADDRT;
+    A.i = (8 - x) * (8 - y);
+    B.i =     (x) * (8 - y);
+    C.i = (8 - x) *     (y);
+    D.i =     (x) *     (y);
 
     av_assert2(x < 8 && y < 8 && x >= 0 && y >= 0);
 
     __asm__ volatile(
         "li         %[tmp0],    0x06                                    \n\t"
-        "xor        %[ftmp0],   %[ftmp0],   %[ftmp0]                    \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],   %[ftmp0]                    \n\t"
         "mtc1       %[tmp0],    %[ftmp9]                                \n\t"
         "pshufh     %[A],       %[A],       %[ftmp0]                    \n\t"
         "pshufh     %[B],       %[B],       %[ftmp0]                    \n\t"
@@ -2287,31 +2280,32 @@ void ff_put_no_rnd_vc1_chroma_mc8_mmi(uint8_t *dst /* align 8 */,
           [src]"+&r"(src),              [dst]"+&r"(dst),
           [h]"+&r"(h)
         : [stride]"r"((mips_reg)stride),
-          [A]"f"(A),                    [B]"f"(B),
-          [C]"f"(C),                    [D]"f"(D),
-          [ff_pw_28]"f"(ff_pw_28)
+          [A]"f"(A.f),                  [B]"f"(B.f),
+          [C]"f"(C.f),                  [D]"f"(D.f),
+          [ff_pw_28]"f"(ff_pw_28.f)
         : "memory"
     );
 }
 
 void ff_put_no_rnd_vc1_chroma_mc4_mmi(uint8_t *dst /* align 8 */,
                                       uint8_t *src /* align 1 */,
-                                      int stride, int h, int x, int y)
+                                      ptrdiff_t stride, int h, int x, int y)
 {
-    const int A = (8 - x) * (8 - y);
-    const int B =     (x) * (8 - y);
-    const int C = (8 - x) *     (y);
-    const int D =     (x) *     (y);
+    union mmi_intfloat64 A, B, C, D;
     double ftmp[6];
     uint32_t tmp[1];
     DECLARE_VAR_LOW32;
     DECLARE_VAR_ADDRT;
+    A.i = (8 - x) * (8 - y);
+    B.i =     (x) * (8 - y);
+    C.i = (8 - x) *     (y);
+    D.i =     (x) *     (y);
 
     av_assert2(x < 8 && y < 8 && x >= 0 && y >= 0);
 
     __asm__ volatile(
         "li         %[tmp0],    0x06                                    \n\t"
-        "xor        %[ftmp0],   %[ftmp0],   %[ftmp0]                    \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],   %[ftmp0]                    \n\t"
         "mtc1       %[tmp0],    %[ftmp5]                                \n\t"
         "pshufh     %[A],       %[A],       %[ftmp0]                    \n\t"
         "pshufh     %[B],       %[B],       %[ftmp0]                    \n\t"
@@ -2340,31 +2334,32 @@ void ff_put_no_rnd_vc1_chroma_mc4_mmi(uint8_t *dst /* align 8 */,
           [src]"+&r"(src),              [dst]"+&r"(dst),
           [h]"+&r"(h)
         : [stride]"r"((mips_reg)stride),
-          [A]"f"(A),                    [B]"f"(B),
-          [C]"f"(C),                    [D]"f"(D),
-          [ff_pw_28]"f"(ff_pw_28)
+          [A]"f"(A.f),                  [B]"f"(B.f),
+          [C]"f"(C.f),                  [D]"f"(D.f),
+          [ff_pw_28]"f"(ff_pw_28.f)
         : "memory"
     );
 }
 
 void ff_avg_no_rnd_vc1_chroma_mc8_mmi(uint8_t *dst /* align 8 */,
                                       uint8_t *src /* align 1 */,
-                                      int stride, int h, int x, int y)
+                                      ptrdiff_t stride, int h, int x, int y)
 {
-    const int A = (8 - x) * (8 - y);
-    const int B =     (x) * (8 - y);
-    const int C = (8 - x) *     (y);
-    const int D =     (x) *     (y);
+    union mmi_intfloat64 A, B, C, D;
     double ftmp[10];
     uint32_t tmp[1];
     DECLARE_VAR_ALL64;
     DECLARE_VAR_ADDRT;
+    A.i = (8 - x) * (8 - y);
+    B.i =     (x) * (8 - y);
+    C.i = (8 - x) *     (y);
+    D.i =     (x) *     (y);
 
     av_assert2(x < 8 && y < 8 && x >= 0 && y >= 0);
 
     __asm__ volatile(
         "li         %[tmp0],    0x06                                    \n\t"
-        "xor        %[ftmp0],   %[ftmp0],   %[ftmp0]                    \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],   %[ftmp0]                    \n\t"
         "mtc1       %[tmp0],    %[ftmp9]                                \n\t"
         "pshufh     %[A],       %[A],       %[ftmp0]                    \n\t"
         "pshufh     %[B],       %[B],       %[ftmp0]                    \n\t"
@@ -2398,31 +2393,32 @@ void ff_avg_no_rnd_vc1_chroma_mc8_mmi(uint8_t *dst /* align 8 */,
           [src]"+&r"(src),              [dst]"+&r"(dst),
           [h]"+&r"(h)
         : [stride]"r"((mips_reg)stride),
-          [A]"f"(A),                    [B]"f"(B),
-          [C]"f"(C),                    [D]"f"(D),
-          [ff_pw_28]"f"(ff_pw_28)
+          [A]"f"(A.f),                 [B]"f"(B.f),
+          [C]"f"(C.f),                 [D]"f"(D.f),
+          [ff_pw_28]"f"(ff_pw_28.f)
         : "memory"
     );
 }
 
 void ff_avg_no_rnd_vc1_chroma_mc4_mmi(uint8_t *dst /* align 8 */,
                                       uint8_t *src /* align 1 */,
-                                      int stride, int h, int x, int y)
+                                      ptrdiff_t stride, int h, int x, int y)
 {
-    const int A = (8 - x) * (8 - y);
-    const int B = (    x) * (8 - y);
-    const int C = (8 - x) * (    y);
-    const int D = (    x) * (    y);
+    union mmi_intfloat64 A, B, C, D;
     double ftmp[6];
     uint32_t tmp[1];
     DECLARE_VAR_LOW32;
     DECLARE_VAR_ADDRT;
+    A.i = (8 - x) * (8 - y);
+    B.i = (x) * (8 - y);
+    C.i = (8 - x) * (y);
+    D.i = (x) * (y);
 
     av_assert2(x < 8 && y < 8 && x >= 0 && y >= 0);
 
     __asm__ volatile(
         "li         %[tmp0],    0x06                                    \n\t"
-        "xor        %[ftmp0],   %[ftmp0],   %[ftmp0]                    \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],   %[ftmp0]                    \n\t"
         "mtc1       %[tmp0],    %[ftmp5]                                \n\t"
         "pshufh     %[A],       %[A],       %[ftmp0]                    \n\t"
         "pshufh     %[B],       %[B],       %[ftmp0]                    \n\t"
@@ -2454,9 +2450,9 @@ void ff_avg_no_rnd_vc1_chroma_mc4_mmi(uint8_t *dst /* align 8 */,
           [src]"+&r"(src),              [dst]"+&r"(dst),
           [h]"+&r"(h)
         : [stride]"r"((mips_reg)stride),
-          [A]"f"(A),                    [B]"f"(B),
-          [C]"f"(C),                    [D]"f"(D),
-          [ff_pw_28]"f"(ff_pw_28)
+          [A]"f"(A.f),                  [B]"f"(B.f),
+          [C]"f"(C.f),                  [D]"f"(D.f),
+          [ff_pw_28]"f"(ff_pw_28.f)
         : "memory"
     );
 }
diff --git a/libavcodec/mips/vc1dsp_msa.c b/libavcodec/mips/vc1dsp_msa.c
new file mode 100644
index 0000000000..6e588e825a
--- /dev/null
+++ b/libavcodec/mips/vc1dsp_msa.c
@@ -0,0 +1,461 @@
+/*
+ * Loongson SIMD optimized vc1dsp
+ *
+ * Copyright (c) 2019 Loongson Technology Corporation Limited
+ *                    gxw <guxiwei-hf@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "vc1dsp_mips.h"
+#include "constants.h"
+#include "libavutil/mips/generic_macros_msa.h"
+
+void ff_vc1_inv_trans_8x8_msa(int16_t block[64])
+{
+    v8i16 in0, in1, in2, in3, in4, in5, in6, in7;
+    v4i32 in_r0, in_r1, in_r2, in_r3, in_r4, in_r5, in_r6, in_r7;
+    v4i32 in_l0, in_l1, in_l2, in_l3, in_l4, in_l5, in_l6, in_l7;
+    v4i32 t_r1, t_r2, t_r3, t_r4, t_r5, t_r6, t_r7, t_r8;
+    v4i32 t_l1, t_l2, t_l3, t_l4, t_l5, t_l6, t_l7, t_l8;
+    v4i32 cnst_12 = {12, 12, 12, 12};
+    v4i32 cnst_4 = {4, 4, 4, 4};
+    v4i32 cnst_16 = {16, 16, 16, 16};
+    v4i32 cnst_6 = {6, 6, 6, 6};
+    v4i32 cnst_15 = {15, 15, 15, 15};
+    v4i32 cnst_9 = {9, 9, 9, 9};
+    v4i32 cnst_1 = {1, 1, 1, 1};
+    v4i32 cnst_64 = {64, 64, 64, 64};
+
+    LD_SH8(block, 8, in0, in1, in2, in3, in4, in5, in6, in7);
+    UNPCK_SH_SW(in0, in_r0, in_l0);
+    UNPCK_SH_SW(in1, in_r1, in_l1);
+    UNPCK_SH_SW(in2, in_r2, in_l2);
+    UNPCK_SH_SW(in3, in_r3, in_l3);
+    UNPCK_SH_SW(in4, in_r4, in_l4);
+    UNPCK_SH_SW(in5, in_r5, in_l5);
+    UNPCK_SH_SW(in6, in_r6, in_l6);
+    UNPCK_SH_SW(in7, in_r7, in_l7);
+    // First loop
+    t_r1 = cnst_12 * (in_r0 + in_r4) + cnst_4;
+    t_l1 = cnst_12 * (in_l0 + in_l4) + cnst_4;
+    t_r2 = cnst_12 * (in_r0 - in_r4) + cnst_4;
+    t_l2 = cnst_12 * (in_l0 - in_l4) + cnst_4;
+    t_r3 = cnst_16 * in_r2 + cnst_6 * in_r6;
+    t_l3 = cnst_16 * in_l2 + cnst_6 * in_l6;
+    t_r4 = cnst_6 * in_r2 - cnst_16 * in_r6;
+    t_l4 = cnst_6 * in_l2 - cnst_16 * in_l6;
+
+    ADD4(t_r1, t_r3, t_l1, t_l3, t_r2, t_r4, t_l2, t_l4, t_r5, t_l5, t_r6, t_l6);
+    SUB4(t_r2, t_r4, t_l2, t_l4, t_r1, t_r3, t_l1, t_l3, t_r7, t_l7, t_r8, t_l8);
+    t_r1 = cnst_16 * in_r1 + cnst_15 * in_r3 + cnst_9 * in_r5 + cnst_4 * in_r7;
+    t_l1 = cnst_16 * in_l1 + cnst_15 * in_l3 + cnst_9 * in_l5 + cnst_4 * in_l7;
+    t_r2 = cnst_15 * in_r1 - cnst_4 * in_r3 - cnst_16 * in_r5 - cnst_9 * in_r7;
+    t_l2 = cnst_15 * in_l1 - cnst_4 * in_l3 - cnst_16 * in_l5 - cnst_9 * in_l7;
+    t_r3 = cnst_9 * in_r1 - cnst_16 * in_r3 + cnst_4 * in_r5 + cnst_15 * in_r7;
+    t_l3 = cnst_9 * in_l1 - cnst_16 * in_l3 + cnst_4 * in_l5 + cnst_15 * in_l7;
+    t_r4 = cnst_4 * in_r1 - cnst_9 * in_r3 + cnst_15 * in_r5 - cnst_16 * in_r7;
+    t_l4 = cnst_4 * in_l1 - cnst_9 * in_l3 + cnst_15 * in_l5 - cnst_16 * in_l7;
+
+    in_r0 = (t_r5 + t_r1) >> 3;
+    in_l0 = (t_l5 + t_l1) >> 3;
+    in_r1 = (t_r6 + t_r2) >> 3;
+    in_l1 = (t_l6 + t_l2) >> 3;
+    in_r2 = (t_r7 + t_r3) >> 3;
+    in_l2 = (t_l7 + t_l3) >> 3;
+    in_r3 = (t_r8 + t_r4) >> 3;
+    in_l3 = (t_l8 + t_l4) >> 3;
+
+    in_r4 = (t_r8 - t_r4) >> 3;
+    in_l4 = (t_l8 - t_l4) >> 3;
+    in_r5 = (t_r7 - t_r3) >> 3;
+    in_l5 = (t_l7 - t_l3) >> 3;
+    in_r6 = (t_r6 - t_r2) >> 3;
+    in_l6 = (t_l6 - t_l2) >> 3;
+    in_r7 = (t_r5 - t_r1) >> 3;
+    in_l7 = (t_l5 - t_l1) >> 3;
+    TRANSPOSE4x4_SW_SW(in_r0, in_r1, in_r2, in_r3, in_r0, in_r1, in_r2, in_r3);
+    TRANSPOSE4x4_SW_SW(in_l0, in_l1, in_l2, in_l3, in_l0, in_l1, in_l2, in_l3);
+    TRANSPOSE4x4_SW_SW(in_r4, in_r5, in_r6, in_r7, in_r4, in_r5, in_r6, in_r7);
+    TRANSPOSE4x4_SW_SW(in_l4, in_l5, in_l6, in_l7, in_l4, in_l5, in_l6, in_l7);
+    // Second loop
+    t_r1 = cnst_12 * (in_r0 + in_l0) + cnst_64;
+    t_l1 = cnst_12 * (in_r4 + in_l4) + cnst_64;
+    t_r2 = cnst_12 * (in_r0 - in_l0) + cnst_64;
+    t_l2 = cnst_12 * (in_r4 - in_l4) + cnst_64;
+    t_r3 = cnst_16 * in_r2 + cnst_6 * in_l2;
+    t_l3 = cnst_16 * in_r6 + cnst_6 * in_l6;
+    t_r4 = cnst_6 * in_r2 - cnst_16 * in_l2;
+    t_l4 = cnst_6 * in_r6 - cnst_16 * in_l6;
+
+    ADD4(t_r1, t_r3, t_l1, t_l3, t_r2, t_r4, t_l2, t_l4, t_r5, t_l5, t_r6, t_l6);
+    SUB4(t_r2, t_r4, t_l2, t_l4, t_r1, t_r3, t_l1, t_l3, t_r7, t_l7, t_r8, t_l8);
+    t_r1 = cnst_16 * in_r1 + cnst_15 * in_r3 + cnst_9 * in_l1 + cnst_4 * in_l3;
+    t_l1 = cnst_16 * in_r5 + cnst_15 * in_r7 + cnst_9 * in_l5 + cnst_4 * in_l7;
+    t_r2 = cnst_15 * in_r1 - cnst_4 * in_r3 - cnst_16 * in_l1 - cnst_9 * in_l3;
+    t_l2 = cnst_15 * in_r5 - cnst_4 * in_r7 - cnst_16 * in_l5 - cnst_9 * in_l7;
+    t_r3 = cnst_9 * in_r1 - cnst_16 * in_r3 + cnst_4 * in_l1 + cnst_15 * in_l3;
+    t_l3 = cnst_9 * in_r5 - cnst_16 * in_r7 + cnst_4 * in_l5 + cnst_15 * in_l7;
+    t_r4 = cnst_4 * in_r1 - cnst_9 * in_r3 + cnst_15 * in_l1 - cnst_16 * in_l3;
+    t_l4 = cnst_4 * in_r5 - cnst_9 * in_r7 + cnst_15 * in_l5 - cnst_16 * in_l7;
+
+    in_r0 = (t_r5 + t_r1) >> 7;
+    in_l0 = (t_l5 + t_l1) >> 7;
+    in_r1 = (t_r6 + t_r2) >> 7;
+    in_l1 = (t_l6 + t_l2) >> 7;
+    in_r2 = (t_r7 + t_r3) >> 7;
+    in_l2 = (t_l7 + t_l3) >> 7;
+    in_r3 = (t_r8 + t_r4) >> 7;
+    in_l3 = (t_l8 + t_l4) >> 7;
+
+    in_r4 = (t_r8 - t_r4 + cnst_1) >> 7;
+    in_l4 = (t_l8 - t_l4 + cnst_1) >> 7;
+    in_r5 = (t_r7 - t_r3 + cnst_1) >> 7;
+    in_l5 = (t_l7 - t_l3 + cnst_1) >> 7;
+    in_r6 = (t_r6 - t_r2 + cnst_1) >> 7;
+    in_l6 = (t_l6 - t_l2 + cnst_1) >> 7;
+    in_r7 = (t_r5 - t_r1 + cnst_1) >> 7;
+    in_l7 = (t_l5 - t_l1 + cnst_1) >> 7;
+    PCKEV_H4_SH(in_l0, in_r0, in_l1, in_r1, in_l2, in_r2, in_l3, in_r3,
+                in0, in1, in2, in3);
+    PCKEV_H4_SH(in_l4, in_r4, in_l5, in_r5, in_l6, in_r6, in_l7, in_r7,
+                in4, in5, in6, in7);
+    ST_SH8(in0, in1, in2, in3, in4, in5, in6, in7, block, 8);
+}
+
+void ff_vc1_inv_trans_4x8_msa(uint8_t *dest, ptrdiff_t linesize, int16_t *block)
+{
+    v8i16 in0, in1, in2, in3, in4, in5, in6, in7;
+    v4i32 in_r0, in_r1, in_r2, in_r3, in_r4, in_r5, in_r6, in_r7;
+    v4i32 t1, t2, t3, t4, t5, t6, t7, t8;
+    v4i32 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    v16i8 zero_m = { 0 };
+    v4i32 cnst_17 = {17, 17, 17, 17};
+    v4i32 cnst_22 = {22, 22, 22, 22};
+    v4i32 cnst_10 = {10, 10, 10, 10};
+    v4i32 cnst_12 = {12, 12, 12, 12};
+    v4i32 cnst_64 = {64, 64, 64, 64};
+    v4i32 cnst_16 = {16, 16, 16, 16};
+    v4i32 cnst_15 = {15, 15, 15, 15};
+    v4i32 cnst_4 = {4, 4, 4, 4};
+    v4i32 cnst_6 = {6, 6, 6, 6};
+    v4i32 cnst_9 = {9, 9, 9, 9};
+    v4i32 cnst_1 = {1, 1, 1, 1};
+
+    LD_SH8(block, 8, in0, in1, in2, in3, in4, in5, in6, in7);
+    UNPCK_R_SH_SW(in0, in_r0);
+    UNPCK_R_SH_SW(in1, in_r1);
+    UNPCK_R_SH_SW(in2, in_r2);
+    UNPCK_R_SH_SW(in3, in_r3);
+    UNPCK_R_SH_SW(in4, in_r4);
+    UNPCK_R_SH_SW(in5, in_r5);
+    UNPCK_R_SH_SW(in6, in_r6);
+    UNPCK_R_SH_SW(in7, in_r7);
+    // First loop
+    TRANSPOSE4x4_SW_SW(in_r0, in_r1, in_r2, in_r3, in_r0, in_r1, in_r2, in_r3);
+    TRANSPOSE4x4_SW_SW(in_r4, in_r5, in_r6, in_r7, in_r4, in_r5, in_r6, in_r7);
+    t1 = cnst_17 * (in_r0 + in_r2) + cnst_4;
+    t5 = cnst_17 * (in_r4 + in_r6) + cnst_4;
+    t2 = cnst_17 * (in_r0 - in_r2) + cnst_4;
+    t6 = cnst_17 * (in_r4 - in_r6) + cnst_4;
+    t3 = cnst_22 * in_r1 + cnst_10 * in_r3;
+    t7 = cnst_22 * in_r5 + cnst_10 * in_r7;
+    t4 = cnst_22 * in_r3 - cnst_10 * in_r1;
+    t8 = cnst_22 * in_r7 - cnst_10 * in_r5;
+
+    in_r0 = (t1 + t3) >> 3;
+    in_r4 = (t5 + t7) >> 3;
+    in_r1 = (t2 - t4) >> 3;
+    in_r5 = (t6 - t8) >> 3;
+    in_r2 = (t2 + t4) >> 3;
+    in_r6 = (t6 + t8) >> 3;
+    in_r3 = (t1 - t3) >> 3;
+    in_r7 = (t5 - t7) >> 3;
+    TRANSPOSE4x4_SW_SW(in_r0, in_r1, in_r2, in_r3, in_r0, in_r1, in_r2, in_r3);
+    TRANSPOSE4x4_SW_SW(in_r4, in_r5, in_r6, in_r7, in_r4, in_r5, in_r6, in_r7);
+    PCKEV_H4_SH(in_r1, in_r0, in_r3, in_r2, in_r5, in_r4, in_r7, in_r6,
+                in0, in1, in2, in3);
+    ST_D8(in0, in1, in2, in3, 0, 1, 0, 1, 0, 1, 0, 1, block, 8);
+    // Second loop
+    t1 = cnst_12 * (in_r0 + in_r4) + cnst_64;
+    t2 = cnst_12 * (in_r0 - in_r4) + cnst_64;
+    t3 = cnst_16 * in_r2 + cnst_6 * in_r6;
+    t4 = cnst_6 * in_r2 - cnst_16 * in_r6;
+    t5 = t1 + t3, t6 = t2 + t4;
+    t7 = t2 - t4, t8 = t1 - t3;
+    t1 = cnst_16 * in_r1 + cnst_15 * in_r3 + cnst_9 * in_r5 + cnst_4 * in_r7;
+    t2 = cnst_15 * in_r1 - cnst_4 * in_r3 - cnst_16 * in_r5 - cnst_9 * in_r7;
+    t3 = cnst_9 * in_r1 - cnst_16 * in_r3 + cnst_4 * in_r5 + cnst_15 * in_r7;
+    t4 = cnst_4 * in_r1 - cnst_9 * in_r3 + cnst_15 * in_r5 - cnst_16 * in_r7;
+    LD_SW8(dest, linesize, dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7);
+    ILVR_B8_SW(zero_m, dst0, zero_m, dst1, zero_m, dst2, zero_m, dst3,
+               zero_m, dst4, zero_m, dst5, zero_m, dst6, zero_m, dst7,
+               dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7);
+    ILVR_H4_SW(zero_m, dst0, zero_m, dst1, zero_m, dst2, zero_m, dst3,
+               dst0, dst1, dst2, dst3);
+    ILVR_H4_SW(zero_m, dst4, zero_m, dst5, zero_m, dst6, zero_m, dst7,
+               dst4, dst5, dst6, dst7);
+    in_r0 = (t5 + t1) >> 7;
+    in_r1 = (t6 + t2) >> 7;
+    in_r2 = (t7 + t3) >> 7;
+    in_r3 = (t8 + t4) >> 7;
+    in_r4 = (t8 - t4 + cnst_1) >> 7;
+    in_r5 = (t7 - t3 + cnst_1) >> 7;
+    in_r6 = (t6 - t2 + cnst_1) >> 7;
+    in_r7 = (t5 - t1 + cnst_1) >> 7;
+    ADD4(in_r0, dst0, in_r1, dst1, in_r2, dst2, in_r3, dst3,
+         in_r0, in_r1, in_r2, in_r3);
+    ADD4(in_r4, dst4, in_r5, dst5, in_r6, dst6, in_r7, dst7,
+         in_r4, in_r5, in_r6, in_r7);
+    CLIP_SW8_0_255(in_r0, in_r1, in_r2, in_r3, in_r4, in_r5, in_r6, in_r7);
+    PCKEV_H4_SH(in_r1, in_r0, in_r3, in_r2, in_r5, in_r4, in_r7, in_r6,
+                in0, in1, in2, in3);
+    PCKEV_B2_SH(in1, in0, in3, in2, in0, in1);
+    ST_W8(in0, in1, 0, 1, 2, 3, 0, 1, 2, 3, dest, linesize);
+}
+
+void ff_vc1_inv_trans_8x4_msa(uint8_t *dest, ptrdiff_t linesize, int16_t *block)
+{
+    v4i32 in0, in1, in2, in3, in4, in5, in6, in7;
+    v4i32 t1, t2, t3, t4, t5, t6, t7, t8;
+    v4i32 dst0, dst1, dst2, dst3, dst4, dst5, dst6, dst7;
+    v16i8 zero_m = { 0 };
+    v4i32 cnst_17 = {17, 17, 17, 17};
+    v4i32 cnst_22 = {22, 22, 22, 22};
+    v4i32 cnst_10 = {10, 10, 10, 10};
+    v4i32 cnst_12 = {12, 12, 12, 12};
+    v4i32 cnst_64 = {64, 64, 64, 64};
+    v4i32 cnst_16 = {16, 16, 16, 16};
+    v4i32 cnst_15 = {15, 15, 15, 15};
+    v4i32 cnst_4 = {4, 4, 4, 4};
+    v4i32 cnst_6 = {6, 6, 6, 6};
+    v4i32 cnst_9 = {9, 9, 9, 9};
+
+    LD_SW4(block, 8, t1, t2, t3, t4);
+    UNPCK_SH_SW(t1, in0, in4);
+    UNPCK_SH_SW(t2, in1, in5);
+    UNPCK_SH_SW(t3, in2, in6);
+    UNPCK_SH_SW(t4, in3, in7);
+    TRANSPOSE4x4_SW_SW(in0, in1, in2, in3, in0, in1, in2, in3);
+    TRANSPOSE4x4_SW_SW(in4, in5, in6, in7, in4, in5, in6, in7);
+    // First loop
+    t1 = cnst_12 * (in0 + in4) + cnst_4;
+    t2 = cnst_12 * (in0 - in4) + cnst_4;
+    t3 = cnst_16 * in2 + cnst_6 * in6;
+    t4 = cnst_6 * in2 - cnst_16 * in6;
+    t5 = t1 + t3, t6 = t2 + t4;
+    t7 = t2 - t4, t8 = t1 - t3;
+    t1 = cnst_16 * in1 + cnst_15 * in3 + cnst_9 * in5 + cnst_4 * in7;
+    t2 = cnst_15 * in1 - cnst_4 * in3 - cnst_16 * in5 - cnst_9 * in7;
+    t3 = cnst_9 * in1 - cnst_16 * in3 + cnst_4 * in5 + cnst_15 * in7;
+    t4 = cnst_4 * in1 - cnst_9 * in3 + cnst_15 * in5 - cnst_16 * in7;
+    in0 = (t5 + t1) >> 3;
+    in1 = (t6 + t2) >> 3;
+    in2 = (t7 + t3) >> 3;
+    in3 = (t8 + t4) >> 3;
+    in4 = (t8 - t4) >> 3;
+    in5 = (t7 - t3) >> 3;
+    in6 = (t6 - t2) >> 3;
+    in7 = (t5 - t1) >> 3;
+    TRANSPOSE4x4_SW_SW(in0, in1, in2, in3, in0, in1, in2, in3);
+    TRANSPOSE4x4_SW_SW(in4, in5, in6, in7, in4, in5, in6, in7);
+    PCKEV_H4_SW(in4, in0, in5, in1, in6, in2, in7, in3, t1, t2, t3, t4);
+    ST_SW4(t1, t2, t3, t4, block, 8);
+    // Second loop
+    LD_SW4(dest, linesize, dst0, dst1, dst2, dst3);
+    ILVR_B4_SW(zero_m, dst0, zero_m, dst1, zero_m, dst2, zero_m, dst3,
+               dst0, dst1, dst2, dst3);
+    ILVL_H4_SW(zero_m, dst0, zero_m, dst1, zero_m, dst2, zero_m, dst3,
+               dst4, dst5, dst6, dst7);
+    ILVR_H4_SW(zero_m, dst0, zero_m, dst1, zero_m, dst2, zero_m, dst3,
+               dst0, dst1, dst2, dst3);
+    // Right part
+    t1 = cnst_17 * (in0 + in2) + cnst_64;
+    t2 = cnst_17 * (in0 - in2) + cnst_64;
+    t3 = cnst_22 * in1 + cnst_10 * in3;
+    t4 = cnst_22 * in3 - cnst_10 * in1;
+    in0 = (t1 + t3) >> 7;
+    in1 = (t2 - t4) >> 7;
+    in2 = (t2 + t4) >> 7;
+    in3 = (t1 - t3) >> 7;
+    ADD4(in0, dst0, in1, dst1, in2, dst2, in3, dst3, in0, in1, in2, in3);
+    CLIP_SW4_0_255(in0, in1, in2, in3);
+    // Left part
+    t5 = cnst_17 * (in4 + in6) + cnst_64;
+    t6 = cnst_17 * (in4 - in6) + cnst_64;
+    t7 = cnst_22 * in5 + cnst_10 * in7;
+    t8 = cnst_22 * in7 - cnst_10 * in5;
+    in4 = (t5 + t7) >> 7;
+    in5 = (t6 - t8) >> 7;
+    in6 = (t6 + t8) >> 7;
+    in7 = (t5 - t7) >> 7;
+    ADD4(in4, dst4, in5, dst5, in6, dst6, in7, dst7, in4, in5, in6, in7);
+    CLIP_SW4_0_255(in4, in5, in6, in7);
+    PCKEV_H4_SW(in4, in0, in5, in1, in6, in2, in7, in3, in0, in1, in2, in3);
+    PCKEV_B2_SW(in1, in0, in3, in2, in0, in1);
+    ST_D4(in0, in1, 0, 1, 0, 1, dest, linesize);
+}
+
+static void put_vc1_mspel_mc_h_v_msa(uint8_t *dst, const uint8_t *src,
+                                     ptrdiff_t stride, int hmode, int vmode,
+                                     int rnd)
+{
+    v8i16 in_r0, in_r1, in_r2, in_r3, in_l0, in_l1, in_l2, in_l3;
+    v8i16 t0, t1, t2, t3, t4, t5, t6, t7;
+    v8i16 t8, t9, t10, t11, t12, t13, t14, t15;
+    v8i16 cnst_para0, cnst_para1, cnst_para2, cnst_para3, cnst_r;
+    static const int para_value[][4] = {{4, 53, 18, 3},
+                                        {1, 9, 9, 1},
+                                        {3, 18, 53, 4}};
+    static const int shift_value[] = {0, 5, 1, 5};
+    int shift = (shift_value[hmode] + shift_value[vmode]) >> 1;
+    int r = (1 << (shift - 1)) + rnd - 1;
+    cnst_r = __msa_fill_h(r);
+    src -= 1, src -= stride;
+    cnst_para0 = __msa_fill_h(para_value[vmode - 1][0]);
+    cnst_para1 = __msa_fill_h(para_value[vmode - 1][1]);
+    cnst_para2 = __msa_fill_h(para_value[vmode - 1][2]);
+    cnst_para3 = __msa_fill_h(para_value[vmode - 1][3]);
+    LD_SH4(src, stride, in_l0, in_l1, in_l2, in_l3);
+    UNPCK_UB_SH(in_l0, in_r0, in_l0);
+    UNPCK_UB_SH(in_l1, in_r1, in_l1);
+    UNPCK_UB_SH(in_l2, in_r2, in_l2);
+    UNPCK_UB_SH(in_l3, in_r3, in_l3);
+    // row 0
+    t0 = cnst_para1 * in_r1 + cnst_para2 * in_r2
+         - cnst_para0 * in_r0 - cnst_para3 * in_r3;
+    t8 = cnst_para1 * in_l1 + cnst_para2 * in_l2
+         - cnst_para0 * in_l0 - cnst_para3 * in_l3;
+    in_l0 = LD_SH(src + 4 * stride);
+    UNPCK_UB_SH(in_l0, in_r0, in_l0);
+    // row 1
+    t1 = cnst_para1 * in_r2 + cnst_para2 * in_r3
+         - cnst_para0 * in_r1 - cnst_para3 * in_r0;
+    t9 = cnst_para1 * in_l2 + cnst_para2 * in_l3
+         - cnst_para0 * in_l1 - cnst_para3 * in_l0;
+    in_l1 = LD_SH(src + 5 * stride);
+    UNPCK_UB_SH(in_l1, in_r1, in_l1);
+    // row 2
+    t2 = cnst_para1 * in_r3 + cnst_para2 * in_r0
+         - cnst_para0 * in_r2 - cnst_para3 * in_r1;
+    t10 = cnst_para1 * in_l3 + cnst_para2 * in_l0
+          - cnst_para0 * in_l2 - cnst_para3 * in_l1;
+    in_l2 = LD_SH(src + 6 * stride);
+    UNPCK_UB_SH(in_l2, in_r2, in_l2);
+    // row 3
+    t3 = cnst_para1 * in_r0 + cnst_para2 * in_r1
+         - cnst_para0 * in_r3 - cnst_para3 * in_r2;
+    t11 = cnst_para1 * in_l0 + cnst_para2 * in_l1
+          - cnst_para0 * in_l3 - cnst_para3 * in_l2;
+    in_l3 = LD_SH(src + 7 * stride);
+    UNPCK_UB_SH(in_l3, in_r3, in_l3);
+    // row 4
+    t4 = cnst_para1 * in_r1 + cnst_para2 * in_r2
+         - cnst_para0 * in_r0 - cnst_para3 * in_r3;
+    t12 = cnst_para1 * in_l1 + cnst_para2 * in_l2
+          - cnst_para0 * in_l0 - cnst_para3 * in_l3;
+    in_l0 = LD_SH(src + 8 * stride);
+    UNPCK_UB_SH(in_l0, in_r0, in_l0);
+    // row 5
+    t5 = cnst_para1 * in_r2 + cnst_para2 * in_r3
+         - cnst_para0 * in_r1 - cnst_para3 * in_r0;
+    t13 = cnst_para1 * in_l2 + cnst_para2 * in_l3
+          - cnst_para0 * in_l1 - cnst_para3 * in_l0;
+    in_l1 = LD_SH(src + 9 * stride);
+    UNPCK_UB_SH(in_l1, in_r1, in_l1);
+    // row 6
+    t6 = cnst_para1 * in_r3 + cnst_para2 * in_r0
+         - cnst_para0 * in_r2 - cnst_para3 * in_r1;
+    t14 = cnst_para1 * in_l3 + cnst_para2 * in_l0
+          - cnst_para0 * in_l2 - cnst_para3 * in_l1;
+    in_l2 = LD_SH(src + 10 * stride);
+    UNPCK_UB_SH(in_l2, in_r2, in_l2);
+    // row 7
+    t7 = cnst_para1 * in_r0 + cnst_para2 * in_r1
+         - cnst_para0 * in_r3 - cnst_para3 * in_r2;
+    t15 = cnst_para1 * in_l0 + cnst_para2 * in_l1
+          - cnst_para0 * in_l3 - cnst_para3 * in_l2;
+
+    ADD4(t0, cnst_r, t1, cnst_r, t2, cnst_r, t3, cnst_r, t0, t1, t2, t3);
+    ADD4(t4, cnst_r, t5, cnst_r, t6, cnst_r, t7, cnst_r, t4, t5, t6, t7);
+    ADD4(t8, cnst_r, t9, cnst_r, t10, cnst_r, t11, cnst_r,
+         t8, t9, t10, t11);
+    ADD4(t12, cnst_r, t13, cnst_r, t14, cnst_r, t15, cnst_r,
+         t12, t13, t14, t15);
+    t0 >>= shift, t1 >>= shift, t2 >>= shift, t3 >>= shift;
+    t4 >>= shift, t5 >>= shift, t6 >>= shift, t7 >>= shift;
+    t8 >>= shift, t9 >>= shift, t10 >>= shift, t11 >>= shift;
+    t12 >>= shift, t13 >>= shift, t14 >>= shift, t15 >>= shift;
+    TRANSPOSE8x8_SH_SH(t0, t1, t2, t3, t4, t5, t6, t7,
+                       t0, t1, t2, t3, t4, t5, t6, t7);
+    TRANSPOSE8x8_SH_SH(t8, t9, t10, t11, t12, t13, t14, t15,
+                       t8, t9, t10, t11, t12, t13, t14, t15);
+    cnst_para0 = __msa_fill_h(para_value[hmode - 1][0]);
+    cnst_para1 = __msa_fill_h(para_value[hmode - 1][1]);
+    cnst_para2 = __msa_fill_h(para_value[hmode - 1][2]);
+    cnst_para3 = __msa_fill_h(para_value[hmode - 1][3]);
+    r = 64 - rnd;
+    cnst_r = __msa_fill_h(r);
+    // col 0 ~ 7
+    t0 = cnst_para1 * t1 + cnst_para2 * t2 - cnst_para0 * t0 - cnst_para3 * t3;
+    t1 = cnst_para1 * t2 + cnst_para2 * t3 - cnst_para0 * t1 - cnst_para3 * t4;
+    t2 = cnst_para1 * t3 + cnst_para2 * t4 - cnst_para0 * t2 - cnst_para3 * t5;
+    t3 = cnst_para1 * t4 + cnst_para2 * t5 - cnst_para0 * t3 - cnst_para3 * t6;
+    t4 = cnst_para1 * t5 + cnst_para2 * t6 - cnst_para0 * t4 - cnst_para3 * t7;
+    t5 = cnst_para1 * t6 + cnst_para2 * t7 - cnst_para0 * t5 - cnst_para3 * t8;
+    t6 = cnst_para1 * t7 + cnst_para2 * t8 - cnst_para0 * t6 - cnst_para3 * t9;
+    t7 = cnst_para1 * t8 + cnst_para2 * t9 - cnst_para0 * t7 - cnst_para3 * t10;
+    ADD4(t0, cnst_r, t1, cnst_r, t2, cnst_r, t3, cnst_r, t0, t1, t2, t3);
+    ADD4(t4, cnst_r, t5, cnst_r, t6, cnst_r, t7, cnst_r, t4, t5, t6, t7);
+    t0 >>= 7, t1 >>= 7, t2 >>= 7, t3 >>= 7;
+    t4 >>= 7, t5 >>= 7, t6 >>= 7, t7 >>= 7;
+    TRANSPOSE8x8_SH_SH(t0, t1, t2, t3, t4, t5, t6, t7,
+                       t0, t1, t2, t3, t4, t5, t6, t7);
+    CLIP_SH8_0_255(t0, t1, t2, t3, t4, t5, t6, t7);
+    PCKEV_B4_SH(t1, t0, t3, t2, t5, t4, t7, t6, t0, t1, t2, t3);
+    ST_D8(t0, t1, t2, t3, 0, 1, 0, 1, 0, 1, 0, 1, dst, stride);
+}
+
+#define PUT_VC1_MSPEL_MC_MSA(hmode, vmode)                                    \
+void ff_put_vc1_mspel_mc ## hmode ## vmode ## _msa(uint8_t *dst,              \
+                                                const uint8_t *src,           \
+                                                ptrdiff_t stride, int rnd)    \
+{                                                                             \
+    put_vc1_mspel_mc_h_v_msa(dst, src, stride, hmode, vmode, rnd);            \
+}                                                                             \
+void ff_put_vc1_mspel_mc ## hmode ## vmode ## _16_msa(uint8_t *dst,           \
+                                                   const uint8_t *src,        \
+                                                   ptrdiff_t stride, int rnd) \
+{                                                                             \
+    put_vc1_mspel_mc_h_v_msa(dst, src, stride, hmode, vmode, rnd);            \
+    put_vc1_mspel_mc_h_v_msa(dst + 8, src + 8, stride, hmode, vmode, rnd);    \
+    dst += 8 * stride, src += 8 * stride;                                     \
+    put_vc1_mspel_mc_h_v_msa(dst, src, stride, hmode, vmode, rnd);            \
+    put_vc1_mspel_mc_h_v_msa(dst + 8, src + 8, stride, hmode, vmode, rnd);    \
+}
+
+PUT_VC1_MSPEL_MC_MSA(1, 1);
+PUT_VC1_MSPEL_MC_MSA(1, 2);
+PUT_VC1_MSPEL_MC_MSA(1, 3);
+
+PUT_VC1_MSPEL_MC_MSA(2, 1);
+PUT_VC1_MSPEL_MC_MSA(2, 2);
+PUT_VC1_MSPEL_MC_MSA(2, 3);
+
+PUT_VC1_MSPEL_MC_MSA(3, 1);
+PUT_VC1_MSPEL_MC_MSA(3, 2);
+PUT_VC1_MSPEL_MC_MSA(3, 3);
diff --git a/libavcodec/mips/videodsp_init.c b/libavcodec/mips/videodsp_init.c
index 817040420b..07c23bcf7e 100644
--- a/libavcodec/mips/videodsp_init.c
+++ b/libavcodec/mips/videodsp_init.c
@@ -18,12 +18,12 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
+#include "libavutil/mips/cpu.h"
 #include "config.h"
 #include "libavutil/attributes.h"
 #include "libavutil/mips/asmdefs.h"
 #include "libavcodec/videodsp.h"
 
-#if HAVE_MSA
 static void prefetch_mips(uint8_t *mem, ptrdiff_t stride, int h)
 {
     register const uint8_t *p = mem;
@@ -41,11 +41,11 @@ static void prefetch_mips(uint8_t *mem, ptrdiff_t stride, int h)
         : [stride] "r" (stride)
     );
 }
-#endif  // #if HAVE_MSA
 
 av_cold void ff_videodsp_init_mips(VideoDSPContext *ctx, int bpc)
 {
-#if HAVE_MSA
-    ctx->prefetch = prefetch_mips;
-#endif  // #if HAVE_MSA
+    int cpu_flags = av_get_cpu_flags();
+
+    if (have_msa(cpu_flags))
+        ctx->prefetch = prefetch_mips;
 }
diff --git a/libavcodec/mips/vp3dsp_idct_mmi.c b/libavcodec/mips/vp3dsp_idct_mmi.c
index c5c4cf3127..d505fcb765 100644
--- a/libavcodec/mips/vp3dsp_idct_mmi.c
+++ b/libavcodec/mips/vp3dsp_idct_mmi.c
@@ -34,7 +34,7 @@ static void idct_row_mmi(int16_t *input)
     double ftmp[23];
     uint64_t tmp[2];
     __asm__ volatile (
-        "xor        %[ftmp10],      %[ftmp10],        %[ftmp10] \n\t"
+        "pxor       %[ftmp10],      %[ftmp10],        %[ftmp10] \n\t"
         LOAD_CONST(%[csth_1], 1)
         "li         %[tmp0],        0x02                        \n\t"
         "1:                                                     \n\t"
@@ -51,14 +51,14 @@ static void idct_row_mmi(int16_t *input)
         LOAD_CONST(%[ftmp9], 12785)
         "pmulhh     %[A],           %[ftmp9],         %[ftmp7]  \n\t"
         "pcmpgth    %[C],           %[ftmp10],        %[ftmp1]  \n\t"
-        "or         %[mask],        %[C],             %[csth_1] \n\t"
+        "por        %[mask],        %[C],             %[csth_1] \n\t"
         "pmullh     %[B],           %[ftmp1],         %[mask]   \n\t"
         "pmulhuh    %[B],           %[ftmp8],         %[B]      \n\t"
         "pmullh     %[B],           %[B],             %[mask]   \n\t"
         "paddh      %[A],           %[A],             %[B]      \n\t"
         "paddh      %[A],           %[A],             %[C]      \n\t"
         "pcmpgth    %[D],           %[ftmp10],        %[ftmp7]  \n\t"
-        "or         %[mask],        %[D],             %[csth_1] \n\t"
+        "por        %[mask],        %[D],             %[csth_1] \n\t"
         "pmullh     %[ftmp7],       %[ftmp7],         %[mask]   \n\t"
         "pmulhuh    %[B],           %[ftmp8],         %[ftmp7]  \n\t"
         "pmullh     %[B],           %[B],             %[mask]   \n\t"
@@ -69,12 +69,12 @@ static void idct_row_mmi(int16_t *input)
         LOAD_CONST(%[ftmp8], 54491)
         LOAD_CONST(%[ftmp9], 36410)
         "pcmpgth    %[Ad],          %[ftmp10],        %[ftmp5]  \n\t"
-        "or         %[mask],        %[Ad],            %[csth_1] \n\t"
+        "por        %[mask],        %[Ad],            %[csth_1] \n\t"
         "pmullh     %[ftmp1],       %[ftmp5],         %[mask]   \n\t"
         "pmulhuh    %[C],           %[ftmp9],         %[ftmp1]  \n\t"
         "pmullh     %[C],           %[C],             %[mask]   \n\t"
         "pcmpgth    %[Bd],          %[ftmp10],        %[ftmp3]  \n\t"
-        "or         %[mask],        %[Bd],            %[csth_1] \n\t"
+        "por        %[mask],        %[Bd],            %[csth_1] \n\t"
         "pmullh     %[D],           %[ftmp3],         %[mask]   \n\t"
         "pmulhuh    %[D],           %[ftmp8],         %[D]      \n\t"
         "pmullh     %[D],           %[D],             %[mask]   \n\t"
@@ -82,12 +82,12 @@ static void idct_row_mmi(int16_t *input)
         "paddh      %[C],           %[C],             %[Ad]     \n\t"
         "paddh      %[C],           %[C],             %[Bd]     \n\t"
         "pcmpgth    %[Bd],          %[ftmp10],        %[ftmp3]  \n\t"
-        "or         %[mask],        %[Bd],            %[csth_1] \n\t"
+        "por        %[mask],        %[Bd],            %[csth_1] \n\t"
         "pmullh     %[ftmp1],       %[ftmp3],         %[mask]   \n\t"
         "pmulhuh    %[D],           %[ftmp9],         %[ftmp1]  \n\t"
         "pmullh     %[D],           %[D],             %[mask]   \n\t"
         "pcmpgth    %[Ed],          %[ftmp10],        %[ftmp5]  \n\t"
-        "or         %[mask],        %[Ed],            %[csth_1] \n\t"
+        "por        %[mask],        %[Ed],            %[csth_1] \n\t"
         "pmullh     %[Ad],          %[ftmp5],         %[mask]   \n\t"
         "pmulhuh    %[Ad],          %[ftmp8],         %[Ad]     \n\t"
         "pmullh     %[Ad],          %[Ad],            %[mask]   \n\t"
@@ -98,14 +98,14 @@ static void idct_row_mmi(int16_t *input)
         LOAD_CONST(%[ftmp8], 46341)
         "psubh      %[Ad],          %[A],             %[C]      \n\t"
         "pcmpgth    %[Bd],          %[ftmp10],        %[Ad]     \n\t"
-        "or         %[mask],        %[Bd],            %[csth_1] \n\t"
+        "por        %[mask],        %[Bd],            %[csth_1] \n\t"
         "pmullh     %[Ad],          %[Ad],            %[mask]   \n\t"
         "pmulhuh    %[Ad],          %[ftmp8],         %[Ad]     \n\t"
         "pmullh     %[Ad],          %[Ad],            %[mask]   \n\t"
         "paddh      %[Ad],          %[Ad],            %[Bd]     \n\t"
         "psubh      %[Bd],          %[B],             %[D]      \n\t"
         "pcmpgth    %[Cd],          %[ftmp10],        %[Bd]     \n\t"
-        "or         %[mask],        %[Cd],            %[csth_1] \n\t"
+        "por        %[mask],        %[Cd],            %[csth_1] \n\t"
         "pmullh     %[Bd],          %[Bd],            %[mask]   \n\t"
         "pmulhuh    %[Bd],          %[ftmp8],         %[Bd]     \n\t"
         "pmullh     %[Bd],          %[Bd],            %[mask]   \n\t"
@@ -114,14 +114,14 @@ static void idct_row_mmi(int16_t *input)
         "paddh      %[Dd],          %[B],             %[D]      \n\t"
         "paddh      %[A],           %[ftmp0],         %[ftmp4]  \n\t"
         "pcmpgth    %[B],           %[ftmp10],        %[A]      \n\t"
-        "or         %[mask],        %[B],             %[csth_1] \n\t"
+        "por        %[mask],        %[B],             %[csth_1] \n\t"
         "pmullh     %[A],           %[A],             %[mask]   \n\t"
         "pmulhuh    %[A],           %[ftmp8],         %[A]      \n\t"
         "pmullh     %[A],           %[A],             %[mask]   \n\t"
         "paddh      %[A],           %[A],             %[B]      \n\t"
         "psubh      %[B],           %[ftmp0],         %[ftmp4]  \n\t"
         "pcmpgth    %[C],           %[ftmp10],        %[B]      \n\t"
-        "or         %[mask],        %[C],             %[csth_1] \n\t"
+        "por        %[mask],        %[C],             %[csth_1] \n\t"
         "pmullh     %[B],           %[B],             %[mask]   \n\t"
         "pmulhuh    %[B],           %[ftmp8],         %[B]      \n\t"
         "pmullh     %[B],           %[B],             %[mask]   \n\t"
@@ -131,14 +131,14 @@ static void idct_row_mmi(int16_t *input)
         LOAD_CONST(%[ftmp9], 25080)
         "pmulhh     %[C],           %[ftmp9],         %[ftmp6]  \n\t"
         "pcmpgth    %[D],           %[ftmp10],        %[ftmp2]  \n\t"
-        "or         %[mask],        %[D],             %[csth_1] \n\t"
+        "por        %[mask],        %[D],             %[csth_1] \n\t"
         "pmullh     %[Ed],          %[ftmp2],         %[mask]   \n\t"
         "pmulhuh    %[Ed],          %[ftmp8],         %[Ed]     \n\t"
         "pmullh     %[Ed],          %[Ed],            %[mask]   \n\t"
         "paddh      %[C],           %[C],             %[Ed]     \n\t"
         "paddh      %[C],           %[C],             %[D]      \n\t"
         "pcmpgth    %[Ed],          %[ftmp10],        %[ftmp6]  \n\t"
-        "or         %[mask],        %[Ed],            %[csth_1] \n\t"
+        "por        %[mask],        %[Ed],            %[csth_1] \n\t"
         "pmullh     %[ftmp6],       %[ftmp6],         %[mask]   \n\t"
         "pmulhuh    %[D],           %[ftmp8],         %[ftmp6]  \n\t"
         "pmullh     %[D],           %[D],             %[mask]   \n\t"
@@ -193,7 +193,7 @@ static void idct_column_true_mmi(uint8_t *dst, int stride, int16_t *input)
     for (int i = 0; i < 8; ++i)
         temp_value[i] = av_clip_uint8(128 + ((46341 * input[i << 3] + (8 << 16)) >> 20));
     __asm__ volatile (
-        "xor        %[ftmp10],      %[ftmp10],          %[ftmp10] \n\t"
+        "pxor       %[ftmp10],      %[ftmp10],          %[ftmp10] \n\t"
         "li         %[tmp0],        0x02                          \n\t"
         "1:                                                       \n\t"
         "ldc1       %[ftmp0],       0x00(%[input])                \n\t"
@@ -213,14 +213,14 @@ static void idct_column_true_mmi(uint8_t *dst, int stride, int16_t *input)
         LOAD_CONST(%[Gd], 1)
         "pmulhh     %[A],           %[ftmp9],           %[ftmp7]  \n\t"
         "pcmpgth    %[C],           %[ftmp10],          %[ftmp1]  \n\t"
-        "or         %[mask],        %[C],               %[Gd]     \n\t"
+        "por        %[mask],        %[C],               %[Gd]     \n\t"
         "pmullh     %[B],           %[ftmp1],           %[mask]   \n\t"
         "pmulhuh    %[B],           %[ftmp8],           %[B]      \n\t"
         "pmullh     %[B],           %[B],               %[mask]   \n\t"
         "paddh      %[A],           %[A],               %[B]      \n\t"
         "paddh      %[A],           %[A],               %[C]      \n\t"
         "pcmpgth    %[D],           %[ftmp10],          %[ftmp7]  \n\t"
-        "or         %[mask],        %[D],               %[Gd]     \n\t"
+        "por        %[mask],        %[D],               %[Gd]     \n\t"
         "pmullh     %[Ad],          %[ftmp7],           %[mask]   \n\t"
         "pmulhuh    %[B],           %[ftmp8],           %[Ad]     \n\t"
         "pmullh     %[B],           %[B],               %[mask]   \n\t"
@@ -231,12 +231,12 @@ static void idct_column_true_mmi(uint8_t *dst, int stride, int16_t *input)
         LOAD_CONST(%[ftmp8], 54491)
         LOAD_CONST(%[ftmp9], 36410)
         "pcmpgth    %[Ad],          %[ftmp10],          %[ftmp5]  \n\t"
-        "or         %[mask],        %[Ad],              %[Gd]     \n\t"
+        "por        %[mask],        %[Ad],              %[Gd]     \n\t"
         "pmullh     %[Cd],          %[ftmp5],           %[mask]   \n\t"
         "pmulhuh    %[C],           %[ftmp9],           %[Cd]     \n\t"
         "pmullh     %[C],           %[C],               %[mask]   \n\t"
         "pcmpgth    %[Bd],          %[ftmp10],          %[ftmp3]  \n\t"
-        "or         %[mask],        %[Bd],              %[Gd]     \n\t"
+        "por        %[mask],        %[Bd],              %[Gd]     \n\t"
         "pmullh     %[D],           %[ftmp3],           %[mask]   \n\t"
         "pmulhuh    %[D],           %[ftmp8],           %[D]      \n\t"
         "pmullh     %[D],           %[D],               %[mask]   \n\t"
@@ -244,12 +244,12 @@ static void idct_column_true_mmi(uint8_t *dst, int stride, int16_t *input)
         "paddh      %[C],           %[C],               %[Ad]     \n\t"
         "paddh      %[C],           %[C],               %[Bd]     \n\t"
         "pcmpgth    %[Bd],          %[ftmp10],          %[ftmp3]  \n\t"
-        "or         %[mask],        %[Bd],              %[Gd]     \n\t"
+        "por        %[mask],        %[Bd],              %[Gd]     \n\t"
         "pmullh     %[Cd],          %[ftmp3],           %[mask]   \n\t"
         "pmulhuh    %[D],           %[ftmp9],           %[Cd]     \n\t"
         "pmullh     %[D],           %[D],               %[mask]   \n\t"
         "pcmpgth    %[Ed],          %[ftmp10],          %[ftmp5]  \n\t"
-        "or         %[mask],        %[Ed],              %[Gd]     \n\t"
+        "por        %[mask],        %[Ed],              %[Gd]     \n\t"
         "pmullh     %[Ad],          %[ftmp5],           %[mask]   \n\t"
         "pmulhuh    %[Ad],          %[ftmp8],           %[Ad]     \n\t"
         "pmullh     %[Ad],          %[Ad],              %[mask]   \n\t"
@@ -260,14 +260,14 @@ static void idct_column_true_mmi(uint8_t *dst, int stride, int16_t *input)
         LOAD_CONST(%[ftmp8], 46341)
         "psubh      %[Ad],          %[A],             %[C]        \n\t"
         "pcmpgth    %[Bd],          %[ftmp10],        %[Ad]       \n\t"
-        "or         %[mask],        %[Bd],            %[Gd]       \n\t"
+        "por        %[mask],        %[Bd],            %[Gd]       \n\t"
         "pmullh     %[Ad],          %[Ad],            %[mask]     \n\t"
         "pmulhuh    %[Ad],          %[ftmp8],         %[Ad]       \n\t"
         "pmullh     %[Ad],          %[Ad],            %[mask]     \n\t"
         "paddh      %[Ad],          %[Ad],            %[Bd]       \n\t"
         "psubh      %[Bd],          %[B],             %[D]        \n\t"
         "pcmpgth    %[Cd],          %[ftmp10],        %[Bd]       \n\t"
-        "or         %[mask],        %[Cd],            %[Gd]       \n\t"
+        "por        %[mask],        %[Cd],            %[Gd]       \n\t"
         "pmullh     %[Bd],          %[Bd],            %[mask]     \n\t"
         "pmulhuh    %[Bd],          %[ftmp8],         %[Bd]       \n\t"
         "pmullh     %[Bd],          %[Bd],            %[mask]     \n\t"
@@ -278,7 +278,7 @@ static void idct_column_true_mmi(uint8_t *dst, int stride, int16_t *input)
         LOAD_CONST(%[Ed], 2056)
         "paddh      %[A],           %[ftmp0],         %[ftmp4]    \n\t"
         "pcmpgth    %[B],           %[ftmp10],        %[A]        \n\t"
-        "or         %[mask],        %[B],             %[Gd]       \n\t"
+        "por        %[mask],        %[B],             %[Gd]       \n\t"
         "pmullh     %[A],           %[A],             %[mask]     \n\t"
         "pmulhuh    %[A],           %[ftmp8],         %[A]        \n\t"
         "pmullh     %[A],           %[A],             %[mask]     \n\t"
@@ -286,7 +286,7 @@ static void idct_column_true_mmi(uint8_t *dst, int stride, int16_t *input)
         "paddh      %[A],           %[A],             %[Ed]       \n\t"
         "psubh      %[B],           %[ftmp0],         %[ftmp4]    \n\t"
         "pcmpgth    %[C],           %[ftmp10],        %[B]        \n\t"
-        "or         %[mask],        %[C],             %[Gd]       \n\t"
+        "por        %[mask],        %[C],             %[Gd]       \n\t"
         "pmullh     %[B],           %[B],             %[mask]     \n\t"
         "pmulhuh    %[B],           %[ftmp8],         %[B]        \n\t"
         "pmullh     %[B],           %[B],             %[mask]     \n\t"
@@ -297,14 +297,14 @@ static void idct_column_true_mmi(uint8_t *dst, int stride, int16_t *input)
         LOAD_CONST(%[ftmp9], 25080)
         "pmulhh     %[C],           %[ftmp9],         %[ftmp6]    \n\t"
         "pcmpgth    %[D],           %[ftmp10],        %[ftmp2]    \n\t"
-        "or         %[mask],        %[D],             %[Gd]       \n\t"
+        "por        %[mask],        %[D],             %[Gd]       \n\t"
         "pmullh     %[Ed],          %[ftmp2],         %[mask]     \n\t"
         "pmulhuh    %[Ed],          %[ftmp8],         %[Ed]       \n\t"
         "pmullh     %[Ed],          %[Ed],            %[mask]     \n\t"
         "paddh      %[C],           %[C],             %[Ed]       \n\t"
         "paddh      %[C],           %[C],             %[D]        \n\t"
         "pcmpgth    %[Ed],          %[ftmp10],        %[ftmp6]    \n\t"
-        "or         %[mask],        %[Ed],            %[Gd]       \n\t"
+        "por        %[mask],        %[Ed],            %[Gd]       \n\t"
         "pmullh     %[D],           %[ftmp6],         %[mask]     \n\t"
         "pmulhuh    %[D],           %[ftmp8],         %[D]        \n\t"
         "pmullh     %[D],           %[D],             %[mask]     \n\t"
@@ -317,12 +317,12 @@ static void idct_column_true_mmi(uint8_t *dst, int stride, int16_t *input)
         "psubh      %[C],           %[B],             %[Ad]       \n\t"
         "psubh      %[B],           %[Bd],            %[D]        \n\t"
         "paddh      %[D],           %[Bd],            %[D]        \n\t"
-        "or         %[mask],        %[ftmp1],         %[ftmp2]    \n\t"
-        "or         %[mask],        %[mask],          %[ftmp3]    \n\t"
-        "or         %[mask],        %[mask],          %[ftmp4]    \n\t"
-        "or         %[mask],        %[mask],          %[ftmp5]    \n\t"
-        "or         %[mask],        %[mask],          %[ftmp6]    \n\t"
-        "or         %[mask],        %[mask],          %[ftmp7]    \n\t"
+        "por        %[mask],        %[ftmp1],         %[ftmp2]    \n\t"
+        "por        %[mask],        %[mask],          %[ftmp3]    \n\t"
+        "por        %[mask],        %[mask],          %[ftmp4]    \n\t"
+        "por        %[mask],        %[mask],          %[ftmp5]    \n\t"
+        "por        %[mask],        %[mask],          %[ftmp6]    \n\t"
+        "por        %[mask],        %[mask],          %[ftmp7]    \n\t"
         "pcmpeqh    %[mask],        %[mask],          %[ftmp10]   \n\t"
         "packushb   %[mask],        %[mask],          %[ftmp10]   \n\t"
         "li         %[tmp1],        0x04                          \n\t"
@@ -361,7 +361,7 @@ static void idct_column_true_mmi(uint8_t *dst, int stride, int16_t *input)
         "packushb   %[ftmp7],       %[ftmp7],         %[ftmp10]   \n\t"
 
         "lwc1       %[Ed],          0x00(%[temp_value])           \n\t"
-        "and        %[Ed],          %[Ed],            %[mask]     \n\t"
+        "pand       %[Ed],          %[Ed],            %[mask]     \n\t"
         "paddb      %[ftmp0],       %[ftmp0],         %[Ed]       \n\t"
         "paddb      %[ftmp1],       %[ftmp1],         %[Ed]       \n\t"
         "paddb      %[ftmp2],       %[ftmp2],         %[Ed]       \n\t"
@@ -412,7 +412,7 @@ static void idct_column_false_mmi(uint8_t *dst, int stride, int16_t *input)
     for (int i = 0; i < 8; ++i)
         temp_value[i] = (46341 * input[i << 3] + (8 << 16)) >> 20;
     __asm__ volatile (
-        "xor        %[ftmp10],      %[ftmp10],          %[ftmp10] \n\t"
+        "pxor       %[ftmp10],      %[ftmp10],          %[ftmp10] \n\t"
         "li         %[tmp0],        0x02                          \n\t"
         "1:                                                       \n\t"
         "ldc1       %[ftmp0],       0x00(%[input])                \n\t"
@@ -432,14 +432,14 @@ static void idct_column_false_mmi(uint8_t *dst, int stride, int16_t *input)
         LOAD_CONST(%[Gd], 1)
         "pmulhh     %[A],           %[ftmp9],           %[ftmp7]  \n\t"
         "pcmpgth    %[C],           %[ftmp10],          %[ftmp1]  \n\t"
-        "or         %[mask],        %[C],               %[Gd]     \n\t"
+        "por        %[mask],        %[C],               %[Gd]     \n\t"
         "pmullh     %[B],           %[ftmp1],           %[mask]   \n\t"
         "pmulhuh    %[B],           %[ftmp8],           %[B]      \n\t"
         "pmullh     %[B],           %[B],               %[mask]   \n\t"
         "paddh      %[A],           %[A],               %[B]      \n\t"
         "paddh      %[A],           %[A],               %[C]      \n\t"
         "pcmpgth    %[D],           %[ftmp10],          %[ftmp7]  \n\t"
-        "or         %[mask],        %[D],               %[Gd]     \n\t"
+        "por        %[mask],        %[D],               %[Gd]     \n\t"
         "pmullh     %[Ad],          %[ftmp7],           %[mask]   \n\t"
         "pmulhuh    %[B],           %[ftmp8],           %[Ad]     \n\t"
         "pmullh     %[B],           %[B],               %[mask]   \n\t"
@@ -450,12 +450,12 @@ static void idct_column_false_mmi(uint8_t *dst, int stride, int16_t *input)
         LOAD_CONST(%[ftmp8], 54491)
         LOAD_CONST(%[ftmp9], 36410)
         "pcmpgth    %[Ad],          %[ftmp10],          %[ftmp5]  \n\t"
-        "or         %[mask],        %[Ad],              %[Gd]     \n\t"
+        "por        %[mask],        %[Ad],              %[Gd]     \n\t"
         "pmullh     %[Cd],          %[ftmp5],           %[mask]   \n\t"
         "pmulhuh    %[C],           %[ftmp9],           %[Cd]     \n\t"
         "pmullh     %[C],           %[C],               %[mask]   \n\t"
         "pcmpgth    %[Bd],          %[ftmp10],          %[ftmp3]  \n\t"
-        "or         %[mask],        %[Bd],              %[Gd]     \n\t"
+        "por        %[mask],        %[Bd],              %[Gd]     \n\t"
         "pmullh     %[D],           %[ftmp3],           %[mask]   \n\t"
         "pmulhuh    %[D],           %[ftmp8],           %[D]      \n\t"
         "pmullh     %[D],           %[D],               %[mask]   \n\t"
@@ -463,12 +463,12 @@ static void idct_column_false_mmi(uint8_t *dst, int stride, int16_t *input)
         "paddh      %[C],           %[C],               %[Ad]     \n\t"
         "paddh      %[C],           %[C],               %[Bd]     \n\t"
         "pcmpgth    %[Bd],          %[ftmp10],          %[ftmp3]  \n\t"
-        "or         %[mask],        %[Bd],              %[Gd]     \n\t"
+        "por        %[mask],        %[Bd],              %[Gd]     \n\t"
         "pmullh     %[Cd],          %[ftmp3],           %[mask]   \n\t"
         "pmulhuh    %[D],           %[ftmp9],           %[Cd]     \n\t"
         "pmullh     %[D],           %[D],               %[mask]   \n\t"
         "pcmpgth    %[Ed],          %[ftmp10],          %[ftmp5]  \n\t"
-        "or         %[mask],        %[Ed],              %[Gd]     \n\t"
+        "por        %[mask],        %[Ed],              %[Gd]     \n\t"
         "pmullh     %[Ad],          %[ftmp5],           %[mask]   \n\t"
         "pmulhuh    %[Ad],          %[ftmp8],           %[Ad]     \n\t"
         "pmullh     %[Ad],          %[Ad],              %[mask]   \n\t"
@@ -479,14 +479,14 @@ static void idct_column_false_mmi(uint8_t *dst, int stride, int16_t *input)
         LOAD_CONST(%[ftmp8], 46341)
         "psubh      %[Ad],          %[A],             %[C]        \n\t"
         "pcmpgth    %[Bd],          %[ftmp10],        %[Ad]       \n\t"
-        "or         %[mask],        %[Bd],            %[Gd]       \n\t"
+        "por        %[mask],        %[Bd],            %[Gd]       \n\t"
         "pmullh     %[Ad],          %[Ad],            %[mask]     \n\t"
         "pmulhuh    %[Ad],          %[ftmp8],         %[Ad]       \n\t"
         "pmullh     %[Ad],          %[Ad],            %[mask]     \n\t"
         "paddh      %[Ad],          %[Ad],            %[Bd]       \n\t"
         "psubh      %[Bd],          %[B],             %[D]        \n\t"
         "pcmpgth    %[Cd],          %[ftmp10],        %[Bd]       \n\t"
-        "or         %[mask],        %[Cd],            %[Gd]       \n\t"
+        "por        %[mask],        %[Cd],            %[Gd]       \n\t"
         "pmullh     %[Bd],          %[Bd],            %[mask]     \n\t"
         "pmulhuh    %[Bd],          %[ftmp8],         %[Bd]       \n\t"
         "pmullh     %[Bd],          %[Bd],            %[mask]     \n\t"
@@ -497,7 +497,7 @@ static void idct_column_false_mmi(uint8_t *dst, int stride, int16_t *input)
         LOAD_CONST(%[Ed], 8)
         "paddh      %[A],           %[ftmp0],         %[ftmp4]    \n\t"
         "pcmpgth    %[B],           %[ftmp10],        %[A]        \n\t"
-        "or         %[mask],        %[B],             %[Gd]       \n\t"
+        "por        %[mask],        %[B],             %[Gd]       \n\t"
         "pmullh     %[A],           %[A],             %[mask]     \n\t"
         "pmulhuh    %[A],           %[ftmp8],         %[A]        \n\t"
         "pmullh     %[A],           %[A],             %[mask]     \n\t"
@@ -505,7 +505,7 @@ static void idct_column_false_mmi(uint8_t *dst, int stride, int16_t *input)
         "paddh      %[A],           %[A],             %[Ed]       \n\t"
         "psubh      %[B],           %[ftmp0],         %[ftmp4]    \n\t"
         "pcmpgth    %[C],           %[ftmp10],        %[B]        \n\t"
-        "or         %[mask],        %[C],             %[Gd]       \n\t"
+        "por        %[mask],        %[C],             %[Gd]       \n\t"
         "pmullh     %[B],           %[B],             %[mask]     \n\t"
         "pmulhuh    %[B],           %[ftmp8],         %[B]        \n\t"
         "pmullh     %[B],           %[B],             %[mask]     \n\t"
@@ -516,14 +516,14 @@ static void idct_column_false_mmi(uint8_t *dst, int stride, int16_t *input)
         LOAD_CONST(%[ftmp9], 25080)
         "pmulhh     %[C],           %[ftmp9],         %[ftmp6]    \n\t"
         "pcmpgth    %[D],           %[ftmp10],        %[ftmp2]    \n\t"
-        "or         %[mask],        %[D],             %[Gd]       \n\t"
+        "por        %[mask],        %[D],             %[Gd]       \n\t"
         "pmullh     %[Ed],          %[ftmp2],         %[mask]     \n\t"
         "pmulhuh    %[Ed],          %[ftmp8],         %[Ed]       \n\t"
         "pmullh     %[Ed],          %[Ed],            %[mask]     \n\t"
         "paddh      %[C],           %[C],             %[Ed]       \n\t"
         "paddh      %[C],           %[C],             %[D]        \n\t"
         "pcmpgth    %[Ed],          %[ftmp10],        %[ftmp6]    \n\t"
-        "or         %[mask],        %[Ed],            %[Gd]       \n\t"
+        "por        %[mask],        %[Ed],            %[Gd]       \n\t"
         "pmullh     %[D],           %[ftmp6],         %[mask]     \n\t"
         "pmulhuh    %[D],           %[ftmp8],         %[D]        \n\t"
         "pmullh     %[D],           %[D],             %[mask]     \n\t"
@@ -536,12 +536,12 @@ static void idct_column_false_mmi(uint8_t *dst, int stride, int16_t *input)
         "psubh      %[C],           %[B],             %[Ad]       \n\t"
         "psubh      %[B],           %[Bd],            %[D]        \n\t"
         "paddh      %[D],           %[Bd],            %[D]        \n\t"
-        "or         %[mask],        %[ftmp1],         %[ftmp2]    \n\t"
-        "or         %[mask],        %[mask],          %[ftmp3]    \n\t"
-        "or         %[mask],        %[mask],          %[ftmp4]    \n\t"
-        "or         %[mask],        %[mask],          %[ftmp5]    \n\t"
-        "or         %[mask],        %[mask],          %[ftmp6]    \n\t"
-        "or         %[mask],        %[mask],          %[ftmp7]    \n\t"
+        "por        %[mask],        %[ftmp1],         %[ftmp2]    \n\t"
+        "por        %[mask],        %[mask],          %[ftmp3]    \n\t"
+        "por        %[mask],        %[mask],          %[ftmp4]    \n\t"
+        "por        %[mask],        %[mask],          %[ftmp5]    \n\t"
+        "por        %[mask],        %[mask],          %[ftmp6]    \n\t"
+        "por        %[mask],        %[mask],          %[ftmp7]    \n\t"
         "pcmpeqh    %[mask],        %[mask],          %[ftmp10]   \n\t"
         "li         %[tmp1],        0x04                          \n\t"
         "dmtc1      %[tmp1],        %[ftmp8]                      \n\t"
@@ -587,16 +587,16 @@ static void idct_column_false_mmi(uint8_t *dst, int stride, int16_t *input)
         "punpcklbh  %[Cd],          %[Cd],            %[ftmp10]   \n\t"
         "punpcklbh  %[Dd],          %[Dd],            %[ftmp10]   \n\t"
         "ldc1       %[Ed],          0x00(%[temp_value])           \n\t"
-        "and        %[Ed],          %[Ed],            %[mask]     \n\t"
-        "nor        %[mask],        %[mask],          %[mask]     \n\t"
-        "and        %[ftmp0],       %[ftmp0],         %[mask]     \n\t"
-        "and        %[ftmp1],       %[ftmp1],         %[mask]     \n\t"
-        "and        %[ftmp2],       %[ftmp2],         %[mask]     \n\t"
-        "and        %[ftmp3],       %[ftmp3],         %[mask]     \n\t"
-        "and        %[ftmp4],       %[ftmp4],         %[mask]     \n\t"
-        "and        %[ftmp5],       %[ftmp5],         %[mask]     \n\t"
-        "and        %[ftmp6],       %[ftmp6],         %[mask]     \n\t"
-        "and        %[ftmp7],       %[ftmp7],         %[mask]     \n\t"
+        "pand       %[Ed],          %[Ed],            %[mask]     \n\t"
+        "pnor       %[mask],        %[mask],          %[mask]     \n\t"
+        "pand       %[ftmp0],       %[ftmp0],         %[mask]     \n\t"
+        "pand       %[ftmp1],       %[ftmp1],         %[mask]     \n\t"
+        "pand       %[ftmp2],       %[ftmp2],         %[mask]     \n\t"
+        "pand       %[ftmp3],       %[ftmp3],         %[mask]     \n\t"
+        "pand       %[ftmp4],       %[ftmp4],         %[mask]     \n\t"
+        "pand       %[ftmp5],       %[ftmp5],         %[mask]     \n\t"
+        "pand       %[ftmp6],       %[ftmp6],         %[mask]     \n\t"
+        "pand       %[ftmp7],       %[ftmp7],         %[mask]     \n\t"
         "paddh      %[ftmp0],       %[ftmp0],         %[A]        \n\t"
         "paddh      %[ftmp1],       %[ftmp1],         %[B]        \n\t"
         "paddh      %[ftmp2],       %[ftmp2],         %[C]        \n\t"
@@ -689,7 +689,7 @@ void ff_vp3_idct_dc_add_mmi(uint8_t *dest, ptrdiff_t line_size, int16_t *block)
     double ftmp[7];
     uint64_t tmp;
     __asm__ volatile (
-        "xor        %[ftmp0],     %[ftmp0],           %[ftmp0]      \n\t"
+        "pxor       %[ftmp0],     %[ftmp0],           %[ftmp0]      \n\t"
         "mtc1       %[dc],        %[ftmp5]                          \n\t"
         "pshufh     %[ftmp5],     %[ftmp5],           %[ftmp0]      \n\t"
         "li         %[tmp0],      0x08                              \n\t"
@@ -722,6 +722,8 @@ void ff_put_no_rnd_pixels_l2_mmi(uint8_t *dst, const uint8_t *src1,
     if (h == 8) {
         double ftmp[6];
         uint64_t tmp[2];
+        DECLARE_VAR_ALL64;
+
         __asm__ volatile (
             "li          %[tmp0],        0x08                            \n\t"
             "li          %[tmp1],        0xfefefefe                      \n\t"
@@ -730,14 +732,12 @@ void ff_put_no_rnd_pixels_l2_mmi(uint8_t *dst, const uint8_t *src1,
             "li          %[tmp1],        0x01                            \n\t"
             "dmtc1       %[tmp1],        %[ftmp5]                        \n\t"
             "1:                                                          \n\t"
-            "gsldlc1     %[ftmp1],       0x07(%[src1])                   \n\t"
-            "gsldrc1     %[ftmp1],       0x00(%[src1])                   \n\t"
-            "gsldlc1     %[ftmp2],       0x07(%[src2])                   \n\t"
-            "gsldrc1     %[ftmp2],       0x00(%[src2])                   \n\t"
-            "xor         %[ftmp3],       %[ftmp1],             %[ftmp2]  \n\t"
-            "and         %[ftmp3],       %[ftmp3],             %[ftmp4]  \n\t"
+            MMI_ULDC1(%[ftmp1], %[src1], 0x0)
+            MMI_ULDC1(%[ftmp2], %[src2], 0x0)
+            "pxor        %[ftmp3],       %[ftmp1],             %[ftmp2]  \n\t"
+            "pand        %[ftmp3],       %[ftmp3],             %[ftmp4]  \n\t"
             "psrlw       %[ftmp3],       %[ftmp3],             %[ftmp5]  \n\t"
-            "and         %[ftmp6],       %[ftmp1],             %[ftmp2]  \n\t"
+            "pand        %[ftmp6],       %[ftmp1],             %[ftmp2]  \n\t"
             "paddw       %[ftmp3],       %[ftmp3],             %[ftmp6]  \n\t"
             "sdc1        %[ftmp3],       0x00(%[dst])                    \n\t"
             PTR_ADDU    "%[src1],        %[src1],              %[stride] \n\t"
@@ -745,7 +745,8 @@ void ff_put_no_rnd_pixels_l2_mmi(uint8_t *dst, const uint8_t *src1,
             PTR_ADDU    "%[dst],         %[dst],               %[stride] \n\t"
             PTR_ADDIU   "%[tmp0],        %[tmp0],              -0x01     \n\t"
             "bnez        %[tmp0],        1b                              \n\t"
-            : [dst]"+&r"(dst), [src1]"+&r"(src1), [src2]"+&r"(src2),
+            : RESTRICT_ASM_ALL64
+              [dst]"+&r"(dst), [src1]"+&r"(src1), [src2]"+&r"(src2),
               [ftmp1]"=&f"(ftmp[0]), [ftmp2]"=&f"(ftmp[1]), [ftmp3]"=&f"(ftmp[2]),
               [ftmp4]"=&f"(ftmp[3]), [ftmp5]"=&f"(ftmp[4]), [ftmp6]"=&f"(ftmp[5]),
               [tmp0]"=&r"(tmp[0]), [tmp1]"=&r"(tmp[1])
diff --git a/libavcodec/mips/vp3dsp_idct_msa.c b/libavcodec/mips/vp3dsp_idct_msa.c
index b2899eea4a..90c578f134 100644
--- a/libavcodec/mips/vp3dsp_idct_msa.c
+++ b/libavcodec/mips/vp3dsp_idct_msa.c
@@ -187,14 +187,7 @@ static void idct_msa(uint8_t *dst, int stride, int16_t *input, int type)
         G += c5;
         H += c6;
     }
-    A = CLIP_SW_0_255(A);
-    B = CLIP_SW_0_255(B);
-    C = CLIP_SW_0_255(C);
-    D = CLIP_SW_0_255(D);
-    E = CLIP_SW_0_255(E);
-    F = CLIP_SW_0_255(F);
-    G = CLIP_SW_0_255(G);
-    H = CLIP_SW_0_255(H);
+    CLIP_SW8_0_255(A, B, C, D, E, F, G, H);
     sign_l = __msa_or_v((v16u8)r1_r, (v16u8)r2_r);
     sign_l = __msa_or_v(sign_l, (v16u8)r3_r);
     sign_l = __msa_or_v(sign_l, (v16u8)r0_l);
@@ -205,7 +198,7 @@ static void idct_msa(uint8_t *dst, int stride, int16_t *input, int type)
     Add = ((r0_r * cnst46341w) + (8 << 16)) >> 20;
     if (type == 1) {
         Bdd = Add + cnst128w;
-        Bdd = CLIP_SW_0_255(Bdd);
+        CLIP_SW_0_255(Bdd);
         Ad = Bdd;
         Bd = Bdd;
         Cd = Bdd;
@@ -223,14 +216,7 @@ static void idct_msa(uint8_t *dst, int stride, int16_t *input, int type)
         Fd = Add + c5;
         Gd = Add + c6;
         Hd = Add + c7;
-        Ad = CLIP_SW_0_255(Ad);
-        Bd = CLIP_SW_0_255(Bd);
-        Cd = CLIP_SW_0_255(Cd);
-        Dd = CLIP_SW_0_255(Dd);
-        Ed = CLIP_SW_0_255(Ed);
-        Fd = CLIP_SW_0_255(Fd);
-        Gd = CLIP_SW_0_255(Gd);
-        Hd = CLIP_SW_0_255(Hd);
+        CLIP_SW8_0_255(Ad, Bd, Cd, Dd, Ed, Fd, Gd, Hd);
     }
     Ad = (v4i32)__msa_and_v((v16u8)Ad, (v16u8)sign_t);
     Bd = (v4i32)__msa_and_v((v16u8)Bd, (v16u8)sign_t);
@@ -309,14 +295,7 @@ static void idct_msa(uint8_t *dst, int stride, int16_t *input, int type)
         G += c5;
         H += c6;
     }
-    A = CLIP_SW_0_255(A);
-    B = CLIP_SW_0_255(B);
-    C = CLIP_SW_0_255(C);
-    D = CLIP_SW_0_255(D);
-    E = CLIP_SW_0_255(E);
-    F = CLIP_SW_0_255(F);
-    G = CLIP_SW_0_255(G);
-    H = CLIP_SW_0_255(H);
+    CLIP_SW8_0_255(A, B, C, D, E, F, G, H);
     sign_l = __msa_or_v((v16u8)r5_r, (v16u8)r6_r);
     sign_l = __msa_or_v(sign_l, (v16u8)r7_r);
     sign_l = __msa_or_v(sign_l, (v16u8)r4_l);
@@ -327,7 +306,7 @@ static void idct_msa(uint8_t *dst, int stride, int16_t *input, int type)
     Add = ((r4_r * cnst46341w) + (8 << 16)) >> 20;
     if (type == 1) {
         Bdd = Add + cnst128w;
-        Bdd = CLIP_SW_0_255(Bdd);
+        CLIP_SW_0_255(Bdd);
         Ad = Bdd;
         Bd = Bdd;
         Cd = Bdd;
@@ -345,14 +324,7 @@ static void idct_msa(uint8_t *dst, int stride, int16_t *input, int type)
         Fd = Add + c5;
         Gd = Add + c6;
         Hd = Add + c7;
-        Ad = CLIP_SW_0_255(Ad);
-        Bd = CLIP_SW_0_255(Bd);
-        Cd = CLIP_SW_0_255(Cd);
-        Dd = CLIP_SW_0_255(Dd);
-        Ed = CLIP_SW_0_255(Ed);
-        Fd = CLIP_SW_0_255(Fd);
-        Gd = CLIP_SW_0_255(Gd);
-        Hd = CLIP_SW_0_255(Hd);
+        CLIP_SW8_0_255(Ad, Bd, Cd, Dd, Ed, Fd, Gd, Hd);
     }
     Ad = (v4i32)__msa_and_v((v16u8)Ad, (v16u8)sign_t);
     Bd = (v4i32)__msa_and_v((v16u8)Bd, (v16u8)sign_t);
@@ -436,14 +408,7 @@ void ff_vp3_idct_dc_add_msa(uint8_t *dest, ptrdiff_t line_size, int16_t *block)
     e5 += dc;
     e6 += dc;
     e7 += dc;
-    e0 = CLIP_SW_0_255(e0);
-    e1 = CLIP_SW_0_255(e1);
-    e2 = CLIP_SW_0_255(e2);
-    e3 = CLIP_SW_0_255(e3);
-    e4 = CLIP_SW_0_255(e4);
-    e5 = CLIP_SW_0_255(e5);
-    e6 = CLIP_SW_0_255(e6);
-    e7 = CLIP_SW_0_255(e7);
+    CLIP_SW8_0_255(e0, e1, e2, e3, e4, e5, e6, e7);
 
     /* Left part */
     ILVL_H4_SW(zero, c0, zero, c1, zero, c2, zero, c3,
@@ -458,14 +423,7 @@ void ff_vp3_idct_dc_add_msa(uint8_t *dest, ptrdiff_t line_size, int16_t *block)
     r5 += dc;
     r6 += dc;
     r7 += dc;
-    r0 = CLIP_SW_0_255(r0);
-    r1 = CLIP_SW_0_255(r1);
-    r2 = CLIP_SW_0_255(r2);
-    r3 = CLIP_SW_0_255(r3);
-    r4 = CLIP_SW_0_255(r4);
-    r5 = CLIP_SW_0_255(r5);
-    r6 = CLIP_SW_0_255(r6);
-    r7 = CLIP_SW_0_255(r7);
+    CLIP_SW8_0_255(r0, r1, r2, r3, r4, r5, r6, r7);
     VSHF_B2_SB(e0, r0, e1, r1, mask, mask, d0, d1);
     VSHF_B2_SB(e2, r2, e3, r3, mask, mask, d2, d3);
     VSHF_B2_SB(e4, r4, e5, r5, mask, mask, d4, d5);
@@ -516,10 +474,7 @@ void ff_vp3_v_loop_filter_msa(uint8_t *first_pixel, ptrdiff_t stride,
     f1 += e1;
     g0 -= e0;
     g1 -= e1;
-    f0 = CLIP_SW_0_255(f0);
-    f1 = CLIP_SW_0_255(f1);
-    g0 = CLIP_SW_0_255(g0);
-    g1 = CLIP_SW_0_255(g1);
+    CLIP_SW4_0_255(f0, f1, g0, g1);
     VSHF_B2_SB(f0, f1, g0, g1, mask, mask, d1, d2);
 
     /* Final move to first_pixel */
@@ -563,10 +518,7 @@ void ff_vp3_h_loop_filter_msa(uint8_t *first_pixel, ptrdiff_t stride,
     f1 += e1;
     g0 -= e0;
     g1 -= e1;
-    f0 = CLIP_SW_0_255(f0);
-    f1 = CLIP_SW_0_255(f1);
-    g0 = CLIP_SW_0_255(g0);
-    g1 = CLIP_SW_0_255(g1);
+    CLIP_SW4_0_255(f0, f1, g0, g1);
     VSHF_B2_SB(f0, g0, f1, g1, mask, mask, d1, d2);
     /* Final move to first_pixel */
     ST_H4(d1, 0, 1, 2, 3, first_pixel - 1, stride);
diff --git a/libavcodec/mips/vp3dsp_init_mips.c b/libavcodec/mips/vp3dsp_init_mips.c
index e183db35b6..4252ff790e 100644
--- a/libavcodec/mips/vp3dsp_init_mips.c
+++ b/libavcodec/mips/vp3dsp_init_mips.c
@@ -19,42 +19,32 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
+#include "libavutil/mips/cpu.h"
 #include "config.h"
 #include "libavutil/attributes.h"
 #include "libavcodec/avcodec.h"
 #include "libavcodec/vp3dsp.h"
 #include "vp3dsp_mips.h"
 
-#if HAVE_MSA
-static av_cold void vp3dsp_init_msa(VP3DSPContext *c, int flags)
+av_cold void ff_vp3dsp_init_mips(VP3DSPContext *c, int flags)
 {
-    c->put_no_rnd_pixels_l2 = ff_put_no_rnd_pixels_l2_msa;
+    int cpu_flags = av_get_cpu_flags();
 
-    c->idct_add      = ff_vp3_idct_add_msa;
-    c->idct_put      = ff_vp3_idct_put_msa;
-    c->idct_dc_add   = ff_vp3_idct_dc_add_msa;
-    c->v_loop_filter = ff_vp3_v_loop_filter_msa;
-    c->h_loop_filter = ff_vp3_h_loop_filter_msa;
-}
-#endif /* HAVE_MSA */
+    if (have_mmi(cpu_flags)) {
+        c->put_no_rnd_pixels_l2 = ff_put_no_rnd_pixels_l2_mmi;
 
-#if HAVE_MMI
-static av_cold void vp3dsp_init_mmi(VP3DSPContext *c, int flags)
-{
-    c->put_no_rnd_pixels_l2 = ff_put_no_rnd_pixels_l2_mmi;
+        c->idct_add      = ff_vp3_idct_add_mmi;
+        c->idct_put      = ff_vp3_idct_put_mmi;
+        c->idct_dc_add   = ff_vp3_idct_dc_add_mmi;
+    }
 
-    c->idct_add      = ff_vp3_idct_add_mmi;
-    c->idct_put      = ff_vp3_idct_put_mmi;
-    c->idct_dc_add   = ff_vp3_idct_dc_add_mmi;
-}
-#endif /* HAVE_MMI */
+    if (have_msa(cpu_flags)) {
+        c->put_no_rnd_pixels_l2 = ff_put_no_rnd_pixels_l2_msa;
 
-av_cold void ff_vp3dsp_init_mips(VP3DSPContext *c, int flags)
-{
-#if HAVE_MMI
-    vp3dsp_init_mmi(c, flags);
-#endif /* HAVE_MMI */
-#if HAVE_MSA
-    vp3dsp_init_msa(c, flags);
-#endif /* HAVE_MSA */
+        c->idct_add      = ff_vp3_idct_add_msa;
+        c->idct_put      = ff_vp3_idct_put_msa;
+        c->idct_dc_add   = ff_vp3_idct_dc_add_msa;
+        c->v_loop_filter = ff_vp3_v_loop_filter_msa;
+        c->h_loop_filter = ff_vp3_h_loop_filter_msa;
+    }
 }
diff --git a/libavcodec/mips/vp8_idct_msa.c b/libavcodec/mips/vp8_idct_msa.c
index ae6fec0d60..ce37ca1881 100644
--- a/libavcodec/mips/vp8_idct_msa.c
+++ b/libavcodec/mips/vp8_idct_msa.c
@@ -71,10 +71,7 @@ void ff_vp8_idct_add_msa(uint8_t *dst, int16_t input[16], ptrdiff_t stride)
     ILVR_H4_SW(zero, res0, zero, res1, zero, res2, zero, res3,
                res0, res1, res2, res3);
     ADD4(res0, vt0, res1, vt1, res2, vt2, res3, vt3, res0, res1, res2, res3);
-    res0 = CLIP_SW_0_255(res0);
-    res1 = CLIP_SW_0_255(res1);
-    res2 = CLIP_SW_0_255(res2);
-    res3 = CLIP_SW_0_255(res3);
+    CLIP_SW4_0_255(res0, res1, res2, res3);
     VSHF_B2_SB(res0, res1, res2, res3, mask, mask, dest0, dest1);
     ST_W2(dest0, 0, 1, dst, stride);
     ST_W2(dest1, 0, 1, dst + 2 * stride, stride);
diff --git a/libavcodec/mips/vp8_mc_msa.c b/libavcodec/mips/vp8_mc_msa.c
index 57af6b45f1..a61320625a 100644
--- a/libavcodec/mips/vp8_mc_msa.c
+++ b/libavcodec/mips/vp8_mc_msa.c
@@ -1995,8 +1995,8 @@ static void common_hv_2ht_2vt_4x8_msa(uint8_t *src, int32_t src_stride,
     hz_out4 = HORIZ_2TAP_FILT_UH(src4, src5, mask, filt_hz, 7);
     hz_out6 = HORIZ_2TAP_FILT_UH(src6, src7, mask, filt_hz, 7);
     hz_out8 = HORIZ_2TAP_FILT_UH(src8, src8, mask, filt_hz, 7);
-    SLDI_B3_UH(hz_out2, hz_out4, hz_out6, hz_out0, hz_out2, hz_out4, hz_out1,
-               hz_out3, hz_out5, 8);
+    SLDI_B3_UH(hz_out2, hz_out0, hz_out4, hz_out2, hz_out6, hz_out4, 8, hz_out1,
+               hz_out3, hz_out5);
     hz_out7 = (v8u16) __msa_pckod_d((v2i64) hz_out8, (v2i64) hz_out6);
 
     ILVEV_B2_UB(hz_out0, hz_out1, hz_out2, hz_out3, vec0, vec1);
diff --git a/libavcodec/mips/vp8dsp_init_mips.c b/libavcodec/mips/vp8dsp_init_mips.c
index 7fd8fb0d32..92d8c792cb 100644
--- a/libavcodec/mips/vp8dsp_init_mips.c
+++ b/libavcodec/mips/vp8dsp_init_mips.c
@@ -24,6 +24,7 @@
  * VP8 compatible video decoder
  */
 
+#include "libavutil/mips/cpu.h"
 #include "config.h"
 #include "libavutil/attributes.h"
 #include "libavcodec/vp8dsp.h"
@@ -71,132 +72,123 @@
     dsp->put_vp8_bilinear_pixels_tab[IDX][0][0] =  \
         ff_put_vp8_pixels##SIZE##_msa;
 
-#if HAVE_MSA
-static av_cold void vp8dsp_init_msa(VP8DSPContext *dsp)
-{
-    dsp->vp8_luma_dc_wht = ff_vp8_luma_dc_wht_msa;
-    dsp->vp8_idct_add = ff_vp8_idct_add_msa;
-    dsp->vp8_idct_dc_add = ff_vp8_idct_dc_add_msa;
-    dsp->vp8_idct_dc_add4y = ff_vp8_idct_dc_add4y_msa;
-    dsp->vp8_idct_dc_add4uv = ff_vp8_idct_dc_add4uv_msa;
-
-    VP8_MC_MIPS_FUNC(0, 16);
-    VP8_MC_MIPS_FUNC(1, 8);
-    VP8_MC_MIPS_FUNC(2, 4);
-
-    VP8_BILINEAR_MC_MIPS_FUNC(0, 16);
-    VP8_BILINEAR_MC_MIPS_FUNC(1, 8);
-    VP8_BILINEAR_MC_MIPS_FUNC(2, 4);
-
-    VP8_MC_MIPS_COPY(0, 16);
-    VP8_MC_MIPS_COPY(1, 8);
-
-    dsp->vp8_v_loop_filter16y = ff_vp8_v_loop_filter16_msa;
-    dsp->vp8_h_loop_filter16y = ff_vp8_h_loop_filter16_msa;
-    dsp->vp8_v_loop_filter8uv = ff_vp8_v_loop_filter8uv_msa;
-    dsp->vp8_h_loop_filter8uv = ff_vp8_h_loop_filter8uv_msa;
-
-    dsp->vp8_v_loop_filter16y_inner = ff_vp8_v_loop_filter16_inner_msa;
-    dsp->vp8_h_loop_filter16y_inner = ff_vp8_h_loop_filter16_inner_msa;
-    dsp->vp8_v_loop_filter8uv_inner = ff_vp8_v_loop_filter8uv_inner_msa;
-    dsp->vp8_h_loop_filter8uv_inner = ff_vp8_h_loop_filter8uv_inner_msa;
-
-    dsp->vp8_v_loop_filter_simple = ff_vp8_v_loop_filter_simple_msa;
-    dsp->vp8_h_loop_filter_simple = ff_vp8_h_loop_filter_simple_msa;
-}
-#endif  // #if HAVE_MSA
 
-#if HAVE_MMI
-static av_cold void vp8dsp_init_mmi(VP8DSPContext *dsp)
-{
-    dsp->vp8_luma_dc_wht    = ff_vp8_luma_dc_wht_mmi;
-    dsp->vp8_luma_dc_wht_dc = ff_vp8_luma_dc_wht_dc_mmi;
-    dsp->vp8_idct_add       = ff_vp8_idct_add_mmi;
-    dsp->vp8_idct_dc_add    = ff_vp8_idct_dc_add_mmi;
-    dsp->vp8_idct_dc_add4y  = ff_vp8_idct_dc_add4y_mmi;
-    dsp->vp8_idct_dc_add4uv = ff_vp8_idct_dc_add4uv_mmi;
-
-    dsp->put_vp8_epel_pixels_tab[0][0][1] = ff_put_vp8_epel16_h4_mmi;
-    dsp->put_vp8_epel_pixels_tab[0][0][2] = ff_put_vp8_epel16_h6_mmi;
-    dsp->put_vp8_epel_pixels_tab[0][1][0] = ff_put_vp8_epel16_v4_mmi;
-    dsp->put_vp8_epel_pixels_tab[0][1][1] = ff_put_vp8_epel16_h4v4_mmi;
-    dsp->put_vp8_epel_pixels_tab[0][1][2] = ff_put_vp8_epel16_h6v4_mmi;
-    dsp->put_vp8_epel_pixels_tab[0][2][0] = ff_put_vp8_epel16_v6_mmi;
-    dsp->put_vp8_epel_pixels_tab[0][2][1] = ff_put_vp8_epel16_h4v6_mmi;
-    dsp->put_vp8_epel_pixels_tab[0][2][2] = ff_put_vp8_epel16_h6v6_mmi;
-
-    dsp->put_vp8_epel_pixels_tab[1][0][1] = ff_put_vp8_epel8_h4_mmi;
-    dsp->put_vp8_epel_pixels_tab[1][0][2] = ff_put_vp8_epel8_h6_mmi;
-    dsp->put_vp8_epel_pixels_tab[1][1][0] = ff_put_vp8_epel8_v4_mmi;
-    dsp->put_vp8_epel_pixels_tab[1][1][1] = ff_put_vp8_epel8_h4v4_mmi;
-    dsp->put_vp8_epel_pixels_tab[1][1][2] = ff_put_vp8_epel8_h6v4_mmi;
-    dsp->put_vp8_epel_pixels_tab[1][2][0] = ff_put_vp8_epel8_v6_mmi;
-    dsp->put_vp8_epel_pixels_tab[1][2][1] = ff_put_vp8_epel8_h4v6_mmi;
-    dsp->put_vp8_epel_pixels_tab[1][2][2] = ff_put_vp8_epel8_h6v6_mmi;
-
-    dsp->put_vp8_epel_pixels_tab[2][0][1] = ff_put_vp8_epel4_h4_mmi;
-    dsp->put_vp8_epel_pixels_tab[2][0][2] = ff_put_vp8_epel4_h6_mmi;
-    dsp->put_vp8_epel_pixels_tab[2][1][0] = ff_put_vp8_epel4_v4_mmi;
-    dsp->put_vp8_epel_pixels_tab[2][1][1] = ff_put_vp8_epel4_h4v4_mmi;
-    dsp->put_vp8_epel_pixels_tab[2][1][2] = ff_put_vp8_epel4_h6v4_mmi;
-    dsp->put_vp8_epel_pixels_tab[2][2][0] = ff_put_vp8_epel4_v6_mmi;
-    dsp->put_vp8_epel_pixels_tab[2][2][1] = ff_put_vp8_epel4_h4v6_mmi;
-    dsp->put_vp8_epel_pixels_tab[2][2][2] = ff_put_vp8_epel4_h6v6_mmi;
-
-    dsp->put_vp8_bilinear_pixels_tab[0][0][1] = ff_put_vp8_bilinear16_h_mmi;
-    dsp->put_vp8_bilinear_pixels_tab[0][0][2] = ff_put_vp8_bilinear16_h_mmi;
-    dsp->put_vp8_bilinear_pixels_tab[0][1][0] = ff_put_vp8_bilinear16_v_mmi;
-    dsp->put_vp8_bilinear_pixels_tab[0][1][1] = ff_put_vp8_bilinear16_hv_mmi;
-    dsp->put_vp8_bilinear_pixels_tab[0][1][2] = ff_put_vp8_bilinear16_hv_mmi;
-    dsp->put_vp8_bilinear_pixels_tab[0][2][0] = ff_put_vp8_bilinear16_v_mmi;
-    dsp->put_vp8_bilinear_pixels_tab[0][2][1] = ff_put_vp8_bilinear16_hv_mmi;
-    dsp->put_vp8_bilinear_pixels_tab[0][2][2] = ff_put_vp8_bilinear16_hv_mmi;
-
-    dsp->put_vp8_bilinear_pixels_tab[1][0][1] = ff_put_vp8_bilinear8_h_mmi;
-    dsp->put_vp8_bilinear_pixels_tab[1][0][2] = ff_put_vp8_bilinear8_h_mmi;
-    dsp->put_vp8_bilinear_pixels_tab[1][1][0] = ff_put_vp8_bilinear8_v_mmi;
-    dsp->put_vp8_bilinear_pixels_tab[1][1][1] = ff_put_vp8_bilinear8_hv_mmi;
-    dsp->put_vp8_bilinear_pixels_tab[1][1][2] = ff_put_vp8_bilinear8_hv_mmi;
-    dsp->put_vp8_bilinear_pixels_tab[1][2][0] = ff_put_vp8_bilinear8_v_mmi;
-    dsp->put_vp8_bilinear_pixels_tab[1][2][1] = ff_put_vp8_bilinear8_hv_mmi;
-    dsp->put_vp8_bilinear_pixels_tab[1][2][2] = ff_put_vp8_bilinear8_hv_mmi;
-
-    dsp->put_vp8_bilinear_pixels_tab[2][0][1] = ff_put_vp8_bilinear4_h_mmi;
-    dsp->put_vp8_bilinear_pixels_tab[2][0][2] = ff_put_vp8_bilinear4_h_mmi;
-    dsp->put_vp8_bilinear_pixels_tab[2][1][0] = ff_put_vp8_bilinear4_v_mmi;
-    dsp->put_vp8_bilinear_pixels_tab[2][1][1] = ff_put_vp8_bilinear4_hv_mmi;
-    dsp->put_vp8_bilinear_pixels_tab[2][1][2] = ff_put_vp8_bilinear4_hv_mmi;
-    dsp->put_vp8_bilinear_pixels_tab[2][2][0] = ff_put_vp8_bilinear4_v_mmi;
-    dsp->put_vp8_bilinear_pixels_tab[2][2][1] = ff_put_vp8_bilinear4_hv_mmi;
-    dsp->put_vp8_bilinear_pixels_tab[2][2][2] = ff_put_vp8_bilinear4_hv_mmi;
-
-    dsp->put_vp8_epel_pixels_tab[0][0][0]     = ff_put_vp8_pixels16_mmi;
-    dsp->put_vp8_bilinear_pixels_tab[0][0][0] = ff_put_vp8_pixels16_mmi;
-
-    dsp->put_vp8_epel_pixels_tab[1][0][0]     = ff_put_vp8_pixels8_mmi;
-    dsp->put_vp8_bilinear_pixels_tab[1][0][0] = ff_put_vp8_pixels8_mmi;
-
-    dsp->vp8_v_loop_filter16y = ff_vp8_v_loop_filter16_mmi;
-    dsp->vp8_h_loop_filter16y = ff_vp8_h_loop_filter16_mmi;
-    dsp->vp8_v_loop_filter8uv = ff_vp8_v_loop_filter8uv_mmi;
-    dsp->vp8_h_loop_filter8uv = ff_vp8_h_loop_filter8uv_mmi;
-
-    dsp->vp8_v_loop_filter16y_inner = ff_vp8_v_loop_filter16_inner_mmi;
-    dsp->vp8_h_loop_filter16y_inner = ff_vp8_h_loop_filter16_inner_mmi;
-    dsp->vp8_v_loop_filter8uv_inner = ff_vp8_v_loop_filter8uv_inner_mmi;
-    dsp->vp8_h_loop_filter8uv_inner = ff_vp8_h_loop_filter8uv_inner_mmi;
-
-    dsp->vp8_v_loop_filter_simple = ff_vp8_v_loop_filter_simple_mmi;
-    dsp->vp8_h_loop_filter_simple = ff_vp8_h_loop_filter_simple_mmi;
-}
-#endif /* HAVE_MMI */
 
 av_cold void ff_vp8dsp_init_mips(VP8DSPContext *dsp)
 {
-#if HAVE_MMI
-    vp8dsp_init_mmi(dsp);
-#endif /* HAVE_MMI */
-#if HAVE_MSA
-    vp8dsp_init_msa(dsp);
-#endif  // #if HAVE_MSA
+    int cpu_flags = av_get_cpu_flags();
+
+    if (have_mmi(cpu_flags)) {
+        dsp->vp8_luma_dc_wht    = ff_vp8_luma_dc_wht_mmi;
+        dsp->vp8_luma_dc_wht_dc = ff_vp8_luma_dc_wht_dc_mmi;
+        dsp->vp8_idct_add       = ff_vp8_idct_add_mmi;
+        dsp->vp8_idct_dc_add    = ff_vp8_idct_dc_add_mmi;
+        dsp->vp8_idct_dc_add4y  = ff_vp8_idct_dc_add4y_mmi;
+        dsp->vp8_idct_dc_add4uv = ff_vp8_idct_dc_add4uv_mmi;
+
+        dsp->put_vp8_epel_pixels_tab[0][0][1] = ff_put_vp8_epel16_h4_mmi;
+        dsp->put_vp8_epel_pixels_tab[0][0][2] = ff_put_vp8_epel16_h6_mmi;
+        dsp->put_vp8_epel_pixels_tab[0][1][0] = ff_put_vp8_epel16_v4_mmi;
+        dsp->put_vp8_epel_pixels_tab[0][1][1] = ff_put_vp8_epel16_h4v4_mmi;
+        dsp->put_vp8_epel_pixels_tab[0][1][2] = ff_put_vp8_epel16_h6v4_mmi;
+        dsp->put_vp8_epel_pixels_tab[0][2][0] = ff_put_vp8_epel16_v6_mmi;
+        dsp->put_vp8_epel_pixels_tab[0][2][1] = ff_put_vp8_epel16_h4v6_mmi;
+        dsp->put_vp8_epel_pixels_tab[0][2][2] = ff_put_vp8_epel16_h6v6_mmi;
+
+        dsp->put_vp8_epel_pixels_tab[1][0][1] = ff_put_vp8_epel8_h4_mmi;
+        dsp->put_vp8_epel_pixels_tab[1][0][2] = ff_put_vp8_epel8_h6_mmi;
+        dsp->put_vp8_epel_pixels_tab[1][1][0] = ff_put_vp8_epel8_v4_mmi;
+        dsp->put_vp8_epel_pixels_tab[1][1][1] = ff_put_vp8_epel8_h4v4_mmi;
+        dsp->put_vp8_epel_pixels_tab[1][1][2] = ff_put_vp8_epel8_h6v4_mmi;
+        dsp->put_vp8_epel_pixels_tab[1][2][0] = ff_put_vp8_epel8_v6_mmi;
+        dsp->put_vp8_epel_pixels_tab[1][2][1] = ff_put_vp8_epel8_h4v6_mmi;
+        dsp->put_vp8_epel_pixels_tab[1][2][2] = ff_put_vp8_epel8_h6v6_mmi;
+
+        dsp->put_vp8_epel_pixels_tab[2][0][1] = ff_put_vp8_epel4_h4_mmi;
+        dsp->put_vp8_epel_pixels_tab[2][0][2] = ff_put_vp8_epel4_h6_mmi;
+        dsp->put_vp8_epel_pixels_tab[2][1][0] = ff_put_vp8_epel4_v4_mmi;
+        dsp->put_vp8_epel_pixels_tab[2][1][1] = ff_put_vp8_epel4_h4v4_mmi;
+        dsp->put_vp8_epel_pixels_tab[2][1][2] = ff_put_vp8_epel4_h6v4_mmi;
+        dsp->put_vp8_epel_pixels_tab[2][2][0] = ff_put_vp8_epel4_v6_mmi;
+        dsp->put_vp8_epel_pixels_tab[2][2][1] = ff_put_vp8_epel4_h4v6_mmi;
+        dsp->put_vp8_epel_pixels_tab[2][2][2] = ff_put_vp8_epel4_h6v6_mmi;
+
+        dsp->put_vp8_bilinear_pixels_tab[0][0][1] = ff_put_vp8_bilinear16_h_mmi;
+        dsp->put_vp8_bilinear_pixels_tab[0][0][2] = ff_put_vp8_bilinear16_h_mmi;
+        dsp->put_vp8_bilinear_pixels_tab[0][1][0] = ff_put_vp8_bilinear16_v_mmi;
+        dsp->put_vp8_bilinear_pixels_tab[0][1][1] = ff_put_vp8_bilinear16_hv_mmi;
+        dsp->put_vp8_bilinear_pixels_tab[0][1][2] = ff_put_vp8_bilinear16_hv_mmi;
+        dsp->put_vp8_bilinear_pixels_tab[0][2][0] = ff_put_vp8_bilinear16_v_mmi;
+        dsp->put_vp8_bilinear_pixels_tab[0][2][1] = ff_put_vp8_bilinear16_hv_mmi;
+        dsp->put_vp8_bilinear_pixels_tab[0][2][2] = ff_put_vp8_bilinear16_hv_mmi;
+
+        dsp->put_vp8_bilinear_pixels_tab[1][0][1] = ff_put_vp8_bilinear8_h_mmi;
+        dsp->put_vp8_bilinear_pixels_tab[1][0][2] = ff_put_vp8_bilinear8_h_mmi;
+        dsp->put_vp8_bilinear_pixels_tab[1][1][0] = ff_put_vp8_bilinear8_v_mmi;
+        dsp->put_vp8_bilinear_pixels_tab[1][1][1] = ff_put_vp8_bilinear8_hv_mmi;
+        dsp->put_vp8_bilinear_pixels_tab[1][1][2] = ff_put_vp8_bilinear8_hv_mmi;
+        dsp->put_vp8_bilinear_pixels_tab[1][2][0] = ff_put_vp8_bilinear8_v_mmi;
+        dsp->put_vp8_bilinear_pixels_tab[1][2][1] = ff_put_vp8_bilinear8_hv_mmi;
+        dsp->put_vp8_bilinear_pixels_tab[1][2][2] = ff_put_vp8_bilinear8_hv_mmi;
+
+        dsp->put_vp8_bilinear_pixels_tab[2][0][1] = ff_put_vp8_bilinear4_h_mmi;
+        dsp->put_vp8_bilinear_pixels_tab[2][0][2] = ff_put_vp8_bilinear4_h_mmi;
+        dsp->put_vp8_bilinear_pixels_tab[2][1][0] = ff_put_vp8_bilinear4_v_mmi;
+        dsp->put_vp8_bilinear_pixels_tab[2][1][1] = ff_put_vp8_bilinear4_hv_mmi;
+        dsp->put_vp8_bilinear_pixels_tab[2][1][2] = ff_put_vp8_bilinear4_hv_mmi;
+        dsp->put_vp8_bilinear_pixels_tab[2][2][0] = ff_put_vp8_bilinear4_v_mmi;
+        dsp->put_vp8_bilinear_pixels_tab[2][2][1] = ff_put_vp8_bilinear4_hv_mmi;
+        dsp->put_vp8_bilinear_pixels_tab[2][2][2] = ff_put_vp8_bilinear4_hv_mmi;
+
+        dsp->put_vp8_epel_pixels_tab[0][0][0]     = ff_put_vp8_pixels16_mmi;
+        dsp->put_vp8_bilinear_pixels_tab[0][0][0] = ff_put_vp8_pixels16_mmi;
+
+        dsp->put_vp8_epel_pixels_tab[1][0][0]     = ff_put_vp8_pixels8_mmi;
+        dsp->put_vp8_bilinear_pixels_tab[1][0][0] = ff_put_vp8_pixels8_mmi;
+
+        dsp->vp8_v_loop_filter16y = ff_vp8_v_loop_filter16_mmi;
+        dsp->vp8_h_loop_filter16y = ff_vp8_h_loop_filter16_mmi;
+        dsp->vp8_v_loop_filter8uv = ff_vp8_v_loop_filter8uv_mmi;
+        dsp->vp8_h_loop_filter8uv = ff_vp8_h_loop_filter8uv_mmi;
+
+        dsp->vp8_v_loop_filter16y_inner = ff_vp8_v_loop_filter16_inner_mmi;
+        dsp->vp8_h_loop_filter16y_inner = ff_vp8_h_loop_filter16_inner_mmi;
+        dsp->vp8_v_loop_filter8uv_inner = ff_vp8_v_loop_filter8uv_inner_mmi;
+        dsp->vp8_h_loop_filter8uv_inner = ff_vp8_h_loop_filter8uv_inner_mmi;
+
+        dsp->vp8_v_loop_filter_simple = ff_vp8_v_loop_filter_simple_mmi;
+        dsp->vp8_h_loop_filter_simple = ff_vp8_h_loop_filter_simple_mmi;
+    }
+
+    if (have_msa(cpu_flags)) {
+        dsp->vp8_luma_dc_wht = ff_vp8_luma_dc_wht_msa;
+        dsp->vp8_idct_add = ff_vp8_idct_add_msa;
+        dsp->vp8_idct_dc_add = ff_vp8_idct_dc_add_msa;
+        dsp->vp8_idct_dc_add4y = ff_vp8_idct_dc_add4y_msa;
+        dsp->vp8_idct_dc_add4uv = ff_vp8_idct_dc_add4uv_msa;
+
+        VP8_MC_MIPS_FUNC(0, 16);
+        VP8_MC_MIPS_FUNC(1, 8);
+        VP8_MC_MIPS_FUNC(2, 4);
+
+        VP8_BILINEAR_MC_MIPS_FUNC(0, 16);
+        VP8_BILINEAR_MC_MIPS_FUNC(1, 8);
+        VP8_BILINEAR_MC_MIPS_FUNC(2, 4);
+
+        VP8_MC_MIPS_COPY(0, 16);
+        VP8_MC_MIPS_COPY(1, 8);
+
+        dsp->vp8_v_loop_filter16y = ff_vp8_v_loop_filter16_msa;
+        dsp->vp8_h_loop_filter16y = ff_vp8_h_loop_filter16_msa;
+        dsp->vp8_v_loop_filter8uv = ff_vp8_v_loop_filter8uv_msa;
+        dsp->vp8_h_loop_filter8uv = ff_vp8_h_loop_filter8uv_msa;
+
+        dsp->vp8_v_loop_filter16y_inner = ff_vp8_v_loop_filter16_inner_msa;
+        dsp->vp8_h_loop_filter16y_inner = ff_vp8_h_loop_filter16_inner_msa;
+        dsp->vp8_v_loop_filter8uv_inner = ff_vp8_v_loop_filter8uv_inner_msa;
+        dsp->vp8_h_loop_filter8uv_inner = ff_vp8_h_loop_filter8uv_inner_msa;
+
+        dsp->vp8_v_loop_filter_simple = ff_vp8_v_loop_filter_simple_msa;
+        dsp->vp8_h_loop_filter_simple = ff_vp8_h_loop_filter_simple_msa;
+    }
 }
diff --git a/libavcodec/mips/vp8dsp_mmi.c b/libavcodec/mips/vp8dsp_mmi.c
index bd80aa1445..447bfb1fd5 100644
--- a/libavcodec/mips/vp8dsp_mmi.c
+++ b/libavcodec/mips/vp8dsp_mmi.c
@@ -36,10 +36,10 @@
         "pcmpeqb    %[db_1],    "#src1",        "#src2"             \n\t"   \
         "pmaxub     %[db_2],    "#src1",        "#src2"             \n\t"   \
         "pcmpeqb    %[db_2],    %[db_2],        "#src1"             \n\t"   \
-        "xor        "#dst",     %[db_2],        %[db_1]             \n\t"
+        "pxor       "#dst",     %[db_2],        %[db_1]             \n\t"
 
 #define MMI_BTOH(dst_l, dst_r, src)                                         \
-        "xor        %[db_1],    %[db_1],        %[db_1]             \n\t"   \
+        "pxor       %[db_1],    %[db_1],        %[db_1]             \n\t"   \
         "pcmpgtb    %[db_2],    %[db_1],        "#src"              \n\t"   \
         "punpcklbh  "#dst_r",   "#src",         %[db_2]             \n\t"   \
         "punpckhbh  "#dst_l",   "#src",         %[db_2]             \n\t"
@@ -82,17 +82,17 @@
         "punpcklwd  %[ftmp3],   %[ftmp3],       %[ftmp3]            \n\t"   \
         MMI_PCMPGTUB(%[mask], %[mask], %[ftmp3])                            \
         "pcmpeqw    %[ftmp3],   %[ftmp3],       %[ftmp3]            \n\t"   \
-        "xor        %[mask],    %[mask],        %[ftmp3]            \n\t"   \
+        "pxor       %[mask],    %[mask],        %[ftmp3]            \n\t"   \
         /* VP8_MBFILTER */                                                  \
         "li         %[tmp0],    0x80808080                          \n\t"   \
         "dmtc1      %[tmp0],    %[ftmp7]                            \n\t"   \
         "punpcklwd  %[ftmp7],   %[ftmp7],       %[ftmp7]            \n\t"   \
-        "xor        %[p2],      %[p2],          %[ftmp7]            \n\t"   \
-        "xor        %[p1],      %[p1],          %[ftmp7]            \n\t"   \
-        "xor        %[p0],      %[p0],          %[ftmp7]            \n\t"   \
-        "xor        %[q0],      %[q0],          %[ftmp7]            \n\t"   \
-        "xor        %[q1],      %[q1],          %[ftmp7]            \n\t"   \
-        "xor        %[q2],      %[q2],          %[ftmp7]            \n\t"   \
+        "pxor       %[p2],      %[p2],          %[ftmp7]            \n\t"   \
+        "pxor       %[p1],      %[p1],          %[ftmp7]            \n\t"   \
+        "pxor       %[p0],      %[p0],          %[ftmp7]            \n\t"   \
+        "pxor       %[q0],      %[q0],          %[ftmp7]            \n\t"   \
+        "pxor       %[q1],      %[q1],          %[ftmp7]            \n\t"   \
+        "pxor       %[q2],      %[q2],          %[ftmp7]            \n\t"   \
         "psubsb     %[ftmp4],   %[p1],          %[q1]               \n\t"   \
         "psubb      %[ftmp5],   %[q0],          %[p0]               \n\t"   \
         MMI_BTOH(%[ftmp1],  %[ftmp0],  %[ftmp5])                            \
@@ -107,8 +107,8 @@
         "paddh      %[ftmp1],   %[ftmp3],       %[ftmp1]            \n\t"   \
         /* Combine left and right part */                                   \
         "packsshb   %[ftmp1],   %[ftmp0],       %[ftmp1]            \n\t"   \
-        "and        %[ftmp1],   %[ftmp1],       %[mask]             \n\t"   \
-        "and        %[ftmp2],   %[ftmp1],       %[hev]              \n\t"   \
+        "pand       %[ftmp1],   %[ftmp1],       %[mask]             \n\t"   \
+        "pand       %[ftmp2],   %[ftmp1],       %[hev]              \n\t"   \
         "li         %[tmp0],    0x04040404                          \n\t"   \
         "dmtc1      %[tmp0],    %[ftmp0]                            \n\t"   \
         "punpcklwd  %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"   \
@@ -127,8 +127,8 @@
         "paddsb     %[p0],      %[p0],          %[ftmp4]            \n\t"   \
         /* filt_val &= ~hev */                                              \
         "pcmpeqw    %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"   \
-        "xor        %[hev],     %[hev],         %[ftmp0]            \n\t"   \
-        "and        %[ftmp1],   %[ftmp1],       %[hev]              \n\t"   \
+        "pxor       %[hev],     %[hev],         %[ftmp0]            \n\t"   \
+        "pand       %[ftmp1],   %[ftmp1],       %[hev]              \n\t"   \
         MMI_BTOH(%[ftmp5],  %[ftmp6],  %[ftmp1])                            \
         "li         %[tmp0],    0x07                                \n\t"   \
         "dmtc1      %[tmp0],    %[ftmp2]                            \n\t"   \
@@ -149,9 +149,9 @@
         /* Combine left and right part */                                   \
         "packsshb   %[ftmp4],   %[ftmp3],       %[ftmp4]            \n\t"   \
         "psubsb     %[q0],      %[q0],          %[ftmp4]            \n\t"   \
-        "xor        %[q0],      %[q0],          %[ftmp7]            \n\t"   \
+        "pxor       %[q0],      %[q0],          %[ftmp7]            \n\t"   \
         "paddsb     %[p0],      %[p0],          %[ftmp4]            \n\t"   \
-        "xor        %[p0],      %[p0],          %[ftmp7]            \n\t"   \
+        "pxor       %[p0],      %[p0],          %[ftmp7]            \n\t"   \
         "li         %[tmp0],    0x00120012                          \n\t"   \
         "dmtc1      %[tmp0],    %[ftmp1]                            \n\t"   \
         "punpcklwd  %[ftmp1],   %[ftmp1],       %[ftmp1]            \n\t"   \
@@ -166,9 +166,9 @@
         /* Combine left and right part */                                   \
         "packsshb   %[ftmp4],   %[ftmp3],       %[ftmp4]            \n\t"   \
         "psubsb     %[q1],      %[q1],          %[ftmp4]            \n\t"   \
-        "xor        %[q1],      %[q1],          %[ftmp7]            \n\t"   \
+        "pxor       %[q1],      %[q1],          %[ftmp7]            \n\t"   \
         "paddsb     %[p1],      %[p1],          %[ftmp4]            \n\t"   \
-        "xor        %[p1],      %[p1],          %[ftmp7]            \n\t"   \
+        "pxor       %[p1],      %[p1],          %[ftmp7]            \n\t"   \
         "li         %[tmp0],    0x03                                \n\t"   \
         "dmtc1      %[tmp0],    %[ftmp1]                            \n\t"   \
         /* Right part */                                                    \
@@ -184,9 +184,9 @@
         /* Combine left and right part */                                   \
         "packsshb   %[ftmp4],   %[ftmp3],       %[ftmp4]            \n\t"   \
         "psubsb     %[q2],      %[q2],          %[ftmp4]            \n\t"   \
-        "xor        %[q2],      %[q2],          %[ftmp7]            \n\t"   \
+        "pxor       %[q2],      %[q2],          %[ftmp7]            \n\t"   \
         "paddsb     %[p2],      %[p2],          %[ftmp4]            \n\t"   \
-        "xor        %[p2],      %[p2],          %[ftmp7]            \n\t"
+        "pxor       %[p2],      %[p2],          %[ftmp7]            \n\t"
 
 #define PUT_VP8_EPEL4_H6_MMI(src, dst)                                      \
         MMI_ULWC1(%[ftmp1], src, 0x00)                                      \
@@ -789,51 +789,40 @@ static av_always_inline void vp8_v_loop_filter8_mmi(uint8_t *dst,
     DECLARE_DOUBLE_1;
     DECLARE_DOUBLE_2;
     DECLARE_UINT32_T;
+    DECLARE_VAR_ALL64;
+
     __asm__ volatile(
         /* Get data from dst */
-        "gsldlc1    %[q0],      0x07(%[dst])                      \n\t"
-        "gsldrc1    %[q0],      0x00(%[dst])                      \n\t"
+        MMI_ULDC1(%[q0], %[dst], 0x0)
         PTR_SUBU    "%[tmp0],   %[dst],         %[stride]         \n\t"
-        "gsldlc1    %[p0],      0x07(%[tmp0])                     \n\t"
-        "gsldrc1    %[p0],      0x00(%[tmp0])                     \n\t"
+        MMI_ULDC1(%[p0], %[tmp0], 0x0)
         PTR_SUBU    "%[tmp0],   %[tmp0],        %[stride]         \n\t"
-        "gsldlc1    %[p1],      0x07(%[tmp0])                     \n\t"
-        "gsldrc1    %[p1],      0x00(%[tmp0])                     \n\t"
+        MMI_ULDC1(%[p1], %[tmp0], 0x0)
         PTR_SUBU    "%[tmp0],   %[tmp0],        %[stride]         \n\t"
-        "gsldlc1    %[p2],      0x07(%[tmp0])                     \n\t"
-        "gsldrc1    %[p2],      0x00(%[tmp0])                     \n\t"
+        MMI_ULDC1(%[p2], %[tmp0], 0x0)
         PTR_SUBU    "%[tmp0],   %[tmp0],        %[stride]         \n\t"
-        "gsldlc1    %[p3],      0x07(%[tmp0])                     \n\t"
-        "gsldrc1    %[p3],      0x00(%[tmp0])                     \n\t"
+        MMI_ULDC1(%[p3], %[tmp0], 0x0)
         PTR_ADDU    "%[tmp0],   %[dst],         %[stride]         \n\t"
-        "gsldlc1    %[q1],      0x07(%[tmp0])                     \n\t"
-        "gsldrc1    %[q1],      0x00(%[tmp0])                     \n\t"
+        MMI_ULDC1(%[q1], %[tmp0], 0x0)
         PTR_ADDU    "%[tmp0],   %[tmp0],        %[stride]         \n\t"
-        "gsldlc1    %[q2],      0x07(%[tmp0])                     \n\t"
-        "gsldrc1    %[q2],      0x00(%[tmp0])                     \n\t"
+        MMI_ULDC1(%[q2], %[tmp0], 0x0)
         PTR_ADDU    "%[tmp0],   %[tmp0],        %[stride]         \n\t"
-        "gsldlc1    %[q3],      0x07(%[tmp0])                     \n\t"
-        "gsldrc1    %[q3],      0x00(%[tmp0])                     \n\t"
+        MMI_ULDC1(%[q3], %[tmp0], 0x0)
         MMI_VP8_LOOP_FILTER
         /* Move to dst */
-        "gssdlc1    %[q0],      0x07(%[dst])                      \n\t"
-        "gssdrc1    %[q0],      0x00(%[dst])                      \n\t"
+        MMI_USDC1(%[q0], %[dst], 0x0)
         PTR_SUBU    "%[tmp0],   %[dst],         %[stride]         \n\t"
-        "gssdlc1    %[p0],      0x07(%[tmp0])                     \n\t"
-        "gssdrc1    %[p0],      0x00(%[tmp0])                     \n\t"
+        MMI_USDC1(%[p0], %[tmp0], 0x0)
         PTR_SUBU    "%[tmp0],   %[tmp0],        %[stride]         \n\t"
-        "gssdlc1    %[p1],      0x07(%[tmp0])                     \n\t"
-        "gssdrc1    %[p1],      0x00(%[tmp0])                     \n\t"
+        MMI_USDC1(%[p1], %[tmp0], 0x0)
         PTR_SUBU    "%[tmp0],   %[tmp0],        %[stride]         \n\t"
-        "gssdlc1    %[p2],      0x07(%[tmp0])                     \n\t"
-        "gssdrc1    %[p2],      0x00(%[tmp0])                     \n\t"
+        MMI_USDC1(%[p2], %[tmp0], 0x0)
         PTR_ADDU    "%[tmp0],   %[dst],         %[stride]         \n\t"
-        "gssdlc1    %[q1],      0x07(%[tmp0])                     \n\t"
-        "gssdrc1    %[q1],      0x00(%[tmp0])                     \n\t"
+        MMI_USDC1(%[q1], %[tmp0], 0x0)
         PTR_ADDU    "%[tmp0],   %[tmp0],        %[stride]         \n\t"
-        "gssdlc1    %[q2],      0x07(%[tmp0])                     \n\t"
-        "gssdrc1    %[q2],      0x00(%[tmp0])                     \n\t"
-        : [p3]"=&f"(ftmp[0]),       [p2]"=&f"(ftmp[1]),
+        MMI_USDC1(%[q2], %[tmp0], 0x0)
+        : RESTRICT_ASM_ALL64
+          [p3]"=&f"(ftmp[0]),       [p2]"=&f"(ftmp[1]),
           [p1]"=&f"(ftmp[2]),       [p0]"=&f"(ftmp[3]),
           [q0]"=&f"(ftmp[4]),       [q1]"=&f"(ftmp[5]),
           [q2]"=&f"(ftmp[6]),       [q3]"=&f"(ftmp[7]),
@@ -874,31 +863,25 @@ static av_always_inline void vp8_h_loop_filter8_mmi(uint8_t *dst,
     DECLARE_DOUBLE_1;
     DECLARE_DOUBLE_2;
     DECLARE_UINT32_T;
+    DECLARE_VAR_ALL64;
+
     __asm__ volatile(
         /* Get data from dst */
-        "gsldlc1    %[p3],        0x03(%[dst])                    \n\t"
-        "gsldrc1    %[p3],        -0x04(%[dst])                   \n\t"
+        MMI_ULDC1(%[p3], %[dst], -0x04)
         PTR_ADDU    "%[tmp0],     %[dst],           %[stride]     \n\t"
-        "gsldlc1    %[p2],        0x03(%[tmp0])                   \n\t"
-        "gsldrc1    %[p2],        -0x04(%[tmp0])                  \n\t"
+        MMI_ULDC1(%[p2], %[tmp0], -0x04)
         PTR_ADDU    "%[tmp0],     %[tmp0],          %[stride]     \n\t"
-        "gsldlc1    %[p1],        0x03(%[tmp0])                   \n\t"
-        "gsldrc1    %[p1],        -0x04(%[tmp0])                  \n\t"
+        MMI_ULDC1(%[p1], %[tmp0], -0x04)
         PTR_ADDU    "%[tmp0],     %[tmp0],          %[stride]     \n\t"
-        "gsldlc1    %[p0],        0x03(%[tmp0])                   \n\t"
-        "gsldrc1    %[p0],        -0x04(%[tmp0])                  \n\t"
+        MMI_ULDC1(%[p0], %[tmp0], -0x04)
         PTR_ADDU    "%[tmp0],     %[tmp0],          %[stride]     \n\t"
-        "gsldlc1    %[q0],        0x03(%[tmp0])                   \n\t"
-        "gsldrc1    %[q0],        -0x04(%[tmp0])                  \n\t"
+        MMI_ULDC1(%[q0], %[tmp0], -0x04)
         PTR_ADDU    "%[tmp0],     %[tmp0],          %[stride]     \n\t"
-        "gsldlc1    %[q1],        0x03(%[tmp0])                   \n\t"
-        "gsldrc1    %[q1],        -0x04(%[tmp0])                  \n\t"
+        MMI_ULDC1(%[q1], %[tmp0], -0x04)
         PTR_ADDU    "%[tmp0],     %[tmp0],          %[stride]     \n\t"
-        "gsldlc1    %[q2],        0x03(%[tmp0])                   \n\t"
-        "gsldrc1    %[q2],        -0x04(%[tmp0])                  \n\t"
+        MMI_ULDC1(%[q2], %[tmp0], -0x04)
         PTR_ADDU    "%[tmp0],     %[tmp0],          %[stride]     \n\t"
-        "gsldlc1    %[q3],        0x03(%[tmp0])                   \n\t"
-        "gsldrc1    %[q3],        -0x04(%[tmp0])                  \n\t"
+        MMI_ULDC1(%[q3], %[tmp0], -0x04)
         /* Matrix transpose */
         TRANSPOSE_8B(%[p3], %[p2], %[p1], %[p0],
                      %[q0], %[q1], %[q2], %[q3],
@@ -909,30 +892,23 @@ static av_always_inline void vp8_h_loop_filter8_mmi(uint8_t *dst,
                      %[q0], %[q1], %[q2], %[q3],
                      %[ftmp1], %[ftmp2], %[ftmp3], %[ftmp4])
         /* Move to dst */
-        "gssdlc1    %[p3],        0x03(%[dst])                    \n\t"
-        "gssdrc1    %[p3],        -0x04(%[dst])                   \n\t"
+        MMI_USDC1(%[p3], %[dst], -0x04)
         PTR_ADDU    "%[dst],      %[dst],           %[stride]     \n\t"
-        "gssdlc1    %[p2],        0x03(%[dst])                    \n\t"
-        "gssdrc1    %[p2],        -0x04(%[dst])                   \n\t"
+        MMI_USDC1(%[p2], %[dst], -0x04)
         PTR_ADDU    "%[dst],      %[dst],           %[stride]     \n\t"
-        "gssdlc1    %[p1],        0x03(%[dst])                    \n\t"
-        "gssdrc1    %[p1],        -0x04(%[dst])                   \n\t"
+        MMI_USDC1(%[p1], %[dst], -0x04)
         PTR_ADDU    "%[dst],      %[dst],           %[stride]     \n\t"
-        "gssdlc1    %[p0],        0x03(%[dst])                    \n\t"
-        "gssdrc1    %[p0],        -0x04(%[dst])                   \n\t"
+        MMI_USDC1(%[p0], %[dst], -0x04)
         PTR_ADDU    "%[dst],      %[dst],           %[stride]     \n\t"
-        "gssdlc1    %[q0],        0x03(%[dst])                    \n\t"
-        "gssdrc1    %[q0],        -0x04(%[dst])                   \n\t"
+        MMI_USDC1(%[q0], %[dst], -0x04)
         PTR_ADDU    "%[dst],      %[dst],           %[stride]     \n\t"
-        "gssdlc1    %[q1],        0x03(%[dst])                    \n\t"
-        "gssdrc1    %[q1],        -0x04(%[dst])                   \n\t"
+        MMI_USDC1(%[q1], %[dst], -0x04)
         PTR_ADDU    "%[dst],      %[dst],           %[stride]     \n\t"
-        "gssdlc1    %[q2],        0x03(%[dst])                    \n\t"
-        "gssdrc1    %[q2],        -0x04(%[dst])                   \n\t"
+        MMI_USDC1(%[q2], %[dst], -0x04)
         PTR_ADDU    "%[dst],      %[dst],           %[stride]     \n\t"
-        "gssdlc1    %[q3],        0x03(%[dst])                    \n\t"
-        "gssdrc1    %[q3],        -0x04(%[dst])                   \n\t"
-        : [p3]"=&f"(ftmp[0]),       [p2]"=&f"(ftmp[1]),
+        MMI_USDC1(%[q3], %[dst], -0x04)
+        : RESTRICT_ASM_ALL64
+          [p3]"=&f"(ftmp[0]),       [p2]"=&f"(ftmp[1]),
           [p1]"=&f"(ftmp[2]),       [p0]"=&f"(ftmp[3]),
           [q0]"=&f"(ftmp[4]),       [q1]"=&f"(ftmp[5]),
           [q2]"=&f"(ftmp[6]),       [q3]"=&f"(ftmp[7]),
@@ -1019,7 +995,7 @@ void ff_vp8_luma_dc_wht_mmi(int16_t block[4][4][16], int16_t dc[16])
     block[3][3][0] = (dc[12] - dc[15] + 3 - dc[13] + dc[14]) >> 3;
 
     __asm__ volatile (
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
         MMI_SDC1(%[ftmp0], %[dc], 0x00)
         MMI_SDC1(%[ftmp0], %[dc], 0x08)
         MMI_SDC1(%[ftmp0], %[dc], 0x10)
@@ -1126,15 +1102,17 @@ void ff_vp8_luma_dc_wht_dc_mmi(int16_t block[4][4][16], int16_t dc[16])
 void ff_vp8_idct_add_mmi(uint8_t *dst, int16_t block[16], ptrdiff_t stride)
 {
 #if 1
-    DECLARE_ALIGNED(8, const uint64_t, ff_ph_4e7b) = {0x4e7b4e7b4e7b4e7bULL};
-    DECLARE_ALIGNED(8, const uint64_t, ff_ph_22a3) = {0x22a322a322a322a3ULL};
     double ftmp[12];
     uint32_t tmp[1];
+    union av_intfloat64 ff_ph_4e7b_u;
+    union av_intfloat64 ff_ph_22a3_u;
     DECLARE_VAR_LOW32;
     DECLARE_VAR_ALL64;
+    ff_ph_4e7b_u.i = 0x4e7b4e7b4e7b4e7bULL;
+    ff_ph_22a3_u.i = 0x22a322a322a322a3ULL;
 
     __asm__ volatile (
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
         MMI_LDC1(%[ftmp1], %[block], 0x00)
         MMI_LDC1(%[ftmp2], %[block], 0x08)
         MMI_LDC1(%[ftmp3], %[block], 0x10)
@@ -1251,8 +1229,8 @@ void ff_vp8_idct_add_mmi(uint8_t *dst, int16_t block[16], ptrdiff_t stride)
           [tmp0]"=&r"(tmp[0])
         : [dst0]"r"(dst),                   [dst1]"r"(dst+stride),
           [dst2]"r"(dst+2*stride),          [dst3]"r"(dst+3*stride),
-          [block]"r"(block),                [ff_pw_4]"f"(ff_pw_4),
-          [ff_ph_4e7b]"f"(ff_ph_4e7b),      [ff_ph_22a3]"f"(ff_ph_22a3)
+          [block]"r"(block),                [ff_pw_4]"f"(ff_pw_4.f),
+          [ff_ph_4e7b]"f"(ff_ph_4e7b_u.f),  [ff_ph_22a3]"f"(ff_ph_22a3_u.f)
         : "memory"
     );
 #else
@@ -1300,7 +1278,7 @@ void ff_vp8_idct_dc_add_mmi(uint8_t *dst, int16_t block[16], ptrdiff_t stride)
     block[0] = 0;
 
     __asm__ volatile (
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
         "mtc1       %[dc],      %[ftmp5]                            \n\t"
         MMI_LWC1(%[ftmp1], %[dst0], 0x00)
         MMI_LWC1(%[ftmp2], %[dst1], 0x00)
@@ -1593,8 +1571,16 @@ void ff_put_vp8_epel16_h4_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
     const uint64_t *filter = fourtap_subpel_filters[mx - 1];
     double ftmp[9];
     uint32_t tmp[1];
+    union av_intfloat64 filter1;
+    union av_intfloat64 filter2;
+    union av_intfloat64 filter3;
+    union av_intfloat64 filter4;
     mips_reg src1, dst1;
     DECLARE_VAR_ALL64;
+    filter1.i = filter[1];
+    filter2.i = filter[2];
+    filter3.i = filter[3];
+    filter4.i = filter[4];
 
     /*
     dst[0] = cm[(filter[2] * src[0] - filter[1] * src[-1] + filter[3] * src[1] - filter[4] * src[2] + 64) >> 7];
@@ -1616,7 +1602,7 @@ void ff_put_vp8_epel16_h4_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
     dst[15] = cm[(filter[2] * src[15] - filter[1] * src[14] + filter[3] * src[16] - filter[4] * src[17] + 64) >> 7];
     */
     __asm__ volatile (
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
         "li         %[tmp0],    0x07                                \n\t"
         "mtc1       %[tmp0],    %[ftmp4]                            \n\t"
 
@@ -1642,11 +1628,11 @@ void ff_put_vp8_epel16_h4_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
           [dst1]"=&r"(dst1),                [src1]"=&r"(src1),
           [h]"+&r"(h),
           [dst]"+&r"(dst),                  [src]"+&r"(src)
-        : [ff_pw_64]"f"(ff_pw_64),
+        : [ff_pw_64]"f"(ff_pw_64.f),
           [srcstride]"r"((mips_reg)srcstride),
           [dststride]"r"((mips_reg)dststride),
-          [filter1]"f"(filter[1]),          [filter2]"f"(filter[2]),
-          [filter3]"f"(filter[3]),          [filter4]"f"(filter[4])
+          [filter1]"f"(filter1.f),          [filter2]"f"(filter2.f),
+          [filter3]"f"(filter3.f),          [filter4]"f"(filter4.f)
         : "memory"
     );
 #else
@@ -1670,7 +1656,16 @@ void ff_put_vp8_epel8_h4_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
     const uint64_t *filter = fourtap_subpel_filters[mx - 1];
     double ftmp[9];
     uint32_t tmp[1];
+    union av_intfloat64 filter1;
+    union av_intfloat64 filter2;
+    union av_intfloat64 filter3;
+    union av_intfloat64 filter4;
     DECLARE_VAR_ALL64;
+    filter1.i = filter[1];
+    filter2.i = filter[2];
+    filter3.i = filter[3];
+    filter4.i = filter[4];
+
 
     /*
     dst[0] = cm[(filter[2] * src[0] - filter[1] * src[-1] + filter[3] * src[1] - filter[4] * src[2] + 64) >> 7];
@@ -1683,7 +1678,7 @@ void ff_put_vp8_epel8_h4_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
     dst[7] = cm[(filter[2] * src[7] - filter[1] * src[ 6] + filter[3] * src[8] - filter[4] * src[9] + 64) >> 7];
     */
     __asm__ volatile (
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
         "li         %[tmp0],    0x07                                \n\t"
         "mtc1       %[tmp0],    %[ftmp4]                            \n\t"
 
@@ -1703,11 +1698,11 @@ void ff_put_vp8_epel8_h4_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
           RESTRICT_ASM_ALL64
           [h]"+&r"(h),
           [dst]"+&r"(dst),                  [src]"+&r"(src)
-        : [ff_pw_64]"f"(ff_pw_64),
+        : [ff_pw_64]"f"(ff_pw_64.f),
           [srcstride]"r"((mips_reg)srcstride),
           [dststride]"r"((mips_reg)dststride),
-          [filter1]"f"(filter[1]),          [filter2]"f"(filter[2]),
-          [filter3]"f"(filter[3]),          [filter4]"f"(filter[4])
+          [filter1]"f"(filter1.f),          [filter2]"f"(filter2.f),
+          [filter3]"f"(filter3.f),          [filter4]"f"(filter4.f)
         : "memory"
     );
 #else
@@ -1731,7 +1726,15 @@ void ff_put_vp8_epel4_h4_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
     const uint64_t *filter = fourtap_subpel_filters[mx - 1];
     double ftmp[6];
     uint32_t tmp[1];
+    union av_intfloat64 filter1;
+    union av_intfloat64 filter2;
+    union av_intfloat64 filter3;
+    union av_intfloat64 filter4;
     DECLARE_VAR_LOW32;
+    filter1.i = filter[1];
+    filter2.i = filter[2];
+    filter3.i = filter[3];
+    filter4.i = filter[4];
 
     /*
     dst[0] = cm[(filter[2] * src[0] - filter[1] * src[-1] + filter[3] * src[1] - filter[4] * src[2] + 64) >> 7];
@@ -1740,7 +1743,7 @@ void ff_put_vp8_epel4_h4_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
     dst[3] = cm[(filter[2] * src[3] - filter[1] * src[ 2] + filter[3] * src[4] - filter[4] * src[5] + 64) >> 7];
     */
     __asm__ volatile (
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
         "li         %[tmp0],    0x07                                \n\t"
         "mtc1       %[tmp0],    %[ftmp4]                            \n\t"
 
@@ -1758,11 +1761,11 @@ void ff_put_vp8_epel4_h4_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
           RESTRICT_ASM_LOW32
           [h]"+&r"(h),
           [dst]"+&r"(dst),                  [src]"+&r"(src)
-        : [ff_pw_64]"f"(ff_pw_64),
+        : [ff_pw_64]"f"(ff_pw_64.f),
           [srcstride]"r"((mips_reg)srcstride),
           [dststride]"r"((mips_reg)dststride),
-          [filter1]"f"(filter[1]),          [filter2]"f"(filter[2]),
-          [filter3]"f"(filter[3]),          [filter4]"f"(filter[4])
+          [filter1]"f"(filter1.f),          [filter2]"f"(filter2.f),
+          [filter3]"f"(filter3.f),          [filter4]"f"(filter4.f)
         : "memory"
     );
 #else
@@ -1787,7 +1790,19 @@ void ff_put_vp8_epel16_h6_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
     double ftmp[9];
     uint32_t tmp[1];
     mips_reg src1, dst1;
+    union av_intfloat64 filter0;
+    union av_intfloat64 filter1;
+    union av_intfloat64 filter2;
+    union av_intfloat64 filter3;
+    union av_intfloat64 filter4;
+    union av_intfloat64 filter5;
     DECLARE_VAR_ALL64;
+    filter0.i = filter[0];
+    filter1.i = filter[1];
+    filter2.i = filter[2];
+    filter3.i = filter[3];
+    filter4.i = filter[4];
+    filter5.i = filter[5];
 
     /*
     dst[ 0] = cm[(filter[2]*src[ 0] - filter[1]*src[-1] + filter[0]*src[-2] + filter[3]*src[ 1] - filter[4]*src[ 2] + filter[5]*src[ 3] + 64) >> 7];
@@ -1809,7 +1824,7 @@ void ff_put_vp8_epel16_h6_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
     dst[15] = cm[(filter[2]*src[15] - filter[1]*src[14] + filter[0]*src[13] + filter[3]*src[16] - filter[4]*src[17] + filter[5]*src[18] + 64) >> 7];
     */
     __asm__ volatile (
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
         "li         %[tmp0],    0x07                                \n\t"
         "mtc1       %[tmp0],    %[ftmp4]                            \n\t"
 
@@ -1835,12 +1850,12 @@ void ff_put_vp8_epel16_h6_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
           [dst1]"=&r"(dst1),                [src1]"=&r"(src1),
           [h]"+&r"(h),
           [dst]"+&r"(dst),                  [src]"+&r"(src)
-        : [ff_pw_64]"f"(ff_pw_64),
+        : [ff_pw_64]"f"(ff_pw_64.f),
           [srcstride]"r"((mips_reg)srcstride),
           [dststride]"r"((mips_reg)dststride),
-          [filter0]"f"(filter[0]),          [filter1]"f"(filter[1]),
-          [filter2]"f"(filter[2]),          [filter3]"f"(filter[3]),
-          [filter4]"f"(filter[4]),          [filter5]"f"(filter[5])
+          [filter0]"f"(filter0.f),          [filter1]"f"(filter1.f),
+          [filter2]"f"(filter2.f),          [filter3]"f"(filter3.f),
+          [filter4]"f"(filter4.f),          [filter5]"f"(filter5.f)
         : "memory"
     );
 #else
@@ -1864,7 +1879,19 @@ void ff_put_vp8_epel8_h6_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
     const uint64_t *filter = fourtap_subpel_filters[mx - 1];
     double ftmp[9];
     uint32_t tmp[1];
+    union av_intfloat64 filter0;
+    union av_intfloat64 filter1;
+    union av_intfloat64 filter2;
+    union av_intfloat64 filter3;
+    union av_intfloat64 filter4;
+    union av_intfloat64 filter5;
     DECLARE_VAR_ALL64;
+    filter0.i = filter[0];
+    filter1.i = filter[1];
+    filter2.i = filter[2];
+    filter3.i = filter[3];
+    filter4.i = filter[4];
+    filter5.i = filter[5];
 
     /*
     dst[0] = cm[(filter[2]*src[0] - filter[1]*src[-1] + filter[0]*src[-2] + filter[3]*src[1] - filter[4]*src[2] + filter[5]*src[ 3] + 64) >> 7];
@@ -1877,7 +1904,7 @@ void ff_put_vp8_epel8_h6_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
     dst[7] = cm[(filter[2]*src[7] - filter[1]*src[ 6] + filter[0]*src[ 5] + filter[3]*src[8] - filter[4]*src[9] + filter[5]*src[10] + 64) >> 7];
     */
     __asm__ volatile (
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
         "li         %[tmp0],    0x07                                \n\t"
         "mtc1       %[tmp0],    %[ftmp4]                            \n\t"
 
@@ -1897,12 +1924,12 @@ void ff_put_vp8_epel8_h6_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
           RESTRICT_ASM_ALL64
           [h]"+&r"(h),
           [dst]"+&r"(dst),                  [src]"+&r"(src)
-        : [ff_pw_64]"f"(ff_pw_64),
+        : [ff_pw_64]"f"(ff_pw_64.f),
           [srcstride]"r"((mips_reg)srcstride),
           [dststride]"r"((mips_reg)dststride),
-          [filter0]"f"(filter[0]),          [filter1]"f"(filter[1]),
-          [filter2]"f"(filter[2]),          [filter3]"f"(filter[3]),
-          [filter4]"f"(filter[4]),          [filter5]"f"(filter[5])
+          [filter0]"f"(filter0.f),          [filter1]"f"(filter1.f),
+          [filter2]"f"(filter2.f),          [filter3]"f"(filter3.f),
+          [filter4]"f"(filter4.f),          [filter5]"f"(filter5.f)
         : "memory"
     );
 #else
@@ -1926,7 +1953,19 @@ void ff_put_vp8_epel4_h6_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
     const uint64_t *filter = fourtap_subpel_filters[mx - 1];
     double ftmp[6];
     uint32_t tmp[1];
+    union av_intfloat64 filter0;
+    union av_intfloat64 filter1;
+    union av_intfloat64 filter2;
+    union av_intfloat64 filter3;
+    union av_intfloat64 filter4;
+    union av_intfloat64 filter5;
     DECLARE_VAR_LOW32;
+    filter0.i = filter[0];
+    filter1.i = filter[1];
+    filter2.i = filter[2];
+    filter3.i = filter[3];
+    filter4.i = filter[4];
+    filter5.i = filter[5];
 
     /*
     dst[0] = cm[(filter[2]*src[0] - filter[1]*src[-1] + filter[0]*src[-2] + filter[3]*src[1] - filter[4]*src[2] + filter[5]*src[ 3] + 64) >> 7];
@@ -1935,7 +1974,7 @@ void ff_put_vp8_epel4_h6_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
     dst[3] = cm[(filter[2]*src[3] - filter[1]*src[ 2] + filter[0]*src[ 1] + filter[3]*src[4] - filter[4]*src[5] + filter[5]*src[ 6] + 64) >> 7];
     */
     __asm__ volatile (
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
         "li         %[tmp0],    0x07                                \n\t"
         "mtc1       %[tmp0],    %[ftmp4]                            \n\t"
 
@@ -1953,12 +1992,12 @@ void ff_put_vp8_epel4_h6_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
           RESTRICT_ASM_LOW32
           [h]"+&r"(h),
           [dst]"+&r"(dst),                  [src]"+&r"(src)
-        : [ff_pw_64]"f"(ff_pw_64),
+        : [ff_pw_64]"f"(ff_pw_64.f),
           [srcstride]"r"((mips_reg)srcstride),
           [dststride]"r"((mips_reg)dststride),
-          [filter0]"f"(filter[0]),          [filter1]"f"(filter[1]),
-          [filter2]"f"(filter[2]),          [filter3]"f"(filter[3]),
-          [filter4]"f"(filter[4]),          [filter5]"f"(filter[5])
+          [filter0]"f"(filter0.f),          [filter1]"f"(filter1.f),
+          [filter2]"f"(filter2.f),          [filter3]"f"(filter3.f),
+          [filter4]"f"(filter4.f),          [filter5]"f"(filter5.f)
         : "memory"
     );
 #else
@@ -1983,7 +2022,15 @@ void ff_put_vp8_epel16_v4_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
     double ftmp[9];
     uint32_t tmp[1];
     mips_reg src0, src1, dst0;
+    union av_intfloat64 filter1;
+    union av_intfloat64 filter2;
+    union av_intfloat64 filter3;
+    union av_intfloat64 filter4;
     DECLARE_VAR_ALL64;
+    filter1.i = filter[1];
+    filter2.i = filter[2];
+    filter3.i = filter[3];
+    filter4.i = filter[4];
 
     /*
     dst[0] = cm[(filter[2] * src[0] - filter[1] * src[ -srcstride] + filter[3] * src[  srcstride] - filter[4] * src[  2*srcstride] + 64) >> 7];
@@ -2005,7 +2052,7 @@ void ff_put_vp8_epel16_v4_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
     dst[15] = cm[(filter[2] * src[15] - filter[1] * src[15-srcstride] + filter[3] * src[15+srcstride] - filter[4] * src[15+2*srcstride] + 64) >> 7];
     */
     __asm__ volatile (
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
         "li         %[tmp0],    0x07                                \n\t"
         "mtc1       %[tmp0],    %[ftmp4]                            \n\t"
 
@@ -2032,11 +2079,11 @@ void ff_put_vp8_epel16_v4_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
           [src1]"=&r"(src1),
           [h]"+&r"(h),
           [dst]"+&r"(dst),                  [src]"+&r"(src)
-        : [ff_pw_64]"f"(ff_pw_64),
+        : [ff_pw_64]"f"(ff_pw_64.f),
           [srcstride]"r"((mips_reg)srcstride),
           [dststride]"r"((mips_reg)dststride),
-          [filter1]"f"(filter[1]),          [filter2]"f"(filter[2]),
-          [filter3]"f"(filter[3]),          [filter4]"f"(filter[4])
+          [filter1]"f"(filter1.f),          [filter2]"f"(filter2.f),
+          [filter3]"f"(filter3.f),          [filter4]"f"(filter4.f)
         : "memory"
     );
 #else
@@ -2061,7 +2108,15 @@ void ff_put_vp8_epel8_v4_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
     double ftmp[9];
     uint32_t tmp[1];
     mips_reg src1;
+    union av_intfloat64 filter1;
+    union av_intfloat64 filter2;
+    union av_intfloat64 filter3;
+    union av_intfloat64 filter4;
     DECLARE_VAR_ALL64;
+    filter1.i = filter[1];
+    filter2.i = filter[2];
+    filter3.i = filter[3];
+    filter4.i = filter[4];
 
     /*
     dst[0] = cm[(filter[2] * src[0] - filter[1] * src[ -srcstride] + filter[3] * src[  srcstride] - filter[4] * src[  2*srcstride] + 64) >> 7];
@@ -2074,7 +2129,7 @@ void ff_put_vp8_epel8_v4_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
     dst[7] = cm[(filter[2] * src[7] - filter[1] * src[7-srcstride] + filter[3] * src[7+srcstride] - filter[4] * src[7+2*srcstride] + 64) >> 7];
     */
     __asm__ volatile (
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
         "li         %[tmp0],    0x07                                \n\t"
         "mtc1       %[tmp0],    %[ftmp4]                            \n\t"
 
@@ -2095,11 +2150,11 @@ void ff_put_vp8_epel8_v4_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
           [src1]"=&r"(src1),
           [h]"+&r"(h),
           [dst]"+&r"(dst),                  [src]"+&r"(src)
-        : [ff_pw_64]"f"(ff_pw_64),
+        : [ff_pw_64]"f"(ff_pw_64.f),
           [srcstride]"r"((mips_reg)srcstride),
           [dststride]"r"((mips_reg)dststride),
-          [filter1]"f"(filter[1]),          [filter2]"f"(filter[2]),
-          [filter3]"f"(filter[3]),          [filter4]"f"(filter[4])
+          [filter1]"f"(filter1.f),          [filter2]"f"(filter2.f),
+          [filter3]"f"(filter3.f),          [filter4]"f"(filter4.f)
         : "memory"
     );
 #else
@@ -2124,7 +2179,15 @@ void ff_put_vp8_epel4_v4_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
     double ftmp[6];
     uint32_t tmp[1];
     mips_reg src1;
+    union av_intfloat64 filter1;
+    union av_intfloat64 filter2;
+    union av_intfloat64 filter3;
+    union av_intfloat64 filter4;
     DECLARE_VAR_LOW32;
+    filter1.i = filter[1];
+    filter2.i = filter[2];
+    filter3.i = filter[3];
+    filter4.i = filter[4];
 
     /*
     dst[0] = cm[(filter[2] * src[0] - filter[1] * src[ -srcstride] + filter[3] * src[  srcstride] - filter[4] * src[  2*srcstride] + 64) >> 7];
@@ -2133,7 +2196,7 @@ void ff_put_vp8_epel4_v4_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
     dst[3] = cm[(filter[2] * src[3] - filter[1] * src[3-srcstride] + filter[3] * src[3+srcstride] - filter[4] * src[3+2*srcstride] + 64) >> 7];
     */
     __asm__ volatile (
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
         "li         %[tmp0],    0x07                                \n\t"
         "mtc1       %[tmp0],    %[ftmp4]                            \n\t"
 
@@ -2152,11 +2215,11 @@ void ff_put_vp8_epel4_v4_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
           [src1]"=&r"(src1),
           [h]"+&r"(h),
           [dst]"+&r"(dst),                  [src]"+&r"(src)
-        : [ff_pw_64]"f"(ff_pw_64),
+        : [ff_pw_64]"f"(ff_pw_64.f),
           [srcstride]"r"((mips_reg)srcstride),
           [dststride]"r"((mips_reg)dststride),
-          [filter1]"f"(filter[1]),          [filter2]"f"(filter[2]),
-          [filter3]"f"(filter[3]),          [filter4]"f"(filter[4])
+          [filter1]"f"(filter1.f),          [filter2]"f"(filter2.f),
+          [filter3]"f"(filter3.f),          [filter4]"f"(filter4.f)
         : "memory"
     );
 #else
@@ -2181,7 +2244,19 @@ void ff_put_vp8_epel16_v6_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
     double ftmp[9];
     uint32_t tmp[1];
     mips_reg src0, src1, dst0;
+    union av_intfloat64 filter0;
+    union av_intfloat64 filter1;
+    union av_intfloat64 filter2;
+    union av_intfloat64 filter3;
+    union av_intfloat64 filter4;
+    union av_intfloat64 filter5;
     DECLARE_VAR_ALL64;
+    filter0.i = filter[0];
+    filter1.i = filter[1];
+    filter2.i = filter[2];
+    filter3.i = filter[3];
+    filter4.i = filter[4];
+    filter5.i = filter[5];
 
     /*
     dst[0] = cm[(filter[2]*src[0] - filter[1]*src[0-srcstride] + filter[0]*src[0-2*srcstride] + filter[3]*src[0+srcstride] - filter[4]*src[0+2*srcstride] + filter[5]*src[0+3*srcstride] + 64) >> 7];
@@ -2203,7 +2278,7 @@ void ff_put_vp8_epel16_v6_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
     dst[15] = cm[(filter[2]*src[15] - filter[1]*src[15-srcstride] + filter[0]*src[15-2*srcstride] + filter[3]*src[15+srcstride] - filter[4]*src[15+2*srcstride] + filter[5]*src[15+3*srcstride] + 64) >> 7];
     */
     __asm__ volatile (
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
         "li         %[tmp0],    0x07                                \n\t"
         "mtc1       %[tmp0],    %[ftmp4]                            \n\t"
 
@@ -2230,12 +2305,12 @@ void ff_put_vp8_epel16_v6_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
           [src1]"=&r"(src1),
           [h]"+&r"(h),
           [dst]"+&r"(dst),                  [src]"+&r"(src)
-        : [ff_pw_64]"f"(ff_pw_64),
+        : [ff_pw_64]"f"(ff_pw_64.f),
           [srcstride]"r"((mips_reg)srcstride),
           [dststride]"r"((mips_reg)dststride),
-          [filter0]"f"(filter[0]),          [filter1]"f"(filter[1]),
-          [filter2]"f"(filter[2]),          [filter3]"f"(filter[3]),
-          [filter4]"f"(filter[4]),          [filter5]"f"(filter[5])
+          [filter0]"f"(filter0.f),          [filter1]"f"(filter1.f),
+          [filter2]"f"(filter2.f),          [filter3]"f"(filter3.f),
+          [filter4]"f"(filter4.f),          [filter5]"f"(filter5.f)
         : "memory"
     );
 #else
@@ -2260,7 +2335,19 @@ void ff_put_vp8_epel8_v6_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
     double ftmp[9];
     uint32_t tmp[1];
     mips_reg src1;
+    union av_intfloat64 filter0;
+    union av_intfloat64 filter1;
+    union av_intfloat64 filter2;
+    union av_intfloat64 filter3;
+    union av_intfloat64 filter4;
+    union av_intfloat64 filter5;
     DECLARE_VAR_ALL64;
+    filter0.i = filter[0];
+    filter1.i = filter[1];
+    filter2.i = filter[2];
+    filter3.i = filter[3];
+    filter4.i = filter[4];
+    filter5.i = filter[5];
 
     /*
     dst[0] = cm[(filter[2]*src[0] - filter[1]*src[0-srcstride] + filter[0]*src[0-2*srcstride] + filter[3]*src[0+srcstride] - filter[4]*src[0+2*srcstride] + filter[5]*src[0+3*srcstride] + 64) >> 7];
@@ -2273,7 +2360,7 @@ void ff_put_vp8_epel8_v6_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
     dst[7] = cm[(filter[2]*src[7] - filter[1]*src[7-srcstride] + filter[0]*src[7-2*srcstride] + filter[3]*src[7+srcstride] - filter[4]*src[7+2*srcstride] + filter[5]*src[7+3*srcstride] + 64) >> 7];
     */
     __asm__ volatile (
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
         "li         %[tmp0],    0x07                                \n\t"
         "mtc1       %[tmp0],    %[ftmp4]                            \n\t"
 
@@ -2294,12 +2381,12 @@ void ff_put_vp8_epel8_v6_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
           [src1]"=&r"(src1),
           [h]"+&r"(h),
           [dst]"+&r"(dst),                  [src]"+&r"(src)
-        : [ff_pw_64]"f"(ff_pw_64),
+        : [ff_pw_64]"f"(ff_pw_64.f),
           [srcstride]"r"((mips_reg)srcstride),
           [dststride]"r"((mips_reg)dststride),
-          [filter0]"f"(filter[0]),          [filter1]"f"(filter[1]),
-          [filter2]"f"(filter[2]),          [filter3]"f"(filter[3]),
-          [filter4]"f"(filter[4]),          [filter5]"f"(filter[5])
+          [filter0]"f"(filter0.f),          [filter1]"f"(filter1.f),
+          [filter2]"f"(filter2.f),          [filter3]"f"(filter3.f),
+          [filter4]"f"(filter4.f),          [filter5]"f"(filter5.f)
         : "memory"
     );
 #else
@@ -2324,7 +2411,19 @@ void ff_put_vp8_epel4_v6_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
     double ftmp[6];
     uint32_t tmp[1];
     mips_reg src1;
+    union av_intfloat64 filter0;
+    union av_intfloat64 filter1;
+    union av_intfloat64 filter2;
+    union av_intfloat64 filter3;
+    union av_intfloat64 filter4;
+    union av_intfloat64 filter5;
     DECLARE_VAR_LOW32;
+    filter0.i = filter[0];
+    filter1.i = filter[1];
+    filter2.i = filter[2];
+    filter3.i = filter[3];
+    filter4.i = filter[4];
+    filter5.i = filter[5];
 
     /*
     dst[0] = cm[(filter[2]*src[0] - filter[1]*src[0-srcstride] + filter[0]*src[0-2*srcstride] + filter[3]*src[0+srcstride] - filter[4]*src[0+2*srcstride] + filter[5]*src[0+3*srcstride] + 64) >> 7];
@@ -2333,7 +2432,7 @@ void ff_put_vp8_epel4_v6_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
     dst[3] = cm[(filter[2]*src[3] - filter[1]*src[3-srcstride] + filter[0]*src[3-2*srcstride] + filter[3]*src[3+srcstride] - filter[4]*src[3+2*srcstride] + filter[5]*src[3+3*srcstride] + 64) >> 7];
     */
     __asm__ volatile (
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
         "li         %[tmp0],    0x07                                \n\t"
         "mtc1       %[tmp0],    %[ftmp4]                            \n\t"
 
@@ -2352,12 +2451,12 @@ void ff_put_vp8_epel4_v6_mmi(uint8_t *dst, ptrdiff_t dststride, uint8_t *src,
           [src1]"=&r"(src1),
           [h]"+&r"(h),
           [dst]"+&r"(dst),                  [src]"+&r"(src)
-        : [ff_pw_64]"f"(ff_pw_64),
+        : [ff_pw_64]"f"(ff_pw_64.f),
           [srcstride]"r"((mips_reg)srcstride),
           [dststride]"r"((mips_reg)dststride),
-          [filter0]"f"(filter[0]),          [filter1]"f"(filter[1]),
-          [filter2]"f"(filter[2]),          [filter3]"f"(filter[3]),
-          [filter4]"f"(filter[4]),          [filter5]"f"(filter[5])
+          [filter0]"f"(filter0.f),          [filter1]"f"(filter1.f),
+          [filter2]"f"(filter2.f),          [filter3]"f"(filter3.f),
+          [filter4]"f"(filter4.f),          [filter5]"f"(filter5.f)
         : "memory"
     );
 #else
@@ -2845,11 +2944,13 @@ void ff_put_vp8_bilinear16_h_mmi(uint8_t *dst, ptrdiff_t dstride, uint8_t *src,
         ptrdiff_t sstride, int h, int mx, int my)
 {
 #if 1
-    int a = 8 - mx, b = mx;
+    union mmi_intfloat64 a, b;
     double ftmp[7];
     uint32_t tmp[1];
     mips_reg dst0, src0;
     DECLARE_VAR_ALL64;
+    a.i = 8 - mx;
+    b.i = mx;
 
     /*
     dst[0] = (a * src[0] + b * src[1] + 4) >> 3;
@@ -2871,7 +2972,7 @@ void ff_put_vp8_bilinear16_h_mmi(uint8_t *dst, ptrdiff_t dstride, uint8_t *src,
     dst[15] = (a * src[15] + b * src[16] + 4) >> 3;
     */
     __asm__ volatile (
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
         "li         %[tmp0],    0x03                                \n\t"
         "mtc1       %[tmp0],    %[ftmp4]                            \n\t"
         "pshufh     %[a],       %[a],           %[ftmp0]            \n\t"
@@ -2898,10 +2999,10 @@ void ff_put_vp8_bilinear16_h_mmi(uint8_t *dst, ptrdiff_t dstride, uint8_t *src,
           [dst0]"=&r"(dst0),            [src0]"=&r"(src0),
           [h]"+&r"(h),
           [dst]"+&r"(dst),              [src]"+&r"(src),
-          [a]"+&f"(a),                  [b]"+&f"(b)
+          [a]"+&f"(a.f),                [b]"+&f"(b.f)
         : [sstride]"r"((mips_reg)sstride),
           [dstride]"r"((mips_reg)dstride),
-          [ff_pw_4]"f"(ff_pw_4)
+          [ff_pw_4]"f"(ff_pw_4.f)
         : "memory"
     );
 #else
@@ -2921,11 +3022,13 @@ void ff_put_vp8_bilinear16_v_mmi(uint8_t *dst, ptrdiff_t dstride, uint8_t *src,
         ptrdiff_t sstride, int h, int mx, int my)
 {
 #if 1
-    int c = 8 - my, d = my;
+    union mmi_intfloat64 c, d;
     double ftmp[7];
     uint32_t tmp[1];
     mips_reg src0, src1, dst0;
     DECLARE_VAR_ALL64;
+    c.i = 8 - my;
+    d.i = my;
 
     /*
     dst[0] = (c * src[0] + d * src[    sstride] + 4) >> 3;
@@ -2938,7 +3041,7 @@ void ff_put_vp8_bilinear16_v_mmi(uint8_t *dst, ptrdiff_t dstride, uint8_t *src,
     dst[7] = (c * src[7] + d * src[7 + sstride] + 4) >> 3;
     */
     __asm__ volatile (
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
         "li         %[tmp0],    0x03                                \n\t"
         "mtc1       %[tmp0],    %[ftmp4]                            \n\t"
         "pshufh     %[c],       %[c],           %[ftmp0]            \n\t"
@@ -2966,10 +3069,10 @@ void ff_put_vp8_bilinear16_v_mmi(uint8_t *dst, ptrdiff_t dstride, uint8_t *src,
           [src1]"=&r"(src1),
           [h]"+&r"(h),
           [dst]"+&r"(dst),              [src]"+&r"(src),
-          [c]"+&f"(c),                  [d]"+&f"(d)
+          [c]"+&f"(c.f),                [d]"+&f"(d.f)
         : [sstride]"r"((mips_reg)sstride),
           [dstride]"r"((mips_reg)dstride),
-          [ff_pw_4]"f"(ff_pw_4)
+          [ff_pw_4]"f"(ff_pw_4.f)
         : "memory"
     );
 #else
@@ -3023,10 +3126,12 @@ void ff_put_vp8_bilinear8_h_mmi(uint8_t *dst, ptrdiff_t dstride, uint8_t *src,
         ptrdiff_t sstride, int h, int mx, int my)
 {
 #if 1
-    int a = 8 - mx, b = mx;
+    union mmi_intfloat64 a, b;
     double ftmp[7];
     uint32_t tmp[1];
     DECLARE_VAR_ALL64;
+    a.i = 8 - mx;
+    b.i = mx;
 
     /*
     dst[0] = (a * src[0] + b * src[1] + 4) >> 3;
@@ -3039,7 +3144,7 @@ void ff_put_vp8_bilinear8_h_mmi(uint8_t *dst, ptrdiff_t dstride, uint8_t *src,
     dst[7] = (a * src[7] + b * src[8] + 4) >> 3;
     */
     __asm__ volatile (
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
         "li         %[tmp0],    0x03                                \n\t"
         "mtc1       %[tmp0],    %[ftmp4]                            \n\t"
         "pshufh     %[a],       %[a],           %[ftmp0]            \n\t"
@@ -3060,10 +3165,10 @@ void ff_put_vp8_bilinear8_h_mmi(uint8_t *dst, ptrdiff_t dstride, uint8_t *src,
           RESTRICT_ASM_ALL64
           [h]"+&r"(h),
           [dst]"+&r"(dst),              [src]"+&r"(src),
-          [a]"+&f"(a),                  [b]"+&f"(b)
+          [a]"+&f"(a.f),                [b]"+&f"(b.f)
         : [sstride]"r"((mips_reg)sstride),
           [dstride]"r"((mips_reg)dstride),
-          [ff_pw_4]"f"(ff_pw_4)
+          [ff_pw_4]"f"(ff_pw_4.f)
         : "memory"
     );
 #else
@@ -3083,11 +3188,13 @@ void ff_put_vp8_bilinear8_v_mmi(uint8_t *dst, ptrdiff_t dstride, uint8_t *src,
         ptrdiff_t sstride, int h, int mx, int my)
 {
 #if 1
-    int c = 8 - my, d = my;
+    union mmi_intfloat64 c, d;
     double ftmp[7];
     uint32_t tmp[1];
     mips_reg src1;
     DECLARE_VAR_ALL64;
+    c.i = 8 - my;
+    d.i = my;
 
     /*
     dst[0] = (c * src[0] + d * src[    sstride] + 4) >> 3;
@@ -3100,7 +3207,7 @@ void ff_put_vp8_bilinear8_v_mmi(uint8_t *dst, ptrdiff_t dstride, uint8_t *src,
     dst[7] = (c * src[7] + d * src[7 + sstride] + 4) >> 3;
     */
     __asm__ volatile (
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
         "li         %[tmp0],    0x03                                \n\t"
         "mtc1       %[tmp0],    %[ftmp4]                            \n\t"
         "pshufh     %[c],       %[c],           %[ftmp0]            \n\t"
@@ -3122,10 +3229,10 @@ void ff_put_vp8_bilinear8_v_mmi(uint8_t *dst, ptrdiff_t dstride, uint8_t *src,
           [src1]"=&r"(src1),
           [h]"+&r"(h),
           [dst]"+&r"(dst),              [src]"+&r"(src),
-          [c]"+&f"(c),                  [d]"+&f"(d)
+          [c]"+&f"(c.f),                [d]"+&f"(d.f)
         : [sstride]"r"((mips_reg)sstride),
           [dstride]"r"((mips_reg)dstride),
-          [ff_pw_4]"f"(ff_pw_4)
+          [ff_pw_4]"f"(ff_pw_4.f)
         : "memory"
     );
 #else
@@ -3179,11 +3286,13 @@ void ff_put_vp8_bilinear4_h_mmi(uint8_t *dst, ptrdiff_t dstride, uint8_t *src,
         ptrdiff_t sstride, int h, int mx, int my)
 {
 #if 1
-    int a = 8 - mx, b = mx;
+    union mmi_intfloat64 a, b;
     double ftmp[5];
     uint32_t tmp[1];
     DECLARE_VAR_LOW32;
     DECLARE_VAR_ALL64;
+    a.i = 8 - mx;
+    b.i = mx;
 
     /*
     dst[0] = (a * src[0] + b * src[1] + 4) >> 3;
@@ -3192,7 +3301,7 @@ void ff_put_vp8_bilinear4_h_mmi(uint8_t *dst, ptrdiff_t dstride, uint8_t *src,
     dst[3] = (a * src[3] + b * src[4] + 4) >> 3;
     */
     __asm__ volatile (
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
         "li         %[tmp0],    0x03                                \n\t"
         "mtc1       %[tmp0],    %[ftmp4]                            \n\t"
         "pshufh     %[a],       %[a],           %[ftmp0]            \n\t"
@@ -3213,10 +3322,10 @@ void ff_put_vp8_bilinear4_h_mmi(uint8_t *dst, ptrdiff_t dstride, uint8_t *src,
           RESTRICT_ASM_ALL64
           [h]"+&r"(h),
           [dst]"+&r"(dst),              [src]"+&r"(src),
-          [a]"+&f"(a),                  [b]"+&f"(b)
+          [a]"+&f"(a.f),                [b]"+&f"(b.f)
         : [sstride]"r"((mips_reg)sstride),
           [dstride]"r"((mips_reg)dstride),
-          [ff_pw_4]"f"(ff_pw_4)
+          [ff_pw_4]"f"(ff_pw_4.f)
         : "memory"
     );
 #else
@@ -3236,12 +3345,14 @@ void ff_put_vp8_bilinear4_v_mmi(uint8_t *dst, ptrdiff_t dstride, uint8_t *src,
         ptrdiff_t sstride, int h, int mx, int my)
 {
 #if 1
-    int c = 8 - my, d = my;
+    union mmi_intfloat64 c, d;
     double ftmp[7];
     uint32_t tmp[1];
     mips_reg src1;
     DECLARE_VAR_LOW32;
     DECLARE_VAR_ALL64;
+    c.i = 8 - my;
+    d.i = my;
 
     /*
     dst[0] = (c * src[0] + d * src[    sstride] + 4) >> 3;
@@ -3250,7 +3361,7 @@ void ff_put_vp8_bilinear4_v_mmi(uint8_t *dst, ptrdiff_t dstride, uint8_t *src,
     dst[3] = (c * src[3] + d * src[3 + sstride] + 4) >> 3;
     */
     __asm__ volatile (
-        "xor        %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],       %[ftmp0]            \n\t"
         "li         %[tmp0],    0x03                                \n\t"
         "mtc1       %[tmp0],    %[ftmp4]                            \n\t"
         "pshufh     %[c],       %[c],           %[ftmp0]            \n\t"
@@ -3272,10 +3383,10 @@ void ff_put_vp8_bilinear4_v_mmi(uint8_t *dst, ptrdiff_t dstride, uint8_t *src,
           [src1]"=&r"(src1),
           [h]"+&r"(h),
           [dst]"+&r"(dst),              [src]"+&r"(src),
-          [c]"+&f"(c),                  [d]"+&f"(d)
+          [c]"+&f"(c.f),                [d]"+&f"(d.f)
         : [sstride]"r"((mips_reg)sstride),
           [dstride]"r"((mips_reg)dstride),
-          [ff_pw_4]"f"(ff_pw_4)
+          [ff_pw_4]"f"(ff_pw_4.f)
         : "memory"
     );
 #else
diff --git a/libavcodec/mips/vp9_idct_msa.c b/libavcodec/mips/vp9_idct_msa.c
index 1f32770139..53bfbb4765 100644
--- a/libavcodec/mips/vp9_idct_msa.c
+++ b/libavcodec/mips/vp9_idct_msa.c
@@ -249,6 +249,7 @@ static const int32_t sinpi_4_9 = 15212;
     v8i16 c0_m, c1_m, c2_m, c3_m;                                     \
     v8i16 step0_m, step1_m;                                           \
     v4i32 tmp0_m, tmp1_m, tmp2_m, tmp3_m;                             \
+    v16i8 zeros = { 0 };                                              \
                                                                       \
     c0_m = VP9_SET_COSPI_PAIR(cospi_16_64, cospi_16_64);              \
     c1_m = VP9_SET_COSPI_PAIR(cospi_16_64, -cospi_16_64);             \
@@ -262,7 +263,7 @@ static const int32_t sinpi_4_9 = 15212;
     SRARI_W4_SW(tmp0_m, tmp1_m, tmp2_m, tmp3_m, VP9_DCT_CONST_BITS);  \
                                                                       \
     PCKEV_H2_SW(tmp1_m, tmp0_m, tmp3_m, tmp2_m, tmp0_m, tmp2_m);      \
-    SLDI_B2_0_SW(tmp0_m, tmp2_m, tmp1_m, tmp3_m, 8);                  \
+    SLDI_B2_SW(zeros, tmp0_m, zeros, tmp2_m, 8, tmp1_m, tmp3_m);      \
     BUTTERFLY_4((v8i16) tmp0_m, (v8i16) tmp1_m,                       \
                 (v8i16) tmp2_m, (v8i16) tmp3_m,                       \
                 out0, out1, out2, out3);                              \
@@ -763,13 +764,13 @@ static void vp9_iadst8x8_colcol_addblk_msa(int16_t *input, uint8_t *dst,
 
     res0 = (v8i16) __msa_ilvr_b((v16i8) zero, (v16i8) dst0);
     res0 += out0;
-    res0 = CLIP_SH_0_255(res0);
+    CLIP_SH_0_255(res0);
     res0 = (v8i16) __msa_pckev_b((v16i8) res0, (v16i8) res0);
     ST_D1(res0, 0, dst);
 
     res7 = (v8i16) __msa_ilvr_b((v16i8) zero, (v16i8) dst7);
     res7 += out7;
-    res7 = CLIP_SH_0_255(res7);
+    CLIP_SH_0_255(res7);
     res7 = (v8i16) __msa_pckev_b((v16i8) res7, (v16i8) res7);
     ST_D1(res7, 0, dst + 7 * dst_stride);
 
@@ -1192,8 +1193,7 @@ static void vp9_idct16x16_1_add_msa(int16_t *input, uint8_t *dst,
              res3);
         ADD4(res4, vec, res5, vec, res6, vec, res7, vec, res4, res5, res6,
              res7);
-        CLIP_SH4_0_255(res0, res1, res2, res3);
-        CLIP_SH4_0_255(res4, res5, res6, res7);
+        CLIP_SH8_0_255(res0, res1, res2, res3, res4, res5, res6, res7);
         PCKEV_B4_UB(res4, res0, res5, res1, res6, res2, res7, res3,
                     tmp0, tmp1, tmp2, tmp3);
         ST_UB4(tmp0, tmp1, tmp2, tmp3, dst, dst_stride);
@@ -1981,8 +1981,7 @@ static void vp9_idct32x32_1_add_msa(int16_t *input, uint8_t *dst,
              res3);
         ADD4(res4, vec, res5, vec, res6, vec, res7, vec, res4, res5, res6,
              res7);
-        CLIP_SH4_0_255(res0, res1, res2, res3);
-        CLIP_SH4_0_255(res4, res5, res6, res7);
+        CLIP_SH8_0_255(res0, res1, res2, res3, res4, res5, res6, res7);
         PCKEV_B4_UB(res4, res0, res5, res1, res6, res2, res7, res3,
                     tmp0, tmp1, tmp2, tmp3);
 
diff --git a/libavcodec/mips/vp9_lpf_msa.c b/libavcodec/mips/vp9_lpf_msa.c
index 2450c741d4..cbb140950e 100644
--- a/libavcodec/mips/vp9_lpf_msa.c
+++ b/libavcodec/mips/vp9_lpf_msa.c
@@ -1673,6 +1673,7 @@ static void vp9_transpose_16x8_to_8x16(uint8_t *input, int32_t in_pitch,
     v16u8 p7_org, p6_org, p5_org, p4_org, p3_org, p2_org, p1_org, p0_org;
     v16i8 tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7;
     v16u8 p7, p6, p5, p4, p3, p2, p1, p0, q0, q1, q2, q3, q4, q5, q6, q7;
+    v16i8 zeros = { 0 };
 
     LD_UB8(input, in_pitch,
            p7_org, p6_org, p5_org, p4_org, p3_org, p2_org, p1_org, p0_org);
@@ -1686,7 +1687,7 @@ static void vp9_transpose_16x8_to_8x16(uint8_t *input, int32_t in_pitch,
     ILVL_B2_SB(tmp1, tmp0, tmp3, tmp2, tmp5, tmp7);
     ILVR_W2_UB(tmp6, tmp4, tmp7, tmp5, q0, q4);
     ILVL_W2_UB(tmp6, tmp4, tmp7, tmp5, q2, q6);
-    SLDI_B4_0_UB(q0, q2, q4, q6, q1, q3, q5, q7, 8);
+    SLDI_B4_UB(zeros, q0, zeros, q2, zeros, q4, zeros, q6, 8, q1, q3, q5, q7);
 
     ST_UB8(p7, p6, p5, p4, p3, p2, p1, p0, output, out_pitch);
     output += (8 * out_pitch);
diff --git a/libavcodec/mips/vp9_mc_mmi.c b/libavcodec/mips/vp9_mc_mmi.c
index e7a83875b9..495cac3d0b 100644
--- a/libavcodec/mips/vp9_mc_mmi.c
+++ b/libavcodec/mips/vp9_mc_mmi.c
@@ -77,29 +77,24 @@ static void convolve_horiz_mmi(const uint8_t *src, int32_t src_stride,
 {
     double ftmp[15];
     uint32_t tmp[2];
+    DECLARE_VAR_ALL64;
     src -= 3;
     src_stride -= w;
     dst_stride -= w;
     __asm__ volatile (
         "move       %[tmp1],    %[width]                   \n\t"
-        "xor        %[ftmp0],   %[ftmp0],    %[ftmp0]      \n\t"
-        "gsldlc1    %[filter1], 0x03(%[filter])            \n\t"
-        "gsldrc1    %[filter1], 0x00(%[filter])            \n\t"
-        "gsldlc1    %[filter2], 0x0b(%[filter])            \n\t"
-        "gsldrc1    %[filter2], 0x08(%[filter])            \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],    %[ftmp0]      \n\t"
+        MMI_ULDC1(%[filter1], %[filter], 0x00)
+        MMI_ULDC1(%[filter2], %[filter], 0x08)
         "li         %[tmp0],    0x07                       \n\t"
         "dmtc1      %[tmp0],    %[ftmp13]                  \n\t"
         "punpcklwd  %[ftmp13],  %[ftmp13],   %[ftmp13]     \n\t"
         "1:                                                \n\t"
         /* Get 8 data per row */
-        "gsldlc1    %[ftmp5],   0x07(%[src])               \n\t"
-        "gsldrc1    %[ftmp5],   0x00(%[src])               \n\t"
-        "gsldlc1    %[ftmp7],   0x08(%[src])               \n\t"
-        "gsldrc1    %[ftmp7],   0x01(%[src])               \n\t"
-        "gsldlc1    %[ftmp9],   0x09(%[src])               \n\t"
-        "gsldrc1    %[ftmp9],   0x02(%[src])               \n\t"
-        "gsldlc1    %[ftmp11],  0x0A(%[src])               \n\t"
-        "gsldrc1    %[ftmp11],  0x03(%[src])               \n\t"
+        MMI_ULDC1(%[ftmp5], %[src], 0x00)
+        MMI_ULDC1(%[ftmp7], %[src], 0x01)
+        MMI_ULDC1(%[ftmp9], %[src], 0x02)
+        MMI_ULDC1(%[ftmp11], %[src], 0x03)
         "punpcklbh  %[ftmp4],   %[ftmp5],    %[ftmp0]      \n\t"
         "punpckhbh  %[ftmp5],   %[ftmp5],    %[ftmp0]      \n\t"
         "punpcklbh  %[ftmp6],   %[ftmp7],    %[ftmp0]      \n\t"
@@ -127,7 +122,8 @@ static void convolve_horiz_mmi(const uint8_t *src, int32_t src_stride,
         PTR_ADDU   "%[dst],     %[dst],      %[dst_stride] \n\t"
         PTR_ADDIU  "%[height],  %[height],   -0x01         \n\t"
         "bnez       %[height],  1b                         \n\t"
-        : [srcl]"=&f"(ftmp[0]),     [srch]"=&f"(ftmp[1]),
+        : RESTRICT_ASM_ALL64
+          [srcl]"=&f"(ftmp[0]),     [srch]"=&f"(ftmp[1]),
           [filter1]"=&f"(ftmp[2]),  [filter2]"=&f"(ftmp[3]),
           [ftmp0]"=&f"(ftmp[4]),    [ftmp4]"=&f"(ftmp[5]),
           [ftmp5]"=&f"(ftmp[6]),    [ftmp6]"=&f"(ftmp[7]),
@@ -153,15 +149,14 @@ static void convolve_vert_mmi(const uint8_t *src, int32_t src_stride,
     double ftmp[17];
     uint32_t tmp[1];
     ptrdiff_t addr = src_stride;
+    DECLARE_VAR_ALL64;
     src_stride -= w;
     dst_stride -= w;
 
     __asm__ volatile (
-        "xor        %[ftmp0],    %[ftmp0],   %[ftmp0]      \n\t"
-        "gsldlc1    %[ftmp4],    0x03(%[filter])           \n\t"
-        "gsldrc1    %[ftmp4],    0x00(%[filter])           \n\t"
-        "gsldlc1    %[ftmp5],    0x0b(%[filter])           \n\t"
-        "gsldrc1    %[ftmp5],    0x08(%[filter])           \n\t"
+        "pxor       %[ftmp0],    %[ftmp0],   %[ftmp0]      \n\t"
+        MMI_ULDC1(%[ftmp4], %[filter], 0x00)
+        MMI_ULDC1(%[ftmp5], %[filter], 0x08)
         "punpcklwd  %[filter10], %[ftmp4],   %[ftmp4]      \n\t"
         "punpckhwd  %[filter32], %[ftmp4],   %[ftmp4]      \n\t"
         "punpcklwd  %[filter54], %[ftmp5],   %[ftmp5]      \n\t"
@@ -171,29 +166,21 @@ static void convolve_vert_mmi(const uint8_t *src, int32_t src_stride,
         "punpcklwd  %[ftmp13],   %[ftmp13],  %[ftmp13]     \n\t"
         "1:                                                \n\t"
         /* Get 8 data per column */
-        "gsldlc1    %[ftmp4],    0x07(%[src])              \n\t"
-        "gsldrc1    %[ftmp4],    0x00(%[src])              \n\t"
+        MMI_ULDC1(%[ftmp4], %[src], 0x0)
         PTR_ADDU   "%[tmp0],     %[src],     %[addr]       \n\t"
-        "gsldlc1    %[ftmp5],    0x07(%[tmp0])             \n\t"
-        "gsldrc1    %[ftmp5],    0x00(%[tmp0])             \n\t"
+        MMI_ULDC1(%[ftmp5], %[tmp0], 0x0)
         PTR_ADDU   "%[tmp0],     %[tmp0],    %[addr]       \n\t"
-        "gsldlc1    %[ftmp6],    0x07(%[tmp0])             \n\t"
-        "gsldrc1    %[ftmp6],    0x00(%[tmp0])             \n\t"
+        MMI_ULDC1(%[ftmp6], %[tmp0], 0x0)
         PTR_ADDU   "%[tmp0],     %[tmp0],    %[addr]       \n\t"
-        "gsldlc1    %[ftmp7],    0x07(%[tmp0])             \n\t"
-        "gsldrc1    %[ftmp7],    0x00(%[tmp0])             \n\t"
+        MMI_ULDC1(%[ftmp7], %[tmp0], 0x0)
         PTR_ADDU   "%[tmp0],     %[tmp0],    %[addr]       \n\t"
-        "gsldlc1    %[ftmp8],    0x07(%[tmp0])             \n\t"
-        "gsldrc1    %[ftmp8],    0x00(%[tmp0])             \n\t"
+        MMI_ULDC1(%[ftmp8], %[tmp0], 0x0)
         PTR_ADDU   "%[tmp0],     %[tmp0],    %[addr]       \n\t"
-        "gsldlc1    %[ftmp9],    0x07(%[tmp0])             \n\t"
-        "gsldrc1    %[ftmp9],    0x00(%[tmp0])             \n\t"
+        MMI_ULDC1(%[ftmp9], %[tmp0], 0x0)
         PTR_ADDU   "%[tmp0],     %[tmp0],    %[addr]       \n\t"
-        "gsldlc1    %[ftmp10],   0x07(%[tmp0])             \n\t"
-        "gsldrc1    %[ftmp10],   0x00(%[tmp0])             \n\t"
+        MMI_ULDC1(%[ftmp10], %[tmp0], 0x0)
         PTR_ADDU   "%[tmp0],     %[tmp0],    %[addr]       \n\t"
-        "gsldlc1    %[ftmp11],   0x07(%[tmp0])             \n\t"
-        "gsldrc1    %[ftmp11],   0x00(%[tmp0])             \n\t"
+        MMI_ULDC1(%[ftmp11], %[tmp0], 0x0)
         "punpcklbh  %[ftmp4],    %[ftmp4],   %[ftmp0]      \n\t"
         "punpcklbh  %[ftmp5],    %[ftmp5],   %[ftmp0]      \n\t"
         "punpcklbh  %[ftmp6],    %[ftmp6],   %[ftmp0]      \n\t"
@@ -221,7 +208,8 @@ static void convolve_vert_mmi(const uint8_t *src, int32_t src_stride,
         PTR_ADDU   "%[dst],      %[dst],     %[dst_stride] \n\t"
         PTR_ADDIU  "%[height],   %[height],  -0x01         \n\t"
         "bnez       %[height],   1b                        \n\t"
-        : [srcl]"=&f"(ftmp[0]),     [srch]"=&f"(ftmp[1]),
+        : RESTRICT_ASM_ALL64
+          [srcl]"=&f"(ftmp[0]),     [srch]"=&f"(ftmp[1]),
           [filter10]"=&f"(ftmp[2]), [filter32]"=&f"(ftmp[3]),
           [filter54]"=&f"(ftmp[4]), [filter76]"=&f"(ftmp[5]),
           [ftmp0]"=&f"(ftmp[6]),    [ftmp4]"=&f"(ftmp[7]),
@@ -247,30 +235,25 @@ static void convolve_avg_horiz_mmi(const uint8_t *src, int32_t src_stride,
 {
     double ftmp[15];
     uint32_t tmp[2];
+    DECLARE_VAR_ALL64;
     src -= 3;
     src_stride -= w;
     dst_stride -= w;
 
     __asm__ volatile (
         "move       %[tmp1],    %[width]                   \n\t"
-        "xor        %[ftmp0],   %[ftmp0],    %[ftmp0]      \n\t"
-        "gsldlc1    %[filter1], 0x03(%[filter])            \n\t"
-        "gsldrc1    %[filter1], 0x00(%[filter])            \n\t"
-        "gsldlc1    %[filter2], 0x0b(%[filter])            \n\t"
-        "gsldrc1    %[filter2], 0x08(%[filter])            \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],    %[ftmp0]      \n\t"
+        MMI_ULDC1(%[filter1], %[filter], 0x00)
+        MMI_ULDC1(%[filter2], %[filter], 0x08)
         "li         %[tmp0],    0x07                       \n\t"
         "dmtc1      %[tmp0],    %[ftmp13]                  \n\t"
         "punpcklwd  %[ftmp13],  %[ftmp13],   %[ftmp13]     \n\t"
         "1:                                                \n\t"
         /* Get 8 data per row */
-        "gsldlc1    %[ftmp5],   0x07(%[src])               \n\t"
-        "gsldrc1    %[ftmp5],   0x00(%[src])               \n\t"
-        "gsldlc1    %[ftmp7],   0x08(%[src])               \n\t"
-        "gsldrc1    %[ftmp7],   0x01(%[src])               \n\t"
-        "gsldlc1    %[ftmp9],   0x09(%[src])               \n\t"
-        "gsldrc1    %[ftmp9],   0x02(%[src])               \n\t"
-        "gsldlc1    %[ftmp11],  0x0A(%[src])               \n\t"
-        "gsldrc1    %[ftmp11],  0x03(%[src])               \n\t"
+        MMI_ULDC1(%[ftmp5], %[src], 0x00)
+        MMI_ULDC1(%[ftmp7], %[src], 0x01)
+        MMI_ULDC1(%[ftmp9], %[src], 0x02)
+        MMI_ULDC1(%[ftmp11], %[src], 0x03)
         "punpcklbh  %[ftmp4],   %[ftmp5],    %[ftmp0]      \n\t"
         "punpckhbh  %[ftmp5],   %[ftmp5],    %[ftmp0]      \n\t"
         "punpcklbh  %[ftmp6],   %[ftmp7],    %[ftmp0]      \n\t"
@@ -289,8 +272,7 @@ static void convolve_avg_horiz_mmi(const uint8_t *src, int32_t src_stride,
         "packsswh   %[srcl],    %[srcl],     %[srch]       \n\t"
         "packushb   %[ftmp12],  %[srcl],     %[ftmp0]      \n\t"
         "punpcklbh  %[ftmp12],  %[ftmp12],   %[ftmp0]      \n\t"
-        "gsldlc1    %[ftmp4],   0x07(%[dst])               \n\t"
-        "gsldrc1    %[ftmp4],   0x00(%[dst])               \n\t"
+        MMI_ULDC1(%[ftmp4], %[dst], 0x0)
         "punpcklbh  %[ftmp4],   %[ftmp4],    %[ftmp0]      \n\t"
         "paddh      %[ftmp12],  %[ftmp12],   %[ftmp4]      \n\t"
         "li         %[tmp0],    0x10001                    \n\t"
@@ -309,7 +291,8 @@ static void convolve_avg_horiz_mmi(const uint8_t *src, int32_t src_stride,
         PTR_ADDU   "%[dst],     %[dst],      %[dst_stride] \n\t"
         PTR_ADDIU  "%[height],  %[height],   -0x01         \n\t"
         "bnez       %[height],  1b                         \n\t"
-        : [srcl]"=&f"(ftmp[0]),     [srch]"=&f"(ftmp[1]),
+        : RESTRICT_ASM_ALL64
+          [srcl]"=&f"(ftmp[0]),     [srch]"=&f"(ftmp[1]),
           [filter1]"=&f"(ftmp[2]),  [filter2]"=&f"(ftmp[3]),
           [ftmp0]"=&f"(ftmp[4]),    [ftmp4]"=&f"(ftmp[5]),
           [ftmp5]"=&f"(ftmp[6]),    [ftmp6]"=&f"(ftmp[7]),
@@ -335,15 +318,14 @@ static void convolve_avg_vert_mmi(const uint8_t *src, int32_t src_stride,
     double ftmp[17];
     uint32_t tmp[1];
     ptrdiff_t addr = src_stride;
+    DECLARE_VAR_ALL64;
     src_stride -= w;
     dst_stride -= w;
 
     __asm__ volatile (
-        "xor        %[ftmp0],    %[ftmp0],   %[ftmp0]      \n\t"
-        "gsldlc1    %[ftmp4],    0x03(%[filter])           \n\t"
-        "gsldrc1    %[ftmp4],    0x00(%[filter])           \n\t"
-        "gsldlc1    %[ftmp5],    0x0b(%[filter])           \n\t"
-        "gsldrc1    %[ftmp5],    0x08(%[filter])           \n\t"
+        "pxor       %[ftmp0],    %[ftmp0],   %[ftmp0]      \n\t"
+        MMI_ULDC1(%[ftmp4], %[filter], 0x00)
+        MMI_ULDC1(%[ftmp5], %[filter], 0x08)
         "punpcklwd  %[filter10], %[ftmp4],   %[ftmp4]      \n\t"
         "punpckhwd  %[filter32], %[ftmp4],   %[ftmp4]      \n\t"
         "punpcklwd  %[filter54], %[ftmp5],   %[ftmp5]      \n\t"
@@ -353,29 +335,21 @@ static void convolve_avg_vert_mmi(const uint8_t *src, int32_t src_stride,
         "punpcklwd  %[ftmp13],   %[ftmp13],  %[ftmp13]     \n\t"
         "1:                                                \n\t"
         /* Get 8 data per column */
-        "gsldlc1    %[ftmp4],    0x07(%[src])              \n\t"
-        "gsldrc1    %[ftmp4],    0x00(%[src])              \n\t"
+        MMI_ULDC1(%[ftmp4], %[src], 0x0)
         PTR_ADDU   "%[tmp0],     %[src],     %[addr]       \n\t"
-        "gsldlc1    %[ftmp5],    0x07(%[tmp0])             \n\t"
-        "gsldrc1    %[ftmp5],    0x00(%[tmp0])             \n\t"
+        MMI_ULDC1(%[ftmp5], %[tmp0], 0x0)
         PTR_ADDU   "%[tmp0],     %[tmp0],    %[addr]       \n\t"
-        "gsldlc1    %[ftmp6],    0x07(%[tmp0])             \n\t"
-        "gsldrc1    %[ftmp6],    0x00(%[tmp0])             \n\t"
+        MMI_ULDC1(%[ftmp6], %[tmp0], 0x0)
         PTR_ADDU   "%[tmp0],     %[tmp0],    %[addr]       \n\t"
-        "gsldlc1    %[ftmp7],    0x07(%[tmp0])             \n\t"
-        "gsldrc1    %[ftmp7],    0x00(%[tmp0])             \n\t"
+        MMI_ULDC1(%[ftmp7], %[tmp0], 0x0)
         PTR_ADDU   "%[tmp0],     %[tmp0],    %[addr]       \n\t"
-        "gsldlc1    %[ftmp8],    0x07(%[tmp0])             \n\t"
-        "gsldrc1    %[ftmp8],    0x00(%[tmp0])             \n\t"
+        MMI_ULDC1(%[ftmp8], %[tmp0], 0x0)
         PTR_ADDU   "%[tmp0],     %[tmp0],    %[addr]       \n\t"
-        "gsldlc1    %[ftmp9],    0x07(%[tmp0])             \n\t"
-        "gsldrc1    %[ftmp9],    0x00(%[tmp0])             \n\t"
+        MMI_ULDC1(%[ftmp9], %[tmp0], 0x0)
         PTR_ADDU   "%[tmp0],     %[tmp0],    %[addr]       \n\t"
-        "gsldlc1    %[ftmp10],   0x07(%[tmp0])             \n\t"
-        "gsldrc1    %[ftmp10],   0x00(%[tmp0])             \n\t"
+        MMI_ULDC1(%[ftmp10], %[tmp0], 0x0)
         PTR_ADDU   "%[tmp0],     %[tmp0],    %[addr]       \n\t"
-        "gsldlc1    %[ftmp11],   0x07(%[tmp0])             \n\t"
-        "gsldrc1    %[ftmp11],   0x00(%[tmp0])             \n\t"
+        MMI_ULDC1(%[ftmp11], %[tmp0], 0x0)
         "punpcklbh  %[ftmp4],    %[ftmp4],   %[ftmp0]      \n\t"
         "punpcklbh  %[ftmp5],    %[ftmp5],   %[ftmp0]      \n\t"
         "punpcklbh  %[ftmp6],    %[ftmp6],   %[ftmp0]      \n\t"
@@ -394,8 +368,7 @@ static void convolve_avg_vert_mmi(const uint8_t *src, int32_t src_stride,
         "packsswh   %[srcl],     %[srcl],    %[srch]       \n\t"
         "packushb   %[ftmp12],   %[srcl],    %[ftmp0]      \n\t"
         "punpcklbh  %[ftmp12],   %[ftmp12],  %[ftmp0]      \n\t"
-        "gsldlc1    %[ftmp4],    0x07(%[dst])              \n\t"
-        "gsldrc1    %[ftmp4],    0x00(%[dst])              \n\t"
+        MMI_ULDC1(%[ftmp4], %[dst], 0x00)
         "punpcklbh  %[ftmp4],    %[ftmp4],   %[ftmp0]      \n\t"
         "paddh      %[ftmp12],   %[ftmp12],  %[ftmp4]      \n\t"
         "li         %[tmp0],     0x10001                   \n\t"
@@ -414,7 +387,8 @@ static void convolve_avg_vert_mmi(const uint8_t *src, int32_t src_stride,
         PTR_ADDU   "%[dst],      %[dst],     %[dst_stride] \n\t"
         PTR_ADDIU  "%[height],   %[height],  -0x01         \n\t"
         "bnez       %[height],   1b                        \n\t"
-        : [srcl]"=&f"(ftmp[0]),     [srch]"=&f"(ftmp[1]),
+        : RESTRICT_ASM_ALL64
+          [srcl]"=&f"(ftmp[0]),     [srch]"=&f"(ftmp[1]),
           [filter10]"=&f"(ftmp[2]), [filter32]"=&f"(ftmp[3]),
           [filter54]"=&f"(ftmp[4]), [filter76]"=&f"(ftmp[5]),
           [ftmp0]"=&f"(ftmp[6]),    [ftmp4]"=&f"(ftmp[7]),
@@ -439,20 +413,19 @@ static void convolve_avg_mmi(const uint8_t *src, int32_t src_stride,
 {
     double ftmp[4];
     uint32_t tmp[2];
+    DECLARE_VAR_ALL64;
     src_stride -= w;
     dst_stride -= w;
 
     __asm__ volatile (
         "move       %[tmp1],    %[width]                  \n\t"
-        "xor        %[ftmp0],   %[ftmp0],   %[ftmp0]      \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],   %[ftmp0]      \n\t"
         "li         %[tmp0],    0x10001                   \n\t"
         "dmtc1      %[tmp0],    %[ftmp3]                  \n\t"
         "punpcklhw  %[ftmp3],   %[ftmp3],   %[ftmp3]      \n\t"
         "1:                                               \n\t"
-        "gslwlc1    %[ftmp1],   0x07(%[src])              \n\t"
-        "gslwrc1    %[ftmp1],   0x00(%[src])              \n\t"
-        "gslwlc1    %[ftmp2],   0x07(%[dst])              \n\t"
-        "gslwrc1    %[ftmp2],   0x00(%[dst])              \n\t"
+        MMI_ULDC1(%[ftmp1], %[src], 0x00)
+        MMI_ULDC1(%[ftmp2], %[dst], 0x00)
         "punpcklbh  %[ftmp1],   %[ftmp1],   %[ftmp0]      \n\t"
         "punpcklbh  %[ftmp2],   %[ftmp2],   %[ftmp0]      \n\t"
         "paddh      %[ftmp1],   %[ftmp1],   %[ftmp2]      \n\t"
@@ -469,7 +442,8 @@ static void convolve_avg_mmi(const uint8_t *src, int32_t src_stride,
         PTR_ADDU   "%[src],     %[src],     %[src_stride] \n\t"
         PTR_ADDIU  "%[height],  %[height],  -0x01         \n\t"
         "bnez       %[height],  1b                        \n\t"
-        : [ftmp0]"=&f"(ftmp[0]),  [ftmp1]"=&f"(ftmp[1]),
+        : RESTRICT_ASM_ALL64
+          [ftmp0]"=&f"(ftmp[0]),  [ftmp1]"=&f"(ftmp[1]),
           [ftmp2]"=&f"(ftmp[2]),  [ftmp3]"=&f"(ftmp[3]),
           [tmp0]"=&r"(tmp[0]),    [tmp1]"=&r"(tmp[1]),
           [src]"+&r"(src),        [dst]"+&r"(dst),
diff --git a/libavcodec/mips/vp9_mc_msa.c b/libavcodec/mips/vp9_mc_msa.c
index 1d8a892768..57ea425727 100644
--- a/libavcodec/mips/vp9_mc_msa.c
+++ b/libavcodec/mips/vp9_mc_msa.c
@@ -795,7 +795,7 @@ static void common_hv_8ht_8vt_4w_msa(const uint8_t *src, int32_t src_stride,
                               filt_hz1, filt_hz2, filt_hz3);
     hz_out5 = HORIZ_8TAP_FILT(src5, src6, mask0, mask1, mask2, mask3, filt_hz0,
                               filt_hz1, filt_hz2, filt_hz3);
-    SLDI_B2_SH(hz_out2, hz_out4, hz_out0, hz_out2, hz_out1, hz_out3, 8);
+    SLDI_B2_SH(hz_out2, hz_out0, hz_out4, hz_out2, 8, hz_out1, hz_out3);
 
     filt = LD_SH(filter_vert);
     SPLATI_H4_SH(filt, 0, 1, 2, 3, filt_vt0, filt_vt1, filt_vt2, filt_vt3);
@@ -1585,7 +1585,7 @@ static void common_hv_8ht_8vt_and_aver_dst_4w_msa(const uint8_t *src,
                               filt_hz1, filt_hz2, filt_hz3);
     hz_out5 = HORIZ_8TAP_FILT(src5, src6, mask0, mask1, mask2, mask3, filt_hz0,
                               filt_hz1, filt_hz2, filt_hz3);
-    SLDI_B2_SH(hz_out2, hz_out4, hz_out0, hz_out2, hz_out1, hz_out3, 8);
+    SLDI_B2_SH(hz_out2, hz_out0, hz_out4, hz_out2, 8, hz_out1, hz_out3);
 
     filt = LD_SH(filter_vert);
     SPLATI_H4_SH(filt, 0, 1, 2, 3, filt_vt0, filt_vt1, filt_vt2, filt_vt3);
@@ -2093,7 +2093,7 @@ void ff_put_bilin_64h_msa(uint8_t *dst, ptrdiff_t dst_stride,
         src4 = LD_SB(src + 32);
         src6 = LD_SB(src + 48);
         src7 = LD_SB(src + 56);
-        SLDI_B3_SB(src2, src4, src6, src0, src2, src4, src1, src3, src5, 8);
+        SLDI_B3_SB(src2, src0, src4, src2, src6, src4, 8, src1, src3, src5);
         src += src_stride;
 
         VSHF_B2_UB(src0, src0, src1, src1, mask, mask, vec0, vec1);
@@ -2544,8 +2544,8 @@ static void common_hv_2ht_2vt_4x8_msa(const uint8_t *src, int32_t src_stride,
     hz_out4 = HORIZ_2TAP_FILT_UH(src4, src5, mask, filt_hz, 7);
     hz_out6 = HORIZ_2TAP_FILT_UH(src6, src7, mask, filt_hz, 7);
     hz_out8 = HORIZ_2TAP_FILT_UH(src8, src8, mask, filt_hz, 7);
-    SLDI_B3_UH(hz_out2, hz_out4, hz_out6, hz_out0, hz_out2, hz_out4, hz_out1,
-               hz_out3, hz_out5, 8);
+    SLDI_B3_UH(hz_out2, hz_out0, hz_out4, hz_out2, hz_out6, hz_out4, 8, hz_out1,
+               hz_out3, hz_out5);
     hz_out7 = (v8u16) __msa_pckod_d((v2i64) hz_out8, (v2i64) hz_out6);
 
     ILVEV_B2_UB(hz_out0, hz_out1, hz_out2, hz_out3, vec0, vec1);
@@ -3146,7 +3146,7 @@ void ff_avg_bilin_64h_msa(uint8_t *dst, ptrdiff_t dst_stride,
     for (loop_cnt = height; loop_cnt--;) {
         LD_SB4(src, 16, src0, src2, src4, src6);
         src7 = LD_SB(src + 56);
-        SLDI_B3_SB(src2, src4, src6, src0, src2, src4, src1, src3, src5, 8);
+        SLDI_B3_SB(src2, src0, src4, src2, src6, src4, 8, src1, src3, src5);
         src += src_stride;
 
         VSHF_B2_UB(src0, src0, src1, src1, mask, mask, vec0, vec1);
@@ -3655,8 +3655,8 @@ static void common_hv_2ht_2vt_and_aver_dst_4x8_msa(const uint8_t *src,
     hz_out4 = HORIZ_2TAP_FILT_UH(src4, src5, mask, filt_hz, 7);
     hz_out6 = HORIZ_2TAP_FILT_UH(src6, src7, mask, filt_hz, 7);
     hz_out8 = HORIZ_2TAP_FILT_UH(src8, src8, mask, filt_hz, 7);
-    SLDI_B3_UH(hz_out2, hz_out4, hz_out6, hz_out0, hz_out2, hz_out4, hz_out1,
-               hz_out3, hz_out5, 8);
+    SLDI_B3_UH(hz_out2, hz_out0, hz_out4, hz_out2, hz_out6, hz_out4, 8, hz_out1,
+               hz_out3, hz_out5);
     hz_out7 = (v8u16) __msa_pckod_d((v2i64) hz_out8, (v2i64) hz_out6);
 
     LW4(dst, dst_stride, tp0, tp1, tp2, tp3);
diff --git a/libavcodec/mips/vp9dsp_init_mips.c b/libavcodec/mips/vp9dsp_init_mips.c
index 5990fa6952..5a8c599e7e 100644
--- a/libavcodec/mips/vp9dsp_init_mips.c
+++ b/libavcodec/mips/vp9dsp_init_mips.c
@@ -18,6 +18,7 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
+#include "libavutil/mips/cpu.h"
 #include "config.h"
 #include "libavutil/common.h"
 #include "libavcodec/vp9dsp.h"
@@ -209,10 +210,17 @@ static av_cold void vp9dsp_init_mmi(VP9DSPContext *dsp, int bpp)
 
 av_cold void ff_vp9dsp_init_mips(VP9DSPContext *dsp, int bpp)
 {
+#if HAVE_MSA || HAVE_MMI
+    int cpu_flags = av_get_cpu_flags();
+#endif
+
 #if HAVE_MMI
-    vp9dsp_init_mmi(dsp, bpp);
-#endif  // #if HAVE_MMI
+    if (have_mmi(cpu_flags))
+        vp9dsp_init_mmi(dsp, bpp);
+#endif
+
 #if HAVE_MSA
-    vp9dsp_init_msa(dsp, bpp);
-#endif  // #if HAVE_MSA
+    if (have_msa(cpu_flags))
+        vp9dsp_init_msa(dsp, bpp);
+#endif
 }
diff --git a/libavcodec/mips/wmv2dsp_init_mips.c b/libavcodec/mips/wmv2dsp_init_mips.c
index 51dd2078d9..af1400731a 100644
--- a/libavcodec/mips/wmv2dsp_init_mips.c
+++ b/libavcodec/mips/wmv2dsp_init_mips.c
@@ -18,21 +18,17 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
+#include "libavutil/mips/cpu.h"
 #include "config.h"
 #include "libavutil/attributes.h"
 #include "wmv2dsp_mips.h"
 
-#if HAVE_MMI
-static av_cold void wmv2dsp_init_mmi(WMV2DSPContext *c)
-{
-    c->idct_add  = ff_wmv2_idct_add_mmi;
-    c->idct_put  = ff_wmv2_idct_put_mmi;
-}
-#endif /* HAVE_MMI */
-
 av_cold void ff_wmv2dsp_init_mips(WMV2DSPContext *c)
 {
-#if HAVE_MMI
-    wmv2dsp_init_mmi(c);
-#endif /* HAVE_MMI */
+    int cpu_flags = av_get_cpu_flags();
+
+    if (have_mmi(cpu_flags)) {
+        c->idct_add  = ff_wmv2_idct_add_mmi;
+        c->idct_put  = ff_wmv2_idct_put_mmi;
+    }
 }
diff --git a/libavcodec/mips/wmv2dsp_mips.h b/libavcodec/mips/wmv2dsp_mips.h
index 22894c505d..c96b3d94c7 100644
--- a/libavcodec/mips/wmv2dsp_mips.h
+++ b/libavcodec/mips/wmv2dsp_mips.h
@@ -23,7 +23,7 @@
 
 #include "libavcodec/wmv2dsp.h"
 
-void ff_wmv2_idct_add_mmi(uint8_t *dest, int line_size, int16_t *block);
-void ff_wmv2_idct_put_mmi(uint8_t *dest, int line_size, int16_t *block);
+void ff_wmv2_idct_add_mmi(uint8_t *dest, ptrdiff_t line_size, int16_t *block);
+void ff_wmv2_idct_put_mmi(uint8_t *dest, ptrdiff_t line_size, int16_t *block);
 
 #endif /* AVCODEC_MIPS_WMV2DSP_MIPS_H */
diff --git a/libavcodec/mips/wmv2dsp_mmi.c b/libavcodec/mips/wmv2dsp_mmi.c
index 1f6ccb299b..1a6781ae77 100644
--- a/libavcodec/mips/wmv2dsp_mmi.c
+++ b/libavcodec/mips/wmv2dsp_mmi.c
@@ -95,7 +95,7 @@ static void wmv2_idct_col_mmi(short * b)
     b[56] = (a0 + a2 - a1 - a5 + 8192) >> 14;
 }
 
-void ff_wmv2_idct_add_mmi(uint8_t *dest, int line_size, int16_t *block)
+void ff_wmv2_idct_add_mmi(uint8_t *dest, ptrdiff_t line_size, int16_t *block)
 {
     int i;
     double ftmp[11];
@@ -106,7 +106,7 @@ void ff_wmv2_idct_add_mmi(uint8_t *dest, int line_size, int16_t *block)
         wmv2_idct_col_mmi(block + i);
 
     __asm__ volatile (
-        "xor        %[ftmp0],   %[ftmp0],   %[ftmp0]                    \n\t"
+        "pxor       %[ftmp0],   %[ftmp0],   %[ftmp0]                    \n\t"
 
         // low 4 loop
         MMI_LDC1(%[ftmp1], %[block], 0x00)
@@ -212,7 +212,7 @@ void ff_wmv2_idct_add_mmi(uint8_t *dest, int line_size, int16_t *block)
     );
 }
 
-void ff_wmv2_idct_put_mmi(uint8_t *dest, int line_size, int16_t *block)
+void ff_wmv2_idct_put_mmi(uint8_t *dest, ptrdiff_t line_size, int16_t *block)
 {
     int i;
     double ftmp[8];
diff --git a/libavcodec/mips/xvid_idct_mmi.c b/libavcodec/mips/xvid_idct_mmi.c
index d3f9acb0e2..b822b8add8 100644
--- a/libavcodec/mips/xvid_idct_mmi.c
+++ b/libavcodec/mips/xvid_idct_mmi.c
@@ -240,13 +240,13 @@ void ff_xvid_idct_mmi(int16_t *block)
     );
 }
 
-void ff_xvid_idct_put_mmi(uint8_t *dest, int32_t line_size, int16_t *block)
+void ff_xvid_idct_put_mmi(uint8_t *dest, ptrdiff_t line_size, int16_t *block)
 {
     ff_xvid_idct_mmi(block);
     ff_put_pixels_clamped_mmi(block, dest, line_size);
 }
 
-void ff_xvid_idct_add_mmi(uint8_t *dest, int32_t line_size, int16_t *block)
+void ff_xvid_idct_add_mmi(uint8_t *dest, ptrdiff_t line_size, int16_t *block)
 {
     ff_xvid_idct_mmi(block);
     ff_add_pixels_clamped_mmi(block, dest, line_size);
diff --git a/libavcodec/mips/xvididct_init_mips.c b/libavcodec/mips/xvididct_init_mips.c
index c1d82cc30c..ed545cfe17 100644
--- a/libavcodec/mips/xvididct_init_mips.c
+++ b/libavcodec/mips/xvididct_init_mips.c
@@ -18,28 +18,23 @@
  * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
  */
 
+#include "libavutil/mips/cpu.h"
 #include "xvididct_mips.h"
 
-#if HAVE_MMI
-static av_cold void xvid_idct_init_mmi(IDCTDSPContext *c, AVCodecContext *avctx,
+av_cold void ff_xvid_idct_init_mips(IDCTDSPContext *c, AVCodecContext *avctx,
         unsigned high_bit_depth)
 {
-    if (!high_bit_depth) {
-        if (avctx->idct_algo == FF_IDCT_AUTO ||
-                avctx->idct_algo == FF_IDCT_XVID) {
-            c->idct_put = ff_xvid_idct_put_mmi;
-            c->idct_add = ff_xvid_idct_add_mmi;
-            c->idct = ff_xvid_idct_mmi;
-            c->perm_type = FF_IDCT_PERM_NONE;
+    int cpu_flags = av_get_cpu_flags();
+
+    if (have_mmi(cpu_flags)) {
+        if (!high_bit_depth) {
+            if (avctx->idct_algo == FF_IDCT_AUTO ||
+                    avctx->idct_algo == FF_IDCT_XVID) {
+                c->idct_put = ff_xvid_idct_put_mmi;
+                c->idct_add = ff_xvid_idct_add_mmi;
+                c->idct = ff_xvid_idct_mmi;
+                c->perm_type = FF_IDCT_PERM_NONE;
+            }
         }
     }
 }
-#endif /* HAVE_MMI */
-
-av_cold void ff_xvid_idct_init_mips(IDCTDSPContext *c, AVCodecContext *avctx,
-        unsigned high_bit_depth)
-{
-#if HAVE_MMI
-    xvid_idct_init_mmi(c, avctx, high_bit_depth);
-#endif /* HAVE_MMI */
-}
diff --git a/libavcodec/mips/xvididct_mips.h b/libavcodec/mips/xvididct_mips.h
index 0768aaa26f..bee03c1391 100644
--- a/libavcodec/mips/xvididct_mips.h
+++ b/libavcodec/mips/xvididct_mips.h
@@ -24,7 +24,7 @@
 #include "libavcodec/xvididct.h"
 
 void ff_xvid_idct_mmi(int16_t *block);
-void ff_xvid_idct_put_mmi(uint8_t *dest, int32_t line_size, int16_t *block);
-void ff_xvid_idct_add_mmi(uint8_t *dest, int32_t line_size, int16_t *block);
+void ff_xvid_idct_put_mmi(uint8_t *dest, ptrdiff_t line_size, int16_t *block);
+void ff_xvid_idct_add_mmi(uint8_t *dest, ptrdiff_t line_size, int16_t *block);
 
 #endif /* AVCODEC_MIPS_XVIDIDCT_MIPS_H */
diff --git a/libavcodec/vc1dsp.c b/libavcodec/vc1dsp.c
index c25a6f3adf..a543792e14 100644
--- a/libavcodec/vc1dsp.c
+++ b/libavcodec/vc1dsp.c
@@ -445,7 +445,6 @@ static void vc1_inv_trans_4x8_c(uint8_t *dest, ptrdiff_t stride, int16_t *block)
         dst[1] = (t2 - t4) >> 3;
         dst[2] = (t2 + t4) >> 3;
         dst[3] = (t1 - t3) >> 3;
-
         src += 8;
         dst += 8;
     }
@@ -1039,4 +1038,6 @@ av_cold void ff_vc1dsp_init(VC1DSPContext *dsp)
         ff_vc1dsp_init_x86(dsp);
     if (ARCH_MIPS)
         ff_vc1dsp_init_mips(dsp);
+    if (ARCH_LOONGARCH)
+        ff_vc1dsp_init_loongarch(dsp);
 }
diff --git a/libavcodec/vc1dsp.h b/libavcodec/vc1dsp.h
index 75db62b1b4..c6443acb20 100644
--- a/libavcodec/vc1dsp.h
+++ b/libavcodec/vc1dsp.h
@@ -88,5 +88,6 @@ void ff_vc1dsp_init_arm(VC1DSPContext* dsp);
 void ff_vc1dsp_init_ppc(VC1DSPContext *c);
 void ff_vc1dsp_init_x86(VC1DSPContext* dsp);
 void ff_vc1dsp_init_mips(VC1DSPContext* dsp);
+void ff_vc1dsp_init_loongarch(VC1DSPContext* dsp);
 
 #endif /* AVCODEC_VC1DSP_H */
diff --git a/libavcodec/videodsp.c b/libavcodec/videodsp.c
index ce9e9eb143..fe64b5e81a 100644
--- a/libavcodec/videodsp.c
+++ b/libavcodec/videodsp.c
@@ -54,4 +54,6 @@ av_cold void ff_videodsp_init(VideoDSPContext *ctx, int bpc)
         ff_videodsp_init_x86(ctx, bpc);
     if (ARCH_MIPS)
         ff_videodsp_init_mips(ctx, bpc);
+    if (ARCH_LOONGARCH)
+        ff_videodsp_init_loongarch(ctx, bpc);
 }
diff --git a/libavcodec/videodsp.h b/libavcodec/videodsp.h
index c0545f22b0..ac971dc57f 100644
--- a/libavcodec/videodsp.h
+++ b/libavcodec/videodsp.h
@@ -84,5 +84,6 @@ void ff_videodsp_init_arm(VideoDSPContext *ctx, int bpc);
 void ff_videodsp_init_ppc(VideoDSPContext *ctx, int bpc);
 void ff_videodsp_init_x86(VideoDSPContext *ctx, int bpc);
 void ff_videodsp_init_mips(VideoDSPContext *ctx, int bpc);
+void ff_videodsp_init_loongarch(VideoDSPContext *ctx, int bpc);
 
 #endif /* AVCODEC_VIDEODSP_H */
diff --git a/libavcodec/vp8dsp.c b/libavcodec/vp8dsp.c
index 4ff63d0784..732a483b62 100644
--- a/libavcodec/vp8dsp.c
+++ b/libavcodec/vp8dsp.c
@@ -743,5 +743,7 @@ av_cold void ff_vp8dsp_init(VP8DSPContext *dsp)
         ff_vp8dsp_init_x86(dsp);
     if (ARCH_MIPS)
         ff_vp8dsp_init_mips(dsp);
+    if (ARCH_LOONGARCH)
+        ff_vp8dsp_init_loongarch(dsp);
 }
 #endif /* CONFIG_VP8_DECODER */
diff --git a/libavcodec/vp8dsp.h b/libavcodec/vp8dsp.h
index cfe1524b0b..7c6208df39 100644
--- a/libavcodec/vp8dsp.h
+++ b/libavcodec/vp8dsp.h
@@ -101,6 +101,7 @@ void ff_vp8dsp_init_aarch64(VP8DSPContext *c);
 void ff_vp8dsp_init_arm(VP8DSPContext *c);
 void ff_vp8dsp_init_x86(VP8DSPContext *c);
 void ff_vp8dsp_init_mips(VP8DSPContext *c);
+void ff_vp8dsp_init_loongarch(VP8DSPContext *c);
 
 #define IS_VP7 1
 #define IS_VP8 0
diff --git a/libavcodec/vp9dsp.c b/libavcodec/vp9dsp.c
index f6d73f73cd..5fe8888da6 100644
--- a/libavcodec/vp9dsp.c
+++ b/libavcodec/vp9dsp.c
@@ -96,4 +96,5 @@ av_cold void ff_vp9dsp_init(VP9DSPContext *dsp, int bpp, int bitexact)
     if (ARCH_ARM) ff_vp9dsp_init_arm(dsp, bpp);
     if (ARCH_X86) ff_vp9dsp_init_x86(dsp, bpp, bitexact);
     if (ARCH_MIPS) ff_vp9dsp_init_mips(dsp, bpp);
+    if (ARCH_LOONGARCH) ff_vp9dsp_init_loongarch(dsp, bpp);
 }
diff --git a/libavcodec/vp9dsp.h b/libavcodec/vp9dsp.h
index e2256316a8..700dd72de8 100644
--- a/libavcodec/vp9dsp.h
+++ b/libavcodec/vp9dsp.h
@@ -132,5 +132,6 @@ void ff_vp9dsp_init_aarch64(VP9DSPContext *dsp, int bpp);
 void ff_vp9dsp_init_arm(VP9DSPContext *dsp, int bpp);
 void ff_vp9dsp_init_x86(VP9DSPContext *dsp, int bpp, int bitexact);
 void ff_vp9dsp_init_mips(VP9DSPContext *dsp, int bpp);
+void ff_vp9dsp_init_loongarch(VP9DSPContext *dsp, int bpp);
 
 #endif /* AVCODEC_VP9DSP_H */
diff --git a/libavutil/cpu.c b/libavutil/cpu.c
index 6548cc3042..efb48eb544 100644
--- a/libavutil/cpu.c
+++ b/libavutil/cpu.c
@@ -51,6 +51,8 @@ static atomic_int cpu_flags = ATOMIC_VAR_INIT(-1);
 
 static int get_cpu_flags(void)
 {
+    if (ARCH_MIPS)
+        return ff_get_cpu_flags_mips();
     if (ARCH_AARCH64)
         return ff_get_cpu_flags_aarch64();
     if (ARCH_ARM)
@@ -59,6 +61,8 @@ static int get_cpu_flags(void)
         return ff_get_cpu_flags_ppc();
     if (ARCH_X86)
         return ff_get_cpu_flags_x86();
+    if (ARCH_LOONGARCH)
+        return ff_get_cpu_flags_loongarch();
     return 0;
 }
 
@@ -169,6 +173,12 @@ int av_parse_cpu_flags(const char *s)
         { "armv8",    NULL, 0, AV_OPT_TYPE_CONST, { .i64 = AV_CPU_FLAG_ARMV8    },    .unit = "flags" },
         { "neon",     NULL, 0, AV_OPT_TYPE_CONST, { .i64 = AV_CPU_FLAG_NEON     },    .unit = "flags" },
         { "vfp",      NULL, 0, AV_OPT_TYPE_CONST, { .i64 = AV_CPU_FLAG_VFP      },    .unit = "flags" },
+#elif ARCH_MIPS
+        { "mmi",      NULL, 0, AV_OPT_TYPE_CONST, { .i64 = AV_CPU_FLAG_MMI      },    .unit = "flags" },
+        { "msa",      NULL, 0, AV_OPT_TYPE_CONST, { .i64 = AV_CPU_FLAG_MSA      },    .unit = "flags" },
+#elif ARCH_LOONGARCH
+        { "lsx",      NULL, 0, AV_OPT_TYPE_CONST, { .i64 = AV_CPU_FLAG_LSX      },    .unit = "flags" },
+        { "lasx",     NULL, 0, AV_OPT_TYPE_CONST, { .i64 = AV_CPU_FLAG_LASX     },    .unit = "flags" },
 #endif
         { NULL },
     };
@@ -250,6 +260,12 @@ int av_parse_cpu_caps(unsigned *flags, const char *s)
         { "armv8",    NULL, 0, AV_OPT_TYPE_CONST, { .i64 = AV_CPU_FLAG_ARMV8    },    .unit = "flags" },
         { "neon",     NULL, 0, AV_OPT_TYPE_CONST, { .i64 = AV_CPU_FLAG_NEON     },    .unit = "flags" },
         { "vfp",      NULL, 0, AV_OPT_TYPE_CONST, { .i64 = AV_CPU_FLAG_VFP      },    .unit = "flags" },
+#elif ARCH_MIPS
+        { "mmi",      NULL, 0, AV_OPT_TYPE_CONST, { .i64 = AV_CPU_FLAG_MMI      },    .unit = "flags" },
+        { "msa",      NULL, 0, AV_OPT_TYPE_CONST, { .i64 = AV_CPU_FLAG_MSA      },    .unit = "flags" },
+#elif ARCH_LOONGARCH
+        { "lsx",      NULL, 0, AV_OPT_TYPE_CONST, { .i64 = AV_CPU_FLAG_LSX      },    .unit = "flags" },
+        { "lasx",     NULL, 0, AV_OPT_TYPE_CONST, { .i64 = AV_CPU_FLAG_LASX     },    .unit = "flags" },
 #endif
         { NULL },
     };
@@ -308,6 +324,8 @@ int av_cpu_count(void)
 
 size_t av_cpu_max_align(void)
 {
+    if (ARCH_MIPS)
+        return ff_get_cpu_max_align_mips();
     if (ARCH_AARCH64)
         return ff_get_cpu_max_align_aarch64();
     if (ARCH_ARM)
@@ -316,6 +334,8 @@ size_t av_cpu_max_align(void)
         return ff_get_cpu_max_align_ppc();
     if (ARCH_X86)
         return ff_get_cpu_max_align_x86();
+    if (ARCH_LOONGARCH)
+        return ff_get_cpu_max_align_loongarch();
 
     return 8;
 }
diff --git a/libavutil/cpu.h b/libavutil/cpu.h
index 8bb9eb606b..c1dbdc0061 100644
--- a/libavutil/cpu.h
+++ b/libavutil/cpu.h
@@ -71,6 +71,13 @@
 #define AV_CPU_FLAG_VFP_VM       (1 << 7) ///< VFPv2 vector mode, deprecated in ARMv7-A and unavailable in various CPUs implementations
 #define AV_CPU_FLAG_SETEND       (1 <<16)
 
+#define AV_CPU_FLAG_MMI          (1 << 0)
+#define AV_CPU_FLAG_MSA          (1 << 1)
+
+//Loongarch SIMD extension.
+#define AV_CPU_FLAG_LSX          (1 << 0)
+#define AV_CPU_FLAG_LASX         (1 << 1)
+
 /**
  * Return the flags which specify extensions supported by the CPU.
  * The returned value is affected by av_force_cpu_flags() if that was used
diff --git a/libavutil/cpu_internal.h b/libavutil/cpu_internal.h
index 37122d1c5f..e207b2d480 100644
--- a/libavutil/cpu_internal.h
+++ b/libavutil/cpu_internal.h
@@ -41,14 +41,18 @@
 #define CPUEXT_FAST(flags, cpuext) CPUEXT_SUFFIX_FAST(flags, , cpuext)
 #define CPUEXT_SLOW(flags, cpuext) CPUEXT_SUFFIX_SLOW(flags, , cpuext)
 
+int ff_get_cpu_flags_mips(void);
 int ff_get_cpu_flags_aarch64(void);
 int ff_get_cpu_flags_arm(void);
 int ff_get_cpu_flags_ppc(void);
 int ff_get_cpu_flags_x86(void);
+int ff_get_cpu_flags_loongarch(void);
 
+size_t ff_get_cpu_max_align_mips(void);
 size_t ff_get_cpu_max_align_aarch64(void);
 size_t ff_get_cpu_max_align_arm(void);
 size_t ff_get_cpu_max_align_ppc(void);
 size_t ff_get_cpu_max_align_x86(void);
+size_t ff_get_cpu_max_align_loongarch(void);
 
 #endif /* AVUTIL_CPU_INTERNAL_H */
diff --git a/libavutil/intmath.h b/libavutil/intmath.h
index 9573109e9d..6d35a6f9e0 100644
--- a/libavutil/intmath.h
+++ b/libavutil/intmath.h
@@ -32,6 +32,9 @@
 #if ARCH_X86
 #   include "x86/intmath.h"
 #endif
+#if ARCH_LOONGARCH64
+#   include "loongarch/intmath.h"
+#endif
 
 #if HAVE_FAST_CLZ
 #if AV_GCC_VERSION_AT_LEAST(3,4)
diff --git a/libavutil/loongarch/Makefile b/libavutil/loongarch/Makefile
new file mode 100644
index 0000000000..2addd9351c
--- /dev/null
+++ b/libavutil/loongarch/Makefile
@@ -0,0 +1 @@
+OBJS += loongarch/cpu.o
diff --git a/libavutil/loongarch/cpu.c b/libavutil/loongarch/cpu.c
new file mode 100644
index 0000000000..e4b240bc44
--- /dev/null
+++ b/libavutil/loongarch/cpu.c
@@ -0,0 +1,69 @@
+/*
+ * Copyright (c) 2020 Loongson Technology Corporation Limited
+ * Contributed by Shiyou Yin <yinshiyou-hf@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include <stdint.h>
+#include "cpu.h"
+
+#define LOONGARCH_CFG2 0x2
+#define LOONGARCH_CFG2_LSX    (1 << 6)
+#define LOONGARCH_CFG2_LASX   (1 << 7)
+
+static int cpu_flags_cpucfg(void)
+{
+    int flags = 0;
+    uint32_t cfg2 = 0;
+
+    __asm__ volatile(
+        "cpucfg %0, %1 \n\t"
+        : "+&r"(cfg2)
+        : "r"(LOONGARCH_CFG2)
+    );
+
+    if (cfg2 & LOONGARCH_CFG2_LSX)
+        flags |= AV_CPU_FLAG_LSX;
+
+    if (cfg2 & LOONGARCH_CFG2_LASX)
+        flags |= AV_CPU_FLAG_LASX;
+
+    return flags;
+}
+
+int ff_get_cpu_flags_loongarch(void)
+{
+#if defined __linux__
+    return cpu_flags_cpucfg();
+#else
+    /* Assume no SIMD ASE supported */
+    return 0;
+#endif
+}
+
+size_t ff_get_cpu_max_align_loongarch(void)
+{
+    int flags = av_get_cpu_flags();
+
+    if (flags & AV_CPU_FLAG_LASX)
+        return 32;
+    if (flags & AV_CPU_FLAG_LSX)
+        return 16;
+
+    return 8;
+}
diff --git a/libavutil/loongarch/cpu.h b/libavutil/loongarch/cpu.h
new file mode 100644
index 0000000000..1a445c69bc
--- /dev/null
+++ b/libavutil/loongarch/cpu.h
@@ -0,0 +1,31 @@
+/*
+ * Copyright (c) 2020 Loongson Technology Corporation Limited
+ * Contributed by Shiyou Yin <yinshiyou-hf@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef AVUTIL_LOONGARCH_CPU_H
+#define AVUTIL_LOONGARCH_CPU_H
+
+#include "libavutil/cpu.h"
+#include "libavutil/cpu_internal.h"
+
+#define have_lsx(flags) CPUEXT(flags, LSX)
+#define have_lasx(flags) CPUEXT(flags, LASX)
+
+#endif /* AVUTIL_LOONGARCH_CPU_H */
diff --git a/libavutil/loongarch/intmath.h b/libavutil/loongarch/intmath.h
new file mode 100644
index 0000000000..c095b3a3e4
--- /dev/null
+++ b/libavutil/loongarch/intmath.h
@@ -0,0 +1,73 @@
+/*
+ * Copyright (c) 2021 Loongson Technology Corporation Limited
+ * Contributed by Hao Chen <chenhao@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef AVUTIL_LOONGARCH_INTMATH_H
+#define AVUTIL_LOONGARCH_INTMATH_H
+
+#include <stdint.h>
+#include <stdlib.h>
+#include "config.h"
+
+#if HAVE_FAST_CLZ
+
+static av_always_inline unsigned ff_loongarch_clz(unsigned _1)
+{
+    unsigned out;
+    __asm__ volatile (
+    "clz.w   %[out],  %[in]  \n\t"
+    : [out]"=&r"(out)
+    : [in]"r"(_1)
+    );
+    return out;
+}
+
+static av_always_inline int ff_loongarch_ctz_w(int _1)
+{
+    int out;
+    __asm__ volatile (
+    "ctz.w   %[out],  %[in]        \n\t"
+    "andi    %[out],  %[out],  31  \n\t"
+    : [out]"=&r"(out)
+    : [in]"r"(_1)
+    );
+    return out;
+}
+
+static av_always_inline int ff_loongarch_ctz_d(long long _1)
+{
+    int out;
+    __asm__ volatile (
+    "ctz.d   %[out],  %[in]        \n\t"
+    "andi    %[out],  %[out],  63  \n\t"
+    : [out]"=&r"(out)
+    : [in]"r"(_1)
+    );
+    return out;
+}
+
+#define ff_log2(x) (31 - ff_loongarch_clz((x)|1))
+
+#define ff_clz(x) ff_loongarch_clz(x)
+#define ff_ctz(x) ff_loongarch_ctz_w(x)
+#define ff_ctzll(x)  ff_loongarch_ctz_d(x)
+
+#endif /* HAVE_FAST_CLZ */
+#endif /* AVUTIL_LOONGARCH_INTMATH_H */
diff --git a/libavutil/loongarch/loongson_intrinsics.h b/libavutil/loongarch/loongson_intrinsics.h
new file mode 100644
index 0000000000..eb256863c8
--- /dev/null
+++ b/libavutil/loongarch/loongson_intrinsics.h
@@ -0,0 +1,1948 @@
+/*
+ * Copyright (c) 2021 Loongson Technology Corporation Limited
+ * All rights reserved.
+ * Contributed by Shiyou Yin <yinshiyou-hf@loongson.cn>
+ *                Xiwei Gu   <guxiwei-hf@loongson.cn>
+ *                Lu Wang    <wanglu@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ *
+ */
+
+#ifndef AVUTIL_LOONGARCH_LOONGSON_INTRINSICS_H
+#define AVUTIL_LOONGARCH_LOONGSON_INTRINSICS_H
+
+/*
+ * Copyright (c) 2021 Loongson Technology Corporation Limited
+ * All rights reserved.
+ * Contributed by Shiyou Yin <yinshiyou-hf@loongson.cn>
+ *                Xiwei Gu   <guxiwei-hf@loongson.cn>
+ *                Lu Wang    <wanglu@loongson.cn>
+ *
+ * This file is a header file for loongarch builtin extension.
+ *
+ */
+
+#ifndef LOONGSON_INTRINSICS_H
+#define LOONGSON_INTRINSICS_H
+
+/**
+ * MAJOR version: Macro usage changes.
+ * MINOR version: Add new functions, or bug fixes.
+ * MICRO version: Comment changes or implementation changes.
+ */
+#define LSOM_VERSION_MAJOR 1
+#define LSOM_VERSION_MINOR 1
+#define LSOM_VERSION_MICRO 0
+
+#define DUP2_ARG1(_INS, _IN0, _IN1, _OUT0, _OUT1) \
+  {                                               \
+    _OUT0 = _INS(_IN0);                           \
+    _OUT1 = _INS(_IN1);                           \
+  }
+
+#define DUP2_ARG2(_INS, _IN0, _IN1, _IN2, _IN3, _OUT0, _OUT1) \
+  {                                                           \
+    _OUT0 = _INS(_IN0, _IN1);                                 \
+    _OUT1 = _INS(_IN2, _IN3);                                 \
+  }
+
+#define DUP2_ARG3(_INS, _IN0, _IN1, _IN2, _IN3, _IN4, _IN5, _OUT0, _OUT1) \
+  {                                                                       \
+    _OUT0 = _INS(_IN0, _IN1, _IN2);                                       \
+    _OUT1 = _INS(_IN3, _IN4, _IN5);                                       \
+  }
+
+#define DUP4_ARG1(_INS, _IN0, _IN1, _IN2, _IN3, _OUT0, _OUT1, _OUT2, _OUT3) \
+  {                                                                         \
+    DUP2_ARG1(_INS, _IN0, _IN1, _OUT0, _OUT1);                              \
+    DUP2_ARG1(_INS, _IN2, _IN3, _OUT2, _OUT3);                              \
+  }
+
+#define DUP4_ARG2(_INS, _IN0, _IN1, _IN2, _IN3, _IN4, _IN5, _IN6, _IN7, _OUT0, \
+                  _OUT1, _OUT2, _OUT3)                                         \
+  {                                                                            \
+    DUP2_ARG2(_INS, _IN0, _IN1, _IN2, _IN3, _OUT0, _OUT1);                     \
+    DUP2_ARG2(_INS, _IN4, _IN5, _IN6, _IN7, _OUT2, _OUT3);                     \
+  }
+
+#define DUP4_ARG3(_INS, _IN0, _IN1, _IN2, _IN3, _IN4, _IN5, _IN6, _IN7, _IN8, \
+                  _IN9, _IN10, _IN11, _OUT0, _OUT1, _OUT2, _OUT3)             \
+  {                                                                           \
+    DUP2_ARG3(_INS, _IN0, _IN1, _IN2, _IN3, _IN4, _IN5, _OUT0, _OUT1);        \
+    DUP2_ARG3(_INS, _IN6, _IN7, _IN8, _IN9, _IN10, _IN11, _OUT2, _OUT3);      \
+  }
+
+#ifdef __loongarch_sx
+#include <lsxintrin.h>
+/*
+ * =============================================================================
+ * Description : Dot product & addition of byte vector elements
+ * Arguments   : Inputs  - in_c, in_h, in_l
+ *               Outputs - out
+ *               Return Type - halfword
+ * Details     : Signed byte elements from in_h are multiplied by
+ *               signed byte elements from in_l, and then added adjacent to
+ *               each other to get results with the twice size of input.
+ *               Then the results plus to signed half-word elements from in_c.
+ * Example     : out = __lsx_vdp2add_h_b(in_c, in_h, in_l)
+ *        in_c : 1,2,3,4, 1,2,3,4
+ *        in_h : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *        in_l : 8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1
+ *         out : 23,40,41,26, 23,40,41,26
+ * =============================================================================
+ */
+static inline __m128i __lsx_vdp2add_h_b(__m128i in_c, __m128i in_h,
+                                        __m128i in_l) {
+  __m128i out;
+
+  out = __lsx_vmaddwev_h_b(in_c, in_h, in_l);
+  out = __lsx_vmaddwod_h_b(out, in_h, in_l);
+  return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product & addition of byte vector elements
+ * Arguments   : Inputs  - in_c, in_h, in_l
+ *               Outputs - out
+ *               Return Type - halfword
+ * Details     : Unsigned byte elements from in_h are multiplied by
+ *               unsigned byte elements from in_l, and then added adjacent to
+ *               each other to get results with the twice size of input.
+ *               The results plus to signed half-word elements from in_c.
+ * Example     : out = __lsx_vdp2add_h_bu(in_c, in_h, in_l)
+ *        in_c : 1,2,3,4, 1,2,3,4
+ *        in_h : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *        in_l : 8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1
+ *         out : 23,40,41,26, 23,40,41,26
+ * =============================================================================
+ */
+static inline __m128i __lsx_vdp2add_h_bu(__m128i in_c, __m128i in_h,
+                                         __m128i in_l) {
+  __m128i out;
+
+  out = __lsx_vmaddwev_h_bu(in_c, in_h, in_l);
+  out = __lsx_vmaddwod_h_bu(out, in_h, in_l);
+  return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product & addition of byte vector elements
+ * Arguments   : Inputs  - in_c, in_h, in_l
+ *               Outputs - out
+ *               Return Type - halfword
+ * Details     : Unsigned byte elements from in_h are multiplied by
+ *               signed byte elements from in_l, and then added adjacent to
+ *               each other to get results with the twice size of input.
+ *               The results plus to signed half-word elements from in_c.
+ * Example     : out = __lsx_vdp2add_h_bu_b(in_c, in_h, in_l)
+ *        in_c : 1,1,1,1, 1,1,1,1
+ *        in_h : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *        in_l : -1,-2,-3,-4, -5,-6,-7,-8, 1,2,3,4, 5,6,7,8
+ *         out : -4,-24,-60,-112, 6,26,62,114
+ * =============================================================================
+ */
+static inline __m128i __lsx_vdp2add_h_bu_b(__m128i in_c, __m128i in_h,
+                                           __m128i in_l) {
+  __m128i out;
+
+  out = __lsx_vmaddwev_h_bu_b(in_c, in_h, in_l);
+  out = __lsx_vmaddwod_h_bu_b(out, in_h, in_l);
+  return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product & addition of half-word vector elements
+ * Arguments   : Inputs  - in_c, in_h, in_l
+ *               Outputs - out
+ *               Return Type - __m128i
+ * Details     : Signed half-word elements from in_h are multiplied by
+ *               signed half-word elements from in_l, and then added adjacent to
+ *               each other to get results with the twice size of input.
+ *               Then the results plus to signed word elements from in_c.
+ * Example     : out = __lsx_vdp2add_h_b(in_c, in_h, in_l)
+ *        in_c : 1,2,3,4
+ *        in_h : 1,2,3,4, 5,6,7,8
+ *        in_l : 8,7,6,5, 4,3,2,1
+ *         out : 23,40,41,26
+ * =============================================================================
+ */
+static inline __m128i __lsx_vdp2add_w_h(__m128i in_c, __m128i in_h,
+                                        __m128i in_l) {
+  __m128i out;
+
+  out = __lsx_vmaddwev_w_h(in_c, in_h, in_l);
+  out = __lsx_vmaddwod_w_h(out, in_h, in_l);
+  return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of byte vector elements
+ * Arguments   : Inputs  - in_h, in_l
+ *               Outputs - out
+ *               Return Type - halfword
+ * Details     : Signed byte elements from in_h are multiplied by
+ *               signed byte elements from in_l, and then added adjacent to
+ *               each other to get results with the twice size of input.
+ * Example     : out = __lsx_vdp2_h_b(in_h, in_l)
+ *        in_h : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *        in_l : 8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1
+ *         out : 22,38,38,22, 22,38,38,22
+ * =============================================================================
+ */
+static inline __m128i __lsx_vdp2_h_b(__m128i in_h, __m128i in_l) {
+  __m128i out;
+
+  out = __lsx_vmulwev_h_b(in_h, in_l);
+  out = __lsx_vmaddwod_h_b(out, in_h, in_l);
+  return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of byte vector elements
+ * Arguments   : Inputs  - in_h, in_l
+ *               Outputs - out
+ *               Return Type - halfword
+ * Details     : Unsigned byte elements from in_h are multiplied by
+ *               unsigned byte elements from in_l, and then added adjacent to
+ *               each other to get results with the twice size of input.
+ * Example     : out = __lsx_vdp2_h_bu(in_h, in_l)
+ *        in_h : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *        in_l : 8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1
+ *         out : 22,38,38,22, 22,38,38,22
+ * =============================================================================
+ */
+static inline __m128i __lsx_vdp2_h_bu(__m128i in_h, __m128i in_l) {
+  __m128i out;
+
+  out = __lsx_vmulwev_h_bu(in_h, in_l);
+  out = __lsx_vmaddwod_h_bu(out, in_h, in_l);
+  return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of byte vector elements
+ * Arguments   : Inputs  - in_h, in_l
+ *               Outputs - out
+ *               Return Type - halfword
+ * Details     : Unsigned byte elements from in_h are multiplied by
+ *               signed byte elements from in_l, and then added adjacent to
+ *               each other to get results with the twice size of input.
+ * Example     : out = __lsx_vdp2_h_bu_b(in_h, in_l)
+ *        in_h : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *        in_l : 8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,-1
+ *         out : 22,38,38,22, 22,38,38,6
+ * =============================================================================
+ */
+static inline __m128i __lsx_vdp2_h_bu_b(__m128i in_h, __m128i in_l) {
+  __m128i out;
+
+  out = __lsx_vmulwev_h_bu_b(in_h, in_l);
+  out = __lsx_vmaddwod_h_bu_b(out, in_h, in_l);
+  return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of byte vector elements
+ * Arguments   : Inputs  - in_h, in_l
+ *               Outputs - out
+ *               Return Type - halfword
+ * Details     : Signed byte elements from in_h are multiplied by
+ *               signed byte elements from in_l, and then added adjacent to
+ *               each other to get results with the twice size of input.
+ * Example     : out = __lsx_vdp2_w_h(in_h, in_l)
+ *        in_h : 1,2,3,4, 5,6,7,8
+ *        in_l : 8,7,6,5, 4,3,2,1
+ *         out : 22,38,38,22
+ * =============================================================================
+ */
+static inline __m128i __lsx_vdp2_w_h(__m128i in_h, __m128i in_l) {
+  __m128i out;
+
+  out = __lsx_vmulwev_w_h(in_h, in_l);
+  out = __lsx_vmaddwod_w_h(out, in_h, in_l);
+  return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Clip all halfword elements of input vector between min & max
+ *               out = ((_in) < (min)) ? (min) : (((_in) > (max)) ? (max) :
+ *               (_in))
+ * Arguments   : Inputs  - _in  (input vector)
+ *                       - min  (min threshold)
+ *                       - max  (max threshold)
+ *               Outputs - out  (output vector with clipped elements)
+ *               Return Type - signed halfword
+ * Example     : out = __lsx_vclip_h(_in)
+ *         _in : -8,2,280,249, -8,255,280,249
+ *         min : 1,1,1,1, 1,1,1,1
+ *         max : 9,9,9,9, 9,9,9,9
+ *         out : 1,2,9,9, 1,9,9,9
+ * =============================================================================
+ */
+static inline __m128i __lsx_vclip_h(__m128i _in, __m128i min, __m128i max) {
+  __m128i out;
+
+  out = __lsx_vmax_h(min, _in);
+  out = __lsx_vmin_h(max, out);
+  return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Set each element of vector between 0 and 255
+ * Arguments   : Inputs  - _in
+ *               Outputs - out
+ *               Return Type - halfword
+ * Details     : Signed byte elements from _in are clamped between 0 and 255.
+ * Example     : out = __lsx_vclip255_h(_in)
+ *         _in : -8,255,280,249, -8,255,280,249
+ *         out : 0,255,255,249, 0,255,255,249
+ * =============================================================================
+ */
+static inline __m128i __lsx_vclip255_h(__m128i _in) {
+  __m128i out;
+
+  out = __lsx_vmaxi_h(_in, 0);
+  out = __lsx_vsat_hu(out, 7);
+  return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Set each element of vector between 0 and 255
+ * Arguments   : Inputs  - _in
+ *               Outputs - out
+ *               Return Type - word
+ * Details     : Signed byte elements from _in are clamped between 0 and 255.
+ * Example     : out = __lsx_vclip255_w(_in)
+ *         _in : -8,255,280,249
+ *         out : 0,255,255,249
+ * =============================================================================
+ */
+static inline __m128i __lsx_vclip255_w(__m128i _in) {
+  __m128i out;
+
+  out = __lsx_vmaxi_w(_in, 0);
+  out = __lsx_vsat_wu(out, 7);
+  return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Swap two variables
+ * Arguments   : Inputs  - _in0, _in1
+ *               Outputs - _in0, _in1 (in-place)
+ * Details     : Swapping of two input variables using xor
+ * Example     : LSX_SWAP(_in0, _in1)
+ *        _in0 : 1,2,3,4
+ *        _in1 : 5,6,7,8
+ *   _in0(out) : 5,6,7,8
+ *   _in1(out) : 1,2,3,4
+ * =============================================================================
+ */
+#define LSX_SWAP(_in0, _in1)         \
+  {                                  \
+    _in0 = __lsx_vxor_v(_in0, _in1); \
+    _in1 = __lsx_vxor_v(_in0, _in1); \
+    _in0 = __lsx_vxor_v(_in0, _in1); \
+  }
+
+/*
+ * =============================================================================
+ * Description : Transpose 4x4 block with word elements in vectors
+ * Arguments   : Inputs  - in0, in1, in2, in3
+ *               Outputs - out0, out1, out2, out3
+ * Details     :
+ * Example     :
+ *               1, 2, 3, 4            1, 5, 9,13
+ *               5, 6, 7, 8    to      2, 6,10,14
+ *               9,10,11,12  =====>    3, 7,11,15
+ *              13,14,15,16            4, 8,12,16
+ * =============================================================================
+ */
+#define LSX_TRANSPOSE4x4_W(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
+  {                                                                            \
+    __m128i _t0, _t1, _t2, _t3;                                                \
+                                                                               \
+    _t0 = __lsx_vilvl_w(_in1, _in0);                                           \
+    _t1 = __lsx_vilvh_w(_in1, _in0);                                           \
+    _t2 = __lsx_vilvl_w(_in3, _in2);                                           \
+    _t3 = __lsx_vilvh_w(_in3, _in2);                                           \
+    _out0 = __lsx_vilvl_d(_t2, _t0);                                           \
+    _out1 = __lsx_vilvh_d(_t2, _t0);                                           \
+    _out2 = __lsx_vilvl_d(_t3, _t1);                                           \
+    _out3 = __lsx_vilvh_d(_t3, _t1);                                           \
+  }
+
+/*
+ * =============================================================================
+ * Description : Transpose 8x8 block with byte elements in vectors
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7
+ *               Outputs - _out0, _out1, _out2, _out3, _out4, _out5, _out6,
+ *               _out7
+ * Details     : The rows of the matrix become columns, and the columns
+ *               become rows.
+ * Example     : LSX_TRANSPOSE8x8_B
+ *        _in0 : 00,01,02,03,04,05,06,07, 00,00,00,00,00,00,00,00
+ *        _in1 : 10,11,12,13,14,15,16,17, 00,00,00,00,00,00,00,00
+ *        _in2 : 20,21,22,23,24,25,26,27, 00,00,00,00,00,00,00,00
+ *        _in3 : 30,31,32,33,34,35,36,37, 00,00,00,00,00,00,00,00
+ *        _in4 : 40,41,42,43,44,45,46,47, 00,00,00,00,00,00,00,00
+ *        _in5 : 50,51,52,53,54,55,56,57, 00,00,00,00,00,00,00,00
+ *        _in6 : 60,61,62,63,64,65,66,67, 00,00,00,00,00,00,00,00
+ *        _in7 : 70,71,72,73,74,75,76,77, 00,00,00,00,00,00,00,00
+ *
+ *      _ out0 : 00,10,20,30,40,50,60,70, 00,00,00,00,00,00,00,00
+ *      _ out1 : 01,11,21,31,41,51,61,71, 00,00,00,00,00,00,00,00
+ *      _ out2 : 02,12,22,32,42,52,62,72, 00,00,00,00,00,00,00,00
+ *      _ out3 : 03,13,23,33,43,53,63,73, 00,00,00,00,00,00,00,00
+ *      _ out4 : 04,14,24,34,44,54,64,74, 00,00,00,00,00,00,00,00
+ *      _ out5 : 05,15,25,35,45,55,65,75, 00,00,00,00,00,00,00,00
+ *      _ out6 : 06,16,26,36,46,56,66,76, 00,00,00,00,00,00,00,00
+ *      _ out7 : 07,17,27,37,47,57,67,77, 00,00,00,00,00,00,00,00
+ * =============================================================================
+ */
+#define LSX_TRANSPOSE8x8_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,  \
+                           _out0, _out1, _out2, _out3, _out4, _out5, _out6, \
+                           _out7)                                           \
+  {                                                                         \
+    __m128i zero = { 0 };                                                   \
+    __m128i shuf8 = { 0x0F0E0D0C0B0A0908, 0x1716151413121110 };             \
+    __m128i _t0, _t1, _t2, _t3, _t4, _t5, _t6, _t7;                         \
+                                                                            \
+    _t0 = __lsx_vilvl_b(_in2, _in0);                                        \
+    _t1 = __lsx_vilvl_b(_in3, _in1);                                        \
+    _t2 = __lsx_vilvl_b(_in6, _in4);                                        \
+    _t3 = __lsx_vilvl_b(_in7, _in5);                                        \
+    _t4 = __lsx_vilvl_b(_t1, _t0);                                          \
+    _t5 = __lsx_vilvh_b(_t1, _t0);                                          \
+    _t6 = __lsx_vilvl_b(_t3, _t2);                                          \
+    _t7 = __lsx_vilvh_b(_t3, _t2);                                          \
+    _out0 = __lsx_vilvl_w(_t6, _t4);                                        \
+    _out2 = __lsx_vilvh_w(_t6, _t4);                                        \
+    _out4 = __lsx_vilvl_w(_t7, _t5);                                        \
+    _out6 = __lsx_vilvh_w(_t7, _t5);                                        \
+    _out1 = __lsx_vshuf_b(zero, _out0, shuf8);                              \
+    _out3 = __lsx_vshuf_b(zero, _out2, shuf8);                              \
+    _out5 = __lsx_vshuf_b(zero, _out4, shuf8);                              \
+    _out7 = __lsx_vshuf_b(zero, _out6, shuf8);                              \
+  }
+
+/*
+ * =============================================================================
+ * Description : Transpose 8x8 block with half-word elements in vectors
+ * Arguments   : Inputs  - in0, in1, in2, in3, in4, in5, in6, in7
+ *               Outputs - out0, out1, out2, out3, out4, out5, out6, out7
+ * Details     :
+ * Example     :
+ *              00,01,02,03,04,05,06,07           00,10,20,30,40,50,60,70
+ *              10,11,12,13,14,15,16,17           01,11,21,31,41,51,61,71
+ *              20,21,22,23,24,25,26,27           02,12,22,32,42,52,62,72
+ *              30,31,32,33,34,35,36,37    to     03,13,23,33,43,53,63,73
+ *              40,41,42,43,44,45,46,47  ======>  04,14,24,34,44,54,64,74
+ *              50,51,52,53,54,55,56,57           05,15,25,35,45,55,65,75
+ *              60,61,62,63,64,65,66,67           06,16,26,36,46,56,66,76
+ *              70,71,72,73,74,75,76,77           07,17,27,37,47,57,67,77
+ * =============================================================================
+ */
+#define LSX_TRANSPOSE8x8_H(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,  \
+                           _out0, _out1, _out2, _out3, _out4, _out5, _out6, \
+                           _out7)                                           \
+  {                                                                         \
+    __m128i _s0, _s1, _t0, _t1, _t2, _t3, _t4, _t5, _t6, _t7;               \
+                                                                            \
+    _s0 = __lsx_vilvl_h(_in6, _in4);                                        \
+    _s1 = __lsx_vilvl_h(_in7, _in5);                                        \
+    _t0 = __lsx_vilvl_h(_s1, _s0);                                          \
+    _t1 = __lsx_vilvh_h(_s1, _s0);                                          \
+    _s0 = __lsx_vilvh_h(_in6, _in4);                                        \
+    _s1 = __lsx_vilvh_h(_in7, _in5);                                        \
+    _t2 = __lsx_vilvl_h(_s1, _s0);                                          \
+    _t3 = __lsx_vilvh_h(_s1, _s0);                                          \
+    _s0 = __lsx_vilvl_h(_in2, _in0);                                        \
+    _s1 = __lsx_vilvl_h(_in3, _in1);                                        \
+    _t4 = __lsx_vilvl_h(_s1, _s0);                                          \
+    _t5 = __lsx_vilvh_h(_s1, _s0);                                          \
+    _s0 = __lsx_vilvh_h(_in2, _in0);                                        \
+    _s1 = __lsx_vilvh_h(_in3, _in1);                                        \
+    _t6 = __lsx_vilvl_h(_s1, _s0);                                          \
+    _t7 = __lsx_vilvh_h(_s1, _s0);                                          \
+                                                                            \
+    _out0 = __lsx_vpickev_d(_t0, _t4);                                      \
+    _out2 = __lsx_vpickev_d(_t1, _t5);                                      \
+    _out4 = __lsx_vpickev_d(_t2, _t6);                                      \
+    _out6 = __lsx_vpickev_d(_t3, _t7);                                      \
+    _out1 = __lsx_vpickod_d(_t0, _t4);                                      \
+    _out3 = __lsx_vpickod_d(_t1, _t5);                                      \
+    _out5 = __lsx_vpickod_d(_t2, _t6);                                      \
+    _out7 = __lsx_vpickod_d(_t3, _t7);                                      \
+  }
+
+/*
+ * =============================================================================
+ * Description : Transpose input 8x4 byte block into 4x8
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3      (input 8x4 byte block)
+ *               Outputs - _out0, _out1, _out2, _out3  (output 4x8 byte block)
+ *               Return Type - as per RTYPE
+ * Details     : The rows of the matrix become columns, and the columns become
+ *               rows.
+ * Example     : LSX_TRANSPOSE8x4_B
+ *        _in0 : 00,01,02,03,00,00,00,00, 00,00,00,00,00,00,00,00
+ *        _in1 : 10,11,12,13,00,00,00,00, 00,00,00,00,00,00,00,00
+ *        _in2 : 20,21,22,23,00,00,00,00, 00,00,00,00,00,00,00,00
+ *        _in3 : 30,31,32,33,00,00,00,00, 00,00,00,00,00,00,00,00
+ *        _in4 : 40,41,42,43,00,00,00,00, 00,00,00,00,00,00,00,00
+ *        _in5 : 50,51,52,53,00,00,00,00, 00,00,00,00,00,00,00,00
+ *        _in6 : 60,61,62,63,00,00,00,00, 00,00,00,00,00,00,00,00
+ *        _in7 : 70,71,72,73,00,00,00,00, 00,00,00,00,00,00,00,00
+ *
+ *       _out0 : 00,10,20,30,40,50,60,70, 00,00,00,00,00,00,00,00
+ *       _out1 : 01,11,21,31,41,51,61,71, 00,00,00,00,00,00,00,00
+ *       _out2 : 02,12,22,32,42,52,62,72, 00,00,00,00,00,00,00,00
+ *       _out3 : 03,13,23,33,43,53,63,73, 00,00,00,00,00,00,00,00
+ * =============================================================================
+ */
+#define LSX_TRANSPOSE8x4_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7, \
+                           _out0, _out1, _out2, _out3)                     \
+  {                                                                        \
+    __m128i _tmp0_m, _tmp1_m, _tmp2_m, _tmp3_m;                            \
+                                                                           \
+    _tmp0_m = __lsx_vpackev_w(_in4, _in0);                                 \
+    _tmp1_m = __lsx_vpackev_w(_in5, _in1);                                 \
+    _tmp2_m = __lsx_vilvl_b(_tmp1_m, _tmp0_m);                             \
+    _tmp0_m = __lsx_vpackev_w(_in6, _in2);                                 \
+    _tmp1_m = __lsx_vpackev_w(_in7, _in3);                                 \
+                                                                           \
+    _tmp3_m = __lsx_vilvl_b(_tmp1_m, _tmp0_m);                             \
+    _tmp0_m = __lsx_vilvl_h(_tmp3_m, _tmp2_m);                             \
+    _tmp1_m = __lsx_vilvh_h(_tmp3_m, _tmp2_m);                             \
+                                                                           \
+    _out0 = __lsx_vilvl_w(_tmp1_m, _tmp0_m);                               \
+    _out2 = __lsx_vilvh_w(_tmp1_m, _tmp0_m);                               \
+    _out1 = __lsx_vilvh_d(_out2, _out0);                                   \
+    _out3 = __lsx_vilvh_d(_out0, _out2);                                   \
+  }
+
+/*
+ * =============================================================================
+ * Description : Transpose 16x8 block with byte elements in vectors
+ * Arguments   : Inputs  - in0, in1, in2, in3, in4, in5, in6, in7, in8
+ *                         in9, in10, in11, in12, in13, in14, in15
+ *               Outputs - out0, out1, out2, out3, out4, out5, out6, out7
+ * Details     :
+ * Example     :
+ *              000,001,002,003,004,005,006,007
+ *              008,009,010,011,012,013,014,015
+ *              016,017,018,019,020,021,022,023
+ *              024,025,026,027,028,029,030,031
+ *              032,033,034,035,036,037,038,039
+ *              040,041,042,043,044,045,046,047        000,008,...,112,120
+ *              048,049,050,051,052,053,054,055        001,009,...,113,121
+ *              056,057,058,059,060,061,062,063   to   002,010,...,114,122
+ *              064,068,066,067,068,069,070,071 =====> 003,011,...,115,123
+ *              072,073,074,075,076,077,078,079        004,012,...,116,124
+ *              080,081,082,083,084,085,086,087        005,013,...,117,125
+ *              088,089,090,091,092,093,094,095        006,014,...,118,126
+ *              096,097,098,099,100,101,102,103        007,015,...,119,127
+ *              104,105,106,107,108,109,110,111
+ *              112,113,114,115,116,117,118,119
+ *              120,121,122,123,124,125,126,127
+ * =============================================================================
+ */
+#define LSX_TRANSPOSE16x8_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,  \
+                            _in8, _in9, _in10, _in11, _in12, _in13, _in14,   \
+                            _in15, _out0, _out1, _out2, _out3, _out4, _out5, \
+                            _out6, _out7)                                    \
+  {                                                                          \
+    __m128i _tmp0, _tmp1, _tmp2, _tmp3, _tmp4, _tmp5, _tmp6, _tmp7;          \
+    __m128i _t0, _t1, _t2, _t3, _t4, _t5, _t6, _t7;                          \
+    DUP4_ARG2(__lsx_vilvl_b, _in2, _in0, _in3, _in1, _in6, _in4, _in7, _in5, \
+              _tmp0, _tmp1, _tmp2, _tmp3);                                   \
+    DUP4_ARG2(__lsx_vilvl_b, _in10, _in8, _in11, _in9, _in14, _in12, _in15,  \
+              _in13, _tmp4, _tmp5, _tmp6, _tmp7);                            \
+    DUP2_ARG2(__lsx_vilvl_b, _tmp1, _tmp0, _tmp3, _tmp2, _t0, _t2);          \
+    DUP2_ARG2(__lsx_vilvh_b, _tmp1, _tmp0, _tmp3, _tmp2, _t1, _t3);          \
+    DUP2_ARG2(__lsx_vilvl_b, _tmp5, _tmp4, _tmp7, _tmp6, _t4, _t6);          \
+    DUP2_ARG2(__lsx_vilvh_b, _tmp5, _tmp4, _tmp7, _tmp6, _t5, _t7);          \
+    DUP2_ARG2(__lsx_vilvl_w, _t2, _t0, _t3, _t1, _tmp0, _tmp4);              \
+    DUP2_ARG2(__lsx_vilvh_w, _t2, _t0, _t3, _t1, _tmp2, _tmp6);              \
+    DUP2_ARG2(__lsx_vilvl_w, _t6, _t4, _t7, _t5, _tmp1, _tmp5);              \
+    DUP2_ARG2(__lsx_vilvh_w, _t6, _t4, _t7, _t5, _tmp3, _tmp7);              \
+    DUP2_ARG2(__lsx_vilvl_d, _tmp1, _tmp0, _tmp3, _tmp2, _out0, _out2);      \
+    DUP2_ARG2(__lsx_vilvh_d, _tmp1, _tmp0, _tmp3, _tmp2, _out1, _out3);      \
+    DUP2_ARG2(__lsx_vilvl_d, _tmp5, _tmp4, _tmp7, _tmp6, _out4, _out6);      \
+    DUP2_ARG2(__lsx_vilvh_d, _tmp5, _tmp4, _tmp7, _tmp6, _out5, _out7);      \
+  }
+
+/*
+ * =============================================================================
+ * Description : Butterfly of 4 input vectors
+ * Arguments   : Inputs  - in0, in1, in2, in3
+ *               Outputs - out0, out1, out2, out3
+ * Details     : Butterfly operation
+ * Example     :
+ *               out0 = in0 + in3;
+ *               out1 = in1 + in2;
+ *               out2 = in1 - in2;
+ *               out3 = in0 - in3;
+ * =============================================================================
+ */
+#define LSX_BUTTERFLY_4_B(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
+  {                                                                           \
+    _out0 = __lsx_vadd_b(_in0, _in3);                                         \
+    _out1 = __lsx_vadd_b(_in1, _in2);                                         \
+    _out2 = __lsx_vsub_b(_in1, _in2);                                         \
+    _out3 = __lsx_vsub_b(_in0, _in3);                                         \
+  }
+#define LSX_BUTTERFLY_4_H(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
+  {                                                                           \
+    _out0 = __lsx_vadd_h(_in0, _in3);                                         \
+    _out1 = __lsx_vadd_h(_in1, _in2);                                         \
+    _out2 = __lsx_vsub_h(_in1, _in2);                                         \
+    _out3 = __lsx_vsub_h(_in0, _in3);                                         \
+  }
+#define LSX_BUTTERFLY_4_W(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
+  {                                                                           \
+    _out0 = __lsx_vadd_w(_in0, _in3);                                         \
+    _out1 = __lsx_vadd_w(_in1, _in2);                                         \
+    _out2 = __lsx_vsub_w(_in1, _in2);                                         \
+    _out3 = __lsx_vsub_w(_in0, _in3);                                         \
+  }
+#define LSX_BUTTERFLY_4_D(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
+  {                                                                           \
+    _out0 = __lsx_vadd_d(_in0, _in3);                                         \
+    _out1 = __lsx_vadd_d(_in1, _in2);                                         \
+    _out2 = __lsx_vsub_d(_in1, _in2);                                         \
+    _out3 = __lsx_vsub_d(_in0, _in3);                                         \
+  }
+
+/*
+ * =============================================================================
+ * Description : Butterfly of 8 input vectors
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3, ~
+ *               Outputs - _out0, _out1, _out2, _out3, ~
+ * Details     : Butterfly operation
+ * Example     :
+ *              _out0 = _in0 + _in7;
+ *              _out1 = _in1 + _in6;
+ *              _out2 = _in2 + _in5;
+ *              _out3 = _in3 + _in4;
+ *              _out4 = _in3 - _in4;
+ *              _out5 = _in2 - _in5;
+ *              _out6 = _in1 - _in6;
+ *              _out7 = _in0 - _in7;
+ * =============================================================================
+ */
+#define LSX_BUTTERFLY_8_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,  \
+                          _out0, _out1, _out2, _out3, _out4, _out5, _out6, \
+                          _out7)                                           \
+  {                                                                        \
+    _out0 = __lsx_vadd_b(_in0, _in7);                                      \
+    _out1 = __lsx_vadd_b(_in1, _in6);                                      \
+    _out2 = __lsx_vadd_b(_in2, _in5);                                      \
+    _out3 = __lsx_vadd_b(_in3, _in4);                                      \
+    _out4 = __lsx_vsub_b(_in3, _in4);                                      \
+    _out5 = __lsx_vsub_b(_in2, _in5);                                      \
+    _out6 = __lsx_vsub_b(_in1, _in6);                                      \
+    _out7 = __lsx_vsub_b(_in0, _in7);                                      \
+  }
+
+#define LSX_BUTTERFLY_8_H(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,  \
+                          _out0, _out1, _out2, _out3, _out4, _out5, _out6, \
+                          _out7)                                           \
+  {                                                                        \
+    _out0 = __lsx_vadd_h(_in0, _in7);                                      \
+    _out1 = __lsx_vadd_h(_in1, _in6);                                      \
+    _out2 = __lsx_vadd_h(_in2, _in5);                                      \
+    _out3 = __lsx_vadd_h(_in3, _in4);                                      \
+    _out4 = __lsx_vsub_h(_in3, _in4);                                      \
+    _out5 = __lsx_vsub_h(_in2, _in5);                                      \
+    _out6 = __lsx_vsub_h(_in1, _in6);                                      \
+    _out7 = __lsx_vsub_h(_in0, _in7);                                      \
+  }
+
+#define LSX_BUTTERFLY_8_W(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,  \
+                          _out0, _out1, _out2, _out3, _out4, _out5, _out6, \
+                          _out7)                                           \
+  {                                                                        \
+    _out0 = __lsx_vadd_w(_in0, _in7);                                      \
+    _out1 = __lsx_vadd_w(_in1, _in6);                                      \
+    _out2 = __lsx_vadd_w(_in2, _in5);                                      \
+    _out3 = __lsx_vadd_w(_in3, _in4);                                      \
+    _out4 = __lsx_vsub_w(_in3, _in4);                                      \
+    _out5 = __lsx_vsub_w(_in2, _in5);                                      \
+    _out6 = __lsx_vsub_w(_in1, _in6);                                      \
+    _out7 = __lsx_vsub_w(_in0, _in7);                                      \
+  }
+
+#define LSX_BUTTERFLY_8_D(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,  \
+                          _out0, _out1, _out2, _out3, _out4, _out5, _out6, \
+                          _out7)                                           \
+  {                                                                        \
+    _out0 = __lsx_vadd_d(_in0, _in7);                                      \
+    _out1 = __lsx_vadd_d(_in1, _in6);                                      \
+    _out2 = __lsx_vadd_d(_in2, _in5);                                      \
+    _out3 = __lsx_vadd_d(_in3, _in4);                                      \
+    _out4 = __lsx_vsub_d(_in3, _in4);                                      \
+    _out5 = __lsx_vsub_d(_in2, _in5);                                      \
+    _out6 = __lsx_vsub_d(_in1, _in6);                                      \
+    _out7 = __lsx_vsub_d(_in0, _in7);                                      \
+  }
+
+#endif  // LSX
+
+#ifdef __loongarch_asx
+#include <lasxintrin.h>
+/*
+ * =============================================================================
+ * Description : Dot product of byte vector elements
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ *               Return Type - signed halfword
+ * Details     : Unsigned byte elements from in_h are multiplied with
+ *               unsigned byte elements from in_l producing a result
+ *               twice the size of input i.e. signed halfword.
+ *               Then this multiplied results of adjacent odd-even elements
+ *               are added to the out vector
+ * Example     : See out = __lasx_xvdp2_w_h(in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2_h_bu(__m256i in_h, __m256i in_l) {
+  __m256i out;
+
+  out = __lasx_xvmulwev_h_bu(in_h, in_l);
+  out = __lasx_xvmaddwod_h_bu(out, in_h, in_l);
+  return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of byte vector elements
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ *               Return Type - signed halfword
+ * Details     : Signed byte elements from in_h are multiplied with
+ *               signed byte elements from in_l producing a result
+ *               twice the size of input i.e. signed halfword.
+ *               Then this multiplication results of adjacent odd-even elements
+ *               are added to the out vector
+ * Example     : See out = __lasx_xvdp2_w_h(in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2_h_b(__m256i in_h, __m256i in_l) {
+  __m256i out;
+
+  out = __lasx_xvmulwev_h_b(in_h, in_l);
+  out = __lasx_xvmaddwod_h_b(out, in_h, in_l);
+  return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of halfword vector elements
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ *               Return Type - signed word
+ * Details     : Signed halfword elements from in_h are multiplied with
+ *               signed halfword elements from in_l producing a result
+ *               twice the size of input i.e. signed word.
+ *               Then this multiplied results of adjacent odd-even elements
+ *               are added to the out vector.
+ * Example     : out = __lasx_xvdp2_w_h(in_h, in_l)
+ *        in_h : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *        in_l : 8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1
+ *         out : 22,38,38,22, 22,38,38,22
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2_w_h(__m256i in_h, __m256i in_l) {
+  __m256i out;
+
+  out = __lasx_xvmulwev_w_h(in_h, in_l);
+  out = __lasx_xvmaddwod_w_h(out, in_h, in_l);
+  return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of word vector elements
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ *               Return Type - signed double
+ * Details     : Signed word elements from in_h are multiplied with
+ *               signed word elements from in_l producing a result
+ *               twice the size of input i.e. signed double-word.
+ *               Then this multiplied results of adjacent odd-even elements
+ *               are added to the out vector.
+ * Example     : See out = __lasx_xvdp2_w_h(in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2_d_w(__m256i in_h, __m256i in_l) {
+  __m256i out;
+
+  out = __lasx_xvmulwev_d_w(in_h, in_l);
+  out = __lasx_xvmaddwod_d_w(out, in_h, in_l);
+  return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of halfword vector elements
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ *               Return Type - signed word
+ * Details     : Unsigned halfword elements from in_h are multiplied with
+ *               signed halfword elements from in_l producing a result
+ *               twice the size of input i.e. unsigned word.
+ *               Multiplication result of adjacent odd-even elements
+ *               are added to the out vector
+ * Example     : See out = __lasx_xvdp2_w_h(in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2_w_hu_h(__m256i in_h, __m256i in_l) {
+  __m256i out;
+
+  out = __lasx_xvmulwev_w_hu_h(in_h, in_l);
+  out = __lasx_xvmaddwod_w_hu_h(out, in_h, in_l);
+  return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product & addition of byte vector elements
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ *               Return Type - halfword
+ * Details     : Signed byte elements from in_h are multiplied with
+ *               signed byte elements from in_l producing a result
+ *               twice the size of input i.e. signed halfword.
+ *               Then this multiplied results of adjacent odd-even elements
+ *               are added to the in_c vector.
+ * Example     : See out = __lasx_xvdp2add_w_h(in_c, in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2add_h_b(__m256i in_c, __m256i in_h,
+                                          __m256i in_l) {
+  __m256i out;
+
+  out = __lasx_xvmaddwev_h_b(in_c, in_h, in_l);
+  out = __lasx_xvmaddwod_h_b(out, in_h, in_l);
+  return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product & addition of byte vector elements
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ *               Return Type - halfword
+ * Details     : Unsigned byte elements from in_h are multiplied with
+ *               unsigned byte elements from in_l producing a result
+ *               twice the size of input i.e. signed halfword.
+ *               Then this multiplied results of adjacent odd-even elements
+ *               are added to the in_c vector.
+ * Example     : See out = __lasx_xvdp2add_w_h(in_c, in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2add_h_bu(__m256i in_c, __m256i in_h,
+                                           __m256i in_l) {
+  __m256i out;
+
+  out = __lasx_xvmaddwev_h_bu(in_c, in_h, in_l);
+  out = __lasx_xvmaddwod_h_bu(out, in_h, in_l);
+  return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product & addition of byte vector elements
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ *               Return Type - halfword
+ * Details     : Unsigned byte elements from in_h are multiplied with
+ *               signed byte elements from in_l producing a result
+ *               twice the size of input i.e. signed halfword.
+ *               Then this multiplied results of adjacent odd-even elements
+ *               are added to the in_c vector.
+ * Example     : See out = __lasx_xvdp2add_w_h(in_c, in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2add_h_bu_b(__m256i in_c, __m256i in_h,
+                                             __m256i in_l) {
+  __m256i out;
+
+  out = __lasx_xvmaddwev_h_bu_b(in_c, in_h, in_l);
+  out = __lasx_xvmaddwod_h_bu_b(out, in_h, in_l);
+  return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of halfword vector elements
+ * Arguments   : Inputs - in_c, in_h, in_l
+ *               Output - out
+ *               Return Type - per RTYPE
+ * Details     : Signed halfword elements from in_h are multiplied with
+ *               signed halfword elements from in_l producing a result
+ *               twice the size of input i.e. signed word.
+ *               Multiplication result of adjacent odd-even elements
+ *               are added to the in_c vector.
+ * Example     : out = __lasx_xvdp2add_w_h(in_c, in_h, in_l)
+ *        in_c : 1,2,3,4, 1,2,3,4
+ *        in_h : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8,
+ *        in_l : 8,7,6,5, 4,3,2,1, 8,7,6,5, 4,3,2,1,
+ *         out : 23,40,41,26, 23,40,41,26
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2add_w_h(__m256i in_c, __m256i in_h,
+                                          __m256i in_l) {
+  __m256i out;
+
+  out = __lasx_xvmaddwev_w_h(in_c, in_h, in_l);
+  out = __lasx_xvmaddwod_w_h(out, in_h, in_l);
+  return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of halfword vector elements
+ * Arguments   : Inputs - in_c, in_h, in_l
+ *               Output - out
+ *               Return Type - signed word
+ * Details     : Unsigned halfword elements from in_h are multiplied with
+ *               unsigned halfword elements from in_l producing a result
+ *               twice the size of input i.e. signed word.
+ *               Multiplication result of adjacent odd-even elements
+ *               are added to the in_c vector.
+ * Example     : See out = __lasx_xvdp2add_w_h(in_c, in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2add_w_hu(__m256i in_c, __m256i in_h,
+                                           __m256i in_l) {
+  __m256i out;
+
+  out = __lasx_xvmaddwev_w_hu(in_c, in_h, in_l);
+  out = __lasx_xvmaddwod_w_hu(out, in_h, in_l);
+  return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of halfword vector elements
+ * Arguments   : Inputs - in_c, in_h, in_l
+ *               Output - out
+ *               Return Type - signed word
+ * Details     : Unsigned halfword elements from in_h are multiplied with
+ *               signed halfword elements from in_l producing a result
+ *               twice the size of input i.e. signed word.
+ *               Multiplication result of adjacent odd-even elements
+ *               are added to the in_c vector
+ * Example     : See out = __lasx_xvdp2add_w_h(in_c, in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2add_w_hu_h(__m256i in_c, __m256i in_h,
+                                             __m256i in_l) {
+  __m256i out;
+
+  out = __lasx_xvmaddwev_w_hu_h(in_c, in_h, in_l);
+  out = __lasx_xvmaddwod_w_hu_h(out, in_h, in_l);
+  return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Vector Unsigned Dot Product and Subtract
+ * Arguments   : Inputs - in_c, in_h, in_l
+ *               Output - out
+ *               Return Type - signed halfword
+ * Details     : Unsigned byte elements from in_h are multiplied with
+ *               unsigned byte elements from in_l producing a result
+ *               twice the size of input i.e. signed halfword.
+ *               Multiplication result of adjacent odd-even elements
+ *               are added together and subtracted from double width elements
+ *               in_c vector.
+ * Example     : See out = __lasx_xvdp2sub_w_h(in_c, in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2sub_h_bu(__m256i in_c, __m256i in_h,
+                                           __m256i in_l) {
+  __m256i out;
+
+  out = __lasx_xvmulwev_h_bu(in_h, in_l);
+  out = __lasx_xvmaddwod_h_bu(out, in_h, in_l);
+  out = __lasx_xvsub_h(in_c, out);
+  return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Vector Signed Dot Product and Subtract
+ * Arguments   : Inputs - in_c, in_h, in_l
+ *               Output - out
+ *               Return Type - signed word
+ * Details     : Signed halfword elements from in_h are multiplied with
+ *               Signed halfword elements from in_l producing a result
+ *               twice the size of input i.e. signed word.
+ *               Multiplication result of adjacent odd-even elements
+ *               are added together and subtracted from double width elements
+ *               in_c vector.
+ * Example     : out = __lasx_xvdp2sub_w_h(in_c, in_h, in_l)
+ *        in_c : 0,0,0,0, 0,0,0,0
+ *        in_h : 3,1,3,0, 0,0,0,1, 0,0,1,1, 0,0,0,1
+ *        in_l : 2,1,1,0, 1,0,0,0, 0,0,1,0, 1,0,0,1
+ *         out : -7,-3,0,0, 0,-1,0,-1
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp2sub_w_h(__m256i in_c, __m256i in_h,
+                                          __m256i in_l) {
+  __m256i out;
+
+  out = __lasx_xvmulwev_w_h(in_h, in_l);
+  out = __lasx_xvmaddwod_w_h(out, in_h, in_l);
+  out = __lasx_xvsub_w(in_c, out);
+  return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Dot product of halfword vector elements
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ *               Return Type - signed word
+ * Details     : Signed halfword elements from in_h are multiplied with
+ *               signed halfword elements from in_l producing a result
+ *               four times the size of input i.e. signed doubleword.
+ *               Then this multiplication results of four adjacent elements
+ *               are added together and stored to the out vector.
+ * Example     : out = __lasx_xvdp4_d_h(in_h, in_l)
+ *        in_h :  3,1,3,0, 0,0,0,1, 0,0,1,-1, 0,0,0,1
+ *        in_l : -2,1,1,0, 1,0,0,0, 0,0,1, 0, 1,0,0,1
+ *         out : -2,0,1,1
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvdp4_d_h(__m256i in_h, __m256i in_l) {
+  __m256i out;
+
+  out = __lasx_xvmulwev_w_h(in_h, in_l);
+  out = __lasx_xvmaddwod_w_h(out, in_h, in_l);
+  out = __lasx_xvhaddw_d_w(out, out);
+  return out;
+}
+
+/*
+ * =============================================================================
+ * Description : The high half of the vector elements are expanded and
+ *               added after being doubled.
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ * Details     : The in_h vector and the in_l vector are added after the
+ *               higher half of the two-fold sign extension (signed byte
+ *               to signed halfword) and stored to the out vector.
+ * Example     : See out = __lasx_xvaddwh_w_h(in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvaddwh_h_b(__m256i in_h, __m256i in_l) {
+  __m256i out;
+
+  out = __lasx_xvilvh_b(in_h, in_l);
+  out = __lasx_xvhaddw_h_b(out, out);
+  return out;
+}
+
+/*
+ * =============================================================================
+ * Description : The high half of the vector elements are expanded and
+ *               added after being doubled.
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ * Details     : The in_h vector and the in_l vector are added after the
+ *               higher half of the two-fold sign extension (signed halfword
+ *               to signed word) and stored to the out vector.
+ * Example     : out = __lasx_xvaddwh_w_h(in_h, in_l)
+ *        in_h : 3, 0,3,0, 0,0,0,-1, 0,0,1,-1, 0,0,0,1
+ *        in_l : 2,-1,1,2, 1,0,0, 0, 1,0,1, 0, 1,0,0,1
+ *         out : 1,0,0,-1, 1,0,0, 2
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvaddwh_w_h(__m256i in_h, __m256i in_l) {
+  __m256i out;
+
+  out = __lasx_xvilvh_h(in_h, in_l);
+  out = __lasx_xvhaddw_w_h(out, out);
+  return out;
+}
+
+/*
+ * =============================================================================
+ * Description : The low half of the vector elements are expanded and
+ *               added after being doubled.
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ * Details     : The in_h vector and the in_l vector are added after the
+ *               lower half of the two-fold sign extension (signed byte
+ *               to signed halfword) and stored to the out vector.
+ * Example     : See out = __lasx_xvaddwl_w_h(in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvaddwl_h_b(__m256i in_h, __m256i in_l) {
+  __m256i out;
+
+  out = __lasx_xvilvl_b(in_h, in_l);
+  out = __lasx_xvhaddw_h_b(out, out);
+  return out;
+}
+
+/*
+ * =============================================================================
+ * Description : The low half of the vector elements are expanded and
+ *               added after being doubled.
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ * Details     : The in_h vector and the in_l vector are added after the
+ *               lower half of the two-fold sign extension (signed halfword
+ *               to signed word) and stored to the out vector.
+ * Example     : out = __lasx_xvaddwl_w_h(in_h, in_l)
+ *        in_h : 3, 0,3,0, 0,0,0,-1, 0,0,1,-1, 0,0,0,1
+ *        in_l : 2,-1,1,2, 1,0,0, 0, 1,0,1, 0, 1,0,0,1
+ *         out : 5,-1,4,2, 1,0,2,-1
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvaddwl_w_h(__m256i in_h, __m256i in_l) {
+  __m256i out;
+
+  out = __lasx_xvilvl_h(in_h, in_l);
+  out = __lasx_xvhaddw_w_h(out, out);
+  return out;
+}
+
+/*
+ * =============================================================================
+ * Description : The low half of the vector elements are expanded and
+ *               added after being doubled.
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ * Details     : The out vector and the out vector are added after the
+ *               lower half of the two-fold zero extension (unsigned byte
+ *               to unsigned halfword) and stored to the out vector.
+ * Example     : See out = __lasx_xvaddwl_w_h(in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvaddwl_h_bu(__m256i in_h, __m256i in_l) {
+  __m256i out;
+
+  out = __lasx_xvilvl_b(in_h, in_l);
+  out = __lasx_xvhaddw_hu_bu(out, out);
+  return out;
+}
+
+/*
+ * =============================================================================
+ * Description : The low half of the vector elements are expanded and
+ *               added after being doubled.
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ * Details     : The in_l vector after double zero extension (unsigned byte to
+ *               signed halfword)，added to the in_h vector.
+ * Example     : See out = __lasx_xvaddw_w_w_h(in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvaddw_h_h_bu(__m256i in_h, __m256i in_l) {
+  __m256i out;
+
+  out = __lasx_xvsllwil_hu_bu(in_l, 0);
+  out = __lasx_xvadd_h(in_h, out);
+  return out;
+}
+
+/*
+ * =============================================================================
+ * Description : The low half of the vector elements are expanded and
+ *               added after being doubled.
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ * Details     : The in_l vector after double sign extension (signed halfword to
+ *               signed word), added to the in_h vector.
+ * Example     : out = __lasx_xvaddw_w_w_h(in_h, in_l)
+ *        in_h : 0, 1,0,0, -1,0,0,1,
+ *        in_l : 2,-1,1,2,  1,0,0,0, 0,0,1,0, 1,0,0,1,
+ *         out : 2, 0,1,2, -1,0,1,1,
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvaddw_w_w_h(__m256i in_h, __m256i in_l) {
+  __m256i out;
+
+  out = __lasx_xvsllwil_w_h(in_l, 0);
+  out = __lasx_xvadd_w(in_h, out);
+  return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Multiplication and addition calculation after expansion
+ *               of the lower half of the vector.
+ * Arguments   : Inputs - in_c, in_h, in_l
+ *               Output - out
+ * Details     : The in_h vector and the in_l vector are multiplied after
+ *               the lower half of the two-fold sign extension (signed halfword
+ *               to signed word), and the result is added to the vector in_c,
+ *               then stored to the out vector.
+ * Example     : out = __lasx_xvmaddwl_w_h(in_c, in_h, in_l)
+ *        in_c : 1,2,3,4, 5,6,7,8
+ *        in_h : 1,2,3,4, 1,2,3,4, 5,6,7,8, 5,6,7,8
+ *        in_l : 200, 300, 400, 500,  2000, 3000, 4000, 5000,
+ *              -200,-300,-400,-500, -2000,-3000,-4000,-5000
+ *         out : 201, 602,1203,2004, -995, -1794,-2793,-3992
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvmaddwl_w_h(__m256i in_c, __m256i in_h,
+                                          __m256i in_l) {
+  __m256i tmp0, tmp1, out;
+
+  tmp0 = __lasx_xvsllwil_w_h(in_h, 0);
+  tmp1 = __lasx_xvsllwil_w_h(in_l, 0);
+  tmp0 = __lasx_xvmul_w(tmp0, tmp1);
+  out = __lasx_xvadd_w(tmp0, in_c);
+  return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Multiplication and addition calculation after expansion
+ *               of the higher half of the vector.
+ * Arguments   : Inputs - in_c, in_h, in_l
+ *               Output - out
+ * Details     : The in_h vector and the in_l vector are multiplied after
+ *               the higher half of the two-fold sign extension (signed
+ *               halfword to signed word), and the result is added to
+ *               the vector in_c, then stored to the out vector.
+ * Example     : See out = __lasx_xvmaddwl_w_h(in_c, in_h, in_l)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvmaddwh_w_h(__m256i in_c, __m256i in_h,
+                                          __m256i in_l) {
+  __m256i tmp0, tmp1, out;
+
+  tmp0 = __lasx_xvilvh_h(in_h, in_h);
+  tmp1 = __lasx_xvilvh_h(in_l, in_l);
+  tmp0 = __lasx_xvmulwev_w_h(tmp0, tmp1);
+  out = __lasx_xvadd_w(tmp0, in_c);
+  return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Multiplication calculation after expansion of the lower
+ *               half of the vector.
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ * Details     : The in_h vector and the in_l vector are multiplied after
+ *               the lower half of the two-fold sign extension (signed
+ *               halfword to signed word), then stored to the out vector.
+ * Example     : out = __lasx_xvmulwl_w_h(in_h, in_l)
+ *        in_h : 3,-1,3,0, 0,0,0,-1, 0,0,1,-1, 0,0,0,1
+ *        in_l : 2,-1,1,2, 1,0,0, 0, 0,0,1, 0, 1,0,0,1
+ *         out : 6,1,3,0, 0,0,1,0
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvmulwl_w_h(__m256i in_h, __m256i in_l) {
+  __m256i tmp0, tmp1, out;
+
+  tmp0 = __lasx_xvsllwil_w_h(in_h, 0);
+  tmp1 = __lasx_xvsllwil_w_h(in_l, 0);
+  out = __lasx_xvmul_w(tmp0, tmp1);
+  return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Multiplication calculation after expansion of the lower
+ *               half of the vector.
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ * Details     : The in_h vector and the in_l vector are multiplied after
+ *               the lower half of the two-fold sign extension (signed
+ *               halfword to signed word), then stored to the out vector.
+ * Example     : out = __lasx_xvmulwh_w_h(in_h, in_l)
+ *        in_h : 3,-1,3,0, 0,0,0,-1, 0,0,1,-1, 0,0,0,1
+ *        in_l : 2,-1,1,2, 1,0,0, 0, 0,0,1, 0, 1,0,0,1
+ *         out : 0,0,0,0, 0,0,0,1
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvmulwh_w_h(__m256i in_h, __m256i in_l) {
+  __m256i tmp0, tmp1, out;
+
+  tmp0 = __lasx_xvilvh_h(in_h, in_h);
+  tmp1 = __lasx_xvilvh_h(in_l, in_l);
+  out = __lasx_xvmulwev_w_h(tmp0, tmp1);
+  return out;
+}
+
+/*
+ * =============================================================================
+ * Description : The low half of the vector elements are added to the high half
+ *               after being doubled, then saturated.
+ * Arguments   : Inputs - in_h, in_l
+ *               Output - out
+ * Details     : The in_h vector adds the in_l vector after the lower half of
+ *               the two-fold zero extension (unsigned byte to unsigned
+ *               halfword) and then saturated. The results are stored to the out
+ *               vector.
+ * Example     : out = __lasx_xvsaddw_hu_hu_bu(in_h, in_l)
+ *        in_h : 2,65532,1,2, 1,0,0,0, 0,0,1,0, 1,0,0,1
+ *        in_l : 3,6,3,0, 0,0,0,1, 0,0,1,1, 0,0,0,1, 3,18,3,0, 0,0,0,1, 0,0,1,1,
+ *               0,0,0,1
+ *        out  : 5,65535,4,2, 1,0,0,1, 3,18,4,0, 1,0,0,2,
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvsaddw_hu_hu_bu(__m256i in_h, __m256i in_l) {
+  __m256i tmp1, out;
+  __m256i zero = { 0 };
+
+  tmp1 = __lasx_xvilvl_b(zero, in_l);
+  out = __lasx_xvsadd_hu(in_h, tmp1);
+  return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Clip all halfword elements of input vector between min & max
+ *               out = ((in) < (min)) ? (min) : (((in) > (max)) ? (max) : (in))
+ * Arguments   : Inputs  - in    (input vector)
+ *                       - min   (min threshold)
+ *                       - max   (max threshold)
+ *               Outputs - in    (output vector with clipped elements)
+ *               Return Type - signed halfword
+ * Example     : out = __lasx_xvclip_h(in, min, max)
+ *          in : -8,2,280,249, -8,255,280,249, 4,4,4,4, 5,5,5,5
+ *         min : 1,1,1,1, 1,1,1,1, 1,1,1,1, 1,1,1,1
+ *         max : 9,9,9,9, 9,9,9,9, 9,9,9,9, 9,9,9,9
+ *         out : 1,2,9,9, 1,9,9,9, 4,4,4,4, 5,5,5,5
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvclip_h(__m256i in, __m256i min, __m256i max) {
+  __m256i out;
+
+  out = __lasx_xvmax_h(min, in);
+  out = __lasx_xvmin_h(max, out);
+  return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Clip all signed halfword elements of input vector
+ *               between 0 & 255
+ * Arguments   : Inputs  - in   (input vector)
+ *               Outputs - out  (output vector with clipped elements)
+ *               Return Type - signed halfword
+ * Example     : See out = __lasx_xvclip255_w(in)
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvclip255_h(__m256i in) {
+  __m256i out;
+
+  out = __lasx_xvmaxi_h(in, 0);
+  out = __lasx_xvsat_hu(out, 7);
+  return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Clip all signed word elements of input vector
+ *               between 0 & 255
+ * Arguments   : Inputs - in   (input vector)
+ *               Output - out  (output vector with clipped elements)
+ *               Return Type - signed word
+ * Example     : out = __lasx_xvclip255_w(in)
+ *          in : -8,255,280,249, -8,255,280,249
+ *         out :  0,255,255,249,  0,255,255,249
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvclip255_w(__m256i in) {
+  __m256i out;
+
+  out = __lasx_xvmaxi_w(in, 0);
+  out = __lasx_xvsat_wu(out, 7);
+  return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Indexed halfword element values are replicated to all
+ *               elements in output vector. If 'idx < 8' use xvsplati_l_*,
+ *               if 'idx >= 8' use xvsplati_h_*.
+ * Arguments   : Inputs - in, idx
+ *               Output - out
+ * Details     : Idx element value from in vector is replicated to all
+ *               elements in out vector.
+ *               Valid index range for halfword operation is 0-7
+ * Example     : out = __lasx_xvsplati_l_h(in, idx)
+ *          in : 20,10,11,12, 13,14,15,16, 0,0,2,0, 0,0,0,0
+ *         idx : 0x02
+ *         out : 11,11,11,11, 11,11,11,11, 11,11,11,11, 11,11,11,11
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvsplati_l_h(__m256i in, int idx) {
+  __m256i out;
+
+  out = __lasx_xvpermi_q(in, in, 0x02);
+  out = __lasx_xvreplve_h(out, idx);
+  return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Indexed halfword element values are replicated to all
+ *               elements in output vector. If 'idx < 8' use xvsplati_l_*,
+ *               if 'idx >= 8' use xvsplati_h_*.
+ * Arguments   : Inputs - in, idx
+ *               Output - out
+ * Details     : Idx element value from in vector is replicated to all
+ *               elements in out vector.
+ *               Valid index range for halfword operation is 0-7
+ * Example     : out = __lasx_xvsplati_h_h(in, idx)
+ *          in : 20,10,11,12, 13,14,15,16, 0,2,0,0, 0,0,0,0
+ *         idx : 0x09
+ *         out : 2,2,2,2, 2,2,2,2, 2,2,2,2, 2,2,2,2
+ * =============================================================================
+ */
+static inline __m256i __lasx_xvsplati_h_h(__m256i in, int idx) {
+  __m256i out;
+
+  out = __lasx_xvpermi_q(in, in, 0x13);
+  out = __lasx_xvreplve_h(out, idx);
+  return out;
+}
+
+/*
+ * =============================================================================
+ * Description : Transpose 4x4 block with double-word elements in vectors
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3
+ *               Outputs - _out0, _out1, _out2, _out3
+ * Example     : LASX_TRANSPOSE4x4_D
+ *        _in0 : 1,2,3,4
+ *        _in1 : 1,2,3,4
+ *        _in2 : 1,2,3,4
+ *        _in3 : 1,2,3,4
+ *
+ *       _out0 : 1,1,1,1
+ *       _out1 : 2,2,2,2
+ *       _out2 : 3,3,3,3
+ *       _out3 : 4,4,4,4
+ * =============================================================================
+ */
+#define LASX_TRANSPOSE4x4_D(_in0, _in1, _in2, _in3, _out0, _out1, _out2, \
+                            _out3)                                       \
+  {                                                                      \
+    __m256i _tmp0, _tmp1, _tmp2, _tmp3;                                  \
+    _tmp0 = __lasx_xvilvl_d(_in1, _in0);                                 \
+    _tmp1 = __lasx_xvilvh_d(_in1, _in0);                                 \
+    _tmp2 = __lasx_xvilvl_d(_in3, _in2);                                 \
+    _tmp3 = __lasx_xvilvh_d(_in3, _in2);                                 \
+    _out0 = __lasx_xvpermi_q(_tmp2, _tmp0, 0x20);                        \
+    _out2 = __lasx_xvpermi_q(_tmp2, _tmp0, 0x31);                        \
+    _out1 = __lasx_xvpermi_q(_tmp3, _tmp1, 0x20);                        \
+    _out3 = __lasx_xvpermi_q(_tmp3, _tmp1, 0x31);                        \
+  }
+
+/*
+ * =============================================================================
+ * Description : Transpose 8x8 block with word elements in vectors
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7
+ *               Outputs - _out0, _out1, _out2, _out3, _out4, _out5, _out6,
+ *               _out7
+ * Example     : LASX_TRANSPOSE8x8_W
+ *        _in0 : 1,2,3,4,5,6,7,8
+ *        _in1 : 2,2,3,4,5,6,7,8
+ *        _in2 : 3,2,3,4,5,6,7,8
+ *        _in3 : 4,2,3,4,5,6,7,8
+ *        _in4 : 5,2,3,4,5,6,7,8
+ *        _in5 : 6,2,3,4,5,6,7,8
+ *        _in6 : 7,2,3,4,5,6,7,8
+ *        _in7 : 8,2,3,4,5,6,7,8
+ *
+ *       _out0 : 1,2,3,4,5,6,7,8
+ *       _out1 : 2,2,2,2,2,2,2,2
+ *       _out2 : 3,3,3,3,3,3,3,3
+ *       _out3 : 4,4,4,4,4,4,4,4
+ *       _out4 : 5,5,5,5,5,5,5,5
+ *       _out5 : 6,6,6,6,6,6,6,6
+ *       _out6 : 7,7,7,7,7,7,7,7
+ *       _out7 : 8,8,8,8,8,8,8,8
+ * =============================================================================
+ */
+#define LASX_TRANSPOSE8x8_W(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,  \
+                            _out0, _out1, _out2, _out3, _out4, _out5, _out6, \
+                            _out7)                                           \
+  {                                                                          \
+    __m256i _s0_m, _s1_m;                                                    \
+    __m256i _tmp0_m, _tmp1_m, _tmp2_m, _tmp3_m;                              \
+    __m256i _tmp4_m, _tmp5_m, _tmp6_m, _tmp7_m;                              \
+                                                                             \
+    _s0_m = __lasx_xvilvl_w(_in2, _in0);                                     \
+    _s1_m = __lasx_xvilvl_w(_in3, _in1);                                     \
+    _tmp0_m = __lasx_xvilvl_w(_s1_m, _s0_m);                                 \
+    _tmp1_m = __lasx_xvilvh_w(_s1_m, _s0_m);                                 \
+    _s0_m = __lasx_xvilvh_w(_in2, _in0);                                     \
+    _s1_m = __lasx_xvilvh_w(_in3, _in1);                                     \
+    _tmp2_m = __lasx_xvilvl_w(_s1_m, _s0_m);                                 \
+    _tmp3_m = __lasx_xvilvh_w(_s1_m, _s0_m);                                 \
+    _s0_m = __lasx_xvilvl_w(_in6, _in4);                                     \
+    _s1_m = __lasx_xvilvl_w(_in7, _in5);                                     \
+    _tmp4_m = __lasx_xvilvl_w(_s1_m, _s0_m);                                 \
+    _tmp5_m = __lasx_xvilvh_w(_s1_m, _s0_m);                                 \
+    _s0_m = __lasx_xvilvh_w(_in6, _in4);                                     \
+    _s1_m = __lasx_xvilvh_w(_in7, _in5);                                     \
+    _tmp6_m = __lasx_xvilvl_w(_s1_m, _s0_m);                                 \
+    _tmp7_m = __lasx_xvilvh_w(_s1_m, _s0_m);                                 \
+    _out0 = __lasx_xvpermi_q(_tmp4_m, _tmp0_m, 0x20);                        \
+    _out1 = __lasx_xvpermi_q(_tmp5_m, _tmp1_m, 0x20);                        \
+    _out2 = __lasx_xvpermi_q(_tmp6_m, _tmp2_m, 0x20);                        \
+    _out3 = __lasx_xvpermi_q(_tmp7_m, _tmp3_m, 0x20);                        \
+    _out4 = __lasx_xvpermi_q(_tmp4_m, _tmp0_m, 0x31);                        \
+    _out5 = __lasx_xvpermi_q(_tmp5_m, _tmp1_m, 0x31);                        \
+    _out6 = __lasx_xvpermi_q(_tmp6_m, _tmp2_m, 0x31);                        \
+    _out7 = __lasx_xvpermi_q(_tmp7_m, _tmp3_m, 0x31);                        \
+  }
+
+/*
+ * =============================================================================
+ * Description : Transpose input 16x8 byte block
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,
+ *                         _in8, _in9, _in10, _in11, _in12, _in13, _in14, _in15
+ *                         (input 16x8 byte block)
+ *               Outputs - _out0, _out1, _out2, _out3, _out4, _out5, _out6,
+ *                         _out7 (output 8x16 byte block)
+ * Details     : The rows of the matrix become columns, and the columns become
+ *               rows.
+ * Example     : See LASX_TRANSPOSE16x8_H
+ * =============================================================================
+ */
+#define LASX_TRANSPOSE16x8_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,  \
+                             _in8, _in9, _in10, _in11, _in12, _in13, _in14,   \
+                             _in15, _out0, _out1, _out2, _out3, _out4, _out5, \
+                             _out6, _out7)                                    \
+  {                                                                           \
+    __m256i _tmp0_m, _tmp1_m, _tmp2_m, _tmp3_m;                               \
+    __m256i _tmp4_m, _tmp5_m, _tmp6_m, _tmp7_m;                               \
+                                                                              \
+    _tmp0_m = __lasx_xvilvl_b(_in2, _in0);                                    \
+    _tmp1_m = __lasx_xvilvl_b(_in3, _in1);                                    \
+    _tmp2_m = __lasx_xvilvl_b(_in6, _in4);                                    \
+    _tmp3_m = __lasx_xvilvl_b(_in7, _in5);                                    \
+    _tmp4_m = __lasx_xvilvl_b(_in10, _in8);                                   \
+    _tmp5_m = __lasx_xvilvl_b(_in11, _in9);                                   \
+    _tmp6_m = __lasx_xvilvl_b(_in14, _in12);                                  \
+    _tmp7_m = __lasx_xvilvl_b(_in15, _in13);                                  \
+    _out0 = __lasx_xvilvl_b(_tmp1_m, _tmp0_m);                                \
+    _out1 = __lasx_xvilvh_b(_tmp1_m, _tmp0_m);                                \
+    _out2 = __lasx_xvilvl_b(_tmp3_m, _tmp2_m);                                \
+    _out3 = __lasx_xvilvh_b(_tmp3_m, _tmp2_m);                                \
+    _out4 = __lasx_xvilvl_b(_tmp5_m, _tmp4_m);                                \
+    _out5 = __lasx_xvilvh_b(_tmp5_m, _tmp4_m);                                \
+    _out6 = __lasx_xvilvl_b(_tmp7_m, _tmp6_m);                                \
+    _out7 = __lasx_xvilvh_b(_tmp7_m, _tmp6_m);                                \
+    _tmp0_m = __lasx_xvilvl_w(_out2, _out0);                                  \
+    _tmp2_m = __lasx_xvilvh_w(_out2, _out0);                                  \
+    _tmp4_m = __lasx_xvilvl_w(_out3, _out1);                                  \
+    _tmp6_m = __lasx_xvilvh_w(_out3, _out1);                                  \
+    _tmp1_m = __lasx_xvilvl_w(_out6, _out4);                                  \
+    _tmp3_m = __lasx_xvilvh_w(_out6, _out4);                                  \
+    _tmp5_m = __lasx_xvilvl_w(_out7, _out5);                                  \
+    _tmp7_m = __lasx_xvilvh_w(_out7, _out5);                                  \
+    _out0 = __lasx_xvilvl_d(_tmp1_m, _tmp0_m);                                \
+    _out1 = __lasx_xvilvh_d(_tmp1_m, _tmp0_m);                                \
+    _out2 = __lasx_xvilvl_d(_tmp3_m, _tmp2_m);                                \
+    _out3 = __lasx_xvilvh_d(_tmp3_m, _tmp2_m);                                \
+    _out4 = __lasx_xvilvl_d(_tmp5_m, _tmp4_m);                                \
+    _out5 = __lasx_xvilvh_d(_tmp5_m, _tmp4_m);                                \
+    _out6 = __lasx_xvilvl_d(_tmp7_m, _tmp6_m);                                \
+    _out7 = __lasx_xvilvh_d(_tmp7_m, _tmp6_m);                                \
+  }
+
+/*
+ * =============================================================================
+ * Description : Transpose input 16x8 byte block
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,
+ *                         _in8, _in9, _in10, _in11, _in12, _in13, _in14, _in15
+ *                         (input 16x8 byte block)
+ *               Outputs - _out0, _out1, _out2, _out3, _out4, _out5, _out6,
+ *                         _out7 (output 8x16 byte block)
+ * Details     : The rows of the matrix become columns, and the columns become
+ *               rows.
+ * Example     : LASX_TRANSPOSE16x8_H
+ *        _in0 : 1,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *        _in1 : 2,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *        _in2 : 3,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *        _in3 : 4,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *        _in4 : 5,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *        _in5 : 6,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *        _in6 : 7,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *        _in7 : 8,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *        _in8 : 9,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *        _in9 : 1,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *       _in10 : 0,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *       _in11 : 2,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *       _in12 : 3,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *       _in13 : 7,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *       _in14 : 5,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *       _in15 : 6,2,3,4,5,6,7,8,0,0,0,0,0,0,0,0
+ *
+ *       _out0 : 1,2,3,4,5,6,7,8,9,1,0,2,3,7,5,6
+ *       _out1 : 2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2
+ *       _out2 : 3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3
+ *       _out3 : 4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4
+ *       _out4 : 5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5
+ *       _out5 : 6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6
+ *       _out6 : 7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7
+ *       _out7 : 8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8
+ * =============================================================================
+ */
+#define LASX_TRANSPOSE16x8_H(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,  \
+                             _in8, _in9, _in10, _in11, _in12, _in13, _in14,   \
+                             _in15, _out0, _out1, _out2, _out3, _out4, _out5, \
+                             _out6, _out7)                                    \
+  {                                                                           \
+    __m256i _tmp0_m, _tmp1_m, _tmp2_m, _tmp3_m;                               \
+    __m256i _tmp4_m, _tmp5_m, _tmp6_m, _tmp7_m;                               \
+    __m256i _t0, _t1, _t2, _t3, _t4, _t5, _t6, _t7;                           \
+                                                                              \
+    _tmp0_m = __lasx_xvilvl_h(_in2, _in0);                                    \
+    _tmp1_m = __lasx_xvilvl_h(_in3, _in1);                                    \
+    _tmp2_m = __lasx_xvilvl_h(_in6, _in4);                                    \
+    _tmp3_m = __lasx_xvilvl_h(_in7, _in5);                                    \
+    _tmp4_m = __lasx_xvilvl_h(_in10, _in8);                                   \
+    _tmp5_m = __lasx_xvilvl_h(_in11, _in9);                                   \
+    _tmp6_m = __lasx_xvilvl_h(_in14, _in12);                                  \
+    _tmp7_m = __lasx_xvilvl_h(_in15, _in13);                                  \
+    _t0 = __lasx_xvilvl_h(_tmp1_m, _tmp0_m);                                  \
+    _t1 = __lasx_xvilvh_h(_tmp1_m, _tmp0_m);                                  \
+    _t2 = __lasx_xvilvl_h(_tmp3_m, _tmp2_m);                                  \
+    _t3 = __lasx_xvilvh_h(_tmp3_m, _tmp2_m);                                  \
+    _t4 = __lasx_xvilvl_h(_tmp5_m, _tmp4_m);                                  \
+    _t5 = __lasx_xvilvh_h(_tmp5_m, _tmp4_m);                                  \
+    _t6 = __lasx_xvilvl_h(_tmp7_m, _tmp6_m);                                  \
+    _t7 = __lasx_xvilvh_h(_tmp7_m, _tmp6_m);                                  \
+    _tmp0_m = __lasx_xvilvl_d(_t2, _t0);                                      \
+    _tmp2_m = __lasx_xvilvh_d(_t2, _t0);                                      \
+    _tmp4_m = __lasx_xvilvl_d(_t3, _t1);                                      \
+    _tmp6_m = __lasx_xvilvh_d(_t3, _t1);                                      \
+    _tmp1_m = __lasx_xvilvl_d(_t6, _t4);                                      \
+    _tmp3_m = __lasx_xvilvh_d(_t6, _t4);                                      \
+    _tmp5_m = __lasx_xvilvl_d(_t7, _t5);                                      \
+    _tmp7_m = __lasx_xvilvh_d(_t7, _t5);                                      \
+    _out0 = __lasx_xvpermi_q(_tmp1_m, _tmp0_m, 0x20);                         \
+    _out1 = __lasx_xvpermi_q(_tmp3_m, _tmp2_m, 0x20);                         \
+    _out2 = __lasx_xvpermi_q(_tmp5_m, _tmp4_m, 0x20);                         \
+    _out3 = __lasx_xvpermi_q(_tmp7_m, _tmp6_m, 0x20);                         \
+                                                                              \
+    _tmp0_m = __lasx_xvilvh_h(_in2, _in0);                                    \
+    _tmp1_m = __lasx_xvilvh_h(_in3, _in1);                                    \
+    _tmp2_m = __lasx_xvilvh_h(_in6, _in4);                                    \
+    _tmp3_m = __lasx_xvilvh_h(_in7, _in5);                                    \
+    _tmp4_m = __lasx_xvilvh_h(_in10, _in8);                                   \
+    _tmp5_m = __lasx_xvilvh_h(_in11, _in9);                                   \
+    _tmp6_m = __lasx_xvilvh_h(_in14, _in12);                                  \
+    _tmp7_m = __lasx_xvilvh_h(_in15, _in13);                                  \
+    _t0 = __lasx_xvilvl_h(_tmp1_m, _tmp0_m);                                  \
+    _t1 = __lasx_xvilvh_h(_tmp1_m, _tmp0_m);                                  \
+    _t2 = __lasx_xvilvl_h(_tmp3_m, _tmp2_m);                                  \
+    _t3 = __lasx_xvilvh_h(_tmp3_m, _tmp2_m);                                  \
+    _t4 = __lasx_xvilvl_h(_tmp5_m, _tmp4_m);                                  \
+    _t5 = __lasx_xvilvh_h(_tmp5_m, _tmp4_m);                                  \
+    _t6 = __lasx_xvilvl_h(_tmp7_m, _tmp6_m);                                  \
+    _t7 = __lasx_xvilvh_h(_tmp7_m, _tmp6_m);                                  \
+    _tmp0_m = __lasx_xvilvl_d(_t2, _t0);                                      \
+    _tmp2_m = __lasx_xvilvh_d(_t2, _t0);                                      \
+    _tmp4_m = __lasx_xvilvl_d(_t3, _t1);                                      \
+    _tmp6_m = __lasx_xvilvh_d(_t3, _t1);                                      \
+    _tmp1_m = __lasx_xvilvl_d(_t6, _t4);                                      \
+    _tmp3_m = __lasx_xvilvh_d(_t6, _t4);                                      \
+    _tmp5_m = __lasx_xvilvl_d(_t7, _t5);                                      \
+    _tmp7_m = __lasx_xvilvh_d(_t7, _t5);                                      \
+    _out4 = __lasx_xvpermi_q(_tmp1_m, _tmp0_m, 0x20);                         \
+    _out5 = __lasx_xvpermi_q(_tmp3_m, _tmp2_m, 0x20);                         \
+    _out6 = __lasx_xvpermi_q(_tmp5_m, _tmp4_m, 0x20);                         \
+    _out7 = __lasx_xvpermi_q(_tmp7_m, _tmp6_m, 0x20);                         \
+  }
+
+/*
+ * =============================================================================
+ * Description : Transpose 4x4 block with halfword elements in vectors
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3
+ *               Outputs - _out0, _out1, _out2, _out3
+ *               Return Type - signed halfword
+ * Details     : The rows of the matrix become columns, and the columns become
+ *               rows.
+ * Example     : See LASX_TRANSPOSE8x8_H
+ * =============================================================================
+ */
+#define LASX_TRANSPOSE4x4_H(_in0, _in1, _in2, _in3, _out0, _out1, _out2, \
+                            _out3)                                       \
+  {                                                                      \
+    __m256i _s0_m, _s1_m;                                                \
+                                                                         \
+    _s0_m = __lasx_xvilvl_h(_in1, _in0);                                 \
+    _s1_m = __lasx_xvilvl_h(_in3, _in2);                                 \
+    _out0 = __lasx_xvilvl_w(_s1_m, _s0_m);                               \
+    _out2 = __lasx_xvilvh_w(_s1_m, _s0_m);                               \
+    _out1 = __lasx_xvilvh_d(_out0, _out0);                               \
+    _out3 = __lasx_xvilvh_d(_out2, _out2);                               \
+  }
+
+/*
+ * =============================================================================
+ * Description : Transpose input 8x8 byte block
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7
+ *                         (input 8x8 byte block)
+ *               Outputs - _out0, _out1, _out2, _out3, _out4, _out5, _out6,
+ *                         _out7 (output 8x8 byte block)
+ * Example     : See LASX_TRANSPOSE8x8_H
+ * =============================================================================
+ */
+#define LASX_TRANSPOSE8x8_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,  \
+                            _out0, _out1, _out2, _out3, _out4, _out5, _out6, \
+                            _out7)                                           \
+  {                                                                          \
+    __m256i _tmp0_m, _tmp1_m, _tmp2_m, _tmp3_m;                              \
+    __m256i _tmp4_m, _tmp5_m, _tmp6_m, _tmp7_m;                              \
+    _tmp0_m = __lasx_xvilvl_b(_in2, _in0);                                   \
+    _tmp1_m = __lasx_xvilvl_b(_in3, _in1);                                   \
+    _tmp2_m = __lasx_xvilvl_b(_in6, _in4);                                   \
+    _tmp3_m = __lasx_xvilvl_b(_in7, _in5);                                   \
+    _tmp4_m = __lasx_xvilvl_b(_tmp1_m, _tmp0_m);                             \
+    _tmp5_m = __lasx_xvilvh_b(_tmp1_m, _tmp0_m);                             \
+    _tmp6_m = __lasx_xvilvl_b(_tmp3_m, _tmp2_m);                             \
+    _tmp7_m = __lasx_xvilvh_b(_tmp3_m, _tmp2_m);                             \
+    _out0 = __lasx_xvilvl_w(_tmp6_m, _tmp4_m);                               \
+    _out2 = __lasx_xvilvh_w(_tmp6_m, _tmp4_m);                               \
+    _out4 = __lasx_xvilvl_w(_tmp7_m, _tmp5_m);                               \
+    _out6 = __lasx_xvilvh_w(_tmp7_m, _tmp5_m);                               \
+    _out1 = __lasx_xvbsrl_v(_out0, 8);                                       \
+    _out3 = __lasx_xvbsrl_v(_out2, 8);                                       \
+    _out5 = __lasx_xvbsrl_v(_out4, 8);                                       \
+    _out7 = __lasx_xvbsrl_v(_out6, 8);                                       \
+  }
+
+/*
+ * =============================================================================
+ * Description : Transpose 8x8 block with halfword elements in vectors.
+ * Arguments   : Inputs  - _in0, _in1, ~
+ *               Outputs - _out0, _out1, ~
+ * Details     : The rows of the matrix become columns, and the columns become
+ *               rows.
+ * Example     : LASX_TRANSPOSE8x8_H
+ *        _in0 : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *        _in1 : 8,2,3,4, 5,6,7,8, 8,2,3,4, 5,6,7,8
+ *        _in2 : 8,2,3,4, 5,6,7,8, 8,2,3,4, 5,6,7,8
+ *        _in3 : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *        _in4 : 9,2,3,4, 5,6,7,8, 9,2,3,4, 5,6,7,8
+ *        _in5 : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *        _in6 : 1,2,3,4, 5,6,7,8, 1,2,3,4, 5,6,7,8
+ *        _in7 : 9,2,3,4, 5,6,7,8, 9,2,3,4, 5,6,7,8
+ *
+ *       _out0 : 1,8,8,1, 9,1,1,9, 1,8,8,1, 9,1,1,9
+ *       _out1 : 2,2,2,2, 2,2,2,2, 2,2,2,2, 2,2,2,2
+ *       _out2 : 3,3,3,3, 3,3,3,3, 3,3,3,3, 3,3,3,3
+ *       _out3 : 4,4,4,4, 4,4,4,4, 4,4,4,4, 4,4,4,4
+ *       _out4 : 5,5,5,5, 5,5,5,5, 5,5,5,5, 5,5,5,5
+ *       _out5 : 6,6,6,6, 6,6,6,6, 6,6,6,6, 6,6,6,6
+ *       _out6 : 7,7,7,7, 7,7,7,7, 7,7,7,7, 7,7,7,7
+ *       _out7 : 8,8,8,8, 8,8,8,8, 8,8,8,8, 8,8,8,8
+ * =============================================================================
+ */
+#define LASX_TRANSPOSE8x8_H(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,  \
+                            _out0, _out1, _out2, _out3, _out4, _out5, _out6, \
+                            _out7)                                           \
+  {                                                                          \
+    __m256i _s0_m, _s1_m;                                                    \
+    __m256i _tmp0_m, _tmp1_m, _tmp2_m, _tmp3_m;                              \
+    __m256i _tmp4_m, _tmp5_m, _tmp6_m, _tmp7_m;                              \
+                                                                             \
+    _s0_m = __lasx_xvilvl_h(_in6, _in4);                                     \
+    _s1_m = __lasx_xvilvl_h(_in7, _in5);                                     \
+    _tmp0_m = __lasx_xvilvl_h(_s1_m, _s0_m);                                 \
+    _tmp1_m = __lasx_xvilvh_h(_s1_m, _s0_m);                                 \
+    _s0_m = __lasx_xvilvh_h(_in6, _in4);                                     \
+    _s1_m = __lasx_xvilvh_h(_in7, _in5);                                     \
+    _tmp2_m = __lasx_xvilvl_h(_s1_m, _s0_m);                                 \
+    _tmp3_m = __lasx_xvilvh_h(_s1_m, _s0_m);                                 \
+                                                                             \
+    _s0_m = __lasx_xvilvl_h(_in2, _in0);                                     \
+    _s1_m = __lasx_xvilvl_h(_in3, _in1);                                     \
+    _tmp4_m = __lasx_xvilvl_h(_s1_m, _s0_m);                                 \
+    _tmp5_m = __lasx_xvilvh_h(_s1_m, _s0_m);                                 \
+    _s0_m = __lasx_xvilvh_h(_in2, _in0);                                     \
+    _s1_m = __lasx_xvilvh_h(_in3, _in1);                                     \
+    _tmp6_m = __lasx_xvilvl_h(_s1_m, _s0_m);                                 \
+    _tmp7_m = __lasx_xvilvh_h(_s1_m, _s0_m);                                 \
+                                                                             \
+    _out0 = __lasx_xvpickev_d(_tmp0_m, _tmp4_m);                             \
+    _out2 = __lasx_xvpickev_d(_tmp1_m, _tmp5_m);                             \
+    _out4 = __lasx_xvpickev_d(_tmp2_m, _tmp6_m);                             \
+    _out6 = __lasx_xvpickev_d(_tmp3_m, _tmp7_m);                             \
+    _out1 = __lasx_xvpickod_d(_tmp0_m, _tmp4_m);                             \
+    _out3 = __lasx_xvpickod_d(_tmp1_m, _tmp5_m);                             \
+    _out5 = __lasx_xvpickod_d(_tmp2_m, _tmp6_m);                             \
+    _out7 = __lasx_xvpickod_d(_tmp3_m, _tmp7_m);                             \
+  }
+
+/*
+ * =============================================================================
+ * Description : Butterfly of 4 input vectors
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3
+ *               Outputs - _out0, _out1, _out2, _out3
+ * Details     : Butterfly operation
+ * Example     : LASX_BUTTERFLY_4
+ *               _out0 = _in0 + _in3;
+ *               _out1 = _in1 + _in2;
+ *               _out2 = _in1 - _in2;
+ *               _out3 = _in0 - _in3;
+ * =============================================================================
+ */
+#define LASX_BUTTERFLY_4_B(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
+  {                                                                            \
+    _out0 = __lasx_xvadd_b(_in0, _in3);                                        \
+    _out1 = __lasx_xvadd_b(_in1, _in2);                                        \
+    _out2 = __lasx_xvsub_b(_in1, _in2);                                        \
+    _out3 = __lasx_xvsub_b(_in0, _in3);                                        \
+  }
+#define LASX_BUTTERFLY_4_H(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
+  {                                                                            \
+    _out0 = __lasx_xvadd_h(_in0, _in3);                                        \
+    _out1 = __lasx_xvadd_h(_in1, _in2);                                        \
+    _out2 = __lasx_xvsub_h(_in1, _in2);                                        \
+    _out3 = __lasx_xvsub_h(_in0, _in3);                                        \
+  }
+#define LASX_BUTTERFLY_4_W(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
+  {                                                                            \
+    _out0 = __lasx_xvadd_w(_in0, _in3);                                        \
+    _out1 = __lasx_xvadd_w(_in1, _in2);                                        \
+    _out2 = __lasx_xvsub_w(_in1, _in2);                                        \
+    _out3 = __lasx_xvsub_w(_in0, _in3);                                        \
+  }
+#define LASX_BUTTERFLY_4_D(_in0, _in1, _in2, _in3, _out0, _out1, _out2, _out3) \
+  {                                                                            \
+    _out0 = __lasx_xvadd_d(_in0, _in3);                                        \
+    _out1 = __lasx_xvadd_d(_in1, _in2);                                        \
+    _out2 = __lasx_xvsub_d(_in1, _in2);                                        \
+    _out3 = __lasx_xvsub_d(_in0, _in3);                                        \
+  }
+
+/*
+ * =============================================================================
+ * Description : Butterfly of 8 input vectors
+ * Arguments   : Inputs  - _in0, _in1, _in2, _in3, ~
+ *               Outputs - _out0, _out1, _out2, _out3, ~
+ * Details     : Butterfly operation
+ * Example     : LASX_BUTTERFLY_8
+ *               _out0 = _in0 + _in7;
+ *               _out1 = _in1 + _in6;
+ *               _out2 = _in2 + _in5;
+ *               _out3 = _in3 + _in4;
+ *               _out4 = _in3 - _in4;
+ *               _out5 = _in2 - _in5;
+ *               _out6 = _in1 - _in6;
+ *               _out7 = _in0 - _in7;
+ * =============================================================================
+ */
+#define LASX_BUTTERFLY_8_B(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,  \
+                           _out0, _out1, _out2, _out3, _out4, _out5, _out6, \
+                           _out7)                                           \
+  {                                                                         \
+    _out0 = __lasx_xvadd_b(_in0, _in7);                                     \
+    _out1 = __lasx_xvadd_b(_in1, _in6);                                     \
+    _out2 = __lasx_xvadd_b(_in2, _in5);                                     \
+    _out3 = __lasx_xvadd_b(_in3, _in4);                                     \
+    _out4 = __lasx_xvsub_b(_in3, _in4);                                     \
+    _out5 = __lasx_xvsub_b(_in2, _in5);                                     \
+    _out6 = __lasx_xvsub_b(_in1, _in6);                                     \
+    _out7 = __lasx_xvsub_b(_in0, _in7);                                     \
+  }
+
+#define LASX_BUTTERFLY_8_H(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,  \
+                           _out0, _out1, _out2, _out3, _out4, _out5, _out6, \
+                           _out7)                                           \
+  {                                                                         \
+    _out0 = __lasx_xvadd_h(_in0, _in7);                                     \
+    _out1 = __lasx_xvadd_h(_in1, _in6);                                     \
+    _out2 = __lasx_xvadd_h(_in2, _in5);                                     \
+    _out3 = __lasx_xvadd_h(_in3, _in4);                                     \
+    _out4 = __lasx_xvsub_h(_in3, _in4);                                     \
+    _out5 = __lasx_xvsub_h(_in2, _in5);                                     \
+    _out6 = __lasx_xvsub_h(_in1, _in6);                                     \
+    _out7 = __lasx_xvsub_h(_in0, _in7);                                     \
+  }
+
+#define LASX_BUTTERFLY_8_W(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,  \
+                           _out0, _out1, _out2, _out3, _out4, _out5, _out6, \
+                           _out7)                                           \
+  {                                                                         \
+    _out0 = __lasx_xvadd_w(_in0, _in7);                                     \
+    _out1 = __lasx_xvadd_w(_in1, _in6);                                     \
+    _out2 = __lasx_xvadd_w(_in2, _in5);                                     \
+    _out3 = __lasx_xvadd_w(_in3, _in4);                                     \
+    _out4 = __lasx_xvsub_w(_in3, _in4);                                     \
+    _out5 = __lasx_xvsub_w(_in2, _in5);                                     \
+    _out6 = __lasx_xvsub_w(_in1, _in6);                                     \
+    _out7 = __lasx_xvsub_w(_in0, _in7);                                     \
+  }
+
+#define LASX_BUTTERFLY_8_D(_in0, _in1, _in2, _in3, _in4, _in5, _in6, _in7,  \
+                           _out0, _out1, _out2, _out3, _out4, _out5, _out6, \
+                           _out7)                                           \
+  {                                                                         \
+    _out0 = __lasx_xvadd_d(_in0, _in7);                                     \
+    _out1 = __lasx_xvadd_d(_in1, _in6);                                     \
+    _out2 = __lasx_xvadd_d(_in2, _in5);                                     \
+    _out3 = __lasx_xvadd_d(_in3, _in4);                                     \
+    _out4 = __lasx_xvsub_d(_in3, _in4);                                     \
+    _out5 = __lasx_xvsub_d(_in2, _in5);                                     \
+    _out6 = __lasx_xvsub_d(_in1, _in6);                                     \
+    _out7 = __lasx_xvsub_d(_in0, _in7);                                     \
+  }
+
+#endif  // LASX
+
+/*
+ * =============================================================================
+ * Description : Print out elements in vector.
+ * Arguments   : Inputs  - RTYPE, _element_num, _in0, _enter
+ *               Outputs -
+ * Details     : Print out '_element_num' elements in 'RTYPE' vector '_in0', if
+ *               '_enter' is TRUE, prefix "\nVP:" will be added first.
+ * Example     : VECT_PRINT(v4i32,4,in0,1); // in0: 1,2,3,4
+ *               VP:1,2,3,4,
+ * =============================================================================
+ */
+#define VECT_PRINT(RTYPE, element_num, in0, enter)                 \
+  {                                                                \
+    RTYPE _tmp0 = (RTYPE)in0;                                      \
+    int _i = 0;                                                    \
+    if (enter) printf("\nVP:");                                    \
+    for (_i = 0; _i < element_num; _i++) printf("%d,", _tmp0[_i]); \
+  }
+
+#endif /* LOONGSON_INTRINSICS_H */
+#endif /* AVUTIL_LOONGARCH_LOONGSON_INTRINSICS_H */
diff --git a/libavutil/loongarch/timer.h b/libavutil/loongarch/timer.h
new file mode 100644
index 0000000000..44ed786409
--- /dev/null
+++ b/libavutil/loongarch/timer.h
@@ -0,0 +1,48 @@
+/*
+ * Copyright (c) 2023 Loongson Technology Corporation Limited
+ * Contributed by Hecai Yuan <yuanhecai@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef AVUTIL_LOONGARCH_TIMER_H
+#define AVUTIL_LOONGARCH_TIMER_H
+
+#include <stdint.h>
+#include "config.h"
+
+#if HAVE_INLINE_ASM
+
+#define AV_READ_TIME read_time
+
+static inline uint64_t read_time(void)
+{
+
+#if ARCH_LOONGARCH64
+    uint64_t a, id = 0;
+    __asm__ volatile ( "rdtime.d  %0, %1" : "=r"(a), "=r"(id) :: "memory" );
+    return a;
+#else
+    uint32_t a, id = 0;
+    __asm__ volatile ( "rdtimel.w  %0, %1" : "=r"(a), "=r"(id) :: "memory" );
+    return (uint64_t)a;
+#endif
+}
+
+#endif /* HAVE_INLINE_ASM */
+
+#endif /* AVUTIL_LOONGARCH_TIMER_H */
diff --git a/libavutil/mips/Makefile b/libavutil/mips/Makefile
index dbfa5aa341..5f8c9b64e9 100644
--- a/libavutil/mips/Makefile
+++ b/libavutil/mips/Makefile
@@ -1 +1 @@
-OBJS += mips/float_dsp_mips.o
+OBJS += mips/float_dsp_mips.o mips/cpu.o
diff --git a/libavutil/mips/asmdefs.h b/libavutil/mips/asmdefs.h
index 748119918a..659342bab9 100644
--- a/libavutil/mips/asmdefs.h
+++ b/libavutil/mips/asmdefs.h
@@ -27,6 +27,8 @@
 #ifndef AVUTIL_MIPS_ASMDEFS_H
 #define AVUTIL_MIPS_ASMDEFS_H
 
+#include <stdint.h>
+
 #if defined(_ABI64) && _MIPS_SIM == _ABI64
 # define mips_reg       int64_t
 # define PTRSIZE        " 8 "
@@ -55,4 +57,52 @@
 # define PTR_SLL        "sll "
 #endif
 
+/*
+ * parse_r var, r - Helper assembler macro for parsing register names.
+ *
+ * This converts the register name in $n form provided in \r to the
+ * corresponding register number, which is assigned to the variable \var. It is
+ * needed to allow explicit encoding of instructions in inline assembly where
+ * registers are chosen by the compiler in $n form, allowing us to avoid using
+ * fixed register numbers.
+ *
+ * It also allows newer instructions (not implemented by the assembler) to be
+ * transparently implemented using assembler macros, instead of needing separate
+ * cases depending on toolchain support.
+ *
+ * Simple usage example:
+ * __asm__ __volatile__("parse_r __rt, %0\n\t"
+ *                      ".insn\n\t"
+ *                      "# di    %0\n\t"
+ *                      ".word   (0x41606000 | (__rt << 16))"
+ *                      : "=r" (status);
+ */
+
+/* Match an individual register number and assign to \var */
+#define _IFC_REG(n)                                \
+        ".ifc        \\r, $" #n "\n\t"             \
+        "\\var        = " #n "\n\t"                \
+        ".endif\n\t"
+
+__asm__(".macro        parse_r var r\n\t"
+        "\\var        = -1\n\t"
+        _IFC_REG(0)  _IFC_REG(1)  _IFC_REG(2)  _IFC_REG(3)
+        _IFC_REG(4)  _IFC_REG(5)  _IFC_REG(6)  _IFC_REG(7)
+        _IFC_REG(8)  _IFC_REG(9)  _IFC_REG(10) _IFC_REG(11)
+        _IFC_REG(12) _IFC_REG(13) _IFC_REG(14) _IFC_REG(15)
+        _IFC_REG(16) _IFC_REG(17) _IFC_REG(18) _IFC_REG(19)
+        _IFC_REG(20) _IFC_REG(21) _IFC_REG(22) _IFC_REG(23)
+        _IFC_REG(24) _IFC_REG(25) _IFC_REG(26) _IFC_REG(27)
+        _IFC_REG(28) _IFC_REG(29) _IFC_REG(30) _IFC_REG(31)
+        ".iflt        \\var\n\t"
+        ".error        \"Unable to parse register name \\r\"\n\t"
+        ".endif\n\t"
+        ".endm");
+
+/* General union structure for clang adaption */
+union mmi_intfloat64 {
+    int64_t i;
+    double  f;
+};
+
 #endif /* AVCODEC_MIPS_ASMDEFS_H */
diff --git a/libavutil/mips/cpu.c b/libavutil/mips/cpu.c
new file mode 100644
index 0000000000..59619d54de
--- /dev/null
+++ b/libavutil/mips/cpu.c
@@ -0,0 +1,134 @@
+/*
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "libavutil/cpu.h"
+#include "libavutil/cpu_internal.h"
+#include "config.h"
+#if defined __linux__ || defined __ANDROID__
+#include <stdint.h>
+#include <stdio.h>
+#include <string.h>
+#include <sys/auxv.h>
+#include "asmdefs.h"
+#include "libavutil/avstring.h"
+#endif
+
+#if defined __linux__ || defined __ANDROID__
+
+#define HWCAP_LOONGSON_CPUCFG (1 << 14)
+
+static int cpucfg_available(void)
+{
+    return getauxval(AT_HWCAP) & HWCAP_LOONGSON_CPUCFG;
+}
+
+/* Most toolchains have no CPUCFG support yet */
+static uint32_t read_cpucfg(uint32_t reg)
+{
+        uint32_t __res;
+
+        __asm__ __volatile__(
+                "parse_r __res,%0\n\t"
+                "parse_r reg,%1\n\t"
+                ".insn \n\t"
+                ".word (0xc8080118 | (reg << 21) | (__res << 11))\n\t"
+                :"=r"(__res)
+                :"r"(reg)
+                :
+                );
+        return __res;
+}
+
+#define LOONGSON_CFG1 0x1
+
+#define LOONGSON_CFG1_MMI    (1 << 4)
+#define LOONGSON_CFG1_MSA1   (1 << 5)
+
+static int cpu_flags_cpucfg(void)
+{
+    int flags = 0;
+    uint32_t cfg1 = read_cpucfg(LOONGSON_CFG1);
+
+    if (cfg1 & LOONGSON_CFG1_MMI)
+        flags |= AV_CPU_FLAG_MMI;
+
+    if (cfg1 & LOONGSON_CFG1_MSA1)
+        flags |= AV_CPU_FLAG_MSA;
+
+    return flags;
+}
+
+static int cpu_flags_cpuinfo(void)
+{
+    FILE *f = fopen("/proc/cpuinfo", "r");
+    char buf[200];
+    int flags = 0;
+
+    if (!f)
+        return -1;
+
+    while (fgets(buf, sizeof(buf), f)) {
+        /* Legacy kernel may not export MMI in ASEs implemented */
+        if (av_strstart(buf, "cpu model", NULL)) {
+            if (strstr(buf, "Loongson-3 "))
+                flags |= AV_CPU_FLAG_MMI;
+        }
+
+        if (av_strstart(buf, "ASEs implemented", NULL)) {
+            if (strstr(buf, " loongson-mmi"))
+                flags |= AV_CPU_FLAG_MMI;
+            if (strstr(buf, " msa"))
+                flags |= AV_CPU_FLAG_MSA;
+
+            break;
+        }
+    }
+    fclose(f);
+    return flags;
+}
+#endif
+
+int ff_get_cpu_flags_mips(void)
+{
+#if defined __linux__ || defined __ANDROID__
+    if (cpucfg_available())
+        return cpu_flags_cpucfg();
+    else
+        return cpu_flags_cpuinfo();
+#else
+    /* Assume no SIMD ASE supported */
+    return 0;
+#endif
+}
+
+size_t ff_get_cpu_max_align_mips(void)
+{
+    int flags = av_get_cpu_flags();
+
+    if (flags & AV_CPU_FLAG_MSA)
+        return 16;
+
+    /*
+     * MMI itself is 64-bit but quad word load & store
+     * needs 128-bit align.
+     */
+    if (flags & AV_CPU_FLAG_MMI)
+        return 16;
+
+    return 8;
+}
diff --git a/libavutil/mips/cpu.h b/libavutil/mips/cpu.h
new file mode 100644
index 0000000000..615dc49759
--- /dev/null
+++ b/libavutil/mips/cpu.h
@@ -0,0 +1,28 @@
+/*
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef AVUTIL_MIPS_CPU_H
+#define AVUTIL_MIPS_CPU_H
+
+#include "libavutil/cpu.h"
+#include "libavutil/cpu_internal.h"
+
+#define have_mmi(flags) CPUEXT(flags, MMI)
+#define have_msa(flags) CPUEXT(flags, MSA)
+
+#endif /* AVUTIL_MIPS_CPU_H */
diff --git a/libavutil/mips/generic_macros_msa.h b/libavutil/mips/generic_macros_msa.h
index 9ac0583765..1486f7296e 100644
--- a/libavutil/mips/generic_macros_msa.h
+++ b/libavutil/mips/generic_macros_msa.h
@@ -25,10 +25,6 @@
 #include <msa.h>
 #include <config.h>
 
-#if HAVE_MSA2
-#include <msa2.h>
-#endif
-
 #define ALIGNMENT           16
 #define ALLOC_ALIGNED(align) __attribute__ ((aligned((align) << 1)))
 
@@ -111,10 +107,11 @@
         uint32_t val_lw_m;                           \
                                                      \
         __asm__ volatile (                           \
-            "ulw  %[val_lw_m],  %[psrc_lw_m]  \n\t"  \
+            "lwr %[val_lw_m], 0(%[psrc_lw_m]) \n\t"  \
+            "lwl %[val_lw_m], 3(%[psrc_lw_m]) \n\t"  \
                                                      \
-            : [val_lw_m] "=r" (val_lw_m)             \
-            : [psrc_lw_m] "m" (*psrc_lw_m)           \
+            : [val_lw_m] "=&r"(val_lw_m)             \
+            : [psrc_lw_m] "r"(psrc_lw_m)             \
         );                                           \
                                                      \
         val_lw_m;                                    \
@@ -127,10 +124,11 @@
             uint64_t val_ld_m = 0;                       \
                                                          \
             __asm__ volatile (                           \
-                "uld  %[val_ld_m],  %[psrc_ld_m]  \n\t"  \
+                "ldr %[val_ld_m], 0(%[psrc_ld_m]) \n\t"  \
+                "ldl %[val_ld_m], 7(%[psrc_ld_m]) \n\t"  \
                                                          \
-                : [val_ld_m] "=r" (val_ld_m)             \
-                : [psrc_ld_m] "m" (*psrc_ld_m)           \
+                : [val_ld_m] "=&r" (val_ld_m)            \
+                : [psrc_ld_m] "r" (psrc_ld_m)            \
             );                                           \
                                                          \
             val_ld_m;                                    \
@@ -299,6 +297,7 @@
 #define LD_SB4(...) LD_V4(v16i8, __VA_ARGS__)
 #define LD_UH4(...) LD_V4(v8u16, __VA_ARGS__)
 #define LD_SH4(...) LD_V4(v8i16, __VA_ARGS__)
+#define LD_SW4(...) LD_V4(v4i32, __VA_ARGS__)
 
 #define LD_V5(RTYPE, psrc, stride, out0, out1, out2, out3, out4)  \
 {                                                                 \
@@ -337,6 +336,7 @@
 #define LD_SB8(...) LD_V8(v16i8, __VA_ARGS__)
 #define LD_UH8(...) LD_V8(v8u16, __VA_ARGS__)
 #define LD_SH8(...) LD_V8(v8i16, __VA_ARGS__)
+#define LD_SW8(...) LD_V8(v4i32, __VA_ARGS__)
 
 #define LD_V16(RTYPE, psrc, stride,                                   \
                out0, out1, out2, out3, out4, out5, out6, out7,        \
@@ -602,67 +602,48 @@
 }
 #define AVER_UB4_UB(...) AVER_UB4(v16u8, __VA_ARGS__)
 
-/* Description : Immediate number of columns to slide with zero
-   Arguments   : Inputs  - in0, in1, slide_val
-                 Outputs - out0, out1
+/* Description : Immediate number of columns to slide
+   Arguments   : Inputs  - s, d, slide_val
+                 Outputs - out
                  Return Type - as per RTYPE
-   Details     : Byte elements from 'zero_m' vector are slide into 'in0' by
+   Details     : Byte elements from 'd' vector are slide into 's' by
                  number of elements specified by 'slide_val'
 */
-#define SLDI_B2_0(RTYPE, in0, in1, out0, out1, slide_val)                 \
-{                                                                         \
-    v16i8 zero_m = { 0 };                                                 \
-    out0 = (RTYPE) __msa_sldi_b((v16i8) zero_m, (v16i8) in0, slide_val);  \
-    out1 = (RTYPE) __msa_sldi_b((v16i8) zero_m, (v16i8) in1, slide_val);  \
-}
-#define SLDI_B2_0_UB(...) SLDI_B2_0(v16u8, __VA_ARGS__)
-#define SLDI_B2_0_SB(...) SLDI_B2_0(v16i8, __VA_ARGS__)
-#define SLDI_B2_0_SW(...) SLDI_B2_0(v4i32, __VA_ARGS__)
-
-#define SLDI_B3_0(RTYPE, in0, in1, in2, out0, out1, out2,  slide_val)     \
-{                                                                         \
-    v16i8 zero_m = { 0 };                                                 \
-    SLDI_B2_0(RTYPE, in0, in1, out0, out1, slide_val);                    \
-    out2 = (RTYPE) __msa_sldi_b((v16i8) zero_m, (v16i8) in2, slide_val);  \
-}
-#define SLDI_B3_0_UB(...) SLDI_B3_0(v16u8, __VA_ARGS__)
-#define SLDI_B3_0_SB(...) SLDI_B3_0(v16i8, __VA_ARGS__)
-
-#define SLDI_B4_0(RTYPE, in0, in1, in2, in3,            \
-                  out0, out1, out2, out3, slide_val)    \
-{                                                       \
-    SLDI_B2_0(RTYPE, in0, in1, out0, out1, slide_val);  \
-    SLDI_B2_0(RTYPE, in2, in3, out2, out3, slide_val);  \
+#define SLDI_B(RTYPE, d, s, slide_val, out)                       \
+{                                                                 \
+    out = (RTYPE) __msa_sldi_b((v16i8) d, (v16i8) s, slide_val);  \
 }
-#define SLDI_B4_0_UB(...) SLDI_B4_0(v16u8, __VA_ARGS__)
-#define SLDI_B4_0_SB(...) SLDI_B4_0(v16i8, __VA_ARGS__)
-#define SLDI_B4_0_SH(...) SLDI_B4_0(v8i16, __VA_ARGS__)
 
-/* Description : Immediate number of columns to slide
-   Arguments   : Inputs  - in0_0, in0_1, in1_0, in1_1, slide_val
-                 Outputs - out0, out1
-                 Return Type - as per RTYPE
-   Details     : Byte elements from 'in0_0' vector are slide into 'in1_0' by
-                 number of elements specified by 'slide_val'
-*/
-#define SLDI_B2(RTYPE, in0_0, in0_1, in1_0, in1_1, out0, out1, slide_val)  \
-{                                                                          \
-    out0 = (RTYPE) __msa_sldi_b((v16i8) in0_0, (v16i8) in1_0, slide_val);  \
-    out1 = (RTYPE) __msa_sldi_b((v16i8) in0_1, (v16i8) in1_1, slide_val);  \
+#define SLDI_B2(RTYPE, d0, s0, d1, s1, slide_val, out0, out1)  \
+{                                                              \
+    SLDI_B(RTYPE, d0, s0, slide_val, out0)                     \
+    SLDI_B(RTYPE, d1, s1, slide_val, out1)                     \
 }
 #define SLDI_B2_UB(...) SLDI_B2(v16u8, __VA_ARGS__)
 #define SLDI_B2_SB(...) SLDI_B2(v16i8, __VA_ARGS__)
 #define SLDI_B2_SH(...) SLDI_B2(v8i16, __VA_ARGS__)
+#define SLDI_B2_SW(...) SLDI_B2(v4i32, __VA_ARGS__)
 
-#define SLDI_B3(RTYPE, in0_0, in0_1, in0_2, in1_0, in1_1, in1_2,           \
-                out0, out1, out2, slide_val)                               \
-{                                                                          \
-    SLDI_B2(RTYPE, in0_0, in0_1, in1_0, in1_1, out0, out1, slide_val)      \
-    out2 = (RTYPE) __msa_sldi_b((v16i8) in0_2, (v16i8) in1_2, slide_val);  \
+#define SLDI_B3(RTYPE, d0, s0, d1, s1, d2, s2, slide_val,  \
+                out0, out1, out2)                          \
+{                                                          \
+    SLDI_B2(RTYPE, d0, s0, d1, s1, slide_val, out0, out1)  \
+    SLDI_B(RTYPE, d2, s2, slide_val, out2)                 \
 }
+#define SLDI_B3_UB(...) SLDI_B3(v16u8, __VA_ARGS__)
 #define SLDI_B3_SB(...) SLDI_B3(v16i8, __VA_ARGS__)
 #define SLDI_B3_UH(...) SLDI_B3(v8u16, __VA_ARGS__)
 
+#define SLDI_B4(RTYPE, d0, s0, d1, s1, d2, s2, d3, s3,     \
+                slide_val, out0, out1, out2, out3)         \
+{                                                          \
+    SLDI_B2(RTYPE, d0, s0, d1, s1, slide_val, out0, out1)  \
+    SLDI_B2(RTYPE, d2, s2, d3, s3, slide_val, out2, out3)  \
+}
+#define SLDI_B4_UB(...) SLDI_B4(v16u8, __VA_ARGS__)
+#define SLDI_B4_SB(...) SLDI_B4(v16i8, __VA_ARGS__)
+#define SLDI_B4_SH(...) SLDI_B4(v8i16, __VA_ARGS__)
+
 /* Description : Shuffle byte vector elements as per mask vector
    Arguments   : Inputs  - in0, in1, in2, in3, mask0, mask1
                  Outputs - out0, out1
@@ -933,99 +914,78 @@
 
 /* Description : Clips all halfword elements of input vector between min & max
                  out = ((in) < (min)) ? (min) : (((in) > (max)) ? (max) : (in))
-   Arguments   : Inputs  - in       (input vector)
-                         - min      (min threshold)
-                         - max      (max threshold)
-                 Outputs - out_m    (output vector with clipped elements)
+   Arguments   : Inputs  - in    (input vector)
+                         - min   (min threshold)
+                         - max   (max threshold)
+                 Outputs - in    (output vector with clipped elements)
                  Return Type - signed halfword
 */
-#define CLIP_SH(in, min, max)                           \
-( {                                                     \
-    v8i16 out_m;                                        \
-                                                        \
-    out_m = __msa_max_s_h((v8i16) min, (v8i16) in);     \
-    out_m = __msa_min_s_h((v8i16) max, (v8i16) out_m);  \
-    out_m;                                              \
-} )
+#define CLIP_SH(in, min, max)                     \
+{                                                 \
+    in = __msa_max_s_h((v8i16) min, (v8i16) in);  \
+    in = __msa_min_s_h((v8i16) max, (v8i16) in);  \
+}
 
 /* Description : Clips all signed halfword elements of input vector
                  between 0 & 255
-   Arguments   : Inputs  - in       (input vector)
-                 Outputs - out_m    (output vector with clipped elements)
-                 Return Type - signed halfword
+   Arguments   : Inputs  - in    (input vector)
+                 Outputs - in    (output vector with clipped elements)
+                 Return Type - signed halfwords
 */
-#define CLIP_SH_0_255(in)                                 \
-( {                                                       \
-    v8i16 max_m = __msa_ldi_h(255);                       \
-    v8i16 out_m;                                          \
-                                                          \
-    out_m = __msa_maxi_s_h((v8i16) in, 0);                \
-    out_m = __msa_min_s_h((v8i16) max_m, (v8i16) out_m);  \
-    out_m;                                                \
-} )
+#define CLIP_SH_0_255(in)                       \
+{                                               \
+    in = __msa_maxi_s_h((v8i16) in, 0);         \
+    in = (v8i16) __msa_sat_u_h((v8u16) in, 7);  \
+}
+
 #define CLIP_SH2_0_255(in0, in1)  \
 {                                 \
-    in0 = CLIP_SH_0_255(in0);     \
-    in1 = CLIP_SH_0_255(in1);     \
+    CLIP_SH_0_255(in0);           \
+    CLIP_SH_0_255(in1);           \
 }
+
 #define CLIP_SH4_0_255(in0, in1, in2, in3)  \
 {                                           \
     CLIP_SH2_0_255(in0, in1);               \
     CLIP_SH2_0_255(in2, in3);               \
 }
 
-#define CLIP_SH_0_255_MAX_SATU(in)                    \
-( {                                                   \
-    v8i16 out_m;                                      \
-                                                      \
-    out_m = __msa_maxi_s_h((v8i16) in, 0);            \
-    out_m = (v8i16) __msa_sat_u_h((v8u16) out_m, 7);  \
-    out_m;                                            \
-} )
-#define CLIP_SH2_0_255_MAX_SATU(in0, in1)  \
-{                                          \
-    in0 = CLIP_SH_0_255_MAX_SATU(in0);     \
-    in1 = CLIP_SH_0_255_MAX_SATU(in1);     \
-}
-#define CLIP_SH4_0_255_MAX_SATU(in0, in1, in2, in3)  \
-{                                                    \
-    CLIP_SH2_0_255_MAX_SATU(in0, in1);               \
-    CLIP_SH2_0_255_MAX_SATU(in2, in3);               \
+#define CLIP_SH8_0_255(in0, in1, in2, in3,  \
+                       in4, in5, in6, in7)  \
+{                                           \
+    CLIP_SH4_0_255(in0, in1, in2, in3);     \
+    CLIP_SH4_0_255(in4, in5, in6, in7);     \
 }
 
 /* Description : Clips all signed word elements of input vector
                  between 0 & 255
-   Arguments   : Inputs  - in       (input vector)
-                 Outputs - out_m    (output vector with clipped elements)
+   Arguments   : Inputs  - in    (input vector)
+                 Outputs - in    (output vector with clipped elements)
                  Return Type - signed word
 */
-#define CLIP_SW_0_255(in)                                 \
-( {                                                       \
-    v4i32 max_m = __msa_ldi_w(255);                       \
-    v4i32 out_m;                                          \
-                                                          \
-    out_m = __msa_maxi_s_w((v4i32) in, 0);                \
-    out_m = __msa_min_s_w((v4i32) max_m, (v4i32) out_m);  \
-    out_m;                                                \
-} )
+#define CLIP_SW_0_255(in)                       \
+{                                               \
+    in = __msa_maxi_s_w((v4i32) in, 0);         \
+    in = (v4i32) __msa_sat_u_w((v4u32) in, 7);  \
+}
 
-#define CLIP_SW_0_255_MAX_SATU(in)                    \
-( {                                                   \
-    v4i32 out_m;                                      \
-                                                      \
-    out_m = __msa_maxi_s_w((v4i32) in, 0);            \
-    out_m = (v4i32) __msa_sat_u_w((v4u32) out_m, 7);  \
-    out_m;                                            \
-} )
-#define CLIP_SW2_0_255_MAX_SATU(in0, in1)  \
-{                                          \
-    in0 = CLIP_SW_0_255_MAX_SATU(in0);     \
-    in1 = CLIP_SW_0_255_MAX_SATU(in1);     \
+#define CLIP_SW2_0_255(in0, in1)  \
+{                                 \
+    CLIP_SW_0_255(in0);           \
+    CLIP_SW_0_255(in1);           \
 }
-#define CLIP_SW4_0_255_MAX_SATU(in0, in1, in2, in3)  \
-{                                                    \
-    CLIP_SW2_0_255_MAX_SATU(in0, in1);               \
-    CLIP_SW2_0_255_MAX_SATU(in2, in3);               \
+
+#define CLIP_SW4_0_255(in0, in1, in2, in3)  \
+{                                           \
+    CLIP_SW2_0_255(in0, in1);               \
+    CLIP_SW2_0_255(in2, in3);               \
+}
+
+#define CLIP_SW8_0_255(in0, in1, in2, in3,  \
+                       in4, in5, in6, in7)  \
+{                                           \
+    CLIP_SW4_0_255(in0, in1, in2, in3);     \
+    CLIP_SW4_0_255(in4, in5, in6, in7);     \
 }
 
 /* Description : Addition of 4 signed word elements
@@ -1155,15 +1115,6 @@
                  unsigned absolute diff values, even-odd pairs are added
                  together to generate 8 halfword results.
 */
-#if HAVE_MSA2
-#define SAD_UB2_UH(in0, in1, ref0, ref1)                                 \
-( {                                                                      \
-    v8u16 sad_m = { 0 };                                                 \
-    sad_m += __builtin_msa2_sad_adj2_u_w2x_b((v16u8) in0, (v16u8) ref0); \
-    sad_m += __builtin_msa2_sad_adj2_u_w2x_b((v16u8) in1, (v16u8) ref1); \
-    sad_m;                                                               \
-} )
-#else
 #define SAD_UB2_UH(in0, in1, ref0, ref1)                        \
 ( {                                                             \
     v16u8 diff0_m, diff1_m;                                     \
@@ -1177,7 +1128,6 @@
                                                                 \
     sad_m;                                                      \
 } )
-#endif // #if HAVE_MSA2
 
 /* Description : Insert specified word elements from input vectors to 1
                  destination vector
@@ -1422,6 +1372,7 @@
             out4, out5, out6, out7);                              \
 }
 #define ILVR_B8_UH(...) ILVR_B8(v8u16, __VA_ARGS__)
+#define ILVR_B8_SW(...) ILVR_B8(v4i32, __VA_ARGS__)
 
 /* Description : Interleave right half of halfword elements from vectors
    Arguments   : Inputs  - in0, in1, in2, in3, in4, in5, in6, in7
@@ -2218,12 +2169,6 @@
                  extracted and interleaved with same vector 'in0' to generate
                  4 word elements keeping sign intact
 */
-#if HAVE_MSA2
-#define UNPCK_R_SH_SW(in, out)                           \
-{                                                        \
-    out = (v4i32) __builtin_msa2_w2x_lo_s_h((v8i16) in); \
-}
-#else
 #define UNPCK_R_SH_SW(in, out)                       \
 {                                                    \
     v8i16 sign_m;                                    \
@@ -2231,7 +2176,6 @@
     sign_m = __msa_clti_s_h((v8i16) in, 0);          \
     out = (v4i32) __msa_ilvr_h(sign_m, (v8i16) in);  \
 }
-#endif // #if HAVE_MSA2
 
 /* Description : Sign extend byte elements from input vector and return
                  halfword results in pair of vectors
@@ -2244,13 +2188,6 @@
                  Then interleaved left with same vector 'in0' to
                  generate 8 signed halfword elements in 'out1'
 */
-#if HAVE_MSA2
-#define UNPCK_SB_SH(in, out0, out1)                       \
-{                                                         \
-    out0 = (v4i32) __builtin_msa2_w2x_lo_s_b((v16i8) in); \
-    out1 = (v4i32) __builtin_msa2_w2x_hi_s_b((v16i8) in); \
-}
-#else
 #define UNPCK_SB_SH(in, out0, out1)                  \
 {                                                    \
     v16i8 tmp_m;                                     \
@@ -2258,7 +2195,6 @@
     tmp_m = __msa_clti_s_b((v16i8) in, 0);           \
     ILVRL_B2_SH(tmp_m, in, out0, out1);              \
 }
-#endif // #if HAVE_MSA2
 
 /* Description : Zero extend unsigned byte elements to halfword elements
    Arguments   : Inputs  - in           (1 input unsigned byte vector)
@@ -2285,13 +2221,6 @@
                  Then interleaved left with same vector 'in0' to
                  generate 4 signed word elements in 'out1'
 */
-#if HAVE_MSA2
-#define UNPCK_SH_SW(in, out0, out1)                       \
-{                                                         \
-    out0 = (v4i32) __builtin_msa2_w2x_lo_s_h((v8i16) in); \
-    out1 = (v4i32) __builtin_msa2_w2x_hi_s_h((v8i16) in); \
-}
-#else
 #define UNPCK_SH_SW(in, out0, out1)                  \
 {                                                    \
     v8i16 tmp_m;                                     \
@@ -2299,7 +2228,6 @@
     tmp_m = __msa_clti_s_h((v8i16) in, 0);           \
     ILVRL_H2_SW(tmp_m, in, out0, out1);              \
 }
-#endif // #if HAVE_MSA2
 
 /* Description : Swap two variables
    Arguments   : Inputs  - in0, in1
@@ -2433,6 +2361,7 @@
 {                                                                        \
     v16i8 tmp0_m, tmp1_m, tmp2_m, tmp3_m;                                \
     v16i8 tmp4_m, tmp5_m, tmp6_m, tmp7_m;                                \
+    v16i8 zeros = { 0 };                                                 \
                                                                          \
     ILVR_B4_SB(in2, in0, in3, in1, in6, in4, in7, in5,                   \
                tmp0_m, tmp1_m, tmp2_m, tmp3_m);                          \
@@ -2440,8 +2369,8 @@
     ILVRL_B2_SB(tmp3_m, tmp2_m, tmp6_m, tmp7_m);                         \
     ILVRL_W2(RTYPE, tmp6_m, tmp4_m, out0, out2);                         \
     ILVRL_W2(RTYPE, tmp7_m, tmp5_m, out4, out6);                         \
-    SLDI_B2_0(RTYPE, out0, out2, out1, out3, 8);                         \
-    SLDI_B2_0(RTYPE, out4, out6, out5, out7, 8);                         \
+    SLDI_B4(RTYPE, zeros, out0, zeros, out2, zeros, out4, zeros, out6,   \
+            8, out1, out3, out5, out7);                                  \
 }
 #define TRANSPOSE8x8_UB_UB(...) TRANSPOSE8x8_UB(v16u8, __VA_ARGS__)
 #define TRANSPOSE8x8_UB_UH(...) TRANSPOSE8x8_UB(v8u16, __VA_ARGS__)
@@ -2523,8 +2452,6 @@
     out5 = (v16u8) __msa_ilvod_w((v4i32) tmp3_m, (v4i32) tmp2_m);            \
                                                                              \
     tmp2_m = (v16u8) __msa_ilvod_h((v8i16) tmp5_m, (v8i16) tmp4_m);          \
-    tmp2_m = (v16u8) __msa_ilvod_h((v8i16) tmp5_m, (v8i16) tmp4_m);          \
-    tmp3_m = (v16u8) __msa_ilvod_h((v8i16) tmp7_m, (v8i16) tmp6_m);          \
     tmp3_m = (v16u8) __msa_ilvod_h((v8i16) tmp7_m, (v8i16) tmp6_m);          \
     out3 = (v16u8) __msa_ilvev_w((v4i32) tmp3_m, (v4i32) tmp2_m);            \
     out7 = (v16u8) __msa_ilvod_w((v4i32) tmp3_m, (v4i32) tmp2_m);            \
diff --git a/libavutil/mips/mmiutils.h b/libavutil/mips/mmiutils.h
index 05f6b31155..8855f98d8f 100644
--- a/libavutil/mips/mmiutils.h
+++ b/libavutil/mips/mmiutils.h
@@ -27,74 +27,103 @@
 #include "config.h"
 #include "libavutil/mips/asmdefs.h"
 
-#if HAVE_LOONGSON2
+/*
+ * These were used to define temporary registers for MMI marcos
+ * however now we're using $at. They're theoretically unnecessary
+ * but just leave them here to avoid mess.
+ */
+#define DECLARE_VAR_LOW32
+#define RESTRICT_ASM_LOW32
+#define DECLARE_VAR_ALL64
+#define RESTRICT_ASM_ALL64
+#define DECLARE_VAR_ADDRT
+#define RESTRICT_ASM_ADDRT
 
-#define DECLARE_VAR_LOW32       int32_t low32
-#define RESTRICT_ASM_LOW32      [low32]"=&r"(low32),
-#define DECLARE_VAR_ALL64       int64_t all64
-#define RESTRICT_ASM_ALL64      [all64]"=&r"(all64),
-#define DECLARE_VAR_ADDRT       mips_reg addrt
-#define RESTRICT_ASM_ADDRT      [addrt]"=&r"(addrt),
+#if HAVE_LOONGSON2
 
 #define MMI_LWX(reg, addr, stride, bias)                                    \
-    PTR_ADDU    "%[addrt],  "#addr",    "#stride"                   \n\t"   \
-    "lw         "#reg",     "#bias"(%[addrt])                       \n\t"
+    ".set noat                                                 \n\t"   \
+    PTR_ADDU    "$at,  "#addr",    "#stride"                   \n\t"   \
+    "lw         "#reg",     "#bias"($at)                       \n\t"   \
+    ".set at                                                   \n\t"
 
 #define MMI_SWX(reg, addr, stride, bias)                                    \
-    PTR_ADDU    "%[addrt],  "#addr",    "#stride"                   \n\t"   \
-    "sw         "#reg",     "#bias"(%[addrt])                       \n\t"
+    ".set noat                                                 \n\t"   \
+    PTR_ADDU    "$at,  "#addr",    "#stride"                   \n\t"   \
+    "sw         "#reg",     "#bias"($at)                       \n\t"   \
+    ".set at                                                   \n\t"
 
 #define MMI_LDX(reg, addr, stride, bias)                                    \
-    PTR_ADDU    "%[addrt],  "#addr",    "#stride"                   \n\t"   \
-    "ld         "#reg",     "#bias"(%[addrt])                       \n\t"
+    ".set noat                                                 \n\t"   \
+    PTR_ADDU    "$at,  "#addr",    "#stride"                   \n\t"   \
+    "ld         "#reg",     "#bias"($at)                       \n\t"   \
+    ".set at                                                   \n\t"
 
 #define MMI_SDX(reg, addr, stride, bias)                                    \
-    PTR_ADDU    "%[addrt],  "#addr",    "#stride"                   \n\t"   \
-    "sd         "#reg",     "#bias"(%[addrt])                       \n\t"
+    ".set noat                                                 \n\t"   \
+    PTR_ADDU    "$at,  "#addr",    "#stride"                   \n\t"   \
+    "sd         "#reg",     "#bias"($at)                       \n\t"   \
+    ".set at                                                   \n\t"
 
 #define MMI_LWC1(fp, addr, bias)                                            \
     "lwc1       "#fp",      "#bias"("#addr")                        \n\t"
 
 #define MMI_ULWC1(fp, addr, bias)                                           \
-    "ulw        %[low32],   "#bias"("#addr")                        \n\t"   \
-    "mtc1       %[low32],   "#fp"                                   \n\t"
+    ".set noat                                                      \n\t"   \
+    "ulw        $at,   "#bias"("#addr")                             \n\t"   \
+    "mtc1       $at,   "#fp"                                        \n\t"   \
+    ".set at                                                        \n\t"
 
 #define MMI_LWXC1(fp, addr, stride, bias)                                   \
-    PTR_ADDU    "%[addrt],  "#addr",    "#stride"                   \n\t"   \
-    MMI_LWC1(fp, %[addrt], bias)
+    ".set noat                                                 \n\t"   \
+    PTR_ADDU    "$at,  "#addr",    "#stride"                   \n\t"   \
+    MMI_LWC1(fp, $at, bias)                                            \
+    ".set at                                                   \n\t"
 
 #define MMI_SWC1(fp, addr, bias)                                            \
     "swc1       "#fp",      "#bias"("#addr")                        \n\t"
 
 #define MMI_USWC1(fp, addr, bias)                                           \
-    "mfc1       %[low32],   "#fp"                                   \n\t"   \
-    "usw        %[low32],   "#bias"("#addr")                        \n\t"
+    ".set noat                                                      \n\t"   \
+    "mfc1       $at,   "#fp"                                        \n\t"   \
+    "usw        $at,   "#bias"("#addr")                             \n\t"   \
+    ".set at                                                        \n\t"
 
 #define MMI_SWXC1(fp, addr, stride, bias)                                   \
-    PTR_ADDU    "%[addrt],  "#addr",    "#stride"                   \n\t"   \
-    MMI_SWC1(fp, %[addrt], bias)
+    ".set noat                                                 \n\t"   \
+    PTR_ADDU    "$at,  "#addr",    "#stride"                   \n\t"   \
+    MMI_SWC1(fp, $at, bias)                                           \
+    ".set at                                                   \n\t"
 
 #define MMI_LDC1(fp, addr, bias)                                            \
     "ldc1       "#fp",      "#bias"("#addr")                        \n\t"
 
 #define MMI_ULDC1(fp, addr, bias)                                           \
-    "uld        %[all64],   "#bias"("#addr")                        \n\t"   \
-    "dmtc1      %[all64],   "#fp"                                   \n\t"
+    ".set noat                                                      \n\t"   \
+    "uld        $at,   "#bias"("#addr")                             \n\t"   \
+    "dmtc1      $at,   "#fp"                                        \n\t"   \
+    ".set at                                                        \n\t"
 
 #define MMI_LDXC1(fp, addr, stride, bias)                                   \
-    PTR_ADDU    "%[addrt],  "#addr",    "#stride"                   \n\t"   \
-    MMI_LDC1(fp, %[addrt], bias)
+    ".set noat                                                 \n\t"   \
+    PTR_ADDU    "$at,  "#addr",    "#stride"                   \n\t"   \
+    MMI_LDC1(fp, $at, bias)                                           \
+    ".set at                                                   \n\t"
 
 #define MMI_SDC1(fp, addr, bias)                                            \
     "sdc1       "#fp",      "#bias"("#addr")                        \n\t"
 
 #define MMI_USDC1(fp, addr, bias)                                           \
-    "dmfc1      %[all64],   "#fp"                                   \n\t"   \
-    "usd        %[all64],   "#bias"("#addr")                        \n\t"
+    ".set noat                                                      \n\t"   \
+    "dmfc1      $at,   "#fp"                                        \n\t"   \
+    "usd        $at,   "#bias"("#addr")                             \n\t"   \
+    ".set at                                                        \n\t"
 
 #define MMI_SDXC1(fp, addr, stride, bias)                                   \
-    PTR_ADDU    "%[addrt],  "#addr",    "#stride"                   \n\t"   \
-    MMI_SDC1(fp, %[addrt], bias)
+    ".set noat                                                 \n\t"   \
+    PTR_ADDU    "$at,  "#addr",    "#stride"                   \n\t"   \
+    MMI_SDC1(fp, $at, bias)                                            \
+    ".set at                                                   \n\t"
 
 #define MMI_LQ(reg1, reg2, addr, bias)                                      \
     "ld         "#reg1",    "#bias"("#addr")                        \n\t"   \
@@ -114,11 +143,6 @@
 
 #elif HAVE_LOONGSON3 /* !HAVE_LOONGSON2 */
 
-#define DECLARE_VAR_ALL64
-#define RESTRICT_ASM_ALL64
-#define DECLARE_VAR_ADDRT
-#define RESTRICT_ASM_ADDRT
-
 #define MMI_LWX(reg, addr, stride, bias)                                    \
     "gslwx      "#reg",     "#bias"("#addr", "#stride")             \n\t"
 
@@ -136,12 +160,12 @@
 
 #if _MIPS_SIM == _ABIO32 /* workaround for 3A2000 gslwlc1 bug */
 
-#define DECLARE_VAR_LOW32       int32_t low32
-#define RESTRICT_ASM_LOW32      [low32]"=&r"(low32),
-
-#define MMI_ULWC1(fp, addr, bias)                                           \
-    "ulw        %[low32],   "#bias"("#addr")                        \n\t"   \
-    "mtc1       %[low32],   "#fp"                                   \n\t"
+#define MMI_LWLRC1(fp, addr, bias, off)                                     \
+    ".set noat                                                 \n\t"   \
+    "lwl        $at,   "#bias"+"#off"("#addr")                 \n\t"   \
+    "lwr        $at,   "#bias"("#addr")                        \n\t"   \
+    "mtc1       $at,   "#fp"                                   \n\t"   \
+    ".set at                                                   \n\t"
 
 #else /* _MIPS_SIM != _ABIO32 */
 
@@ -202,25 +226,27 @@
 #endif /* HAVE_LOONGSON2 */
 
 /**
- * backup register
+ * Backup saved registers
+ * We're not using compiler's clobber list as it's not smart enough
+ * to take advantage of quad word load/store.
  */
 #define BACKUP_REG \
-  double temp_backup_reg[8];                                    \
+  LOCAL_ALIGNED_16(double, temp_backup_reg, [8]);               \
   if (_MIPS_SIM == _ABI64)                                      \
     __asm__ volatile (                                          \
-      "gssqc1       $f25,      $f24,       0x00(%[temp])  \n\t" \
-      "gssqc1       $f27,      $f26,       0x10(%[temp])  \n\t" \
-      "gssqc1       $f29,      $f28,       0x20(%[temp])  \n\t" \
-      "gssqc1       $f31,      $f30,       0x30(%[temp])  \n\t" \
+      MMI_SQC1($f25, $f24, %[temp], 0x00)                       \
+      MMI_SQC1($f27, $f26, %[temp], 0x10)                       \
+      MMI_SQC1($f29, $f28, %[temp], 0x20)                       \
+      MMI_SQC1($f31, $f30, %[temp], 0x30)                       \
       :                                                         \
       : [temp]"r"(temp_backup_reg)                              \
       : "memory"                                                \
     );                                                          \
   else                                                          \
     __asm__ volatile (                                          \
-      "gssqc1       $f22,      $f20,       0x00(%[temp])  \n\t" \
-      "gssqc1       $f26,      $f24,       0x10(%[temp])  \n\t" \
-      "gssqc1       $f30,      $f28,       0x20(%[temp])  \n\t" \
+      MMI_SQC1($f22, $f20, %[temp], 0x10)                       \
+      MMI_SQC1($f26, $f24, %[temp], 0x10)                       \
+      MMI_SQC1($f30, $f28, %[temp], 0x20)                       \
       :                                                         \
       : [temp]"r"(temp_backup_reg)                              \
       : "memory"                                                \
@@ -232,19 +258,19 @@
 #define RECOVER_REG \
   if (_MIPS_SIM == _ABI64)                                      \
     __asm__ volatile (                                          \
-      "gslqc1       $f25,      $f24,       0x00(%[temp])  \n\t" \
-      "gslqc1       $f27,      $f26,       0x10(%[temp])  \n\t" \
-      "gslqc1       $f29,      $f28,       0x20(%[temp])  \n\t" \
-      "gslqc1       $f31,      $f30,       0x30(%[temp])  \n\t" \
+      MMI_LQC1($f25, $f24, %[temp], 0x00)                       \
+      MMI_LQC1($f27, $f26, %[temp], 0x10)                       \
+      MMI_LQC1($f29, $f28, %[temp], 0x20)                       \
+      MMI_LQC1($f31, $f30, %[temp], 0x30)                       \
       :                                                         \
       : [temp]"r"(temp_backup_reg)                              \
       : "memory"                                                \
     );                                                          \
   else                                                          \
     __asm__ volatile (                                          \
-      "gslqc1       $f22,      $f20,       0x00(%[temp])  \n\t" \
-      "gslqc1       $f26,      $f24,       0x10(%[temp])  \n\t" \
-      "gslqc1       $f30,      $f28,       0x20(%[temp])  \n\t" \
+      MMI_LQC1($f22, $f20, %[temp], 0x10)                       \
+      MMI_LQC1($f26, $f24, %[temp], 0x10)                       \
+      MMI_LQC1($f30, $f28, %[temp], 0x20)                       \
       :                                                         \
       : [temp]"r"(temp_backup_reg)                              \
       : "memory"                                                \
diff --git a/libavutil/tests/cpu.c b/libavutil/tests/cpu.c
index ce45b715a0..0a6c0cd32e 100644
--- a/libavutil/tests/cpu.c
+++ b/libavutil/tests/cpu.c
@@ -49,6 +49,9 @@ static const struct {
     { AV_CPU_FLAG_SETEND,    "setend"     },
 #elif ARCH_PPC
     { AV_CPU_FLAG_ALTIVEC,   "altivec"    },
+#elif ARCH_MIPS
+    { AV_CPU_FLAG_MMI,       "mmi"        },
+    { AV_CPU_FLAG_MSA,       "msa"        },
 #elif ARCH_X86
     { AV_CPU_FLAG_MMX,       "mmx"        },
     { AV_CPU_FLAG_MMXEXT,    "mmxext"     },
@@ -74,6 +77,9 @@ static const struct {
     { AV_CPU_FLAG_BMI2,      "bmi2"       },
     { AV_CPU_FLAG_AESNI,     "aesni"      },
     { AV_CPU_FLAG_AVX512,    "avx512"     },
+#elif ARCH_LOONGARCH
+    { AV_CPU_FLAG_LSX,       "lsx"        },
+    { AV_CPU_FLAG_LASX,      "lasx"       },
 #endif
     { 0 }
 };
diff --git a/libavutil/timer.h b/libavutil/timer.h
index 0bb353cfce..50f36ae4e8 100644
--- a/libavutil/timer.h
+++ b/libavutil/timer.h
@@ -56,6 +56,8 @@
 #   include "ppc/timer.h"
 #elif ARCH_X86
 #   include "x86/timer.h"
+#elif ARCH_LOONGARCH
+#   include "loongarch/timer.h"
 #endif
 
 #if !defined(AV_READ_TIME)
diff --git a/libswscale/loongarch/Makefile b/libswscale/loongarch/Makefile
new file mode 100644
index 0000000000..8bb9254ca0
--- /dev/null
+++ b/libswscale/loongarch/Makefile
@@ -0,0 +1,12 @@
+OBJS-$(CONFIG_SWSCALE)      += loongarch/swscale_init_loongarch.o
+LASX-OBJS-$(CONFIG_SWSCALE) += loongarch/swscale_lasx.o \
+                               loongarch/output_lasx.o  \
+                               loongarch/input_lasx.o   \
+                               loongarch/yuv2rgb_lasx.o \
+                               loongarch/rgb2rgb_lasx.o
+LSX-OBJS-$(CONFIG_SWSCALE)  += loongarch/swscale.o \
+                               loongarch/swscale_lsx.o \
+                               loongarch/input.o   \
+                               loongarch/output.o  \
+                               loongarch/yuv2rgb_lsx.o \
+                               loongarch/output_lsx.o
diff --git a/libswscale/loongarch/input.S b/libswscale/loongarch/input.S
new file mode 100644
index 0000000000..d01f7384b1
--- /dev/null
+++ b/libswscale/loongarch/input.S
@@ -0,0 +1,285 @@
+/*
+ * Loongson LSX optimized swscale
+ *
+ * Copyright (c) 2023 Loongson Technology Corporation Limited
+ * Contributed by Lu Wang <wanglu@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "libavcodec/loongarch/loongson_asm.S"
+
+/* void planar_rgb_to_y_lsx(uint8_t *_dst, const uint8_t *src[4],
+ *                          int width, int32_t *rgb2yuv)
+ */
+function planar_rgb_to_y_lsx
+    ld.d            a5,     a1,    0
+    ld.d            a6,     a1,    8
+    ld.d            a7,     a1,    16
+
+    ld.w            t1,     a3,    0     // ry
+    ld.w            t2,     a3,    4     // gy
+    ld.w            t3,     a3,    8     // by
+    li.w            t4,     9
+    li.w            t5,     524544
+    li.w            t7,     4
+    li.w            t8,     8
+    vldi            vr7,    0
+    vreplgr2vr.w    vr1,    t1
+    vreplgr2vr.w    vr2,    t2
+    vreplgr2vr.w    vr3,    t3
+    vreplgr2vr.w    vr4,    t4
+    vreplgr2vr.w    vr5,    t5
+    bge             a2,     t8,    .WIDTH8
+    bge             a2,     t7,    .WIDTH4
+    blt             zero,   a2,    .WIDTH
+    b               .END
+
+.WIDTH8:
+    vld             vr8,    a5,    0
+    vld             vr9,    a6,    0
+    vld             vr10,   a7,    0
+    vilvl.b         vr11,   vr7,   vr8
+    vilvl.b         vr12,   vr7,   vr9
+    vilvl.b         vr13,   vr7,   vr10
+    vilvl.h         vr14,   vr7,   vr11
+    vilvl.h         vr15,   vr7,   vr12
+    vilvl.h         vr16,   vr7,   vr13
+    vilvh.h         vr17,   vr7,   vr11
+    vilvh.h         vr18,   vr7,   vr12
+    vilvh.h         vr19,   vr7,   vr13
+    vmul.w          vr20,   vr1,   vr16
+    vmul.w          vr21,   vr1,   vr19
+    vmadd.w         vr20,   vr2,   vr14
+    vmadd.w         vr20,   vr3,   vr15
+    vmadd.w         vr21,   vr2,   vr17
+    vmadd.w         vr21,   vr3,   vr18
+    vadd.w          vr20,   vr20,  vr5
+    vadd.w          vr21,   vr21,  vr5
+    vsra.w          vr20,   vr20,  vr4
+    vsra.w          vr21,   vr21,  vr4
+    vpickev.h       vr20,   vr21,  vr20
+    vst             vr20,   a0,    0
+    addi.d          a2,     a2,    -8
+    addi.d          a5,     a5,    8
+    addi.d          a6,     a6,    8
+    addi.d          a7,     a7,    8
+    addi.d          a0,     a0,    16
+    bge             a2,     t8,    .WIDTH8
+    bge             a2,     t7,    .WIDTH4
+    blt             zero,   a2,    .WIDTH
+    b               .END
+
+.WIDTH4:
+    vld             vr8,    a5,    0
+    vld             vr9,    a6,    0
+    vld             vr10,   a7,    0
+    vilvl.b         vr11,   vr7,   vr8
+    vilvl.b         vr12,   vr7,   vr9
+    vilvl.b         vr13,   vr7,   vr10
+    vilvl.h         vr14,   vr7,   vr11
+    vilvl.h         vr15,   vr7,   vr12
+    vilvl.h         vr16,   vr7,   vr13
+    vmul.w          vr17,   vr1,   vr16
+    vmadd.w         vr17,   vr2,   vr14
+    vmadd.w         vr17,   vr3,   vr15
+    vadd.w          vr17,   vr17,  vr5
+    vsra.w          vr17,   vr17,  vr4
+    vpickev.h       vr17,   vr17,  vr17
+    vstelm.d        vr17,   a0,    0,    0
+    addi.d          a2,     a2,    -4
+    addi.d          a5,     a5,    4
+    addi.d          a6,     a6,    4
+    addi.d          a7,     a7,    4
+    addi.d          a0,     a0,    8
+    bge             a2,     t7,    .WIDTH4
+    blt             zero,   a2,    .WIDTH
+    b               .END
+
+.WIDTH:
+    ld.bu           t0,     a5,    0
+    ld.bu           t4,     a6,    0
+    ld.bu           t6,     a7,    0
+    mul.w           t8,     t6,    t1
+    mul.w           t7,     t0,    t2
+    add.w           t8,     t8,    t7
+    mul.w           t7,     t4,    t3
+    add.w           t8,     t8,    t7
+    add.w           t8,     t8,    t5
+    srai.w          t8,     t8,    9
+    st.h            t8,     a0,    0
+    addi.d          a2,     a2,    -1
+    addi.d          a5,     a5,    1
+    addi.d          a6,     a6,    1
+    addi.d          a7,     a7,    1
+    addi.d          a0,     a0,    2
+    blt             zero,   a2,    .WIDTH
+.END:
+endfunc
+
+/* void planar_rgb_to_uv_lsx(uint8_t *_dstU, uint8_t *_dstV, const uint8_t *src[4],
+ *                           int width, int32_t *rgb2yuv)
+ */
+function planar_rgb_to_uv_lsx
+    addi.d          sp,     sp,    -24
+    st.d            s1,     sp,    0
+    st.d            s2,     sp,    8
+    st.d            s3,     sp,    16
+
+    ld.d            a5,     a2,    0
+    ld.d            a6,     a2,    8
+    ld.d            a7,     a2,    16
+    ld.w            t1,     a4,    12     // ru
+    ld.w            t2,     a4,    16     // gu
+    ld.w            t3,     a4,    20     // bu
+    ld.w            s1,     a4,    24     // rv
+    ld.w            s2,     a4,    28     // gv
+    ld.w            s3,     a4,    32     // bv
+    li.w            t4,     9
+    li.w            t5,     4194560
+    li.w            t7,     4
+    li.w            t8,     8
+    vldi            vr0,    0
+    vreplgr2vr.w    vr1,    t1
+    vreplgr2vr.w    vr2,    t2
+    vreplgr2vr.w    vr3,    t3
+    vreplgr2vr.w    vr4,    s1
+    vreplgr2vr.w    vr5,    s2
+    vreplgr2vr.w    vr6,    s3
+    vreplgr2vr.w    vr7,    t4
+    vreplgr2vr.w    vr8,    t5
+    bge             a2,     t8,    .LOOP_WIDTH8
+    bge             a2,     t7,    .LOOP_WIDTH4
+    blt             zero,   a2,    .LOOP_WIDTH
+    b               .LOOP_END
+
+.LOOP_WIDTH8:
+    vld             vr9,    a5,    0
+    vld             vr10,   a6,    0
+    vld             vr11,   a7,    0
+    vilvl.b         vr9,    vr0,   vr9
+    vilvl.b         vr10,   vr0,   vr10
+    vilvl.b         vr11,   vr0,   vr11
+    vilvl.h         vr12,   vr0,   vr9
+    vilvl.h         vr13,   vr0,   vr10
+    vilvl.h         vr14,   vr0,   vr11
+    vilvh.h         vr15,   vr0,   vr9
+    vilvh.h         vr16,   vr0,   vr10
+    vilvh.h         vr17,   vr0,   vr11
+    vmul.w          vr18,   vr1,   vr14
+    vmul.w          vr19,   vr1,   vr17
+    vmul.w          vr20,   vr4,   vr14
+    vmul.w          vr21,   vr4,   vr17
+    vmadd.w         vr18,   vr2,   vr12
+    vmadd.w         vr18,   vr3,   vr13
+    vmadd.w         vr19,   vr2,   vr15
+    vmadd.w         vr19,   vr3,   vr16
+    vmadd.w         vr20,   vr5,   vr12
+    vmadd.w         vr20,   vr6,   vr13
+    vmadd.w         vr21,   vr5,   vr15
+    vmadd.w         vr21,   vr6,   vr16
+    vadd.w          vr18,   vr18,  vr8
+    vadd.w          vr19,   vr19,  vr8
+    vadd.w          vr20,   vr20,  vr8
+    vadd.w          vr21,   vr21,  vr8
+    vsra.w          vr18,   vr18,  vr7
+    vsra.w          vr19,   vr19,  vr7
+    vsra.w          vr20,   vr20,  vr7
+    vsra.w          vr21,   vr21,  vr7
+    vpickev.h       vr18,   vr19,  vr18
+    vpickev.h       vr20,   vr21,  vr20
+    vst             vr18,   a0,    0
+    vst             vr20,   a1,    0
+    addi.d          a3,     a3,    -8
+    addi.d          a5,     a5,    8
+    addi.d          a6,     a6,    8
+    addi.d          a7,     a7,    8
+    addi.d          a0,     a0,    16
+    addi.d          a1,     a1,    16
+    bge             a3,     t8,    .LOOP_WIDTH8
+    bge             a3,     t7,    .LOOP_WIDTH4
+    blt             zero,   a3,    .LOOP_WIDTH
+    b               .LOOP_END
+
+.LOOP_WIDTH4:
+    vld             vr9,    a5,    0
+    vld             vr10,   a6,    0
+    vld             vr11,   a7,    0
+    vilvl.b         vr9,    vr0,   vr9
+    vilvl.b         vr10,   vr0,   vr10
+    vilvl.b         vr11,   vr0,   vr11
+    vilvl.h         vr12,   vr0,   vr9
+    vilvl.h         vr13,   vr0,   vr10
+    vilvl.h         vr14,   vr0,   vr11
+    vmul.w          vr18,   vr1,   vr14
+    vmul.w          vr19,   vr4,   vr14
+    vmadd.w         vr18,   vr2,   vr12
+    vmadd.w         vr18,   vr3,   vr13
+    vmadd.w         vr19,   vr5,   vr12
+    vmadd.w         vr19,   vr6,   vr13
+    vadd.w          vr18,   vr18,  vr8
+    vadd.w          vr19,   vr19,  vr8
+    vsra.w          vr18,   vr18,  vr7
+    vsra.w          vr19,   vr19,  vr7
+    vpickev.h       vr18,   vr18,  vr18
+    vpickev.h       vr19,   vr19,  vr19
+    vstelm.d        vr18,   a0,    0,    0
+    vstelm.d        vr19,   a1,    0,    0
+    addi.d          a3,     a3,    -4
+    addi.d          a5,     a5,    4
+    addi.d          a6,     a6,    4
+    addi.d          a7,     a7,    4
+    addi.d          a0,     a0,    8
+    addi.d          a1,     a1,    8
+    bge             a3,     t7,    .LOOP_WIDTH4
+    blt             zero,   a3,    .LOOP_WIDTH
+    b               .LOOP_END
+
+.LOOP_WIDTH:
+    ld.bu           t0,     a5,    0
+    ld.bu           t4,     a6,    0
+    ld.bu           t6,     a7,    0
+    mul.w           t8,     t6,    t1
+    mul.w           t7,     t0,    t2
+    add.w           t8,     t8,    t7
+    mul.w           t7,     t4,    t3
+    add.w           t8,     t8,    t7
+    add.w           t8,     t8,    t5
+    srai.w          t8,     t8,    9
+    st.h            t8,     a0,    0
+    mul.w           t8,     t6,    s1
+    mul.w           t7,     t0,    s2
+    add.w           t8,     t8,    t7
+    mul.w           t7,     t4,    s3
+    add.w           t8,     t8,    t7
+    add.w           t8,     t8,    t5
+    srai.w          t8,     t8,    9
+    st.h            t8,     a1,    0
+    addi.d          a3,     a3,    -1
+    addi.d          a5,     a5,    1
+    addi.d          a6,     a6,    1
+    addi.d          a7,     a7,    1
+    addi.d          a0,     a0,    2
+    addi.d          a1,     a1,    2
+    blt             zero,   a3,    .LOOP_WIDTH
+
+.LOOP_END:
+    ld.d            s1,     sp,    0
+    ld.d            s2,     sp,    8
+    ld.d            s3,     sp,    16
+    addi.d          sp,     sp,    24
+endfunc
diff --git a/libswscale/loongarch/input_lasx.c b/libswscale/loongarch/input_lasx.c
new file mode 100644
index 0000000000..bc18dd5c14
--- /dev/null
+++ b/libswscale/loongarch/input_lasx.c
@@ -0,0 +1,189 @@
+/*
+ * Copyright (C) 2021 Loongson Technology Corporation Limited
+ * Contributed by Hao Chen(chenhao@loongson.cn)
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "swscale_loongarch.h"
+#include "libavutil/loongarch/loongson_intrinsics.h"
+
+void planar_rgb_to_uv_lasx(uint8_t *_dstU, uint8_t *_dstV, const uint8_t *src[4],
+                           int width, int32_t *rgb2yuv)
+{
+    int i;
+    uint16_t *dstU   = (uint16_t *)_dstU;
+    uint16_t *dstV   = (uint16_t *)_dstV;
+    int set          = 0x4001 << (RGB2YUV_SHIFT - 7);
+    int len          = width - 15;
+    int32_t tem_ru   = rgb2yuv[RU_IDX], tem_gu = rgb2yuv[GU_IDX], tem_bu = rgb2yuv[BU_IDX];
+    int32_t tem_rv   = rgb2yuv[RV_IDX], tem_gv = rgb2yuv[GV_IDX], tem_bv = rgb2yuv[BV_IDX];
+    int shift        = RGB2YUV_SHIFT - 6;
+    const uint8_t *src0 = src[0], *src1 = src[1], *src2 = src[2];
+    __m256i ru, gu, bu, rv, gv, bv;
+    __m256i mask = {0x0D0C090805040100, 0x1D1C191815141110, 0x0D0C090805040100, 0x1D1C191815141110};
+    __m256i temp = __lasx_xvreplgr2vr_w(set);
+    __m256i sra  = __lasx_xvreplgr2vr_w(shift);
+
+    ru = __lasx_xvreplgr2vr_w(tem_ru);
+    gu = __lasx_xvreplgr2vr_w(tem_gu);
+    bu = __lasx_xvreplgr2vr_w(tem_bu);
+    rv = __lasx_xvreplgr2vr_w(tem_rv);
+    gv = __lasx_xvreplgr2vr_w(tem_gv);
+    bv = __lasx_xvreplgr2vr_w(tem_bv);
+    for (i = 0; i < len; i += 16) {
+        __m256i _g, _b, _r;
+        __m256i g_l, g_h, b_l, b_h, r_l, r_h;
+        __m256i v_l, v_h, u_l, u_h, u_lh, v_lh;
+
+        DUP2_ARG2(__lasx_xvld, src0 + i, 0, src1 + i, 0, _g, _b);
+        _r   = __lasx_xvld(src2 + i, 0);
+        g_l = __lasx_vext2xv_wu_bu(_g);
+        DUP2_ARG1(__lasx_vext2xv_wu_bu, _b, _r, b_l, r_l);
+        _g   = __lasx_xvpermi_d(_g, 0x01);
+        _b   = __lasx_xvpermi_d(_b, 0x01);
+        _r   = __lasx_xvpermi_d(_r, 0x01);
+        g_h = __lasx_vext2xv_wu_bu(_g);
+        DUP2_ARG1(__lasx_vext2xv_wu_bu, _b, _r, b_h, r_h);
+        u_l  = __lasx_xvmadd_w(temp, ru, r_l);
+        u_h  = __lasx_xvmadd_w(temp, ru, r_h);
+        v_l  = __lasx_xvmadd_w(temp, rv, r_l);
+        v_h  = __lasx_xvmadd_w(temp, rv, r_h);
+        u_l  = __lasx_xvmadd_w(u_l, gu, g_l);
+        u_l  = __lasx_xvmadd_w(u_l, bu, b_l);
+        u_h  = __lasx_xvmadd_w(u_h, gu, g_h);
+        u_h  = __lasx_xvmadd_w(u_h, bu, b_h);
+        v_l  = __lasx_xvmadd_w(v_l, gv, g_l);
+        v_l  = __lasx_xvmadd_w(v_l, bv, b_l);
+        v_h  = __lasx_xvmadd_w(v_h, gv, g_h);
+        v_h  = __lasx_xvmadd_w(v_h, bv, b_h);
+        u_l  = __lasx_xvsra_w(u_l, sra);
+        u_h  = __lasx_xvsra_w(u_h, sra);
+        v_l  = __lasx_xvsra_w(v_l, sra);
+        v_h  = __lasx_xvsra_w(v_h, sra);
+        DUP2_ARG3(__lasx_xvshuf_b, u_h, u_l, mask, v_h, v_l, mask, u_lh, v_lh);
+        u_lh = __lasx_xvpermi_d(u_lh, 0xD8);
+        v_lh = __lasx_xvpermi_d(v_lh, 0xD8);
+        __lasx_xvst(u_lh, (dstU + i), 0);
+        __lasx_xvst(v_lh, (dstV + i), 0);
+    }
+    if (width - i >= 8) {
+        __m256i _g, _b, _r;
+        __m256i g_l, b_l, r_l;
+        __m256i v_l, u_l, u, v;
+
+        _g  = __lasx_xvldrepl_d((src0 + i), 0);
+        _b  = __lasx_xvldrepl_d((src1 + i), 0);
+        _r  = __lasx_xvldrepl_d((src2 + i), 0);
+        g_l = __lasx_vext2xv_wu_bu(_g);
+        DUP2_ARG1(__lasx_vext2xv_wu_bu, _b, _r, b_l, r_l);
+        u_l = __lasx_xvmadd_w(temp, ru, r_l);
+        v_l = __lasx_xvmadd_w(temp, rv, r_l);
+        u_l = __lasx_xvmadd_w(u_l, gu, g_l);
+        u_l = __lasx_xvmadd_w(u_l, bu, b_l);
+        v_l = __lasx_xvmadd_w(v_l, gv, g_l);
+        v_l = __lasx_xvmadd_w(v_l, bv, b_l);
+        u_l = __lasx_xvsra_w(u_l, sra);
+        v_l = __lasx_xvsra_w(v_l, sra);
+        DUP2_ARG3(__lasx_xvshuf_b, u_l, u_l, mask, v_l, v_l, mask, u, v);
+        __lasx_xvstelm_d(u, (dstU + i), 0, 0);
+        __lasx_xvstelm_d(u, (dstU + i), 8, 2);
+        __lasx_xvstelm_d(v, (dstV + i), 0, 0);
+        __lasx_xvstelm_d(v, (dstV + i), 8, 2);
+        i += 8;
+    }
+    for (; i < width; i++) {
+        int g = src[0][i];
+        int b = src[1][i];
+        int r = src[2][i];
+
+        dstU[i] = (tem_ru * r + tem_gu * g + tem_bu * b + set) >> shift;
+        dstV[i] = (tem_rv * r + tem_gv * g + tem_bv * b + set) >> shift;
+    }
+}
+
+void planar_rgb_to_y_lasx(uint8_t *_dst, const uint8_t *src[4], int width,
+                          int32_t *rgb2yuv)
+{
+    int i;
+    int shift        = (RGB2YUV_SHIFT - 6);
+    int set          = 0x801 << (RGB2YUV_SHIFT - 7);
+    int len          = width - 15;
+    uint16_t *dst    = (uint16_t *)_dst;
+    int32_t tem_ry   = rgb2yuv[RY_IDX], tem_gy = rgb2yuv[GY_IDX];
+    int32_t tem_by   = rgb2yuv[BY_IDX];
+    const uint8_t *src0 = src[0], *src1 = src[1], *src2 = src[2];
+    __m256i mask = {0x0D0C090805040100, 0x1D1C191815141110, 0x0D0C090805040100, 0x1D1C191815141110};
+    __m256i temp = __lasx_xvreplgr2vr_w(set);
+    __m256i sra  = __lasx_xvreplgr2vr_w(shift);
+    __m256i ry   = __lasx_xvreplgr2vr_w(tem_ry);
+    __m256i gy   = __lasx_xvreplgr2vr_w(tem_gy);
+    __m256i by   = __lasx_xvreplgr2vr_w(tem_by);
+
+    for (i = 0; i < len; i += 16) {
+        __m256i _g, _b, _r;
+        __m256i g_l, g_h, b_l, b_h, r_l, r_h;
+        __m256i y_l, y_h, y_lh;
+
+        DUP2_ARG2(__lasx_xvld, src0 + i, 0, src1 + i, 0, _g, _b);
+        _r   = __lasx_xvld(src2 + i, 0);
+        g_l = __lasx_vext2xv_wu_bu(_g);
+        DUP2_ARG1(__lasx_vext2xv_wu_bu, _b, _r, b_l, r_l);
+        _g   = __lasx_xvpermi_d(_g, 0x01);
+        _b   = __lasx_xvpermi_d(_b, 0x01);
+        _r   = __lasx_xvpermi_d(_r, 0x01);
+        g_h  = __lasx_vext2xv_wu_bu(_g);
+        DUP2_ARG1(__lasx_vext2xv_wu_bu,_b, _r, b_h, r_h);
+        y_l  = __lasx_xvmadd_w(temp, ry, r_l);
+        y_h  = __lasx_xvmadd_w(temp, ry, r_h);
+        y_l  = __lasx_xvmadd_w(y_l, gy, g_l);
+        y_l  = __lasx_xvmadd_w(y_l, by, b_l);
+        y_h  = __lasx_xvmadd_w(y_h, gy, g_h);
+        y_h  = __lasx_xvmadd_w(y_h, by, b_h);
+        y_l  = __lasx_xvsra_w(y_l, sra);
+        y_h  = __lasx_xvsra_w(y_h, sra);
+        y_lh = __lasx_xvshuf_b(y_h, y_l, mask);
+        y_lh = __lasx_xvpermi_d(y_lh, 0xD8);
+        __lasx_xvst(y_lh, (dst + i), 0);
+    }
+    if (width - i >= 8) {
+        __m256i _g, _b, _r;
+        __m256i g_l, b_l, r_l;
+        __m256i y_l, y;
+
+        _g  = __lasx_xvldrepl_d((src0 + i), 0);
+        _b  = __lasx_xvldrepl_d((src1 + i), 0);
+        _r  = __lasx_xvldrepl_d((src2 + i), 0);
+        g_l = __lasx_vext2xv_wu_bu(_g);
+        DUP2_ARG1(__lasx_vext2xv_wu_bu, _b, _r, b_l, r_l);
+        y_l = __lasx_xvmadd_w(temp, ry, r_l);
+        y_l = __lasx_xvmadd_w(y_l, gy, g_l);
+        y_l = __lasx_xvmadd_w(y_l, by, b_l);
+        y_l = __lasx_xvsra_w(y_l, sra);
+        y = __lasx_xvshuf_b(y_l, y_l, mask);
+        __lasx_xvstelm_d(y, (dst + i), 0, 0);
+        __lasx_xvstelm_d(y, (dst + i), 8, 2);
+        i += 8;
+    }
+    for (; i < width; i++) {
+        int g = src[0][i];
+        int b = src[1][i];
+        int r = src[2][i];
+
+        dst[i] = (tem_ry * r + tem_gy * g + tem_by * b + set) >> shift;
+    }
+}
diff --git a/libswscale/loongarch/output.S b/libswscale/loongarch/output.S
new file mode 100644
index 0000000000..b44bac502a
--- /dev/null
+++ b/libswscale/loongarch/output.S
@@ -0,0 +1,138 @@
+/*
+ * Loongson LSX optimized swscale
+ *
+ * Copyright (c) 2023 Loongson Technology Corporation Limited
+ * Contributed by Lu Wang <wanglu@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "libavcodec/loongarch/loongson_asm.S"
+
+/* static void ff_yuv2planeX_8_lsx(const int16_t *filter, int filterSize,
+ *                                 const int16_t **src, uint8_t *dest, int dstW,
+ *                                 const uint8_t *dither, int offset)
+ */
+function ff_yuv2planeX_8_lsx
+    addi.w          t1,     a6,     1
+    addi.w          t2,     a6,     2
+    addi.w          t3,     a6,     3
+    addi.w          t4,     a6,     4
+    addi.w          t5,     a6,     5
+    addi.w          t6,     a6,     6
+    addi.w          t7,     a6,     7
+    andi            t0,     a6,     7
+    andi            t1,     t1,     7
+    andi            t2,     t2,     7
+    andi            t3,     t3,     7
+    andi            t4,     t4,     7
+    andi            t5,     t5,     7
+    andi            t6,     t6,     7
+    andi            t7,     t7,     7
+    ldx.bu          t0,     a5,     t0
+    ldx.bu          t1,     a5,     t1
+    ldx.bu          t2,     a5,     t2
+    ldx.bu          t3,     a5,     t3
+    ldx.bu          t4,     a5,     t4
+    ldx.bu          t5,     a5,     t5
+    ldx.bu          t6,     a5,     t6
+    ldx.bu          t7,     a5,     t7
+    vreplgr2vr.w    vr0,    t0
+    vreplgr2vr.w    vr1,    t1
+    vreplgr2vr.w    vr2,    t2
+    vreplgr2vr.w    vr3,    t3
+    vreplgr2vr.w    vr4,    t4
+    vreplgr2vr.w    vr5,    t5
+    vreplgr2vr.w    vr6,    t6
+    vreplgr2vr.w    vr7,    t7
+    vilvl.w         vr0,    vr2,    vr0
+    vilvl.w         vr4,    vr6,    vr4
+    vilvl.w         vr1,    vr3,    vr1
+    vilvl.w         vr5,    vr7,    vr5
+    vilvl.d         vr12,   vr4,    vr0
+    vilvl.d         vr13,   vr5,    vr1
+    li.w            t5,     0
+    li.w            t8,     8
+    bge             a4,     t8,     .WIDTH8
+    blt             zero,   a4,     .WIDTH
+    b               .END
+
+.WIDTH8:
+    li.d            t1,     0
+    li.d            t4,     0
+    vslli.w         vr2,    vr12,   12
+    vslli.w         vr3,    vr13,   12
+    move            t3,     a0
+
+.FILTERSIZE8:
+    ldx.d           t2,     a2,     t1
+    vldx            vr4,    t2,     t5
+    vldrepl.h       vr5,    t3,     0
+    vmaddwev.w.h    vr2,    vr4,    vr5
+    vmaddwod.w.h    vr3,    vr4,    vr5
+    addi.d          t1,     t1,     8
+    addi.d          t3,     t3,     2
+    addi.d          t4,     t4,     1
+    blt             t4,     a1,     .FILTERSIZE8
+    vsrai.w         vr2,    vr2,    19
+    vsrai.w         vr3,    vr3,    19
+    vclip255.w      vr2,    vr2
+    vclip255.w      vr3,    vr3
+    vpickev.h       vr2,    vr3,    vr2
+    vpickev.b       vr2,    vr2,    vr2
+    vbsrl.v         vr3,    vr2,    4
+    vilvl.b         vr2,    vr3,    vr2
+    fst.d           f2,     a3,     0
+    addi.d          t5,     t5,     16
+    addi.d          a4,     a4,     -8
+    addi.d          a3,     a3,     8
+    bge             a4,     t8,     .WIDTH8
+    blt             zero,   a4,     .WIDTH
+    b               .END
+
+.WIDTH:
+    li.d            t1,     0
+    li.d            t4,     0
+    vslli.w         vr2,    vr12,   12
+    vslli.w         vr3,    vr13,   12
+.FILTERSIZE:
+    ldx.d           t2,     a2,     t1
+    vldx            vr4,    t2,     t5
+    vldrepl.h       vr5,    a0,     0
+    vmaddwev.w.h    vr2,    vr4,    vr5
+    vmaddwod.w.h    vr3,    vr4,    vr5
+    addi.d          t1,     t1,     8
+    addi.d          a0,     a0,     2
+    addi.d          t4,     t4,     1
+    blt             t4,     a1,     .FILTERSIZE
+    vsrai.w         vr2,    vr2,    19
+    vsrai.w         vr3,    vr3,    19
+    vclip255.w      vr2,    vr2
+    vclip255.w      vr3,    vr3
+    vpickev.h       vr2,    vr3,    vr2
+    vpickev.b       vr2,    vr2,    vr2
+    vbsrl.v         vr3,    vr2,    4
+    vilvl.b         vr2,    vr3,    vr2
+
+.DEST:
+    vstelm.b        vr2,    a3,     0,    0
+    vbsrl.v         vr2,    vr2,    1
+    addi.d          a4,     a4,     -1
+    addi.d          a3,     a3,     1
+    blt             zero,   a4,     .DEST
+.END:
+endfunc
diff --git a/libswscale/loongarch/output_lasx.c b/libswscale/loongarch/output_lasx.c
new file mode 100644
index 0000000000..cc1f25cdcb
--- /dev/null
+++ b/libswscale/loongarch/output_lasx.c
@@ -0,0 +1,1980 @@
+/*
+ * Copyright (C) 2021 Loongson Technology Corporation Limited
+ * Contributed by Hao Chen(chenhao@loongson.cn)
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "swscale_loongarch.h"
+#include "libavutil/loongarch/loongson_intrinsics.h"
+
+void ff_yuv2planeX_8_lasx(const int16_t *filter, int filterSize,
+                          const int16_t **src, uint8_t *dest, int dstW,
+                          const uint8_t *dither, int offset)
+{
+    int i;
+    int len = dstW - 15;
+    __m256i mask = {0x1C0C180814041000, 0x1C1814100C080400,
+                    0x1C0C180814041000, 0x1C1814100C080400};
+    __m256i val1, val2, val3;
+    uint8_t dither0 = dither[offset & 7];
+    uint8_t dither1 = dither[(offset + 1) & 7];
+    uint8_t dither2 = dither[(offset + 2) & 7];
+    uint8_t dither3 = dither[(offset + 3) & 7];
+    uint8_t dither4 = dither[(offset + 4) & 7];
+    uint8_t dither5 = dither[(offset + 5) & 7];
+    uint8_t dither6 = dither[(offset + 6) & 7];
+    uint8_t dither7 = dither[(offset + 7) & 7];
+    int val_1[8] = {dither0, dither2, dither4, dither6,
+                    dither0, dither2, dither4, dither6};
+    int val_2[8] = {dither1, dither3, dither5, dither7,
+                    dither1, dither3, dither5, dither7};
+    int val_3[8] = {dither0, dither1, dither2, dither3,
+                    dither4, dither5, dither6, dither7};
+
+    DUP2_ARG2(__lasx_xvld, val_1, 0, val_2, 0, val1, val2);
+    val3 = __lasx_xvld(val_3, 0);
+
+    for (i = 0; i < len; i += 16) {
+        int j;
+        __m256i src0, filter0, val;
+        __m256i val_ev, val_od;
+
+        val_ev = __lasx_xvslli_w(val1, 12);
+        val_od = __lasx_xvslli_w(val2, 12);
+
+        for (j = 0; j < filterSize; j++) {
+            src0  = __lasx_xvld(src[j]+ i, 0);
+            filter0 = __lasx_xvldrepl_h((filter + j), 0);
+            val_ev = __lasx_xvmaddwev_w_h(val_ev, src0, filter0);
+            val_od = __lasx_xvmaddwod_w_h(val_od, src0, filter0);
+        }
+        val_ev = __lasx_xvsrai_w(val_ev, 19);
+        val_od = __lasx_xvsrai_w(val_od, 19);
+        val_ev = __lasx_xvclip255_w(val_ev);
+        val_od = __lasx_xvclip255_w(val_od);
+        val    = __lasx_xvshuf_b(val_od, val_ev, mask);
+        __lasx_xvstelm_d(val, (dest + i), 0, 0);
+        __lasx_xvstelm_d(val, (dest + i), 8, 2);
+    }
+    if (dstW - i >= 8){
+        int j;
+        __m256i src0, filter0, val_h;
+        __m256i val_l;
+
+        val_l = __lasx_xvslli_w(val3, 12);
+
+        for (j = 0; j < filterSize; j++) {
+            src0  = __lasx_xvld(src[j] + i, 0);
+            src0  = __lasx_vext2xv_w_h(src0);
+            filter0 = __lasx_xvldrepl_h((filter + j), 0);
+            filter0 = __lasx_vext2xv_w_h(filter0);
+            val_l = __lasx_xvmadd_w(val_l, src0, filter0);
+        }
+        val_l = __lasx_xvsrai_w(val_l, 19);
+        val_l = __lasx_xvclip255_w(val_l);
+        val_h = __lasx_xvpermi_d(val_l, 0x4E);
+        val_l = __lasx_xvshuf_b(val_h, val_l, mask);
+        __lasx_xvstelm_d(val_l, (dest + i), 0, 1);
+        i += 8;
+    }
+    for (; i < dstW; i++) {
+        int val = dither[(i + offset) & 7] << 12;
+        int j;
+        for (j = 0; j< filterSize; j++)
+            val += src[j][i] * filter[j];
+
+        dest[i] = av_clip_uint8(val >> 19);
+    }
+}
+
+/*Copy from libswscale/output.c*/
+static av_always_inline void
+yuv2rgb_write(uint8_t *_dest, int i, int Y1, int Y2,
+              unsigned A1, unsigned A2,
+              const void *_r, const void *_g, const void *_b, int y,
+              enum AVPixelFormat target, int hasAlpha)
+{
+    if (target == AV_PIX_FMT_ARGB || target == AV_PIX_FMT_RGBA ||
+        target == AV_PIX_FMT_ABGR || target == AV_PIX_FMT_BGRA) {
+        uint32_t *dest = (uint32_t *) _dest;
+        const uint32_t *r = (const uint32_t *) _r;
+        const uint32_t *g = (const uint32_t *) _g;
+        const uint32_t *b = (const uint32_t *) _b;
+
+#if CONFIG_SMALL
+        dest[i * 2 + 0] = r[Y1] + g[Y1] + b[Y1];
+        dest[i * 2 + 1] = r[Y2] + g[Y2] + b[Y2];
+#else
+#if defined(ASSERT_LEVEL) && ASSERT_LEVEL > 1
+        int sh = (target == AV_PIX_FMT_RGB32_1 ||
+                  target == AV_PIX_FMT_BGR32_1) ? 0 : 24;
+        av_assert2((((r[Y1] + g[Y1] + b[Y1]) >> sh) & 0xFF) == 0xFF);
+#endif
+        dest[i * 2 + 0] = r[Y1] + g[Y1] + b[Y1];
+        dest[i * 2 + 1] = r[Y2] + g[Y2] + b[Y2];
+#endif
+    } else if (target == AV_PIX_FMT_RGB24 || target == AV_PIX_FMT_BGR24) {
+        uint8_t *dest = (uint8_t *) _dest;
+        const uint8_t *r = (const uint8_t *) _r;
+        const uint8_t *g = (const uint8_t *) _g;
+        const uint8_t *b = (const uint8_t *) _b;
+
+#define r_b ((target == AV_PIX_FMT_RGB24) ? r : b)
+#define b_r ((target == AV_PIX_FMT_RGB24) ? b : r)
+
+        dest[i * 6 + 0] = r_b[Y1];
+        dest[i * 6 + 1] =   g[Y1];
+        dest[i * 6 + 2] = b_r[Y1];
+        dest[i * 6 + 3] = r_b[Y2];
+        dest[i * 6 + 4] =   g[Y2];
+        dest[i * 6 + 5] = b_r[Y2];
+#undef r_b
+#undef b_r
+    } else if (target == AV_PIX_FMT_RGB565 || target == AV_PIX_FMT_BGR565 ||
+               target == AV_PIX_FMT_RGB555 || target == AV_PIX_FMT_BGR555 ||
+               target == AV_PIX_FMT_RGB444 || target == AV_PIX_FMT_BGR444) {
+        uint16_t *dest = (uint16_t *) _dest;
+        const uint16_t *r = (const uint16_t *) _r;
+        const uint16_t *g = (const uint16_t *) _g;
+        const uint16_t *b = (const uint16_t *) _b;
+        int dr1, dg1, db1, dr2, dg2, db2;
+
+        if (target == AV_PIX_FMT_RGB565 || target == AV_PIX_FMT_BGR565) {
+            dr1 = ff_dither_2x2_8[ y & 1     ][0];
+            dg1 = ff_dither_2x2_4[ y & 1     ][0];
+            db1 = ff_dither_2x2_8[(y & 1) ^ 1][0];
+            dr2 = ff_dither_2x2_8[ y & 1     ][1];
+            dg2 = ff_dither_2x2_4[ y & 1     ][1];
+            db2 = ff_dither_2x2_8[(y & 1) ^ 1][1];
+    } else if (target == AV_PIX_FMT_RGB555 || target == AV_PIX_FMT_BGR555) {
+            dr1 = ff_dither_2x2_8[ y & 1     ][0];
+            dg1 = ff_dither_2x2_8[ y & 1     ][1];
+            db1 = ff_dither_2x2_8[(y & 1) ^ 1][0];
+            dr2 = ff_dither_2x2_8[ y & 1     ][1];
+            dg2 = ff_dither_2x2_8[ y & 1     ][0];
+            db2 = ff_dither_2x2_8[(y & 1) ^ 1][1];
+        } else {
+            dr1 = ff_dither_4x4_16[ y & 3     ][0];
+            dg1 = ff_dither_4x4_16[ y & 3     ][1];
+            db1 = ff_dither_4x4_16[(y & 3) ^ 3][0];
+            dr2 = ff_dither_4x4_16[ y & 3     ][1];
+            dg2 = ff_dither_4x4_16[ y & 3     ][0];
+            db2 = ff_dither_4x4_16[(y & 3) ^ 3][1];
+        }
+
+        dest[i * 2 + 0] = r[Y1 + dr1] + g[Y1 + dg1] + b[Y1 + db1];
+        dest[i * 2 + 1] = r[Y2 + dr2] + g[Y2 + dg2] + b[Y2 + db2];
+    } else /* 8/4 bits */ {
+        uint8_t *dest = (uint8_t *) _dest;
+        const uint8_t *r = (const uint8_t *) _r;
+        const uint8_t *g = (const uint8_t *) _g;
+        const uint8_t *b = (const uint8_t *) _b;
+        int dr1, dg1, db1, dr2, dg2, db2;
+
+        if (target == AV_PIX_FMT_RGB8 || target == AV_PIX_FMT_BGR8) {
+            const uint8_t * const d64 = ff_dither_8x8_73[y & 7];
+            const uint8_t * const d32 = ff_dither_8x8_32[y & 7];
+            dr1 = dg1 = d32[(i * 2 + 0) & 7];
+            db1 =       d64[(i * 2 + 0) & 7];
+            dr2 = dg2 = d32[(i * 2 + 1) & 7];
+            db2 =       d64[(i * 2 + 1) & 7];
+        } else {
+            const uint8_t * const d64  = ff_dither_8x8_73 [y & 7];
+            const uint8_t * const d128 = ff_dither_8x8_220[y & 7];
+            dr1 = db1 = d128[(i * 2 + 0) & 7];
+            dg1 =        d64[(i * 2 + 0) & 7];
+            dr2 = db2 = d128[(i * 2 + 1) & 7];
+            dg2 =        d64[(i * 2 + 1) & 7];
+        }
+
+        if (target == AV_PIX_FMT_RGB4 || target == AV_PIX_FMT_BGR4) {
+            dest[i] = r[Y1 + dr1] + g[Y1 + dg1] + b[Y1 + db1] +
+                    ((r[Y2 + dr2] + g[Y2 + dg2] + b[Y2 + db2]) << 4);
+        } else {
+            dest[i * 2 + 0] = r[Y1 + dr1] + g[Y1 + dg1] + b[Y1 + db1];
+            dest[i * 2 + 1] = r[Y2 + dr2] + g[Y2 + dg2] + b[Y2 + db2];
+        }
+    }
+}
+
+#define WRITE_YUV2RGB(vec_y1, vec_y2, vec_u, vec_v, t1, t2, t3, t4)    \
+{                                                                      \
+    Y1 = __lasx_xvpickve2gr_w(vec_y1, t1);                             \
+    Y2 = __lasx_xvpickve2gr_w(vec_y2, t2);                             \
+    U  = __lasx_xvpickve2gr_w(vec_u, t3);                              \
+    V  = __lasx_xvpickve2gr_w(vec_v, t4);                              \
+    r  =  c->table_rV[V];                                              \
+    g  = (c->table_gU[U] + c->table_gV[V]);                            \
+    b  =  c->table_bU[U];                                              \
+    yuv2rgb_write(dest, count, Y1, Y2, 0, 0,                           \
+                  r, g, b, y, target, 0);                              \
+    count++;                                                           \
+}
+
+static void
+yuv2rgb_X_template_lasx(SwsContext *c, const int16_t *lumFilter,
+                        const int16_t **lumSrc, int lumFilterSize,
+                        const int16_t *chrFilter, const int16_t **chrUSrc,
+                        const int16_t **chrVSrc, int chrFilterSize,
+                        const int16_t **alpSrc, uint8_t *dest, int dstW,
+                        int y, enum AVPixelFormat target, int hasAlpha)
+{
+    int i, j;
+    int count = 0;
+    int t     = 1 << 18;
+    int len   = dstW >> 6;
+    int res   = dstW & 63;
+    int len_count = (dstW + 1) >> 1;
+    const void *r, *g, *b;
+    int head = YUVRGB_TABLE_HEADROOM;
+    __m256i headroom  = __lasx_xvreplgr2vr_w(head);
+
+    for (i = 0; i < len; i++) {
+        int Y1, Y2, U, V, count_lum = count << 1;
+        __m256i l_src1, l_src2, l_src3, l_src4, u_src1, u_src2, v_src1, v_src2;
+        __m256i yl1_ev, yl1_od, yh1_ev, yh1_od, yl2_ev, yl2_od, yh2_ev, yh2_od;
+        __m256i u1_ev, u1_od, v1_ev, v1_od, u2_ev, u2_od, v2_ev, v2_od, temp;
+
+        yl1_ev = __lasx_xvldrepl_w(&t, 0);
+        yl1_od = yl1_ev;
+        yh1_ev = yl1_ev;
+        yh1_od = yl1_ev;
+        u1_ev  = yl1_ev;
+        v1_ev  = yl1_ev;
+        u1_od  = yl1_ev;
+        v1_od  = yl1_ev;
+        yl2_ev = yl1_ev;
+        yl2_od = yl1_ev;
+        yh2_ev = yl1_ev;
+        yh2_od = yl1_ev;
+        u2_ev  = yl1_ev;
+        v2_ev  = yl1_ev;
+        u2_od  = yl1_ev;
+        v2_od  = yl1_ev;
+        for (j = 0; j < lumFilterSize; j++) {
+            int16_t *src_lum = lumSrc[j] + count_lum;
+            temp    = __lasx_xvldrepl_h((lumFilter + j), 0);
+            DUP4_ARG2(__lasx_xvld, src_lum, 0, src_lum, 32, src_lum, 64,
+                      src_lum, 96, l_src1, l_src2, l_src3, l_src4);
+
+            yl1_ev  = __lasx_xvmaddwev_w_h(yl1_ev, temp, l_src1);
+            yl1_od  = __lasx_xvmaddwod_w_h(yl1_od, temp, l_src1);
+            yh1_ev  = __lasx_xvmaddwev_w_h(yh1_ev, temp, l_src2);
+            yh1_od  = __lasx_xvmaddwod_w_h(yh1_od, temp, l_src2);
+            yl2_ev  = __lasx_xvmaddwev_w_h(yl2_ev, temp, l_src3);
+            yl2_od  = __lasx_xvmaddwod_w_h(yl2_od, temp, l_src3);
+            yh2_ev  = __lasx_xvmaddwev_w_h(yh2_ev, temp, l_src4);
+            yh2_od  = __lasx_xvmaddwod_w_h(yh2_od, temp, l_src4);
+        }
+        for (j = 0; j < chrFilterSize; j++) {
+            DUP2_ARG2(__lasx_xvld, chrUSrc[j] + count, 0, chrUSrc[j] + count, 32,
+                      u_src1, u_src2);
+            DUP2_ARG2(__lasx_xvld, chrVSrc[j] + count, 0, chrVSrc[j] + count, 32,
+                      v_src1, v_src2);
+            temp  = __lasx_xvldrepl_h((chrFilter + j), 0);
+            u1_ev  = __lasx_xvmaddwev_w_h(u1_ev, temp, u_src1);
+            u1_od  = __lasx_xvmaddwod_w_h(u1_od, temp, u_src1);
+            v1_ev  = __lasx_xvmaddwev_w_h(v1_ev, temp, v_src1);
+            v1_od  = __lasx_xvmaddwod_w_h(v1_od, temp, v_src1);
+            u2_ev  = __lasx_xvmaddwev_w_h(u2_ev, temp, u_src2);
+            u2_od  = __lasx_xvmaddwod_w_h(u2_od, temp, u_src2);
+            v2_ev  = __lasx_xvmaddwev_w_h(v2_ev, temp, v_src2);
+            v2_od  = __lasx_xvmaddwod_w_h(v2_od, temp, v_src2);
+        }
+        yl1_ev = __lasx_xvsrai_w(yl1_ev, 19);
+        yh1_ev = __lasx_xvsrai_w(yh1_ev, 19);
+        yl1_od = __lasx_xvsrai_w(yl1_od, 19);
+        yh1_od = __lasx_xvsrai_w(yh1_od, 19);
+        u1_ev  = __lasx_xvsrai_w(u1_ev, 19);
+        v1_ev  = __lasx_xvsrai_w(v1_ev, 19);
+        u1_od  = __lasx_xvsrai_w(u1_od, 19);
+        v1_od  = __lasx_xvsrai_w(v1_od, 19);
+        yl2_ev = __lasx_xvsrai_w(yl2_ev, 19);
+        yh2_ev = __lasx_xvsrai_w(yh2_ev, 19);
+        yl2_od = __lasx_xvsrai_w(yl2_od, 19);
+        yh2_od = __lasx_xvsrai_w(yh2_od, 19);
+        u2_ev  = __lasx_xvsrai_w(u2_ev, 19);
+        v2_ev  = __lasx_xvsrai_w(v2_ev, 19);
+        u2_od  = __lasx_xvsrai_w(u2_od, 19);
+        v2_od  = __lasx_xvsrai_w(v2_od, 19);
+        u1_ev  = __lasx_xvadd_w(u1_ev, headroom);
+        v1_ev  = __lasx_xvadd_w(v1_ev, headroom);
+        u1_od  = __lasx_xvadd_w(u1_od, headroom);
+        v1_od  = __lasx_xvadd_w(v1_od, headroom);
+        u2_ev  = __lasx_xvadd_w(u2_ev, headroom);
+        v2_ev  = __lasx_xvadd_w(v2_ev, headroom);
+        u2_od  = __lasx_xvadd_w(u2_od, headroom);
+        v2_od  = __lasx_xvadd_w(v2_od, headroom);
+        WRITE_YUV2RGB(yl1_ev, yl1_od, u1_ev, v1_ev, 0, 0, 0, 0);
+        WRITE_YUV2RGB(yl1_ev, yl1_od, u1_od, v1_od, 1, 1, 0, 0);
+        WRITE_YUV2RGB(yl1_ev, yl1_od, u1_ev, v1_ev, 2, 2, 1, 1);
+        WRITE_YUV2RGB(yl1_ev, yl1_od, u1_od, v1_od, 3, 3, 1, 1);
+        WRITE_YUV2RGB(yl1_ev, yl1_od, u1_ev, v1_ev, 4, 4, 2, 2);
+        WRITE_YUV2RGB(yl1_ev, yl1_od, u1_od, v1_od, 5, 5, 2, 2);
+        WRITE_YUV2RGB(yl1_ev, yl1_od, u1_ev, v1_ev, 6, 6, 3, 3);
+        WRITE_YUV2RGB(yl1_ev, yl1_od, u1_od, v1_od, 7, 7, 3, 3);
+        WRITE_YUV2RGB(yh1_ev, yh1_od, u1_ev, v1_ev, 0, 0, 4, 4);
+        WRITE_YUV2RGB(yh1_ev, yh1_od, u1_od, v1_od, 1, 1, 4, 4);
+        WRITE_YUV2RGB(yh1_ev, yh1_od, u1_ev, v1_ev, 2, 2, 5, 5);
+        WRITE_YUV2RGB(yh1_ev, yh1_od, u1_od, v1_od, 3, 3, 5, 5);
+        WRITE_YUV2RGB(yh1_ev, yh1_od, u1_ev, v1_ev, 4, 4, 6, 6);
+        WRITE_YUV2RGB(yh1_ev, yh1_od, u1_od, v1_od, 5, 5, 6, 6);
+        WRITE_YUV2RGB(yh1_ev, yh1_od, u1_ev, v1_ev, 6, 6, 7, 7);
+        WRITE_YUV2RGB(yh1_ev, yh1_od, u1_od, v1_od, 7, 7, 7, 7);
+        WRITE_YUV2RGB(yl2_ev, yl2_od, u2_ev, v2_ev, 0, 0, 0, 0);
+        WRITE_YUV2RGB(yl2_ev, yl2_od, u2_od, v2_od, 1, 1, 0, 0);
+        WRITE_YUV2RGB(yl2_ev, yl2_od, u2_ev, v2_ev, 2, 2, 1, 1);
+        WRITE_YUV2RGB(yl2_ev, yl2_od, u2_od, v2_od, 3, 3, 1, 1);
+        WRITE_YUV2RGB(yl2_ev, yl2_od, u2_ev, v2_ev, 4, 4, 2, 2);
+        WRITE_YUV2RGB(yl2_ev, yl2_od, u2_od, v2_od, 5, 5, 2, 2);
+        WRITE_YUV2RGB(yl2_ev, yl2_od, u2_ev, v2_ev, 6, 6, 3, 3);
+        WRITE_YUV2RGB(yl2_ev, yl2_od, u2_od, v2_od, 7, 7, 3, 3);
+        WRITE_YUV2RGB(yh2_ev, yh2_od, u2_ev, v2_ev, 0, 0, 4, 4);
+        WRITE_YUV2RGB(yh2_ev, yh2_od, u2_od, v2_od, 1, 1, 4, 4);
+        WRITE_YUV2RGB(yh2_ev, yh2_od, u2_ev, v2_ev, 2, 2, 5, 5);
+        WRITE_YUV2RGB(yh2_ev, yh2_od, u2_od, v2_od, 3, 3, 5, 5);
+        WRITE_YUV2RGB(yh2_ev, yh2_od, u2_ev, v2_ev, 4, 4, 6, 6);
+        WRITE_YUV2RGB(yh2_ev, yh2_od, u2_od, v2_od, 5, 5, 6, 6);
+        WRITE_YUV2RGB(yh2_ev, yh2_od, u2_ev, v2_ev, 6, 6, 7, 7);
+        WRITE_YUV2RGB(yh2_ev, yh2_od, u2_od, v2_od, 7, 7, 7, 7);
+    }
+    if (res >= 32) {
+        int Y1, Y2, U, V, count_lum = count << 1;
+        __m256i l_src1, l_src2, u_src, v_src;
+        __m256i yl_ev, yl_od, yh_ev, yh_od;
+        __m256i u_ev, u_od, v_ev, v_od, temp;
+
+        yl_ev = __lasx_xvldrepl_w(&t, 0);
+        yl_od = yl_ev;
+        yh_ev = yl_ev;
+        yh_od = yl_ev;
+        u_ev  = yl_ev;
+        v_ev  = yl_ev;
+        u_od  = yl_ev;
+        v_od  = yl_ev;
+        for (j = 0; j < lumFilterSize; j++) {
+            temp   = __lasx_xvldrepl_h((lumFilter + j), 0);
+            DUP2_ARG2(__lasx_xvld, lumSrc[j] + count_lum, 0, lumSrc[j] + count_lum,
+                      32, l_src1, l_src2);
+            yl_ev  = __lasx_xvmaddwev_w_h(yl_ev, temp, l_src1);
+            yl_od  = __lasx_xvmaddwod_w_h(yl_od, temp, l_src1);
+            yh_ev  = __lasx_xvmaddwev_w_h(yh_ev, temp, l_src2);
+            yh_od  = __lasx_xvmaddwod_w_h(yh_od, temp, l_src2);
+        }
+        for (j = 0; j < chrFilterSize; j++) {
+            DUP2_ARG2(__lasx_xvld, chrUSrc[j] + count, 0, chrVSrc[j] + count, 0,
+                      u_src, v_src);
+            temp  = __lasx_xvldrepl_h((chrFilter + j), 0);
+            u_ev  = __lasx_xvmaddwev_w_h(u_ev, temp, u_src);
+            u_od  = __lasx_xvmaddwod_w_h(u_od, temp, u_src);
+            v_ev  = __lasx_xvmaddwev_w_h(v_ev, temp, v_src);
+            v_od  = __lasx_xvmaddwod_w_h(v_od, temp, v_src);
+        }
+        yl_ev = __lasx_xvsrai_w(yl_ev, 19);
+        yh_ev = __lasx_xvsrai_w(yh_ev, 19);
+        yl_od = __lasx_xvsrai_w(yl_od, 19);
+        yh_od = __lasx_xvsrai_w(yh_od, 19);
+        u_ev  = __lasx_xvsrai_w(u_ev, 19);
+        v_ev  = __lasx_xvsrai_w(v_ev, 19);
+        u_od  = __lasx_xvsrai_w(u_od, 19);
+        v_od  = __lasx_xvsrai_w(v_od, 19);
+        u_ev  = __lasx_xvadd_w(u_ev, headroom);
+        v_ev  = __lasx_xvadd_w(v_ev, headroom);
+        u_od  = __lasx_xvadd_w(u_od, headroom);
+        v_od  = __lasx_xvadd_w(v_od, headroom);
+        WRITE_YUV2RGB(yl_ev, yl_od, u_ev, v_ev, 0, 0, 0, 0);
+        WRITE_YUV2RGB(yl_ev, yl_od, u_od, v_od, 1, 1, 0, 0);
+        WRITE_YUV2RGB(yl_ev, yl_od, u_ev, v_ev, 2, 2, 1, 1);
+        WRITE_YUV2RGB(yl_ev, yl_od, u_od, v_od, 3, 3, 1, 1);
+        WRITE_YUV2RGB(yl_ev, yl_od, u_ev, v_ev, 4, 4, 2, 2);
+        WRITE_YUV2RGB(yl_ev, yl_od, u_od, v_od, 5, 5, 2, 2);
+        WRITE_YUV2RGB(yl_ev, yl_od, u_ev, v_ev, 6, 6, 3, 3);
+        WRITE_YUV2RGB(yl_ev, yl_od, u_od, v_od, 7, 7, 3, 3);
+        WRITE_YUV2RGB(yh_ev, yh_od, u_ev, v_ev, 0, 0, 4, 4);
+        WRITE_YUV2RGB(yh_ev, yh_od, u_od, v_od, 1, 1, 4, 4);
+        WRITE_YUV2RGB(yh_ev, yh_od, u_ev, v_ev, 2, 2, 5, 5);
+        WRITE_YUV2RGB(yh_ev, yh_od, u_od, v_od, 3, 3, 5, 5);
+        WRITE_YUV2RGB(yh_ev, yh_od, u_ev, v_ev, 4, 4, 6, 6);
+        WRITE_YUV2RGB(yh_ev, yh_od, u_od, v_od, 5, 5, 6, 6);
+        WRITE_YUV2RGB(yh_ev, yh_od, u_ev, v_ev, 6, 6, 7, 7);
+        WRITE_YUV2RGB(yh_ev, yh_od, u_od, v_od, 7, 7, 7, 7);
+        res -= 32;
+    }
+    if (res >= 16) {
+        int Y1, Y2, U, V;
+        int count_lum = count << 1;
+        __m256i l_src, u_src, v_src;
+        __m256i y_ev, y_od, u, v, temp;
+
+        y_ev = __lasx_xvldrepl_w(&t, 0);
+        y_od = y_ev;
+        u    = y_ev;
+        v    = y_ev;
+        for (j = 0; j < lumFilterSize; j++) {
+            temp  = __lasx_xvldrepl_h((lumFilter + j), 0);
+            l_src = __lasx_xvld(lumSrc[j] + count_lum, 0);
+            y_ev  = __lasx_xvmaddwev_w_h(y_ev, temp, l_src);
+            y_od  = __lasx_xvmaddwod_w_h(y_od, temp, l_src);
+        }
+        for (j = 0; j < chrFilterSize; j++) {
+            DUP2_ARG2(__lasx_xvld, chrUSrc[j] + count, 0, chrVSrc[j] + count,
+                      0, u_src, v_src);
+            temp  = __lasx_xvldrepl_h((chrFilter + j), 0);
+            u_src = __lasx_vext2xv_w_h(u_src);
+            v_src = __lasx_vext2xv_w_h(v_src);
+            u     = __lasx_xvmaddwev_w_h(u, temp, u_src);
+            v     = __lasx_xvmaddwev_w_h(v, temp, v_src);
+        }
+        y_ev = __lasx_xvsrai_w(y_ev, 19);
+        y_od = __lasx_xvsrai_w(y_od, 19);
+        u    = __lasx_xvsrai_w(u, 19);
+        v    = __lasx_xvsrai_w(v, 19);
+        u    = __lasx_xvadd_w(u, headroom);
+        v    = __lasx_xvadd_w(v, headroom);
+        WRITE_YUV2RGB(y_ev, y_od, u, v, 0, 0, 0, 0);
+        WRITE_YUV2RGB(y_ev, y_od, u, v, 1, 1, 1, 1);
+        WRITE_YUV2RGB(y_ev, y_od, u, v, 2, 2, 2, 2);
+        WRITE_YUV2RGB(y_ev, y_od, u, v, 3, 3, 3, 3);
+        WRITE_YUV2RGB(y_ev, y_od, u, v, 4, 4, 4, 4);
+        WRITE_YUV2RGB(y_ev, y_od, u, v, 5, 5, 5, 5);
+        WRITE_YUV2RGB(y_ev, y_od, u, v, 6, 6, 6, 6);
+        WRITE_YUV2RGB(y_ev, y_od, u, v, 7, 7, 7, 7);
+        res -= 16;
+    }
+    if (res >= 8) {
+        int Y1, Y2, U, V;
+        int count_lum = count << 1;
+        __m256i l_src, u_src, v_src;
+        __m256i y_ev, uv, temp;
+
+        y_ev = __lasx_xvldrepl_w(&t, 0);
+        uv   = y_ev;
+        for (j = 0; j < lumFilterSize; j++) {
+            temp  = __lasx_xvldrepl_h((lumFilter + j), 0);
+            l_src = __lasx_xvld(lumSrc[j] + count_lum, 0);
+            l_src = __lasx_vext2xv_w_h(l_src);
+            y_ev  = __lasx_xvmaddwev_w_h(y_ev, temp, l_src);
+        }
+        for (j = 0; j < chrFilterSize; j++) {
+            u_src = __lasx_xvldrepl_d((chrUSrc[j] + count), 0);
+            v_src = __lasx_xvldrepl_d((chrVSrc[j] + count), 0);
+            temp  = __lasx_xvldrepl_h((chrFilter + j), 0);
+            u_src = __lasx_xvilvl_d(v_src, u_src);
+            u_src = __lasx_vext2xv_w_h(u_src);
+            uv    = __lasx_xvmaddwev_w_h(uv, temp, u_src);
+        }
+        y_ev = __lasx_xvsrai_w(y_ev, 19);
+        uv   = __lasx_xvsrai_w(uv, 19);
+        uv   = __lasx_xvadd_w(uv, headroom);
+        WRITE_YUV2RGB(y_ev, y_ev, uv, uv, 0, 1, 0, 4);
+        WRITE_YUV2RGB(y_ev, y_ev, uv, uv, 2, 3, 1, 5);
+        WRITE_YUV2RGB(y_ev, y_ev, uv, uv, 4, 5, 2, 6);
+        WRITE_YUV2RGB(y_ev, y_ev, uv, uv, 6, 7, 3, 7);
+    }
+    for (; count < len_count; count++) {
+        int Y1 = 1 << 18;
+        int Y2 = Y1;
+        int U  = Y1;
+        int V  = Y1;
+
+        for (j = 0; j < lumFilterSize; j++) {
+            Y1 += lumSrc[j][count * 2]     * lumFilter[j];
+            Y2 += lumSrc[j][count * 2 + 1] * lumFilter[j];
+        }
+        for (j = 0; j < chrFilterSize; j++) {
+            U += chrUSrc[j][count] * chrFilter[j];
+            V += chrVSrc[j][count] * chrFilter[j];
+        }
+        Y1 >>= 19;
+        Y2 >>= 19;
+        U  >>= 19;
+        V  >>= 19;
+        r =  c->table_rV[V + YUVRGB_TABLE_HEADROOM];
+        g = (c->table_gU[U + YUVRGB_TABLE_HEADROOM] +
+             c->table_gV[V + YUVRGB_TABLE_HEADROOM]);
+        b =  c->table_bU[U + YUVRGB_TABLE_HEADROOM];
+
+        yuv2rgb_write(dest, count, Y1, Y2, 0, 0,
+                      r, g, b, y, target, 0);
+    }
+}
+
+static void
+yuv2rgb_2_template_lasx(SwsContext *c, const int16_t *buf[2],
+                        const int16_t *ubuf[2], const int16_t *vbuf[2],
+                        const int16_t *abuf[2], uint8_t *dest, int dstW,
+                        int yalpha, int uvalpha, int y,
+                        enum AVPixelFormat target, int hasAlpha)
+{
+    const int16_t *buf0  = buf[0],  *buf1  = buf[1],
+                  *ubuf0 = ubuf[0], *ubuf1 = ubuf[1],
+                  *vbuf0 = vbuf[0], *vbuf1 = vbuf[1];
+    int yalpha1   = 4096 - yalpha;
+    int uvalpha1  = 4096 - uvalpha;
+    int i, count  = 0;
+    int len       = dstW - 15;
+    int len_count = (dstW + 1) >> 1;
+    const void *r, *g, *b;
+    int head  = YUVRGB_TABLE_HEADROOM;
+    __m256i v_yalpha1  = __lasx_xvreplgr2vr_w(yalpha1);
+    __m256i v_uvalpha1 = __lasx_xvreplgr2vr_w(uvalpha1);
+    __m256i v_yalpha   = __lasx_xvreplgr2vr_w(yalpha);
+    __m256i v_uvalpha  = __lasx_xvreplgr2vr_w(uvalpha);
+    __m256i headroom   = __lasx_xvreplgr2vr_w(head);
+
+    for (i = 0; i < len; i += 16) {
+        int Y1, Y2, U, V;
+        int i_dex = i << 1;
+        int c_dex = count << 1;
+        __m256i y0_h, y0_l, y0, u0, v0;
+        __m256i y1_h, y1_l, y1, u1, v1;
+        __m256i y_l, y_h, u, v;
+
+        DUP4_ARG2(__lasx_xvldx, buf0, i_dex, ubuf0, c_dex, vbuf0, c_dex,
+                  buf1, i_dex, y0, u0, v0, y1);
+        DUP2_ARG2(__lasx_xvldx, ubuf1, c_dex, vbuf1, c_dex, u1, v1);
+        DUP2_ARG2(__lasx_xvsllwil_w_h, y0, 0, y1, 0, y0_l, y1_l);
+        DUP2_ARG1(__lasx_xvexth_w_h, y0, y1, y0_h, y1_h);
+        DUP4_ARG1(__lasx_vext2xv_w_h, u0, u1, v0, v1, u0, u1, v0, v1);
+        y0_l = __lasx_xvmul_w(y0_l, v_yalpha1);
+        y0_h = __lasx_xvmul_w(y0_h, v_yalpha1);
+        u0   = __lasx_xvmul_w(u0, v_uvalpha1);
+        v0   = __lasx_xvmul_w(v0, v_uvalpha1);
+        y_l  = __lasx_xvmadd_w(y0_l, v_yalpha, y1_l);
+        y_h  = __lasx_xvmadd_w(y0_h, v_yalpha, y1_h);
+        u    = __lasx_xvmadd_w(u0, v_uvalpha, u1);
+        v    = __lasx_xvmadd_w(v0, v_uvalpha, v1);
+        y_l  = __lasx_xvsrai_w(y_l, 19);
+        y_h  = __lasx_xvsrai_w(y_h, 19);
+        u    = __lasx_xvsrai_w(u, 19);
+        v    = __lasx_xvsrai_w(v, 19);
+        u    = __lasx_xvadd_w(u, headroom);
+        v    = __lasx_xvadd_w(v, headroom);
+        WRITE_YUV2RGB(y_l, y_l, u, v, 0, 1, 0, 0);
+        WRITE_YUV2RGB(y_l, y_l, u, v, 2, 3, 1, 1);
+        WRITE_YUV2RGB(y_h, y_h, u, v, 0, 1, 2, 2);
+        WRITE_YUV2RGB(y_h, y_h, u, v, 2, 3, 3, 3);
+        WRITE_YUV2RGB(y_l, y_l, u, v, 4, 5, 4, 4);
+        WRITE_YUV2RGB(y_l, y_l, u, v, 6, 7, 5, 5);
+        WRITE_YUV2RGB(y_h, y_h, u, v, 4, 5, 6, 6);
+        WRITE_YUV2RGB(y_h, y_h, u, v, 6, 7, 7, 7);
+    }
+    if (dstW - i >= 8) {
+        int Y1, Y2, U, V;
+        int i_dex = i << 1;
+        __m256i y0_l, y0, u0, v0;
+        __m256i y1_l, y1, u1, v1;
+        __m256i y_l, u, v;
+
+        y0   = __lasx_xvldx(buf0, i_dex);
+        u0   = __lasx_xvldrepl_d((ubuf0 + count), 0);
+        v0   = __lasx_xvldrepl_d((vbuf0 + count), 0);
+        y1   = __lasx_xvldx(buf1, i_dex);
+        u1   = __lasx_xvldrepl_d((ubuf1 + count), 0);
+        v1   = __lasx_xvldrepl_d((vbuf1 + count), 0);
+        DUP2_ARG1(__lasx_vext2xv_w_h, y0, y1, y0_l, y1_l);
+        DUP4_ARG1(__lasx_vext2xv_w_h, u0, u1, v0, v1, u0, u1, v0, v1);
+        y0_l = __lasx_xvmul_w(y0_l, v_yalpha1);
+        u0   = __lasx_xvmul_w(u0, v_uvalpha1);
+        v0   = __lasx_xvmul_w(v0, v_uvalpha1);
+        y_l  = __lasx_xvmadd_w(y0_l, v_yalpha, y1_l);
+        u    = __lasx_xvmadd_w(u0, v_uvalpha, u1);
+        v    = __lasx_xvmadd_w(v0, v_uvalpha, v1);
+        y_l  = __lasx_xvsrai_w(y_l, 19);
+        u    = __lasx_xvsrai_w(u, 19);
+        v    = __lasx_xvsrai_w(v, 19);
+        u    = __lasx_xvadd_w(u, headroom);
+        v    = __lasx_xvadd_w(v, headroom);
+        WRITE_YUV2RGB(y_l, y_l, u, v, 0, 1, 0, 0);
+        WRITE_YUV2RGB(y_l, y_l, u, v, 2, 3, 1, 1);
+        WRITE_YUV2RGB(y_l, y_l, u, v, 4, 5, 2, 2);
+        WRITE_YUV2RGB(y_l, y_l, u, v, 6, 7, 3, 3);
+        i += 8;
+    }
+    for (; count < len_count; count++) {
+        int Y1 = (buf0[count * 2]     * yalpha1  +
+                  buf1[count * 2]     * yalpha)  >> 19;
+        int Y2 = (buf0[count * 2 + 1] * yalpha1  +
+                  buf1[count * 2 + 1] * yalpha) >> 19;
+        int U  = (ubuf0[count] * uvalpha1 + ubuf1[count] * uvalpha) >> 19;
+        int V  = (vbuf0[count] * uvalpha1 + vbuf1[count] * uvalpha) >> 19;
+
+        r =  c->table_rV[V + YUVRGB_TABLE_HEADROOM],
+        g = (c->table_gU[U + YUVRGB_TABLE_HEADROOM] +
+             c->table_gV[V + YUVRGB_TABLE_HEADROOM]),
+        b =  c->table_bU[U + YUVRGB_TABLE_HEADROOM];
+
+        yuv2rgb_write(dest, count, Y1, Y2, 0, 0,
+                      r, g, b, y, target, 0);
+    }
+}
+
+static void
+yuv2rgb_1_template_lasx(SwsContext *c, const int16_t *buf0,
+                        const int16_t *ubuf[2], const int16_t *vbuf[2],
+                        const int16_t *abuf0, uint8_t *dest, int dstW,
+                        int uvalpha, int y, enum AVPixelFormat target,
+                        int hasAlpha)
+{
+    const int16_t *ubuf0 = ubuf[0], *vbuf0 = vbuf[0];
+    int i;
+    int len       = (dstW - 15);
+    int len_count = (dstW + 1) >> 1;
+    const void *r, *g, *b;
+
+    if (uvalpha < 2048) {
+        int count    = 0;
+        int head = YUVRGB_TABLE_HEADROOM;
+        __m256i headroom  = __lasx_xvreplgr2vr_h(head);
+
+        for (i = 0; i < len; i += 16) {
+            int Y1, Y2, U, V;
+            int i_dex = i << 1;
+            int c_dex = count << 1;
+            __m256i src_y, src_u, src_v;
+            __m256i u, v, y_l, y_h;
+
+            DUP2_ARG2(__lasx_xvldx, buf0, i_dex, ubuf0, c_dex, src_y, src_u);
+            src_v = __lasx_xvldx(vbuf0, c_dex);
+            src_u = __lasx_xvpermi_q(src_u, src_v, 0x02);
+            src_y = __lasx_xvsrari_h(src_y, 7);
+            src_u = __lasx_xvsrari_h(src_u, 7);
+            y_l   = __lasx_xvsllwil_w_h(src_y, 0);
+            y_h   = __lasx_xvexth_w_h(src_y);
+            u     = __lasx_xvaddwev_w_h(src_u, headroom);
+            v     = __lasx_xvaddwod_w_h(src_u, headroom);
+            WRITE_YUV2RGB(y_l, y_l, u, u, 0, 1, 0, 4);
+            WRITE_YUV2RGB(y_l, y_l, v, v, 2, 3, 0, 4);
+            WRITE_YUV2RGB(y_h, y_h, u, u, 0, 1, 1, 5);
+            WRITE_YUV2RGB(y_h, y_h, v, v, 2, 3, 1, 5);
+            WRITE_YUV2RGB(y_l, y_l, u, u, 4, 5, 2, 6);
+            WRITE_YUV2RGB(y_l, y_l, v, v, 6, 7, 2, 6);
+            WRITE_YUV2RGB(y_h, y_h, u, u, 4, 5, 3, 7);
+            WRITE_YUV2RGB(y_h, y_h, v, v, 6, 7, 3, 7);
+        }
+        if (dstW - i >= 8){
+            int Y1, Y2, U, V;
+            int i_dex = i << 1;
+            __m256i src_y, src_u, src_v;
+            __m256i y_l, uv;
+
+            src_y  = __lasx_xvldx(buf0, i_dex);
+            src_u  = __lasx_xvldrepl_d((ubuf0 + count), 0);
+            src_v  = __lasx_xvldrepl_d((vbuf0 + count), 0);
+            src_u  = __lasx_xvilvl_d(src_v, src_u);
+            y_l    = __lasx_xvsrari_h(src_y, 7);
+            uv     = __lasx_xvsrari_h(src_u, 7);
+            y_l    = __lasx_vext2xv_w_h(y_l);
+            uv     = __lasx_vext2xv_w_h(uv);
+            uv     = __lasx_xvaddwev_w_h(uv, headroom);
+            WRITE_YUV2RGB(y_l, y_l, uv, uv, 0, 1, 0, 4);
+            WRITE_YUV2RGB(y_l, y_l, uv, uv, 2, 3, 1, 5);
+            WRITE_YUV2RGB(y_l, y_l, uv, uv, 4, 5, 2, 6);
+            WRITE_YUV2RGB(y_l, y_l, uv, uv, 6, 7, 3, 7);
+            i += 8;
+        }
+        for (; count < len_count; count++) {
+            int Y1 = (buf0[count * 2    ] + 64) >> 7;
+            int Y2 = (buf0[count * 2 + 1] + 64) >> 7;
+            int U  = (ubuf0[count]        + 64) >> 7;
+            int V  = (vbuf0[count]        + 64) >> 7;
+
+            r =  c->table_rV[V + YUVRGB_TABLE_HEADROOM],
+            g = (c->table_gU[U + YUVRGB_TABLE_HEADROOM] +
+                 c->table_gV[V + YUVRGB_TABLE_HEADROOM]),
+            b =  c->table_bU[U + YUVRGB_TABLE_HEADROOM];
+
+            yuv2rgb_write(dest, count, Y1, Y2, 0, 0,
+                          r, g, b, y, target, 0);
+        }
+    } else {
+        const int16_t *ubuf1 = ubuf[1], *vbuf1 = vbuf[1];
+        int count = 0;
+        int HEADROOM = YUVRGB_TABLE_HEADROOM;
+        __m256i headroom    = __lasx_xvreplgr2vr_w(HEADROOM);
+
+        for (i = 0; i < len; i += 16) {
+            int Y1, Y2, U, V;
+            int i_dex = i << 1;
+            int c_dex = count << 1;
+            __m256i src_y, src_u0, src_v0, src_u1, src_v1;
+            __m256i y_l, y_h, u, v;
+
+            DUP4_ARG2(__lasx_xvldx, buf0, i_dex, ubuf0, c_dex, vbuf0, c_dex,
+                      ubuf1, c_dex, src_y, src_u0, src_v0, src_u1);
+            src_v1 = __lasx_xvldx(vbuf1, c_dex);
+            src_u0 = __lasx_xvpermi_q(src_u0, src_v0, 0x02);
+            src_u1 = __lasx_xvpermi_q(src_u1, src_v1, 0x02);
+            src_y  = __lasx_xvsrari_h(src_y, 7);
+            u      = __lasx_xvaddwev_w_h(src_u0, src_u1);
+            v      = __lasx_xvaddwod_w_h(src_u0, src_u1);
+            y_l    = __lasx_xvsllwil_w_h(src_y, 0);
+            y_h    = __lasx_xvexth_w_h(src_y);
+            u      = __lasx_xvsrari_w(u, 8);
+            v      = __lasx_xvsrari_w(v, 8);
+            u      = __lasx_xvadd_w(u, headroom);
+            v      = __lasx_xvadd_w(v, headroom);
+            WRITE_YUV2RGB(y_l, y_l, u, u, 0, 1, 0, 4);
+            WRITE_YUV2RGB(y_l, y_l, v, v, 2, 3, 0, 4);
+            WRITE_YUV2RGB(y_h, y_h, u, u, 0, 1, 1, 5);
+            WRITE_YUV2RGB(y_h, y_h, v, v, 2, 3, 1, 5);
+            WRITE_YUV2RGB(y_l, y_l, u, u, 4, 5, 2, 6);
+            WRITE_YUV2RGB(y_l, y_l, v, v, 6, 7, 2, 6);
+            WRITE_YUV2RGB(y_h, y_h, u, u, 4, 5, 3, 7);
+            WRITE_YUV2RGB(y_h, y_h, v, v, 6, 7, 3, 7);
+        }
+        if (dstW - i >= 8) {
+            int Y1, Y2, U, V;
+            int i_dex = i << 1;
+            __m256i src_y, src_u0, src_v0, src_u1, src_v1;
+            __m256i uv;
+
+            src_y  = __lasx_xvldx(buf0, i_dex);
+            src_u0 = __lasx_xvldrepl_d((ubuf0 + count), 0);
+            src_v0 = __lasx_xvldrepl_d((vbuf0 + count), 0);
+            src_u1 = __lasx_xvldrepl_d((ubuf1 + count), 0);
+            src_v1 = __lasx_xvldrepl_d((vbuf1 + count), 0);
+
+            src_u0 = __lasx_xvilvl_h(src_u1, src_u0);
+            src_v0 = __lasx_xvilvl_h(src_v1, src_v0);
+            src_u0 = __lasx_xvpermi_q(src_u0, src_v0, 0x02);
+            src_y  = __lasx_xvsrari_h(src_y, 7);
+            uv     = __lasx_xvhaddw_w_h(src_u0, src_u0);
+            src_y  = __lasx_vext2xv_w_h(src_y);
+            uv     = __lasx_xvsrari_w(uv, 8);
+            uv     = __lasx_xvadd_w(uv, headroom);
+            WRITE_YUV2RGB(src_y, src_y, uv, uv, 0, 1, 0, 4);
+            WRITE_YUV2RGB(src_y, src_y, uv, uv, 2, 3, 1, 5);
+            WRITE_YUV2RGB(src_y, src_y, uv, uv, 4, 5, 2, 6);
+            WRITE_YUV2RGB(src_y, src_y, uv, uv, 6, 7, 3, 7);
+            i += 8;
+        }
+        for (; count < len_count; count++) {
+            int Y1 = (buf0[count * 2    ]         +  64) >> 7;
+            int Y2 = (buf0[count * 2 + 1]         +  64) >> 7;
+            int U  = (ubuf0[count] + ubuf1[count] + 128) >> 8;
+            int V  = (vbuf0[count] + vbuf1[count] + 128) >> 8;
+
+            r =  c->table_rV[V + YUVRGB_TABLE_HEADROOM],
+            g = (c->table_gU[U + YUVRGB_TABLE_HEADROOM] +
+                 c->table_gV[V + YUVRGB_TABLE_HEADROOM]),
+            b =  c->table_bU[U + YUVRGB_TABLE_HEADROOM];
+
+            yuv2rgb_write(dest, count, Y1, Y2, 0, 0,
+                          r, g, b, y, target, 0);
+        }
+    }
+}
+
+#define YUV2RGBWRAPPERX(name, base, ext, fmt, hasAlpha)                                \
+static void name ## ext ## _X_lasx(SwsContext *c, const int16_t *lumFilter,            \
+                                   const int16_t **lumSrc, int lumFilterSize,          \
+                                   const int16_t *chrFilter, const int16_t **chrUSrc,  \
+                                   const int16_t **chrVSrc, int chrFilterSize,         \
+                                   const int16_t **alpSrc, uint8_t *dest, int dstW,    \
+                                   int y)                                              \
+{                                                                                      \
+    name ## base ## _X_template_lasx(c, lumFilter, lumSrc, lumFilterSize,              \
+                                     chrFilter, chrUSrc, chrVSrc, chrFilterSize,       \
+                                     alpSrc, dest, dstW, y, fmt, hasAlpha);            \
+}
+
+#define YUV2RGBWRAPPERX2(name, base, ext, fmt, hasAlpha)                               \
+YUV2RGBWRAPPERX(name, base, ext, fmt, hasAlpha)                                        \
+static void name ## ext ## _2_lasx(SwsContext *c, const int16_t *buf[2],               \
+                                   const int16_t *ubuf[2], const int16_t *vbuf[2],     \
+                                   const int16_t *abuf[2], uint8_t *dest, int dstW,    \
+                                   int yalpha, int uvalpha, int y)                     \
+{                                                                                      \
+    name ## base ## _2_template_lasx(c, buf, ubuf, vbuf, abuf, dest,                   \
+                                     dstW, yalpha, uvalpha, y, fmt, hasAlpha);         \
+}
+
+#define YUV2RGBWRAPPER(name, base, ext, fmt, hasAlpha)                                 \
+YUV2RGBWRAPPERX2(name, base, ext, fmt, hasAlpha)                                       \
+static void name ## ext ## _1_lasx(SwsContext *c, const int16_t *buf0,                 \
+                                   const int16_t *ubuf[2], const int16_t *vbuf[2],     \
+                                   const int16_t *abuf0, uint8_t *dest, int dstW,      \
+                                   int uvalpha, int y)                                 \
+{                                                                                      \
+    name ## base ## _1_template_lasx(c, buf0, ubuf, vbuf, abuf0, dest,                 \
+                                     dstW, uvalpha, y, fmt, hasAlpha);                 \
+}
+
+
+#if CONFIG_SMALL
+#else
+#if CONFIG_SWSCALE_ALPHA
+#endif
+YUV2RGBWRAPPER(yuv2rgb,, x32_1,  AV_PIX_FMT_RGB32_1, 0)
+YUV2RGBWRAPPER(yuv2rgb,, x32,    AV_PIX_FMT_RGB32,   0)
+#endif
+YUV2RGBWRAPPER(yuv2, rgb, rgb24, AV_PIX_FMT_RGB24,     0)
+YUV2RGBWRAPPER(yuv2, rgb, bgr24, AV_PIX_FMT_BGR24,     0)
+YUV2RGBWRAPPER(yuv2rgb,,  16,    AV_PIX_FMT_RGB565,    0)
+YUV2RGBWRAPPER(yuv2rgb,,  15,    AV_PIX_FMT_RGB555,    0)
+YUV2RGBWRAPPER(yuv2rgb,,  12,    AV_PIX_FMT_RGB444,    0)
+YUV2RGBWRAPPER(yuv2rgb,,   8,    AV_PIX_FMT_RGB8,      0)
+YUV2RGBWRAPPER(yuv2rgb,,   4,    AV_PIX_FMT_RGB4,      0)
+YUV2RGBWRAPPER(yuv2rgb,,   4b,   AV_PIX_FMT_RGB4_BYTE, 0)
+
+// This function is copied from libswscale/output.c
+static av_always_inline void yuv2rgb_write_full(SwsContext *c,
+    uint8_t *dest, int i, int R, int A, int G, int B,
+    int y, enum AVPixelFormat target, int hasAlpha, int err[4])
+{
+    int isrgb8 = target == AV_PIX_FMT_BGR8 || target == AV_PIX_FMT_RGB8;
+
+    if ((R | G | B) & 0xC0000000) {
+        R = av_clip_uintp2(R, 30);
+        G = av_clip_uintp2(G, 30);
+        B = av_clip_uintp2(B, 30);
+    }
+
+    switch(target) {
+    case AV_PIX_FMT_ARGB:
+        dest[0] = hasAlpha ? A : 255;
+        dest[1] = R >> 22;
+        dest[2] = G >> 22;
+        dest[3] = B >> 22;
+        break;
+    case AV_PIX_FMT_RGB24:
+        dest[0] = R >> 22;
+        dest[1] = G >> 22;
+        dest[2] = B >> 22;
+        break;
+    case AV_PIX_FMT_RGBA:
+        dest[0] = R >> 22;
+        dest[1] = G >> 22;
+        dest[2] = B >> 22;
+        dest[3] = hasAlpha ? A : 255;
+        break;
+    case AV_PIX_FMT_ABGR:
+        dest[0] = hasAlpha ? A : 255;
+        dest[1] = B >> 22;
+        dest[2] = G >> 22;
+        dest[3] = R >> 22;
+        break;
+    case AV_PIX_FMT_BGR24:
+        dest[0] = B >> 22;
+        dest[1] = G >> 22;
+        dest[2] = R >> 22;
+        break;
+    case AV_PIX_FMT_BGRA:
+        dest[0] = B >> 22;
+        dest[1] = G >> 22;
+        dest[2] = R >> 22;
+        dest[3] = hasAlpha ? A : 255;
+        break;
+    case AV_PIX_FMT_BGR4_BYTE:
+    case AV_PIX_FMT_RGB4_BYTE:
+    case AV_PIX_FMT_BGR8:
+    case AV_PIX_FMT_RGB8:
+    {
+        int r,g,b;
+
+        switch (c->dither) {
+        default:
+        case SWS_DITHER_AUTO:
+        case SWS_DITHER_ED:
+            R >>= 22;
+            G >>= 22;
+            B >>= 22;
+            R += (7*err[0] + 1*c->dither_error[0][i] + 5*c->dither_error[0][i+1] + 3*c->dither_error[0][i+2])>>4;
+            G += (7*err[1] + 1*c->dither_error[1][i] + 5*c->dither_error[1][i+1] + 3*c->dither_error[1][i+2])>>4;
+            B += (7*err[2] + 1*c->dither_error[2][i] + 5*c->dither_error[2][i+1] + 3*c->dither_error[2][i+2])>>4;
+            c->dither_error[0][i] = err[0];
+            c->dither_error[1][i] = err[1];
+            c->dither_error[2][i] = err[2];
+            r = R >> (isrgb8 ? 5 : 7);
+            g = G >> (isrgb8 ? 5 : 6);
+            b = B >> (isrgb8 ? 6 : 7);
+            r = av_clip(r, 0, isrgb8 ? 7 : 1);
+            g = av_clip(g, 0, isrgb8 ? 7 : 3);
+            b = av_clip(b, 0, isrgb8 ? 3 : 1);
+            err[0] = R - r*(isrgb8 ? 36 : 255);
+            err[1] = G - g*(isrgb8 ? 36 : 85);
+            err[2] = B - b*(isrgb8 ? 85 : 255);
+            break;
+        case SWS_DITHER_A_DITHER:
+            if (isrgb8) {
+  /* see http://pippin.gimp.org/a_dither/ for details/origin */
+#define A_DITHER(u,v)   (((((u)+((v)*236))*119)&0xff))
+                r = (((R >> 19) + A_DITHER(i,y)  -96)>>8);
+                g = (((G >> 19) + A_DITHER(i + 17,y) - 96)>>8);
+                b = (((B >> 20) + A_DITHER(i + 17*2,y) -96)>>8);
+                r = av_clip_uintp2(r, 3);
+                g = av_clip_uintp2(g, 3);
+                b = av_clip_uintp2(b, 2);
+            } else {
+                r = (((R >> 21) + A_DITHER(i,y)-256)>>8);
+                g = (((G >> 19) + A_DITHER(i + 17,y)-256)>>8);
+                b = (((B >> 21) + A_DITHER(i + 17*2,y)-256)>>8);
+                r = av_clip_uintp2(r, 1);
+                g = av_clip_uintp2(g, 2);
+                b = av_clip_uintp2(b, 1);
+            }
+            break;
+        case SWS_DITHER_X_DITHER:
+            if (isrgb8) {
+  /* see http://pippin.gimp.org/a_dither/ for details/origin */
+#define X_DITHER(u,v)   (((((u)^((v)*237))*181)&0x1ff)/2)
+                r = (((R >> 19) + X_DITHER(i,y) - 96)>>8);
+                g = (((G >> 19) + X_DITHER(i + 17,y) - 96)>>8);
+                b = (((B >> 20) + X_DITHER(i + 17*2,y) - 96)>>8);
+                r = av_clip_uintp2(r, 3);
+                g = av_clip_uintp2(g, 3);
+                b = av_clip_uintp2(b, 2);
+            } else {
+                r = (((R >> 21) + X_DITHER(i,y)-256)>>8);
+                g = (((G >> 19) + X_DITHER(i + 17,y)-256)>>8);
+                b = (((B >> 21) + X_DITHER(i + 17*2,y)-256)>>8);
+                r = av_clip_uintp2(r, 1);
+                g = av_clip_uintp2(g, 2);
+                b = av_clip_uintp2(b, 1);
+            }
+
+            break;
+        }
+
+        if(target == AV_PIX_FMT_BGR4_BYTE) {
+            dest[0] = r + 2*g + 8*b;
+        } else if(target == AV_PIX_FMT_RGB4_BYTE) {
+            dest[0] = b + 2*g + 8*r;
+        } else if(target == AV_PIX_FMT_BGR8) {
+            dest[0] = r + 8*g + 64*b;
+        } else if(target == AV_PIX_FMT_RGB8) {
+            dest[0] = b + 4*g + 32*r;
+        } else
+            av_assert2(0);
+        break; }
+    }
+}
+
+#define YUVTORGB_SETUP                                           \
+    int y_offset   = c->yuv2rgb_y_offset;                        \
+    int y_coeff    = c->yuv2rgb_y_coeff;                         \
+    int v2r_coe    = c->yuv2rgb_v2r_coeff;                       \
+    int v2g_coe    = c->yuv2rgb_v2g_coeff;                       \
+    int u2g_coe    = c->yuv2rgb_u2g_coeff;                       \
+    int u2b_coe    = c->yuv2rgb_u2b_coeff;                       \
+    __m256i offset = __lasx_xvreplgr2vr_w(y_offset);             \
+    __m256i coeff  = __lasx_xvreplgr2vr_w(y_coeff);              \
+    __m256i v2r    = __lasx_xvreplgr2vr_w(v2r_coe);              \
+    __m256i v2g    = __lasx_xvreplgr2vr_w(v2g_coe);              \
+    __m256i u2g    = __lasx_xvreplgr2vr_w(u2g_coe);              \
+    __m256i u2b    = __lasx_xvreplgr2vr_w(u2b_coe);              \
+
+
+#define YUVTORGB(y, u, v, R, G, B, offset, coeff,              \
+                 y_temp, v2r, v2g, u2g, u2b)                   \
+{                                                              \
+     y = __lasx_xvsub_w(y, offset);                            \
+     y = __lasx_xvmul_w(y, coeff);                             \
+     y = __lasx_xvadd_w(y, y_temp);                            \
+     R = __lasx_xvmadd_w(y, v, v2r);                           \
+     v = __lasx_xvmadd_w(y, v, v2g);                           \
+     G = __lasx_xvmadd_w(v, u, u2g);                           \
+     B = __lasx_xvmadd_w(y, u, u2b);                           \
+}
+
+#define WRITE_FULL_A(r, g, b, a, t1, s)                                      \
+{                                                                            \
+    R = __lasx_xvpickve2gr_w(r, t1);                                         \
+    G = __lasx_xvpickve2gr_w(g, t1);                                         \
+    B = __lasx_xvpickve2gr_w(b, t1);                                         \
+    A = __lasx_xvpickve2gr_w(a, t1);                                         \
+    if (A & 0x100)                                                           \
+        A = av_clip_uint8(A);                                                \
+    yuv2rgb_write_full(c, dest, i + s, R, A, G, B, y, target, hasAlpha, err);\
+    dest += step;                                                            \
+}
+
+#define WRITE_FULL(r, g, b, t1, s)                                            \
+{                                                                             \
+    R = __lasx_xvpickve2gr_w(r, t1);                                          \
+    G = __lasx_xvpickve2gr_w(g, t1);                                          \
+    B = __lasx_xvpickve2gr_w(b, t1);                                          \
+    yuv2rgb_write_full(c, dest, i + s, R, 0, G, B, y, target, hasAlpha, err); \
+    dest += step;                                                             \
+}
+
+static void
+yuv2rgb_full_X_template_lasx(SwsContext *c, const int16_t *lumFilter,
+                             const int16_t **lumSrc, int lumFilterSize,
+                             const int16_t *chrFilter, const int16_t **chrUSrc,
+                             const int16_t **chrVSrc, int chrFilterSize,
+                             const int16_t **alpSrc, uint8_t *dest,
+                             int dstW, int y, enum AVPixelFormat target,
+                             int hasAlpha)
+{
+    int i, j, B, G, R, A;
+    int step       = (target == AV_PIX_FMT_RGB24 ||
+                      target == AV_PIX_FMT_BGR24) ? 3 : 4;
+    int err[4]     = {0};
+    int a_temp     = 1 << 18;
+    int templ      = 1 << 9;
+    int tempc      = templ - (128 << 19);
+    int ytemp      = 1 << 21;
+    int len        = dstW - 15;
+    __m256i y_temp = __lasx_xvreplgr2vr_w(ytemp);
+    YUVTORGB_SETUP
+
+    if(   target == AV_PIX_FMT_BGR4_BYTE || target == AV_PIX_FMT_RGB4_BYTE
+       || target == AV_PIX_FMT_BGR8      || target == AV_PIX_FMT_RGB8)
+        step = 1;
+
+    for (i = 0; i < len; i += 16) {
+        __m256i l_src, u_src, v_src;
+        __m256i y_ev, y_od, u_ev, u_od, v_ev, v_od, temp;
+        __m256i R_ev, R_od, G_ev, G_od, B_ev, B_od;
+        int n = i << 1;
+
+        y_ev = y_od = __lasx_xvreplgr2vr_w(templ);
+        u_ev = u_od = v_ev = v_od = __lasx_xvreplgr2vr_w(tempc);
+        for (j = 0; j < lumFilterSize; j++) {
+            temp  = __lasx_xvldrepl_h((lumFilter + j), 0);
+            l_src = __lasx_xvldx(lumSrc[j], n);
+            y_ev  = __lasx_xvmaddwev_w_h(y_ev, l_src, temp);
+            y_od  = __lasx_xvmaddwod_w_h(y_od, l_src, temp);
+        }
+        for (j = 0; j < chrFilterSize; j++) {
+            temp  = __lasx_xvldrepl_h((chrFilter + j), 0);
+            DUP2_ARG2(__lasx_xvldx, chrUSrc[j], n, chrVSrc[j], n,
+                      u_src, v_src);
+            DUP2_ARG3(__lasx_xvmaddwev_w_h, u_ev, u_src, temp, v_ev,
+                      v_src, temp, u_ev, v_ev);
+            DUP2_ARG3(__lasx_xvmaddwod_w_h, u_od, u_src, temp, v_od,
+                      v_src, temp, u_od, v_od);
+        }
+        y_ev = __lasx_xvsrai_w(y_ev, 10);
+        y_od = __lasx_xvsrai_w(y_od, 10);
+        u_ev = __lasx_xvsrai_w(u_ev, 10);
+        u_od = __lasx_xvsrai_w(u_od, 10);
+        v_ev = __lasx_xvsrai_w(v_ev, 10);
+        v_od = __lasx_xvsrai_w(v_od, 10);
+        YUVTORGB(y_ev, u_ev, v_ev, R_ev, G_ev, B_ev, offset, coeff,
+                 y_temp, v2r, v2g, u2g, u2b);
+        YUVTORGB(y_od, u_od, v_od, R_od, G_od, B_od, offset, coeff,
+                 y_temp, v2r, v2g, u2g, u2b);
+
+        if (hasAlpha) {
+            __m256i a_src, a_ev, a_od;
+
+            a_ev = a_od = __lasx_xvreplgr2vr_w(a_temp);
+            for (j = 0; j < lumFilterSize; j++) {
+                temp  = __lasx_xvldrepl_h(lumFilter + j, 0);
+                a_src = __lasx_xvldx(alpSrc[j], n);
+                a_ev  = __lasx_xvmaddwev_w_h(a_ev, a_src, temp);
+                a_od  = __lasx_xvmaddwod_w_h(a_od, a_src, temp);
+            }
+            a_ev = __lasx_xvsrai_w(a_ev, 19);
+            a_od = __lasx_xvsrai_w(a_od, 19);
+            WRITE_FULL_A(R_ev, G_ev, B_ev, a_ev, 0, 0);
+            WRITE_FULL_A(R_od, G_od, B_od, a_od, 0, 1);
+            WRITE_FULL_A(R_ev, G_ev, B_ev, a_ev, 1, 2);
+            WRITE_FULL_A(R_od, G_od, B_od, a_od, 1, 3);
+            WRITE_FULL_A(R_ev, G_ev, B_ev, a_ev, 2, 4);
+            WRITE_FULL_A(R_od, G_od, B_od, a_od, 2, 5);
+            WRITE_FULL_A(R_ev, G_ev, B_ev, a_ev, 3, 6);
+            WRITE_FULL_A(R_od, G_od, B_od, a_od, 3, 7);
+            WRITE_FULL_A(R_ev, G_ev, B_ev, a_ev, 4, 8);
+            WRITE_FULL_A(R_od, G_od, B_od, a_od, 4, 9);
+            WRITE_FULL_A(R_ev, G_ev, B_ev, a_ev, 5, 10);
+            WRITE_FULL_A(R_od, G_od, B_od, a_od, 5, 11);
+            WRITE_FULL_A(R_ev, G_ev, B_ev, a_ev, 6, 12);
+            WRITE_FULL_A(R_od, G_od, B_od, a_od, 6, 13);
+            WRITE_FULL_A(R_ev, G_ev, B_ev, a_ev, 7, 14);
+            WRITE_FULL_A(R_od, G_od, B_od, a_od, 7, 15);
+        } else {
+            WRITE_FULL(R_ev, G_ev, B_ev, 0, 0);
+            WRITE_FULL(R_od, G_od, B_od, 0, 1);
+            WRITE_FULL(R_ev, G_ev, B_ev, 1, 2);
+            WRITE_FULL(R_od, G_od, B_od, 1, 3);
+            WRITE_FULL(R_ev, G_ev, B_ev, 2, 4);
+            WRITE_FULL(R_od, G_od, B_od, 2, 5);
+            WRITE_FULL(R_ev, G_ev, B_ev, 3, 6);
+            WRITE_FULL(R_od, G_od, B_od, 3, 7);
+            WRITE_FULL(R_ev, G_ev, B_ev, 4, 8);
+            WRITE_FULL(R_od, G_od, B_od, 4, 9);
+            WRITE_FULL(R_ev, G_ev, B_ev, 5, 10);
+            WRITE_FULL(R_od, G_od, B_od, 5, 11);
+            WRITE_FULL(R_ev, G_ev, B_ev, 6, 12);
+            WRITE_FULL(R_od, G_od, B_od, 6, 13);
+            WRITE_FULL(R_ev, G_ev, B_ev, 7, 14);
+            WRITE_FULL(R_od, G_od, B_od, 7, 15);
+        }
+    }
+    if (dstW - i >= 8) {
+        __m256i l_src, u_src, v_src;
+        __m256i y_ev, u_ev, v_ev, uv, temp;
+        __m256i R_ev, G_ev, B_ev;
+        int n = i << 1;
+
+        y_ev = __lasx_xvreplgr2vr_w(templ);
+        u_ev = v_ev = __lasx_xvreplgr2vr_w(tempc);
+        for (j = 0; j < lumFilterSize; j++) {
+            temp  = __lasx_xvldrepl_h((lumFilter + j), 0);
+            l_src = __lasx_xvldx(lumSrc[j], n);
+            l_src = __lasx_xvpermi_d(l_src, 0xD8);
+            l_src = __lasx_xvilvl_h(l_src, l_src);
+            y_ev  = __lasx_xvmaddwev_w_h(y_ev, l_src, temp);
+        }
+        for (j = 0; j < chrFilterSize; j++) {
+            temp  = __lasx_xvldrepl_h((chrFilter + j), 0);
+            DUP2_ARG2(__lasx_xvldx, chrUSrc[j], n, chrVSrc[j], n, u_src, v_src);
+            u_src = __lasx_xvpermi_d(u_src, 0xD8);
+            v_src = __lasx_xvpermi_d(v_src, 0xD8);
+            uv    = __lasx_xvilvl_h(v_src, u_src);
+            u_ev  = __lasx_xvmaddwev_w_h(u_ev, uv, temp);
+            v_ev  = __lasx_xvmaddwod_w_h(v_ev, uv, temp);
+        }
+        y_ev = __lasx_xvsrai_w(y_ev, 10);
+        u_ev = __lasx_xvsrai_w(u_ev, 10);
+        v_ev = __lasx_xvsrai_w(v_ev, 10);
+        YUVTORGB(y_ev, u_ev, v_ev, R_ev, G_ev, B_ev, offset, coeff,
+                 y_temp, v2r, v2g, u2g, u2b);
+
+        if (hasAlpha) {
+            __m256i a_src, a_ev;
+
+            a_ev = __lasx_xvreplgr2vr_w(a_temp);
+            for (j = 0; j < lumFilterSize; j++) {
+                temp  = __lasx_xvldrepl_h(lumFilter + j, 0);
+                a_src = __lasx_xvldx(alpSrc[j], n);
+                a_src = __lasx_xvpermi_d(a_src, 0xD8);
+                a_src = __lasx_xvilvl_h(a_src, a_src);
+                a_ev  =  __lasx_xvmaddwev_w_h(a_ev, a_src, temp);
+            }
+            a_ev = __lasx_xvsrai_w(a_ev, 19);
+            WRITE_FULL_A(R_ev, G_ev, B_ev, a_ev, 0, 0);
+            WRITE_FULL_A(R_ev, G_ev, B_ev, a_ev, 1, 1);
+            WRITE_FULL_A(R_ev, G_ev, B_ev, a_ev, 2, 2);
+            WRITE_FULL_A(R_ev, G_ev, B_ev, a_ev, 3, 3);
+            WRITE_FULL_A(R_ev, G_ev, B_ev, a_ev, 4, 4);
+            WRITE_FULL_A(R_ev, G_ev, B_ev, a_ev, 5, 5);
+            WRITE_FULL_A(R_ev, G_ev, B_ev, a_ev, 6, 6);
+            WRITE_FULL_A(R_ev, G_ev, B_ev, a_ev, 7, 7);
+        } else {
+            WRITE_FULL(R_ev, G_ev, B_ev, 0, 0);
+            WRITE_FULL(R_ev, G_ev, B_ev, 1, 1);
+            WRITE_FULL(R_ev, G_ev, B_ev, 2, 2);
+            WRITE_FULL(R_ev, G_ev, B_ev, 3, 3);
+            WRITE_FULL(R_ev, G_ev, B_ev, 4, 4);
+            WRITE_FULL(R_ev, G_ev, B_ev, 5, 5);
+            WRITE_FULL(R_ev, G_ev, B_ev, 6, 6);
+            WRITE_FULL(R_ev, G_ev, B_ev, 7, 7);
+        }
+        i += 8;
+    }
+    for (; i < dstW; i++) {
+        int Y = templ;
+        int V, U = V = tempc;
+
+        A = 0;
+        for (j = 0; j < lumFilterSize; j++) {
+            Y += lumSrc[j][i] * lumFilter[j];
+        }
+        for (j = 0; j < chrFilterSize; j++) {
+            U += chrUSrc[j][i] * chrFilter[j];
+            V += chrVSrc[j][i] * chrFilter[j];
+
+        }
+        Y >>= 10;
+        U >>= 10;
+        V >>= 10;
+        if (hasAlpha) {
+            A = 1 << 18;
+            for (j = 0; j < lumFilterSize; j++) {
+                A += alpSrc[j][i] * lumFilter[j];
+            }
+            A >>= 19;
+            if (A & 0x100)
+                A = av_clip_uint8(A);
+        }
+        Y -= y_offset;
+        Y *= y_coeff;
+        Y += ytemp;
+        R  = (unsigned)Y + V * v2r_coe;
+        G  = (unsigned)Y + V * v2g_coe + U * u2g_coe;
+        B  = (unsigned)Y + U * u2b_coe;
+        yuv2rgb_write_full(c, dest, i, R, A, G, B, y, target, hasAlpha, err);
+        dest += step;
+    }
+    c->dither_error[0][i] = err[0];
+    c->dither_error[1][i] = err[1];
+    c->dither_error[2][i] = err[2];
+}
+
+static void
+yuv2rgb_full_2_template_lasx(SwsContext *c, const int16_t *buf[2],
+                             const int16_t *ubuf[2], const int16_t *vbuf[2],
+                             const int16_t *abuf[2], uint8_t *dest, int dstW,
+                             int yalpha, int uvalpha, int y,
+                             enum AVPixelFormat target, int hasAlpha)
+{
+    const int16_t *buf0  = buf[0],  *buf1  = buf[1],
+                  *ubuf0 = ubuf[0], *ubuf1 = ubuf[1],
+                  *vbuf0 = vbuf[0], *vbuf1 = vbuf[1],
+                  *abuf0 = hasAlpha ? abuf[0] : NULL,
+                  *abuf1 = hasAlpha ? abuf[1] : NULL;
+    int yalpha1  = 4096 - yalpha;
+    int uvalpha1 = 4096 - uvalpha;
+    int uvtemp   = 128 << 19;
+    int atemp    = 1 << 18;
+    int err[4]   = {0};
+    int ytemp    = 1 << 21;
+    int len      = dstW - 15;
+    int i, R, G, B, A;
+    int step = (target == AV_PIX_FMT_RGB24 ||
+                target == AV_PIX_FMT_BGR24) ? 3 : 4;
+    __m256i v_uvalpha1 = __lasx_xvreplgr2vr_w(uvalpha1);
+    __m256i v_yalpha1  = __lasx_xvreplgr2vr_w(yalpha1);
+    __m256i v_uvalpha  = __lasx_xvreplgr2vr_w(uvalpha);
+    __m256i v_yalpha   = __lasx_xvreplgr2vr_w(yalpha);
+    __m256i uv         = __lasx_xvreplgr2vr_w(uvtemp);
+    __m256i a_bias     = __lasx_xvreplgr2vr_w(atemp);
+    __m256i y_temp     = __lasx_xvreplgr2vr_w(ytemp);
+    YUVTORGB_SETUP
+
+    av_assert2(yalpha  <= 4096U);
+    av_assert2(uvalpha <= 4096U);
+
+    if(   target == AV_PIX_FMT_BGR4_BYTE || target == AV_PIX_FMT_RGB4_BYTE
+       || target == AV_PIX_FMT_BGR8      || target == AV_PIX_FMT_RGB8)
+        step = 1;
+
+    for (i = 0; i < len; i += 16) {
+        __m256i b0, b1, ub0, ub1, vb0, vb1;
+        __m256i y0_l, y0_h, y1_l, y1_h, u0_l, u0_h;
+        __m256i v0_l, v0_h, u1_l, u1_h, v1_l, v1_h;
+        __m256i y_l, y_h, v_l, v_h, u_l, u_h;
+        __m256i R_l, R_h, G_l, G_h, B_l, B_h;
+        int n = i << 1;
+
+        DUP4_ARG2(__lasx_xvldx, buf0, n, buf1, n, ubuf0,
+                  n, ubuf1, n, b0, b1, ub0, ub1);
+        DUP2_ARG2(__lasx_xvldx, vbuf0, n, vbuf1, n, vb0 , vb1);
+        DUP2_ARG2(__lasx_xvsllwil_w_h, b0, 0, b1, 0, y0_l, y1_l);
+        DUP4_ARG2(__lasx_xvsllwil_w_h, ub0, 0, ub1, 0, vb0, 0, vb1, 0,
+                  u0_l, u1_l, v0_l, v1_l);
+        DUP2_ARG1(__lasx_xvexth_w_h, b0, b1, y0_h, y1_h);
+        DUP4_ARG1(__lasx_xvexth_w_h, ub0, ub1, vb0, vb1,
+                  u0_h, u1_h, v0_h, v1_h);
+        y0_l = __lasx_xvmul_w(y0_l, v_yalpha1);
+        y0_h = __lasx_xvmul_w(y0_h, v_yalpha1);
+        u0_l = __lasx_xvmul_w(u0_l, v_uvalpha1);
+        u0_h = __lasx_xvmul_w(u0_h, v_uvalpha1);
+        v0_l = __lasx_xvmul_w(v0_l, v_uvalpha1);
+        v0_h = __lasx_xvmul_w(v0_h, v_uvalpha1);
+        y_l  = __lasx_xvmadd_w(y0_l, v_yalpha, y1_l);
+        y_h  = __lasx_xvmadd_w(y0_h, v_yalpha, y1_h);
+        u_l  = __lasx_xvmadd_w(u0_l, v_uvalpha, u1_l);
+        u_h  = __lasx_xvmadd_w(u0_h, v_uvalpha, u1_h);
+        v_l  = __lasx_xvmadd_w(v0_l, v_uvalpha, v1_l);
+        v_h  = __lasx_xvmadd_w(v0_h, v_uvalpha, v1_h);
+        u_l  = __lasx_xvsub_w(u_l, uv);
+        u_h  = __lasx_xvsub_w(u_h, uv);
+        v_l  = __lasx_xvsub_w(v_l, uv);
+        v_h  = __lasx_xvsub_w(v_h, uv);
+        y_l  = __lasx_xvsrai_w(y_l, 10);
+        y_h  = __lasx_xvsrai_w(y_h, 10);
+        u_l  = __lasx_xvsrai_w(u_l, 10);
+        u_h  = __lasx_xvsrai_w(u_h, 10);
+        v_l  = __lasx_xvsrai_w(v_l, 10);
+        v_h  = __lasx_xvsrai_w(v_h, 10);
+        YUVTORGB(y_l, u_l, v_l, R_l, G_l, B_l, offset, coeff,
+                 y_temp, v2r, v2g, u2g, u2b);
+        YUVTORGB(y_h, u_h, v_h, R_h, G_h, B_h, offset, coeff,
+                 y_temp, v2r, v2g, u2g, u2b);
+
+        if (hasAlpha) {
+            __m256i a0, a1, a0_l, a0_h;
+            __m256i a_l, a_h, a1_l, a1_h;
+
+            DUP2_ARG2(__lasx_xvldx, abuf0, n, abuf1, n, a0, a1);
+            DUP2_ARG2(__lasx_xvsllwil_w_h, a0, 0, a1, 0, a0_l, a1_l);
+            DUP2_ARG1(__lasx_xvexth_w_h, a0, a1, a0_h, a1_h);
+            a_l = __lasx_xvmadd_w(a_bias, a0_l, v_yalpha1);
+            a_h = __lasx_xvmadd_w(a_bias, a0_h, v_yalpha1);
+            a_l = __lasx_xvmadd_w(a_l, v_yalpha, a1_l);
+            a_h = __lasx_xvmadd_w(a_h, v_yalpha, a1_h);
+            a_l = __lasx_xvsrai_w(a_l, 19);
+            a_h = __lasx_xvsrai_w(a_h, 19);
+            WRITE_FULL_A(R_l, G_l, B_l, a_l, 0, 0);
+            WRITE_FULL_A(R_l, G_l, B_l, a_l, 1, 1);
+            WRITE_FULL_A(R_l, G_l, B_l, a_l, 2, 2);
+            WRITE_FULL_A(R_l, G_l, B_l, a_l, 3, 3);
+            WRITE_FULL_A(R_h, G_h, B_h, a_h, 0, 4);
+            WRITE_FULL_A(R_h, G_h, B_h, a_h, 1, 5);
+            WRITE_FULL_A(R_h, G_h, B_h, a_h, 2, 6);
+            WRITE_FULL_A(R_h, G_h, B_h, a_h, 3, 7);
+            WRITE_FULL_A(R_l, G_l, B_l, a_l, 4, 8);
+            WRITE_FULL_A(R_l, G_l, B_l, a_l, 5, 9);
+            WRITE_FULL_A(R_l, G_l, B_l, a_l, 6, 10);
+            WRITE_FULL_A(R_l, G_l, B_l, a_l, 7, 11);
+            WRITE_FULL_A(R_h, G_h, B_h, a_h, 4, 12);
+            WRITE_FULL_A(R_h, G_h, B_h, a_h, 5, 13);
+            WRITE_FULL_A(R_h, G_h, B_h, a_h, 6, 14);
+            WRITE_FULL_A(R_h, G_h, B_h, a_h, 7, 15);
+        } else {
+            WRITE_FULL(R_l, G_l, B_l, 0, 0);
+            WRITE_FULL(R_l, G_l, B_l, 1, 1);
+            WRITE_FULL(R_l, G_l, B_l, 2, 2);
+            WRITE_FULL(R_l, G_l, B_l, 3, 3);
+            WRITE_FULL(R_h, G_h, B_h, 0, 4);
+            WRITE_FULL(R_h, G_h, B_h, 1, 5);
+            WRITE_FULL(R_h, G_h, B_h, 2, 6);
+            WRITE_FULL(R_h, G_h, B_h, 3, 7);
+            WRITE_FULL(R_l, G_l, B_l, 4, 8);
+            WRITE_FULL(R_l, G_l, B_l, 5, 9);
+            WRITE_FULL(R_l, G_l, B_l, 6, 10);
+            WRITE_FULL(R_l, G_l, B_l, 7, 11);
+            WRITE_FULL(R_h, G_h, B_h, 4, 12);
+            WRITE_FULL(R_h, G_h, B_h, 5, 13);
+            WRITE_FULL(R_h, G_h, B_h, 6, 14);
+            WRITE_FULL(R_h, G_h, B_h, 7, 15);
+        }
+    }
+    if (dstW - i >= 8) {
+        __m256i b0, b1, ub0, ub1, vb0, vb1;
+        __m256i y0_l, y1_l, u0_l;
+        __m256i v0_l, u1_l, v1_l;
+        __m256i y_l, u_l, v_l;
+        __m256i R_l, G_l, B_l;
+        int n = i << 1;
+
+        DUP4_ARG2(__lasx_xvldx, buf0, n, buf1, n, ubuf0, n,
+                  ubuf1, n, b0, b1, ub0, ub1);
+        DUP2_ARG2(__lasx_xvldx, vbuf0, n, vbuf1, n, vb0, vb1);
+        DUP2_ARG1(__lasx_vext2xv_w_h, b0, b1, y0_l, y1_l);
+        DUP4_ARG1(__lasx_vext2xv_w_h, ub0, ub1, vb0, vb1,
+                  u0_l, u1_l, v0_l, v1_l);
+        y0_l = __lasx_xvmul_w(y0_l, v_yalpha1);
+        u0_l = __lasx_xvmul_w(u0_l, v_uvalpha1);
+        v0_l = __lasx_xvmul_w(v0_l, v_uvalpha1);
+        y_l  = __lasx_xvmadd_w(y0_l, v_yalpha, y1_l);
+        u_l  = __lasx_xvmadd_w(u0_l, v_uvalpha, u1_l);
+        v_l  = __lasx_xvmadd_w(v0_l, v_uvalpha, v1_l);
+        u_l  = __lasx_xvsub_w(u_l, uv);
+        v_l  = __lasx_xvsub_w(v_l, uv);
+        y_l  = __lasx_xvsrai_w(y_l, 10);
+        u_l  = __lasx_xvsrai_w(u_l, 10);
+        v_l  = __lasx_xvsrai_w(v_l, 10);
+        YUVTORGB(y_l, u_l, v_l, R_l, G_l, B_l, offset, coeff,
+                 y_temp, v2r, v2g, u2g, u2b);
+
+        if (hasAlpha) {
+            __m256i a0, a1, a0_l;
+            __m256i a_l, a1_l;
+
+            DUP2_ARG2(__lasx_xvldx, abuf0, n, abuf1, n, a0, a1);
+            DUP2_ARG1(__lasx_vext2xv_w_h, a0, a1, a0_l, a1_l);
+            a_l = __lasx_xvmadd_w(a_bias, a0_l, v_yalpha1);
+            a_l = __lasx_xvmadd_w(a_l, v_yalpha, a1_l);
+            a_l = __lasx_xvsrai_w(a_l, 19);
+            WRITE_FULL_A(R_l, G_l, B_l, a_l, 0, 0);
+            WRITE_FULL_A(R_l, G_l, B_l, a_l, 1, 1);
+            WRITE_FULL_A(R_l, G_l, B_l, a_l, 2, 2);
+            WRITE_FULL_A(R_l, G_l, B_l, a_l, 3, 3);
+            WRITE_FULL_A(R_l, G_l, B_l, a_l, 4, 4);
+            WRITE_FULL_A(R_l, G_l, B_l, a_l, 5, 5);
+            WRITE_FULL_A(R_l, G_l, B_l, a_l, 6, 6);
+            WRITE_FULL_A(R_l, G_l, B_l, a_l, 7, 7);
+        } else {
+            WRITE_FULL(R_l, G_l, B_l, 0, 0);
+            WRITE_FULL(R_l, G_l, B_l, 1, 1);
+            WRITE_FULL(R_l, G_l, B_l, 2, 2);
+            WRITE_FULL(R_l, G_l, B_l, 3, 3);
+            WRITE_FULL(R_l, G_l, B_l, 4, 4);
+            WRITE_FULL(R_l, G_l, B_l, 5, 5);
+            WRITE_FULL(R_l, G_l, B_l, 6, 6);
+            WRITE_FULL(R_l, G_l, B_l, 7, 7);
+        }
+        i += 8;
+    }
+    for (; i < dstW; i++){
+        int Y = ( buf0[i] * yalpha1  +  buf1[i] * yalpha         ) >> 10;
+        int U = (ubuf0[i] * uvalpha1 + ubuf1[i] * uvalpha- uvtemp) >> 10;
+        int V = (vbuf0[i] * uvalpha1 + vbuf1[i] * uvalpha- uvtemp) >> 10;
+
+        A = 0;
+        if (hasAlpha){
+            A = (abuf0[i] * yalpha1 + abuf1[i] * yalpha + atemp) >> 19;
+            if (A & 0x100)
+                A = av_clip_uint8(A);
+        }
+
+        Y -= y_offset;
+        Y *= y_coeff;
+        Y += ytemp;
+        R  = (unsigned)Y + V * v2r_coe;
+        G  = (unsigned)Y + V * v2g_coe + U * u2g_coe;
+        B  = (unsigned)Y + U * u2b_coe;
+        yuv2rgb_write_full(c, dest, i, R, A, G, B, y, target, hasAlpha, err);
+        dest += step;
+    }
+    c->dither_error[0][i] = err[0];
+    c->dither_error[1][i] = err[1];
+    c->dither_error[2][i] = err[2];
+}
+
+static void
+yuv2rgb_full_1_template_lasx(SwsContext *c, const int16_t *buf0,
+                             const int16_t *ubuf[2], const int16_t *vbuf[2],
+                             const int16_t *abuf0, uint8_t *dest, int dstW,
+                             int uvalpha, int y, enum AVPixelFormat target,
+                             int hasAlpha)
+{
+    const int16_t *ubuf0 = ubuf[0], *vbuf0 = vbuf[0];
+    int i, B, G, R, A;
+    int step = (target == AV_PIX_FMT_RGB24 || target == AV_PIX_FMT_BGR24) ? 3 : 4;
+    int err[4]     = {0};
+    int ytemp      = 1 << 21;
+    int bias_int   = 64;
+    int len        = dstW - 15;
+    __m256i y_temp = __lasx_xvreplgr2vr_w(ytemp);
+    YUVTORGB_SETUP
+
+    if(   target == AV_PIX_FMT_BGR4_BYTE || target == AV_PIX_FMT_RGB4_BYTE
+       || target == AV_PIX_FMT_BGR8      || target == AV_PIX_FMT_RGB8)
+        step = 1;
+    if (uvalpha < 2048) {
+        int uvtemp   = 128 << 7;
+        __m256i uv   = __lasx_xvreplgr2vr_w(uvtemp);
+        __m256i bias = __lasx_xvreplgr2vr_w(bias_int);
+
+        for (i = 0; i < len; i += 16) {
+            __m256i b, ub, vb, ub_l, ub_h, vb_l, vb_h;
+            __m256i y_l, y_h, u_l, u_h, v_l, v_h;
+            __m256i R_l, R_h, G_l, G_h, B_l, B_h;
+            int n = i << 1;
+
+            DUP2_ARG2(__lasx_xvldx, buf0, n, ubuf0, n, b, ub);
+            vb  = __lasx_xvldx(vbuf0, n);
+            y_l = __lasx_xvsllwil_w_h(b, 2);
+            y_h = __lasx_xvexth_w_h(b);
+            DUP2_ARG2(__lasx_xvsllwil_w_h, ub, 0, vb, 0, ub_l, vb_l);
+            DUP2_ARG1(__lasx_xvexth_w_h, ub, vb, ub_h, vb_h);
+            y_h = __lasx_xvslli_w(y_h, 2);
+            u_l = __lasx_xvsub_w(ub_l, uv);
+            u_h = __lasx_xvsub_w(ub_h, uv);
+            v_l = __lasx_xvsub_w(vb_l, uv);
+            v_h = __lasx_xvsub_w(vb_h, uv);
+            u_l = __lasx_xvslli_w(u_l, 2);
+            u_h = __lasx_xvslli_w(u_h, 2);
+            v_l = __lasx_xvslli_w(v_l, 2);
+            v_h = __lasx_xvslli_w(v_h, 2);
+            YUVTORGB(y_l, u_l, v_l, R_l, G_l, B_l, offset, coeff,
+                     y_temp, v2r, v2g, u2g, u2b);
+            YUVTORGB(y_h, u_h, v_h, R_h, G_h, B_h, offset, coeff,
+                     y_temp, v2r, v2g, u2g, u2b);
+
+            if(hasAlpha) {
+                __m256i a_src;
+                __m256i a_l, a_h;
+
+                a_src = __lasx_xvld(abuf0 + i, 0);
+                a_l   = __lasx_xvsllwil_w_h(a_src, 0);
+                a_h   = __lasx_xvexth_w_h(a_src);
+                a_l   = __lasx_xvadd_w(a_l, bias);
+                a_h   = __lasx_xvadd_w(a_h, bias);
+                a_l   = __lasx_xvsrai_w(a_l, 7);
+                a_h   = __lasx_xvsrai_w(a_h, 7);
+                WRITE_FULL_A(R_l, G_l, B_l, a_l, 0, 0);
+                WRITE_FULL_A(R_l, G_l, B_l, a_l, 1, 1);
+                WRITE_FULL_A(R_l, G_l, B_l, a_l, 2, 2);
+                WRITE_FULL_A(R_l, G_l, B_l, a_l, 3, 3);
+                WRITE_FULL_A(R_h, G_h, B_h, a_h, 0, 4);
+                WRITE_FULL_A(R_h, G_h, B_h, a_h, 1, 5);
+                WRITE_FULL_A(R_h, G_h, B_h, a_h, 2, 6);
+                WRITE_FULL_A(R_h, G_h, B_h, a_h, 3, 7);
+                WRITE_FULL_A(R_l, G_l, B_l, a_l, 4, 8);
+                WRITE_FULL_A(R_l, G_l, B_l, a_l, 5, 9);
+                WRITE_FULL_A(R_l, G_l, B_l, a_l, 6, 10);
+                WRITE_FULL_A(R_l, G_l, B_l, a_l, 7, 11);
+                WRITE_FULL_A(R_h, G_h, B_h, a_h, 4, 12);
+                WRITE_FULL_A(R_h, G_h, B_h, a_h, 5, 13);
+                WRITE_FULL_A(R_h, G_h, B_h, a_h, 6, 14);
+                WRITE_FULL_A(R_h, G_h, B_h, a_h, 7, 15);
+            } else {
+                WRITE_FULL(R_l, G_l, B_l, 0, 0);
+                WRITE_FULL(R_l, G_l, B_l, 1, 1);
+                WRITE_FULL(R_l, G_l, B_l, 2, 2);
+                WRITE_FULL(R_l, G_l, B_l, 3, 3);
+                WRITE_FULL(R_h, G_h, B_h, 0, 4);
+                WRITE_FULL(R_h, G_h, B_h, 1, 5);
+                WRITE_FULL(R_h, G_h, B_h, 2, 6);
+                WRITE_FULL(R_h, G_h, B_h, 3, 7);
+                WRITE_FULL(R_l, G_l, B_l, 4, 8);
+                WRITE_FULL(R_l, G_l, B_l, 5, 9);
+                WRITE_FULL(R_l, G_l, B_l, 6, 10);
+                WRITE_FULL(R_l, G_l, B_l, 7, 11);
+                WRITE_FULL(R_h, G_h, B_h, 4, 12);
+                WRITE_FULL(R_h, G_h, B_h, 5, 13);
+                WRITE_FULL(R_h, G_h, B_h, 6, 14);
+                WRITE_FULL(R_h, G_h, B_h, 7, 15);
+            }
+        }
+        if (dstW - i >= 8) {
+            __m256i b, ub, vb, ub_l, vb_l;
+            __m256i y_l, u_l, v_l;
+            __m256i R_l, G_l, B_l;
+            int n = i << 1;
+
+            DUP2_ARG2(__lasx_xvldx, buf0, n, ubuf0, n, b, ub);
+            vb  = __lasx_xvldx(vbuf0, n);
+            y_l = __lasx_vext2xv_w_h(b);
+            DUP2_ARG1(__lasx_vext2xv_w_h, ub, vb, ub_l, vb_l);
+            y_l = __lasx_xvslli_w(y_l, 2);
+            u_l = __lasx_xvsub_w(ub_l, uv);
+            v_l = __lasx_xvsub_w(vb_l, uv);
+            u_l = __lasx_xvslli_w(u_l, 2);
+            v_l = __lasx_xvslli_w(v_l, 2);
+            YUVTORGB(y_l, u_l, v_l, R_l, G_l, B_l, offset, coeff,
+                     y_temp, v2r, v2g, u2g, u2b);
+
+            if(hasAlpha) {
+                __m256i a_src, a_l;
+
+                a_src = __lasx_xvldx(abuf0, n);
+                a_src = __lasx_vext2xv_w_h(a_src);
+                a_l   = __lasx_xvadd_w(bias, a_src);
+                a_l   = __lasx_xvsrai_w(a_l, 7);
+                WRITE_FULL_A(R_l, G_l, B_l, a_l, 0, 0);
+                WRITE_FULL_A(R_l, G_l, B_l, a_l, 1, 1);
+                WRITE_FULL_A(R_l, G_l, B_l, a_l, 2, 2);
+                WRITE_FULL_A(R_l, G_l, B_l, a_l, 3, 3);
+                WRITE_FULL_A(R_l, G_l, B_l, a_l, 4, 4);
+                WRITE_FULL_A(R_l, G_l, B_l, a_l, 5, 5);
+                WRITE_FULL_A(R_l, G_l, B_l, a_l, 6, 6);
+                WRITE_FULL_A(R_l, G_l, B_l, a_l, 7, 7);
+            } else {
+                WRITE_FULL(R_l, G_l, B_l, 0, 0);
+                WRITE_FULL(R_l, G_l, B_l, 1, 1);
+                WRITE_FULL(R_l, G_l, B_l, 2, 2);
+                WRITE_FULL(R_l, G_l, B_l, 3, 3);
+                WRITE_FULL(R_l, G_l, B_l, 4, 4);
+                WRITE_FULL(R_l, G_l, B_l, 5, 5);
+                WRITE_FULL(R_l, G_l, B_l, 6, 6);
+                WRITE_FULL(R_l, G_l, B_l, 7, 7);
+            }
+            i += 8;
+        }
+        for (; i < dstW; i++) {
+            int Y = buf0[i] << 2;
+            int U = (ubuf0[i] - uvtemp) << 2;
+            int V = (vbuf0[i] - uvtemp) << 2;
+
+            A = 0;
+            if(hasAlpha) {
+                A = (abuf0[i] + 64) >> 7;
+                if (A & 0x100)
+                    A = av_clip_uint8(A);
+            }
+            Y -= y_offset;
+            Y *= y_coeff;
+            Y += ytemp;
+            R  = (unsigned)Y + V * v2r_coe;
+            G  = (unsigned)Y + V * v2g_coe + U * u2g_coe;
+            B  = (unsigned)Y + U * u2b_coe;
+            yuv2rgb_write_full(c, dest, i, R, A, G, B, y, target, hasAlpha, err);
+            dest += step;
+        }
+    } else {
+        const int16_t *ubuf1 = ubuf[1], *vbuf1 = vbuf[1];
+        int uvtemp   = 128 << 8;
+        __m256i uv   = __lasx_xvreplgr2vr_w(uvtemp);
+        __m256i zero = __lasx_xvldi(0);
+        __m256i bias = __lasx_xvreplgr2vr_h(bias_int);
+
+        for (i = 0; i < len; i += 16) {
+            __m256i b, ub0, ub1, vb0, vb1;
+            __m256i y_ev, y_od, u_ev, u_od, v_ev, v_od;
+            __m256i R_ev, R_od, G_ev, G_od, B_ev, B_od;
+            int n = i << 1;
+
+            DUP4_ARG2(__lasx_xvldx, buf0, n, ubuf0, n, vbuf0, n,
+                      ubuf1, n, b, ub0, vb0, ub1);
+            vb1 = __lasx_xvldx(vbuf, n);
+            y_ev = __lasx_xvaddwev_w_h(b, zero);
+            y_od = __lasx_xvaddwod_w_h(b, zero);
+            DUP2_ARG2(__lasx_xvaddwev_w_h, ub0, vb0, ub1, vb1, u_ev, v_ev);
+            DUP2_ARG2(__lasx_xvaddwod_w_h, ub0, vb0, ub1, vb1, u_od, v_od);
+            DUP2_ARG2(__lasx_xvslli_w, y_ev, 2, y_od, 2, y_ev, y_od);
+            DUP4_ARG2(__lasx_xvsub_w, u_ev, uv, u_od, uv, v_ev, uv, v_od, uv,
+                      u_ev, u_od, v_ev, v_od);
+            DUP4_ARG2(__lasx_xvslli_w, u_ev, 1, u_od, 1, v_ev, 1, v_od, 1,
+                      u_ev, u_od, v_ev, v_od);
+            YUVTORGB(y_ev, u_ev, v_ev, R_ev, G_ev, B_ev, offset, coeff,
+                     y_temp, v2r, v2g, u2g, u2b);
+            YUVTORGB(y_od, u_od, v_od, R_od, G_od, B_od, offset, coeff,
+                     y_temp, v2r, v2g, u2g, u2b);
+
+            if(hasAlpha) {
+                __m256i a_src;
+                __m256i a_ev, a_od;
+
+                a_src = __lasx_xvld(abuf0 + i, 0);
+                a_ev  = __lasx_xvaddwev_w_h(bias, a_src);
+                a_od  = __lasx_xvaddwod_w_h(bias, a_src);
+                a_ev  = __lasx_xvsrai_w(a_ev, 7);
+                a_od  = __lasx_xvsrai_w(a_od, 7);
+                WRITE_FULL_A(R_ev, G_ev, B_ev, a_ev, 0, 0);
+                WRITE_FULL_A(R_od, G_od, B_od, a_od, 0, 1);
+                WRITE_FULL_A(R_ev, G_ev, B_ev, a_ev, 1, 2);
+                WRITE_FULL_A(R_od, G_od, B_od, a_od, 1, 3);
+                WRITE_FULL_A(R_ev, G_ev, B_ev, a_ev, 2, 4);
+                WRITE_FULL_A(R_od, G_od, B_od, a_od, 2, 5);
+                WRITE_FULL_A(R_ev, G_ev, B_ev, a_ev, 3, 6);
+                WRITE_FULL_A(R_od, G_od, B_od, a_od, 3, 7);
+                WRITE_FULL_A(R_ev, G_ev, B_ev, a_ev, 4, 8);
+                WRITE_FULL_A(R_od, G_od, B_od, a_od, 4, 9);
+                WRITE_FULL_A(R_ev, G_ev, B_ev, a_ev, 5, 10);
+                WRITE_FULL_A(R_od, G_od, B_od, a_od, 5, 11);
+                WRITE_FULL_A(R_ev, G_ev, B_ev, a_ev, 6, 12);
+                WRITE_FULL_A(R_od, G_od, B_od, a_od, 6, 13);
+                WRITE_FULL_A(R_ev, G_ev, B_ev, a_ev, 7, 14);
+                WRITE_FULL_A(R_od, G_od, B_od, a_od, 7, 15);
+            } else {
+                WRITE_FULL(R_ev, G_ev, B_ev, 0, 0);
+                WRITE_FULL(R_od, G_od, B_od, 0, 1);
+                WRITE_FULL(R_ev, G_ev, B_ev, 1, 2);
+                WRITE_FULL(R_od, G_od, B_od, 1, 3);
+                WRITE_FULL(R_ev, G_ev, B_ev, 2, 4);
+                WRITE_FULL(R_od, G_od, B_od, 2, 5);
+                WRITE_FULL(R_ev, G_ev, B_ev, 3, 6);
+                WRITE_FULL(R_od, G_od, B_od, 3, 7);
+                WRITE_FULL(R_ev, G_ev, B_ev, 4, 8);
+                WRITE_FULL(R_od, G_od, B_od, 4, 9);
+                WRITE_FULL(R_ev, G_ev, B_ev, 5, 10);
+                WRITE_FULL(R_od, G_od, B_od, 5, 11);
+                WRITE_FULL(R_ev, G_ev, B_ev, 6, 12);
+                WRITE_FULL(R_od, G_od, B_od, 6, 13);
+                WRITE_FULL(R_ev, G_ev, B_ev, 7, 14);
+                WRITE_FULL(R_od, G_od, B_od, 7, 15);
+            }
+        }
+        if (dstW - i >= 8) {
+            __m256i b, ub0, ub1, vb0, vb1;
+            __m256i y_l, u_l, v_l;
+            __m256i R_l, G_l, B_l;
+            int n = i << 1;
+
+            DUP4_ARG2(__lasx_xvldx, buf0, n, ubuf0, n, vbuf0, n,
+                      ubuf1, n, b, ub0, vb0, ub1);
+            vb1 = __lasx_xvldx(vbuf1, n);
+            y_l = __lasx_vext2xv_w_h(b);
+            y_l = __lasx_xvslli_w(y_l, 2);
+            DUP4_ARG1(__lasx_vext2xv_w_h, ub0, vb0, ub1, vb1,
+                      ub0, vb0, ub1, vb1);
+            DUP2_ARG2(__lasx_xvadd_w, ub0, ub1, vb0, vb1, u_l, v_l);
+            u_l = __lasx_xvsub_w(u_l, uv);
+            v_l = __lasx_xvsub_w(v_l, uv);
+            u_l = __lasx_xvslli_w(u_l, 1);
+            v_l = __lasx_xvslli_w(v_l, 1);
+            YUVTORGB(y_l, u_l, v_l, R_l, G_l, B_l, offset, coeff,
+                     y_temp, v2r, v2g, u2g, u2b);
+
+            if(hasAlpha) {
+                __m256i a_src;
+                __m256i a_l;
+
+                a_src  = __lasx_xvld(abuf0 + i, 0);
+                a_src  = __lasx_xvpermi_d(a_src, 0xD8);
+                a_src  = __lasx_xvilvl_h(a_src, a_src);
+                a_l    = __lasx_xvaddwev_w_h(bias, a_l);
+                a_l   = __lasx_xvsrai_w(a_l, 7);
+                WRITE_FULL_A(R_l, G_l, B_l, a_l, 0, 0);
+                WRITE_FULL_A(R_l, G_l, B_l, a_l, 1, 1);
+                WRITE_FULL_A(R_l, G_l, B_l, a_l, 2, 2);
+                WRITE_FULL_A(R_l, G_l, B_l, a_l, 3, 3);
+                WRITE_FULL_A(R_l, G_l, B_l, a_l, 4, 4);
+                WRITE_FULL_A(R_l, G_l, B_l, a_l, 5, 5);
+                WRITE_FULL_A(R_l, G_l, B_l, a_l, 6, 6);
+                WRITE_FULL_A(R_l, G_l, B_l, a_l, 7, 7);
+            } else {
+                WRITE_FULL(R_l, G_l, B_l, 0, 0);
+                WRITE_FULL(R_l, G_l, B_l, 1, 1);
+                WRITE_FULL(R_l, G_l, B_l, 2, 2);
+                WRITE_FULL(R_l, G_l, B_l, 3, 3);
+                WRITE_FULL(R_l, G_l, B_l, 4, 4);
+                WRITE_FULL(R_l, G_l, B_l, 5, 5);
+                WRITE_FULL(R_l, G_l, B_l, 6, 6);
+                WRITE_FULL(R_l, G_l, B_l, 7, 7);
+            }
+            i += 8;
+        }
+        for (; i < dstW; i++) {
+            int Y = buf0[i] << 2;
+            int U = (ubuf0[i] + ubuf1[i] - uvtemp) << 1;
+            int V = (vbuf0[i] + vbuf1[i] - uvtemp) << 1;
+
+            A = 0;
+            if(hasAlpha) {
+                A = (abuf0[i] + 64) >> 7;
+                if (A & 0x100)
+                    A = av_clip_uint8(A);
+            }
+            Y -= y_offset;
+            Y *= y_coeff;
+            Y += ytemp;
+            R  = (unsigned)Y + V * v2r_coe;
+            G  = (unsigned)Y + V * v2g_coe + U * u2g_coe;
+            B  = (unsigned)Y + U * u2b_coe;
+            yuv2rgb_write_full(c, dest, i, R, A, G, B, y, target, hasAlpha, err);
+            dest += step;
+        }
+    }
+    c->dither_error[0][i] = err[0];
+    c->dither_error[1][i] = err[1];
+    c->dither_error[2][i] = err[2];
+}
+#if CONFIG_SMALL
+YUV2RGBWRAPPER(yuv2, rgb_full, bgra32_full, AV_PIX_FMT_BGRA,
+               CONFIG_SWSCALE_ALPHA && c->needAlpha)
+YUV2RGBWRAPPER(yuv2, rgb_full, abgr32_full, AV_PIX_FMT_ABGR,
+               CONFIG_SWSCALE_ALPHA && c->needAlpha)
+YUV2RGBWRAPPER(yuv2, rgb_full, rgba32_full, AV_PIX_FMT_RGBA,
+               CONFIG_SWSCALE_ALPHA && c->needAlpha)
+YUV2RGBWRAPPER(yuv2, rgb_full, argb32_full, AV_PIX_FMT_ARGB,
+               CONFIG_SWSCALE_ALPHA && c->needAlpha)
+#else
+#if CONFIG_SWSCALE_ALPHA
+YUV2RGBWRAPPER(yuv2, rgb_full, bgra32_full, AV_PIX_FMT_BGRA,  1)
+YUV2RGBWRAPPER(yuv2, rgb_full, abgr32_full, AV_PIX_FMT_ABGR,  1)
+YUV2RGBWRAPPER(yuv2, rgb_full, rgba32_full, AV_PIX_FMT_RGBA,  1)
+YUV2RGBWRAPPER(yuv2, rgb_full, argb32_full, AV_PIX_FMT_ARGB,  1)
+#endif
+YUV2RGBWRAPPER(yuv2, rgb_full, bgrx32_full, AV_PIX_FMT_BGRA,  0)
+YUV2RGBWRAPPER(yuv2, rgb_full, xbgr32_full, AV_PIX_FMT_ABGR,  0)
+YUV2RGBWRAPPER(yuv2, rgb_full, rgbx32_full, AV_PIX_FMT_RGBA,  0)
+YUV2RGBWRAPPER(yuv2, rgb_full, xrgb32_full, AV_PIX_FMT_ARGB,  0)
+#endif
+YUV2RGBWRAPPER(yuv2, rgb_full, bgr24_full,  AV_PIX_FMT_BGR24, 0)
+YUV2RGBWRAPPER(yuv2, rgb_full, rgb24_full,  AV_PIX_FMT_RGB24, 0)
+
+YUV2RGBWRAPPER(yuv2, rgb_full, bgr4_byte_full,  AV_PIX_FMT_BGR4_BYTE, 0)
+YUV2RGBWRAPPER(yuv2, rgb_full, rgb4_byte_full,  AV_PIX_FMT_RGB4_BYTE, 0)
+YUV2RGBWRAPPER(yuv2, rgb_full, bgr8_full,   AV_PIX_FMT_BGR8,  0)
+YUV2RGBWRAPPER(yuv2, rgb_full, rgb8_full,   AV_PIX_FMT_RGB8,  0)
+
+
+av_cold void ff_sws_init_output_loongarch(SwsContext *c)
+{
+
+    if(c->flags & SWS_FULL_CHR_H_INT) {
+        switch (c->dstFormat) {
+        case AV_PIX_FMT_RGBA:
+#if CONFIG_SMALL
+            c->yuv2packedX = yuv2rgba32_full_X_lasx;
+            c->yuv2packed2 = yuv2rgba32_full_2_lasx;
+            c->yuv2packed1 = yuv2rgba32_full_1_lasx;
+#else
+#if CONFIG_SWSCALE_ALPHA
+            if (c->needAlpha) {
+                c->yuv2packedX = yuv2rgba32_full_X_lasx;
+                c->yuv2packed2 = yuv2rgba32_full_2_lasx;
+                c->yuv2packed1 = yuv2rgba32_full_1_lasx;
+            } else
+#endif /* CONFIG_SWSCALE_ALPHA */
+            {
+                c->yuv2packedX = yuv2rgbx32_full_X_lasx;
+                c->yuv2packed2 = yuv2rgbx32_full_2_lasx;
+                c->yuv2packed1 = yuv2rgbx32_full_1_lasx;
+            }
+#endif /* !CONFIG_SMALL */
+            break;
+        case AV_PIX_FMT_ARGB:
+#if CONFIG_SMALL
+            c->yuv2packedX = yuv2argb32_full_X_lasx;
+            c->yuv2packed2 = yuv2argb32_full_2_lasx;
+            c->yuv2packed1 = yuv2argb32_full_1_lasx;
+#else
+#if CONFIG_SWSCALE_ALPHA
+            if (c->needAlpha) {
+                c->yuv2packedX = yuv2argb32_full_X_lasx;
+                c->yuv2packed2 = yuv2argb32_full_2_lasx;
+                c->yuv2packed1 = yuv2argb32_full_1_lasx;
+            } else
+#endif /* CONFIG_SWSCALE_ALPHA */
+            {
+                c->yuv2packedX = yuv2xrgb32_full_X_lasx;
+                c->yuv2packed2 = yuv2xrgb32_full_2_lasx;
+                c->yuv2packed1 = yuv2xrgb32_full_1_lasx;
+            }
+#endif /* !CONFIG_SMALL */
+            break;
+        case AV_PIX_FMT_BGRA:
+#if CONFIG_SMALL
+            c->yuv2packedX = yuv2bgra32_full_X_lasx;
+            c->yuv2packed2 = yuv2bgra32_full_2_lasx;
+            c->yuv2packed1 = yuv2bgra32_full_1_lasx;
+#else
+#if CONFIG_SWSCALE_ALPHA
+            if (c->needAlpha) {
+                c->yuv2packedX = yuv2bgra32_full_X_lasx;
+                c->yuv2packed2 = yuv2bgra32_full_2_lasx;
+                c->yuv2packed1 = yuv2bgra32_full_1_lasx;
+            } else
+#endif /* CONFIG_SWSCALE_ALPHA */
+            {
+                c->yuv2packedX = yuv2bgrx32_full_X_lasx;
+                c->yuv2packed2 = yuv2bgrx32_full_2_lasx;
+                c->yuv2packed1 = yuv2bgrx32_full_1_lasx;
+            }
+#endif /* !CONFIG_SMALL */
+            break;
+        case AV_PIX_FMT_ABGR:
+#if CONFIG_SMALL
+            c->yuv2packedX = yuv2abgr32_full_X_lasx;
+            c->yuv2packed2 = yuv2abgr32_full_2_lasx;
+            c->yuv2packed1 = yuv2abgr32_full_1_lasx;
+#else
+#if CONFIG_SWSCALE_ALPHA
+            if (c->needAlpha) {
+                c->yuv2packedX = yuv2abgr32_full_X_lasx;
+                c->yuv2packed2 = yuv2abgr32_full_2_lasx;
+                c->yuv2packed1 = yuv2abgr32_full_1_lasx;
+            } else
+#endif /* CONFIG_SWSCALE_ALPHA */
+            {
+                c->yuv2packedX = yuv2xbgr32_full_X_lasx;
+                c->yuv2packed2 = yuv2xbgr32_full_2_lasx;
+                c->yuv2packed1 = yuv2xbgr32_full_1_lasx;
+            }
+#endif /* !CONFIG_SMALL */
+            break;
+        case AV_PIX_FMT_RGB24:
+            c->yuv2packedX = yuv2rgb24_full_X_lasx;
+            c->yuv2packed2 = yuv2rgb24_full_2_lasx;
+            c->yuv2packed1 = yuv2rgb24_full_1_lasx;
+            break;
+        case AV_PIX_FMT_BGR24:
+            c->yuv2packedX = yuv2bgr24_full_X_lasx;
+            c->yuv2packed2 = yuv2bgr24_full_2_lasx;
+            c->yuv2packed1 = yuv2bgr24_full_1_lasx;
+            break;
+        case AV_PIX_FMT_BGR4_BYTE:
+            c->yuv2packedX = yuv2bgr4_byte_full_X_lasx;
+            c->yuv2packed2 = yuv2bgr4_byte_full_2_lasx;
+            c->yuv2packed1 = yuv2bgr4_byte_full_1_lasx;
+            break;
+        case AV_PIX_FMT_RGB4_BYTE:
+            c->yuv2packedX = yuv2rgb4_byte_full_X_lasx;
+            c->yuv2packed2 = yuv2rgb4_byte_full_2_lasx;
+            c->yuv2packed1 = yuv2rgb4_byte_full_1_lasx;
+            break;
+        case AV_PIX_FMT_BGR8:
+            c->yuv2packedX = yuv2bgr8_full_X_lasx;
+            c->yuv2packed2 = yuv2bgr8_full_2_lasx;
+            c->yuv2packed1 = yuv2bgr8_full_1_lasx;
+            break;
+        case AV_PIX_FMT_RGB8:
+            c->yuv2packedX = yuv2rgb8_full_X_lasx;
+            c->yuv2packed2 = yuv2rgb8_full_2_lasx;
+            c->yuv2packed1 = yuv2rgb8_full_1_lasx;
+            break;
+    }
+    } else {
+        switch (c->dstFormat) {
+        case AV_PIX_FMT_RGB32:
+        case AV_PIX_FMT_BGR32:
+#if CONFIG_SMALL
+#else
+#if CONFIG_SWSCALE_ALPHA
+            if (c->needAlpha) {
+            } else
+#endif /* CONFIG_SWSCALE_ALPHA */
+            {
+                c->yuv2packed1 = yuv2rgbx32_1_lasx;
+                c->yuv2packed2 = yuv2rgbx32_2_lasx;
+                c->yuv2packedX = yuv2rgbx32_X_lasx;
+            }
+#endif /* !CONFIG_SMALL */
+            break;
+        case AV_PIX_FMT_RGB32_1:
+        case AV_PIX_FMT_BGR32_1:
+#if CONFIG_SMALL
+#else
+#if CONFIG_SWSCALE_ALPHA
+            if (c->needAlpha) {
+            } else
+#endif /* CONFIG_SWSCALE_ALPHA */
+            {
+                c->yuv2packed1 = yuv2rgbx32_1_1_lasx;
+                c->yuv2packed2 = yuv2rgbx32_1_2_lasx;
+                c->yuv2packedX = yuv2rgbx32_1_X_lasx;
+            }
+#endif /* !CONFIG_SMALL */
+            break;
+        case AV_PIX_FMT_RGB24:
+            c->yuv2packed1 = yuv2rgb24_1_lasx;
+            c->yuv2packed2 = yuv2rgb24_2_lasx;
+            c->yuv2packedX = yuv2rgb24_X_lasx;
+            break;
+        case AV_PIX_FMT_BGR24:
+            c->yuv2packed1 = yuv2bgr24_1_lasx;
+            c->yuv2packed2 = yuv2bgr24_2_lasx;
+            c->yuv2packedX = yuv2bgr24_X_lasx;
+            break;
+        case AV_PIX_FMT_RGB565LE:
+        case AV_PIX_FMT_RGB565BE:
+        case AV_PIX_FMT_BGR565LE:
+        case AV_PIX_FMT_BGR565BE:
+            c->yuv2packed1 = yuv2rgb16_1_lasx;
+            c->yuv2packed2 = yuv2rgb16_2_lasx;
+            c->yuv2packedX = yuv2rgb16_X_lasx;
+            break;
+        case AV_PIX_FMT_RGB555LE:
+        case AV_PIX_FMT_RGB555BE:
+        case AV_PIX_FMT_BGR555LE:
+        case AV_PIX_FMT_BGR555BE:
+            c->yuv2packed1 = yuv2rgb15_1_lasx;
+            c->yuv2packed2 = yuv2rgb15_2_lasx;
+            c->yuv2packedX = yuv2rgb15_X_lasx;
+            break;
+        case AV_PIX_FMT_RGB444LE:
+        case AV_PIX_FMT_RGB444BE:
+        case AV_PIX_FMT_BGR444LE:
+        case AV_PIX_FMT_BGR444BE:
+            c->yuv2packed1 = yuv2rgb12_1_lasx;
+            c->yuv2packed2 = yuv2rgb12_2_lasx;
+            c->yuv2packedX = yuv2rgb12_X_lasx;
+            break;
+        case AV_PIX_FMT_RGB8:
+        case AV_PIX_FMT_BGR8:
+            c->yuv2packed1 = yuv2rgb8_1_lasx;
+            c->yuv2packed2 = yuv2rgb8_2_lasx;
+            c->yuv2packedX = yuv2rgb8_X_lasx;
+            break;
+        case AV_PIX_FMT_RGB4:
+        case AV_PIX_FMT_BGR4:
+            c->yuv2packed1 = yuv2rgb4_1_lasx;
+            c->yuv2packed2 = yuv2rgb4_2_lasx;
+            c->yuv2packedX = yuv2rgb4_X_lasx;
+            break;
+        case AV_PIX_FMT_RGB4_BYTE:
+        case AV_PIX_FMT_BGR4_BYTE:
+            c->yuv2packed1 = yuv2rgb4b_1_lasx;
+            c->yuv2packed2 = yuv2rgb4b_2_lasx;
+            c->yuv2packedX = yuv2rgb4b_X_lasx;
+            break;
+        }
+    }
+}
diff --git a/libswscale/loongarch/output_lsx.c b/libswscale/loongarch/output_lsx.c
new file mode 100644
index 0000000000..768cc3abc6
--- /dev/null
+++ b/libswscale/loongarch/output_lsx.c
@@ -0,0 +1,1828 @@
+/*
+ * Copyright (C) 2023 Loongson Technology Corporation Limited
+ * Contributed by Lu Wang <wanglu@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "swscale_loongarch.h"
+#include "libavutil/loongarch/loongson_intrinsics.h"
+
+
+/*Copy from libswscale/output.c*/
+static av_always_inline void
+yuv2rgb_write(uint8_t *_dest, int i, int Y1, int Y2,
+              unsigned A1, unsigned A2,
+              const void *_r, const void *_g, const void *_b, int y,
+              enum AVPixelFormat target, int hasAlpha)
+{
+    if (target == AV_PIX_FMT_ARGB || target == AV_PIX_FMT_RGBA ||
+        target == AV_PIX_FMT_ABGR || target == AV_PIX_FMT_BGRA) {
+        uint32_t *dest = (uint32_t *) _dest;
+        const uint32_t *r = (const uint32_t *) _r;
+        const uint32_t *g = (const uint32_t *) _g;
+        const uint32_t *b = (const uint32_t *) _b;
+
+#if CONFIG_SMALL
+        dest[i * 2 + 0] = r[Y1] + g[Y1] + b[Y1];
+        dest[i * 2 + 1] = r[Y2] + g[Y2] + b[Y2];
+#else
+#if defined(ASSERT_LEVEL) && ASSERT_LEVEL > 1
+        int sh = (target == AV_PIX_FMT_RGB32_1 ||
+                  target == AV_PIX_FMT_BGR32_1) ? 0 : 24;
+        av_assert2((((r[Y1] + g[Y1] + b[Y1]) >> sh) & 0xFF) == 0xFF);
+#endif
+        dest[i * 2 + 0] = r[Y1] + g[Y1] + b[Y1];
+        dest[i * 2 + 1] = r[Y2] + g[Y2] + b[Y2];
+#endif
+    } else if (target == AV_PIX_FMT_RGB24 || target == AV_PIX_FMT_BGR24) {
+        uint8_t *dest = (uint8_t *) _dest;
+        const uint8_t *r = (const uint8_t *) _r;
+        const uint8_t *g = (const uint8_t *) _g;
+        const uint8_t *b = (const uint8_t *) _b;
+
+#define r_b ((target == AV_PIX_FMT_RGB24) ? r : b)
+#define b_r ((target == AV_PIX_FMT_RGB24) ? b : r)
+
+        dest[i * 6 + 0] = r_b[Y1];
+        dest[i * 6 + 1] =   g[Y1];
+        dest[i * 6 + 2] = b_r[Y1];
+        dest[i * 6 + 3] = r_b[Y2];
+        dest[i * 6 + 4] =   g[Y2];
+        dest[i * 6 + 5] = b_r[Y2];
+#undef r_b
+#undef b_r
+    } else if (target == AV_PIX_FMT_RGB565 || target == AV_PIX_FMT_BGR565 ||
+               target == AV_PIX_FMT_RGB555 || target == AV_PIX_FMT_BGR555 ||
+               target == AV_PIX_FMT_RGB444 || target == AV_PIX_FMT_BGR444) {
+        uint16_t *dest = (uint16_t *) _dest;
+        const uint16_t *r = (const uint16_t *) _r;
+        const uint16_t *g = (const uint16_t *) _g;
+        const uint16_t *b = (const uint16_t *) _b;
+        int dr1, dg1, db1, dr2, dg2, db2;
+
+        if (target == AV_PIX_FMT_RGB565 || target == AV_PIX_FMT_BGR565) {
+            dr1 = ff_dither_2x2_8[ y & 1     ][0];
+            dg1 = ff_dither_2x2_4[ y & 1     ][0];
+            db1 = ff_dither_2x2_8[(y & 1) ^ 1][0];
+            dr2 = ff_dither_2x2_8[ y & 1     ][1];
+            dg2 = ff_dither_2x2_4[ y & 1     ][1];
+            db2 = ff_dither_2x2_8[(y & 1) ^ 1][1];
+        } else if (target == AV_PIX_FMT_RGB555 || target == AV_PIX_FMT_BGR555) {
+            dr1 = ff_dither_2x2_8[ y & 1     ][0];
+            dg1 = ff_dither_2x2_8[ y & 1     ][1];
+            db1 = ff_dither_2x2_8[(y & 1) ^ 1][0];
+            dr2 = ff_dither_2x2_8[ y & 1     ][1];
+            dg2 = ff_dither_2x2_8[ y & 1     ][0];
+            db2 = ff_dither_2x2_8[(y & 1) ^ 1][1];
+        } else {
+            dr1 = ff_dither_4x4_16[ y & 3     ][0];
+            dg1 = ff_dither_4x4_16[ y & 3     ][1];
+            db1 = ff_dither_4x4_16[(y & 3) ^ 3][0];
+            dr2 = ff_dither_4x4_16[ y & 3     ][1];
+            dg2 = ff_dither_4x4_16[ y & 3     ][0];
+            db2 = ff_dither_4x4_16[(y & 3) ^ 3][1];
+        }
+
+        dest[i * 2 + 0] = r[Y1 + dr1] + g[Y1 + dg1] + b[Y1 + db1];
+        dest[i * 2 + 1] = r[Y2 + dr2] + g[Y2 + dg2] + b[Y2 + db2];
+    } else { /* 8/4 bits */
+        uint8_t *dest = (uint8_t *) _dest;
+        const uint8_t *r = (const uint8_t *) _r;
+        const uint8_t *g = (const uint8_t *) _g;
+        const uint8_t *b = (const uint8_t *) _b;
+        int dr1, dg1, db1, dr2, dg2, db2;
+
+        if (target == AV_PIX_FMT_RGB8 || target == AV_PIX_FMT_BGR8) {
+            const uint8_t * const d64 = ff_dither_8x8_73[y & 7];
+            const uint8_t * const d32 = ff_dither_8x8_32[y & 7];
+            dr1 = dg1 = d32[(i * 2 + 0) & 7];
+            db1 =       d64[(i * 2 + 0) & 7];
+            dr2 = dg2 = d32[(i * 2 + 1) & 7];
+            db2 =       d64[(i * 2 + 1) & 7];
+        } else {
+            const uint8_t * const d64  = ff_dither_8x8_73 [y & 7];
+            const uint8_t * const d128 = ff_dither_8x8_220[y & 7];
+            dr1 = db1 = d128[(i * 2 + 0) & 7];
+            dg1 =        d64[(i * 2 + 0) & 7];
+            dr2 = db2 = d128[(i * 2 + 1) & 7];
+            dg2 =        d64[(i * 2 + 1) & 7];
+        }
+
+        if (target == AV_PIX_FMT_RGB4 || target == AV_PIX_FMT_BGR4) {
+            dest[i] = r[Y1 + dr1] + g[Y1 + dg1] + b[Y1 + db1] +
+                    ((r[Y2 + dr2] + g[Y2 + dg2] + b[Y2 + db2]) << 4);
+        } else {
+            dest[i * 2 + 0] = r[Y1 + dr1] + g[Y1 + dg1] + b[Y1 + db1];
+            dest[i * 2 + 1] = r[Y2 + dr2] + g[Y2 + dg2] + b[Y2 + db2];
+        }
+    }
+}
+
+#define WRITE_YUV2RGB_LSX(vec_y1, vec_y2, vec_u, vec_v, t1, t2, t3, t4) \
+{                                                                       \
+    Y1 = __lsx_vpickve2gr_w(vec_y1, t1);                                \
+    Y2 = __lsx_vpickve2gr_w(vec_y2, t2);                                \
+    U  = __lsx_vpickve2gr_w(vec_u, t3);                                 \
+    V  = __lsx_vpickve2gr_w(vec_v, t4);                                 \
+    r  =  c->table_rV[V];                                               \
+    g  = (c->table_gU[U] + c->table_gV[V]);                             \
+    b  =  c->table_bU[U];                                               \
+    yuv2rgb_write(dest, count, Y1, Y2, 0, 0,                            \
+                  r, g, b, y, target, 0);                               \
+    count++;                                                            \
+}
+
+static void
+yuv2rgb_X_template_lsx(SwsContext *c, const int16_t *lumFilter,
+                       const int16_t **lumSrc, int lumFilterSize,
+                       const int16_t *chrFilter, const int16_t **chrUSrc,
+                       const int16_t **chrVSrc, int chrFilterSize,
+                       const int16_t **alpSrc, uint8_t *dest, int dstW,
+                       int y, enum AVPixelFormat target, int hasAlpha)
+{
+    int i, j;
+    int count = 0;
+    int t     = 1 << 18;
+    int len   = dstW >> 5;
+    int res   = dstW & 31;
+    int len_count = (dstW + 1) >> 1;
+    const void *r, *g, *b;
+    int head = YUVRGB_TABLE_HEADROOM;
+    __m128i headroom  = __lsx_vreplgr2vr_w(head);
+
+    for (i = 0; i < len; i++) {
+        int Y1, Y2, U, V, count_lum = count << 1;
+        __m128i l_src1, l_src2, l_src3, l_src4, u_src1, u_src2, v_src1, v_src2;
+        __m128i yl_ev, yl_ev1, yl_ev2, yl_od1, yl_od2, yh_ev1, yh_ev2, yh_od1, yh_od2;
+        __m128i u_ev1, u_ev2, u_od1, u_od2, v_ev1, v_ev2, v_od1, v_od2, temp;
+
+        yl_ev  = __lsx_vldrepl_w(&t, 0);
+        yl_ev1 = yl_ev;
+        yl_od1 = yl_ev;
+        yh_ev1 = yl_ev;
+        yh_od1 = yl_ev;
+        u_ev1  = yl_ev;
+        v_ev1  = yl_ev;
+        u_od1  = yl_ev;
+        v_od1  = yl_ev;
+        yl_ev2 = yl_ev;
+        yl_od2 = yl_ev;
+        yh_ev2 = yl_ev;
+        yh_od2 = yl_ev;
+        u_ev2  = yl_ev;
+        v_ev2  = yl_ev;
+        u_od2  = yl_ev;
+        v_od2  = yl_ev;
+
+        for (j = 0; j < lumFilterSize; j++) {
+            temp   = __lsx_vldrepl_h((lumFilter + j), 0);
+            DUP2_ARG2(__lsx_vld, lumSrc[j] + count_lum, 0, lumSrc[j] + count_lum,
+                      16, l_src1, l_src2);
+            DUP2_ARG2(__lsx_vld, lumSrc[j] + count_lum, 32, lumSrc[j] + count_lum,
+                      48, l_src3, l_src4);
+            yl_ev1  = __lsx_vmaddwev_w_h(yl_ev1, temp, l_src1);
+            yl_od1  = __lsx_vmaddwod_w_h(yl_od1, temp, l_src1);
+            yh_ev1  = __lsx_vmaddwev_w_h(yh_ev1, temp, l_src3);
+            yh_od1  = __lsx_vmaddwod_w_h(yh_od1, temp, l_src3);
+            yl_ev2  = __lsx_vmaddwev_w_h(yl_ev2, temp, l_src2);
+            yl_od2  = __lsx_vmaddwod_w_h(yl_od2, temp, l_src2);
+            yh_ev2  = __lsx_vmaddwev_w_h(yh_ev2, temp, l_src4);
+            yh_od2  = __lsx_vmaddwod_w_h(yh_od2, temp, l_src4);
+        }
+        for (j = 0; j < chrFilterSize; j++) {
+            DUP2_ARG2(__lsx_vld, chrUSrc[j] + count, 0, chrVSrc[j] + count, 0,
+                      u_src1, v_src1);
+            DUP2_ARG2(__lsx_vld, chrUSrc[j] + count, 16, chrVSrc[j] + count, 16,
+                      u_src2, v_src2);
+            temp  = __lsx_vldrepl_h((chrFilter + j), 0);
+            u_ev1 = __lsx_vmaddwev_w_h(u_ev1, temp, u_src1);
+            u_od1 = __lsx_vmaddwod_w_h(u_od1, temp, u_src1);
+            v_ev1 = __lsx_vmaddwev_w_h(v_ev1, temp, v_src1);
+            v_od1 = __lsx_vmaddwod_w_h(v_od1, temp, v_src1);
+            u_ev2 = __lsx_vmaddwev_w_h(u_ev2, temp, u_src2);
+            u_od2 = __lsx_vmaddwod_w_h(u_od2, temp, u_src2);
+            v_ev2 = __lsx_vmaddwev_w_h(v_ev2, temp, v_src2);
+            v_od2 = __lsx_vmaddwod_w_h(v_od2, temp, v_src2);
+        }
+        yl_ev1 = __lsx_vsrai_w(yl_ev1, 19);
+        yh_ev1 = __lsx_vsrai_w(yh_ev1, 19);
+        yl_od1 = __lsx_vsrai_w(yl_od1, 19);
+        yh_od1 = __lsx_vsrai_w(yh_od1, 19);
+        u_ev1  = __lsx_vsrai_w(u_ev1, 19);
+        v_ev1  = __lsx_vsrai_w(v_ev1, 19);
+        u_od1  = __lsx_vsrai_w(u_od1, 19);
+        v_od1  = __lsx_vsrai_w(v_od1, 19);
+        yl_ev2 = __lsx_vsrai_w(yl_ev2, 19);
+        yh_ev2 = __lsx_vsrai_w(yh_ev2, 19);
+        yl_od2 = __lsx_vsrai_w(yl_od2, 19);
+        yh_od2 = __lsx_vsrai_w(yh_od2, 19);
+        u_ev2  = __lsx_vsrai_w(u_ev2, 19);
+        v_ev2  = __lsx_vsrai_w(v_ev2, 19);
+        u_od2  = __lsx_vsrai_w(u_od2, 19);
+        v_od2  = __lsx_vsrai_w(v_od2, 19);
+        u_ev1  = __lsx_vadd_w(u_ev1, headroom);
+        v_ev1  = __lsx_vadd_w(v_ev1, headroom);
+        u_od1  = __lsx_vadd_w(u_od1, headroom);
+        v_od1  = __lsx_vadd_w(v_od1, headroom);
+        u_ev2  = __lsx_vadd_w(u_ev2, headroom);
+        v_ev2  = __lsx_vadd_w(v_ev2, headroom);
+        u_od2  = __lsx_vadd_w(u_od2, headroom);
+        v_od2  = __lsx_vadd_w(v_od2, headroom);
+
+        WRITE_YUV2RGB_LSX(yl_ev1, yl_od1, u_ev1, v_ev1, 0, 0, 0, 0);
+        WRITE_YUV2RGB_LSX(yl_ev1, yl_od1, u_od1, v_od1, 1, 1, 0, 0);
+        WRITE_YUV2RGB_LSX(yl_ev1, yl_od1, u_ev1, v_ev1, 2, 2, 1, 1);
+        WRITE_YUV2RGB_LSX(yl_ev1, yl_od1, u_od1, v_od1, 3, 3, 1, 1);
+        WRITE_YUV2RGB_LSX(yl_ev2, yl_od2, u_ev1, v_ev1, 0, 0, 2, 2);
+        WRITE_YUV2RGB_LSX(yl_ev2, yl_od2, u_od1, v_od1, 1, 1, 2, 2);
+        WRITE_YUV2RGB_LSX(yl_ev2, yl_od2, u_ev1, v_ev1, 2, 2, 3, 3);
+        WRITE_YUV2RGB_LSX(yl_ev2, yl_od2, u_od1, v_od1, 3, 3, 3, 3);
+        WRITE_YUV2RGB_LSX(yh_ev1, yh_od1, u_ev2, v_ev2, 0, 0, 0, 0);
+        WRITE_YUV2RGB_LSX(yh_ev1, yh_od1, u_od2, v_od2, 1, 1, 0, 0);
+        WRITE_YUV2RGB_LSX(yh_ev1, yh_od1, u_ev2, v_ev2, 2, 2, 1, 1);
+        WRITE_YUV2RGB_LSX(yh_ev1, yh_od1, u_od2, v_od2, 3, 3, 1, 1);
+        WRITE_YUV2RGB_LSX(yh_ev2, yh_od2, u_ev2, v_ev2, 0, 0, 2, 2);
+        WRITE_YUV2RGB_LSX(yh_ev2, yh_od2, u_od2, v_od2, 1, 1, 2, 2);
+        WRITE_YUV2RGB_LSX(yh_ev2, yh_od2, u_ev2, v_ev2, 2, 2, 3, 3);
+        WRITE_YUV2RGB_LSX(yh_ev2, yh_od2, u_od2, v_od2, 3, 3, 3, 3);
+    }
+
+    if (res >= 16) {
+        int Y1, Y2, U, V, count_lum = count << 1;
+        __m128i l_src1, l_src2, u_src1, v_src1;
+        __m128i yl_ev, yl_ev1, yl_ev2, yl_od1, yl_od2;
+        __m128i u_ev1, u_od1, v_ev1, v_od1, temp;
+
+        yl_ev  = __lsx_vldrepl_w(&t, 0);
+        yl_ev1 = yl_ev;
+        yl_od1 = yl_ev;
+        u_ev1  = yl_ev;
+        v_ev1  = yl_ev;
+        u_od1  = yl_ev;
+        v_od1  = yl_ev;
+        yl_ev2 = yl_ev;
+        yl_od2 = yl_ev;
+
+        for (j = 0; j < lumFilterSize; j++) {
+            temp   = __lsx_vldrepl_h((lumFilter + j), 0);
+            DUP2_ARG2(__lsx_vld, lumSrc[j] + count_lum, 0, lumSrc[j] + count_lum,
+                      16, l_src1, l_src2);
+            yl_ev1  = __lsx_vmaddwev_w_h(yl_ev1, temp, l_src1);
+            yl_od1  = __lsx_vmaddwod_w_h(yl_od1, temp, l_src1);
+            yl_ev2  = __lsx_vmaddwev_w_h(yl_ev2, temp, l_src2);
+            yl_od2  = __lsx_vmaddwod_w_h(yl_od2, temp, l_src2);
+        }
+        for (j = 0; j < chrFilterSize; j++) {
+            DUP2_ARG2(__lsx_vld, chrUSrc[j] + count, 0, chrVSrc[j] + count, 0,
+                      u_src1, v_src1);
+            temp  = __lsx_vldrepl_h((chrFilter + j), 0);
+            u_ev1 = __lsx_vmaddwev_w_h(u_ev1, temp, u_src1);
+            u_od1 = __lsx_vmaddwod_w_h(u_od1, temp, u_src1);
+            v_ev1 = __lsx_vmaddwev_w_h(v_ev1, temp, v_src1);
+            v_od1 = __lsx_vmaddwod_w_h(v_od1, temp, v_src1);
+        }
+        yl_ev1 = __lsx_vsrai_w(yl_ev1, 19);
+        yl_od1 = __lsx_vsrai_w(yl_od1, 19);
+        u_ev1  = __lsx_vsrai_w(u_ev1, 19);
+        v_ev1  = __lsx_vsrai_w(v_ev1, 19);
+        u_od1  = __lsx_vsrai_w(u_od1, 19);
+        v_od1  = __lsx_vsrai_w(v_od1, 19);
+        yl_ev2 = __lsx_vsrai_w(yl_ev2, 19);
+        yl_od2 = __lsx_vsrai_w(yl_od2, 19);
+        u_ev1  = __lsx_vadd_w(u_ev1, headroom);
+        v_ev1  = __lsx_vadd_w(v_ev1, headroom);
+        u_od1  = __lsx_vadd_w(u_od1, headroom);
+        v_od1  = __lsx_vadd_w(v_od1, headroom);
+
+        WRITE_YUV2RGB_LSX(yl_ev1, yl_od1, u_ev1, v_ev1, 0, 0, 0, 0);
+        WRITE_YUV2RGB_LSX(yl_ev1, yl_od1, u_od1, v_od1, 1, 1, 0, 0);
+        WRITE_YUV2RGB_LSX(yl_ev1, yl_od1, u_ev1, v_ev1, 2, 2, 1, 1);
+        WRITE_YUV2RGB_LSX(yl_ev1, yl_od1, u_od1, v_od1, 3, 3, 1, 1);
+        WRITE_YUV2RGB_LSX(yl_ev2, yl_od2, u_ev1, v_ev1, 0, 0, 2, 2);
+        WRITE_YUV2RGB_LSX(yl_ev2, yl_od2, u_od1, v_od1, 1, 1, 2, 2);
+        WRITE_YUV2RGB_LSX(yl_ev2, yl_od2, u_ev1, v_ev1, 2, 2, 3, 3);
+        WRITE_YUV2RGB_LSX(yl_ev2, yl_od2, u_od1, v_od1, 3, 3, 3, 3);
+        res -= 16;
+    }
+
+    if (res >= 8) {
+        int Y1, Y2, U, V, count_lum = count << 1;
+        __m128i l_src1, u_src, v_src;
+        __m128i yl_ev, yl_od;
+        __m128i u_ev, u_od, v_ev, v_od, temp;
+
+        yl_ev = __lsx_vldrepl_w(&t, 0);
+        yl_od = yl_ev;
+        u_ev  = yl_ev;
+        v_ev  = yl_ev;
+        u_od  = yl_ev;
+        v_od  = yl_ev;
+        for (j = 0; j < lumFilterSize; j++) {
+            temp   = __lsx_vldrepl_h((lumFilter + j), 0);
+            l_src1 = __lsx_vld(lumSrc[j] + count_lum, 0);
+            yl_ev  = __lsx_vmaddwev_w_h(yl_ev, temp, l_src1);
+            yl_od  = __lsx_vmaddwod_w_h(yl_od, temp, l_src1);
+        }
+        for (j = 0; j < chrFilterSize; j++) {
+            DUP2_ARG2(__lsx_vld, chrUSrc[j] + count, 0, chrVSrc[j] + count, 0,
+                      u_src, v_src);
+            temp  = __lsx_vldrepl_h((chrFilter + j), 0);
+            u_ev  = __lsx_vmaddwev_w_h(u_ev, temp, u_src);
+            u_od  = __lsx_vmaddwod_w_h(u_od, temp, u_src);
+            v_ev  = __lsx_vmaddwev_w_h(v_ev, temp, v_src);
+            v_od  = __lsx_vmaddwod_w_h(v_od, temp, v_src);
+        }
+        yl_ev = __lsx_vsrai_w(yl_ev, 19);
+        yl_od = __lsx_vsrai_w(yl_od, 19);
+        u_ev  = __lsx_vsrai_w(u_ev, 19);
+        v_ev  = __lsx_vsrai_w(v_ev, 19);
+        u_od  = __lsx_vsrai_w(u_od, 19);
+        v_od  = __lsx_vsrai_w(v_od, 19);
+        u_ev  = __lsx_vadd_w(u_ev, headroom);
+        v_ev  = __lsx_vadd_w(v_ev, headroom);
+        u_od  = __lsx_vadd_w(u_od, headroom);
+        v_od  = __lsx_vadd_w(v_od, headroom);
+        WRITE_YUV2RGB_LSX(yl_ev, yl_od, u_ev, v_ev, 0, 0, 0, 0);
+        WRITE_YUV2RGB_LSX(yl_ev, yl_od, u_od, v_od, 1, 1, 0, 0);
+        WRITE_YUV2RGB_LSX(yl_ev, yl_od, u_ev, v_ev, 2, 2, 1, 1);
+        WRITE_YUV2RGB_LSX(yl_ev, yl_od, u_od, v_od, 3, 3, 1, 1);
+        res -= 8;
+    }
+
+    if (res >= 4) {
+        int Y1, Y2, U, V, count_lum = count << 1;
+        __m128i l_src1, u_src, v_src;
+        __m128i yl_ev, yl_od;
+        __m128i u_ev, u_od, v_ev, v_od, temp;
+
+        yl_ev = __lsx_vldrepl_w(&t, 0);
+        yl_od = yl_ev;
+        u_ev  = yl_ev;
+        v_ev  = yl_ev;
+        u_od  = yl_ev;
+        v_od  = yl_ev;
+        for (j = 0; j < lumFilterSize; j++) {
+            temp   = __lsx_vldrepl_h((lumFilter + j), 0);
+            l_src1 = __lsx_vld(lumSrc[j] + count_lum, 0);
+            yl_ev  = __lsx_vmaddwev_w_h(yl_ev, temp, l_src1);
+            yl_od  = __lsx_vmaddwod_w_h(yl_od, temp, l_src1);
+        }
+        for (j = 0; j < chrFilterSize; j++) {
+            DUP2_ARG2(__lsx_vld, chrUSrc[j] + count, 0, chrVSrc[j] + count, 0,
+                      u_src, v_src);
+            temp  = __lsx_vldrepl_h((chrFilter + j), 0);
+            u_ev  = __lsx_vmaddwev_w_h(u_ev, temp, u_src);
+            u_od  = __lsx_vmaddwod_w_h(u_od, temp, u_src);
+            v_ev  = __lsx_vmaddwev_w_h(v_ev, temp, v_src);
+            v_od  = __lsx_vmaddwod_w_h(v_od, temp, v_src);
+        }
+        yl_ev = __lsx_vsrai_w(yl_ev, 19);
+        yl_od = __lsx_vsrai_w(yl_od, 19);
+        u_ev  = __lsx_vsrai_w(u_ev, 19);
+        v_ev  = __lsx_vsrai_w(v_ev, 19);
+        u_od  = __lsx_vsrai_w(u_od, 19);
+        v_od  = __lsx_vsrai_w(v_od, 19);
+        u_ev  = __lsx_vadd_w(u_ev, headroom);
+        v_ev  = __lsx_vadd_w(v_ev, headroom);
+        u_od  = __lsx_vadd_w(u_od, headroom);
+        v_od  = __lsx_vadd_w(v_od, headroom);
+        WRITE_YUV2RGB_LSX(yl_ev, yl_od, u_ev, v_ev, 0, 0, 0, 0);
+        WRITE_YUV2RGB_LSX(yl_ev, yl_od, u_od, v_od, 1, 1, 0, 0);
+        res -= 4;
+    }
+
+    if (res >= 2) {
+        int Y1, Y2, U, V, count_lum = count << 1;
+        __m128i l_src1, u_src, v_src;
+        __m128i yl_ev, yl_od;
+        __m128i u_ev, u_od, v_ev, v_od, temp;
+
+        yl_ev = __lsx_vldrepl_w(&t, 0);
+        yl_od = yl_ev;
+        u_ev  = yl_ev;
+        v_ev  = yl_ev;
+        u_od  = yl_ev;
+        v_od  = yl_ev;
+        for (j = 0; j < lumFilterSize; j++) {
+            temp   = __lsx_vldrepl_h((lumFilter + j), 0);
+            l_src1 = __lsx_vld(lumSrc[j] + count_lum, 0);
+            yl_ev  = __lsx_vmaddwev_w_h(yl_ev, temp, l_src1);
+            yl_od  = __lsx_vmaddwod_w_h(yl_od, temp, l_src1);
+        }
+        for (j = 0; j < chrFilterSize; j++) {
+            DUP2_ARG2(__lsx_vld, chrUSrc[j] + count, 0, chrVSrc[j] + count, 0,
+                      u_src, v_src);
+            temp  = __lsx_vldrepl_h((chrFilter + j), 0);
+            u_ev  = __lsx_vmaddwev_w_h(u_ev, temp, u_src);
+            u_od  = __lsx_vmaddwod_w_h(u_od, temp, u_src);
+            v_ev  = __lsx_vmaddwev_w_h(v_ev, temp, v_src);
+            v_od  = __lsx_vmaddwod_w_h(v_od, temp, v_src);
+        }
+        yl_ev = __lsx_vsrai_w(yl_ev, 19);
+        yl_od = __lsx_vsrai_w(yl_od, 19);
+        u_ev  = __lsx_vsrai_w(u_ev, 19);
+        v_ev  = __lsx_vsrai_w(v_ev, 19);
+        u_od  = __lsx_vsrai_w(u_od, 19);
+        v_od  = __lsx_vsrai_w(v_od, 19);
+        u_ev  = __lsx_vadd_w(u_ev, headroom);
+        v_ev  = __lsx_vadd_w(v_ev, headroom);
+        u_od  = __lsx_vadd_w(u_od, headroom);
+        v_od  = __lsx_vadd_w(v_od, headroom);
+        WRITE_YUV2RGB_LSX(yl_ev, yl_od, u_ev, v_ev, 0, 0, 0, 0);
+        res -= 2;
+    }
+
+    for (; count < len_count; count++) {
+        int Y1 = 1 << 18;
+        int Y2 = Y1;
+        int U  = Y1;
+        int V  = Y1;
+
+        for (j = 0; j < lumFilterSize; j++) {
+            Y1 += lumSrc[j][count * 2]     * lumFilter[j];
+            Y2 += lumSrc[j][count * 2 + 1] * lumFilter[j];
+        }
+        for (j = 0; j < chrFilterSize; j++) {
+            U += chrUSrc[j][count] * chrFilter[j];
+            V += chrVSrc[j][count] * chrFilter[j];
+        }
+        Y1 >>= 19;
+        Y2 >>= 19;
+        U  >>= 19;
+        V  >>= 19;
+        r =  c->table_rV[V + YUVRGB_TABLE_HEADROOM];
+        g = (c->table_gU[U + YUVRGB_TABLE_HEADROOM] +
+             c->table_gV[V + YUVRGB_TABLE_HEADROOM]);
+        b =  c->table_bU[U + YUVRGB_TABLE_HEADROOM];
+
+        yuv2rgb_write(dest, count, Y1, Y2, 0, 0,
+                      r, g, b, y, target, 0);
+    }
+}
+
+static void
+yuv2rgb_2_template_lsx(SwsContext *c, const int16_t *buf[2],
+                       const int16_t *ubuf[2], const int16_t *vbuf[2],
+                       const int16_t *abuf[2], uint8_t *dest, int dstW,
+                       int yalpha, int uvalpha, int y,
+                       enum AVPixelFormat target, int hasAlpha)
+{
+    const int16_t *buf0  = buf[0],  *buf1  = buf[1],
+                  *ubuf0 = ubuf[0], *ubuf1 = ubuf[1],
+                  *vbuf0 = vbuf[0], *vbuf1 = vbuf[1];
+    int yalpha1   = 4096 - yalpha;
+    int uvalpha1  = 4096 - uvalpha;
+    int i, count  = 0;
+    int len       = dstW - 7;
+    int len_count = (dstW + 1) >> 1;
+    const void *r, *g, *b;
+    int head  = YUVRGB_TABLE_HEADROOM;
+    __m128i v_yalpha1  = __lsx_vreplgr2vr_w(yalpha1);
+    __m128i v_uvalpha1 = __lsx_vreplgr2vr_w(uvalpha1);
+    __m128i v_yalpha   = __lsx_vreplgr2vr_w(yalpha);
+    __m128i v_uvalpha  = __lsx_vreplgr2vr_w(uvalpha);
+    __m128i headroom   = __lsx_vreplgr2vr_w(head);
+    __m128i zero       = __lsx_vldi(0);
+
+    for (i = 0; i < len; i += 8) {
+        int Y1, Y2, U, V;
+        int i_dex = i << 1;
+        int c_dex = count << 1;
+        __m128i y0_h, y0_l, y0, u0, v0;
+        __m128i y1_h, y1_l, y1, u1, v1;
+        __m128i y_l, y_h, u, v;
+
+        DUP4_ARG2(__lsx_vldx, buf0, i_dex, ubuf0, c_dex, vbuf0, c_dex,
+                  buf1, i_dex, y0, u0, v0, y1);
+        DUP2_ARG2(__lsx_vldx, ubuf1, c_dex, vbuf1, c_dex, u1, v1);
+        DUP2_ARG2(__lsx_vsllwil_w_h, y0, 0, y1, 0, y0_l, y1_l);
+        DUP2_ARG1(__lsx_vexth_w_h, y0, y1, y0_h, y1_h);
+        DUP4_ARG2(__lsx_vilvl_h, zero, u0, zero, u1, zero, v0, zero, v1,
+                  u0, u1, v0, v1);
+        y0_l = __lsx_vmul_w(y0_l, v_yalpha1);
+        y0_h = __lsx_vmul_w(y0_h, v_yalpha1);
+        u0   = __lsx_vmul_w(u0, v_uvalpha1);
+        v0   = __lsx_vmul_w(v0, v_uvalpha1);
+        y_l  = __lsx_vmadd_w(y0_l, v_yalpha, y1_l);
+        y_h  = __lsx_vmadd_w(y0_h, v_yalpha, y1_h);
+        u    = __lsx_vmadd_w(u0, v_uvalpha, u1);
+        v    = __lsx_vmadd_w(v0, v_uvalpha, v1);
+        y_l  = __lsx_vsrai_w(y_l, 19);
+        y_h  = __lsx_vsrai_w(y_h, 19);
+        u    = __lsx_vsrai_w(u, 19);
+        v    = __lsx_vsrai_w(v, 19);
+        u    = __lsx_vadd_w(u, headroom);
+        v    = __lsx_vadd_w(v, headroom);
+        WRITE_YUV2RGB_LSX(y_l, y_l, u, v, 0, 1, 0, 0);
+        WRITE_YUV2RGB_LSX(y_l, y_l, u, v, 2, 3, 1, 1);
+        WRITE_YUV2RGB_LSX(y_h, y_h, u, v, 0, 1, 2, 2);
+        WRITE_YUV2RGB_LSX(y_h, y_h, u, v, 2, 3, 3, 3);
+    }
+    if (dstW - i >= 4) {
+        int Y1, Y2, U, V;
+        int i_dex = i << 1;
+        __m128i y0_l, y0, u0, v0;
+        __m128i y1_l, y1, u1, v1;
+        __m128i y_l, u, v;
+
+        y0   = __lsx_vldx(buf0, i_dex);
+        u0   = __lsx_vldrepl_d((ubuf0 + count), 0);
+        v0   = __lsx_vldrepl_d((vbuf0 + count), 0);
+        y1   = __lsx_vldx(buf1, i_dex);
+        u1   = __lsx_vldrepl_d((ubuf1 + count), 0);
+        v1   = __lsx_vldrepl_d((vbuf1 + count), 0);
+        DUP2_ARG2(__lsx_vilvl_h, zero, y0, zero, y1, y0_l, y1_l);
+        DUP4_ARG2(__lsx_vilvl_h, zero, u0, zero, u1, zero, v0, zero, v1,
+                  u0, u1, v0, v1);
+        y0_l = __lsx_vmul_w(y0_l, v_yalpha1);
+        u0   = __lsx_vmul_w(u0, v_uvalpha1);
+        v0   = __lsx_vmul_w(v0, v_uvalpha1);
+        y_l  = __lsx_vmadd_w(y0_l, v_yalpha, y1_l);
+        u    = __lsx_vmadd_w(u0, v_uvalpha, u1);
+        v    = __lsx_vmadd_w(v0, v_uvalpha, v1);
+        y_l  = __lsx_vsrai_w(y_l, 19);
+        u    = __lsx_vsrai_w(u, 19);
+        v    = __lsx_vsrai_w(v, 19);
+        u    = __lsx_vadd_w(u, headroom);
+        v    = __lsx_vadd_w(v, headroom);
+        WRITE_YUV2RGB_LSX(y_l, y_l, u, v, 0, 1, 0, 0);
+        WRITE_YUV2RGB_LSX(y_l, y_l, u, v, 2, 3, 1, 1);
+        i += 4;
+    }
+    for (; count < len_count; count++) {
+        int Y1 = (buf0[count * 2]     * yalpha1  +
+                  buf1[count * 2]     * yalpha)  >> 19;
+        int Y2 = (buf0[count * 2 + 1] * yalpha1  +
+                  buf1[count * 2 + 1] * yalpha) >> 19;
+        int U  = (ubuf0[count] * uvalpha1 + ubuf1[count] * uvalpha) >> 19;
+        int V  = (vbuf0[count] * uvalpha1 + vbuf1[count] * uvalpha) >> 19;
+
+        r =  c->table_rV[V + YUVRGB_TABLE_HEADROOM],
+        g = (c->table_gU[U + YUVRGB_TABLE_HEADROOM] +
+             c->table_gV[V + YUVRGB_TABLE_HEADROOM]),
+        b =  c->table_bU[U + YUVRGB_TABLE_HEADROOM];
+
+        yuv2rgb_write(dest, count, Y1, Y2, 0, 0,
+                      r, g, b, y, target, 0);
+    }
+}
+
+static void
+yuv2rgb_1_template_lsx(SwsContext *c, const int16_t *buf0,
+                       const int16_t *ubuf[2], const int16_t *vbuf[2],
+                       const int16_t *abuf0, uint8_t *dest, int dstW,
+                       int uvalpha, int y, enum AVPixelFormat target,
+                       int hasAlpha)
+{
+    const int16_t *ubuf0 = ubuf[0], *vbuf0 = vbuf[0];
+    int i;
+    int len       = (dstW - 7);
+    int len_count = (dstW + 1) >> 1;
+    const void *r, *g, *b;
+
+    if (uvalpha < 2048) {
+        int count    = 0;
+        int head = YUVRGB_TABLE_HEADROOM;
+        __m128i headroom  = __lsx_vreplgr2vr_h(head);
+
+        for (i = 0; i < len; i += 8) {
+            int Y1, Y2, U, V;
+            int i_dex = i << 1;
+            int c_dex = count << 1;
+            __m128i src_y, src_u, src_v;
+            __m128i u, v, uv, y_l, y_h;
+
+            src_y = __lsx_vldx(buf0, i_dex);
+            DUP2_ARG2(__lsx_vldx, ubuf0, c_dex, vbuf0, c_dex, src_u, src_v);
+            src_y = __lsx_vsrari_h(src_y, 7);
+            src_u = __lsx_vsrari_h(src_u, 7);
+            src_v = __lsx_vsrari_h(src_v, 7);
+            y_l   = __lsx_vsllwil_w_h(src_y, 0);
+            y_h   = __lsx_vexth_w_h(src_y);
+            uv    = __lsx_vilvl_h(src_v, src_u);
+            u     = __lsx_vaddwev_w_h(uv, headroom);
+            v     = __lsx_vaddwod_w_h(uv, headroom);
+            WRITE_YUV2RGB_LSX(y_l, y_l, u, v, 0, 1, 0, 0);
+            WRITE_YUV2RGB_LSX(y_l, y_l, u, v, 2, 3, 1, 1);
+            WRITE_YUV2RGB_LSX(y_h, y_h, u, v, 0, 1, 2, 2);
+            WRITE_YUV2RGB_LSX(y_h, y_h, u, v, 2, 3, 3, 3);
+        }
+        if (dstW - i >= 4){
+            int Y1, Y2, U, V;
+            int i_dex = i << 1;
+            __m128i src_y, src_u, src_v;
+            __m128i y_l, u, v, uv;
+
+            src_y  = __lsx_vldx(buf0, i_dex);
+            src_u  = __lsx_vldrepl_d((ubuf0 + count), 0);
+            src_v  = __lsx_vldrepl_d((vbuf0 + count), 0);
+            y_l    = __lsx_vsrari_h(src_y, 7);
+            y_l    = __lsx_vsllwil_w_h(y_l, 0);
+            uv     = __lsx_vilvl_h(src_v, src_u);
+            uv     = __lsx_vsrari_h(uv, 7);
+            u      = __lsx_vaddwev_w_h(uv, headroom);
+            v      = __lsx_vaddwod_w_h(uv, headroom);
+            WRITE_YUV2RGB_LSX(y_l, y_l, u, v, 0, 1, 0, 0);
+            WRITE_YUV2RGB_LSX(y_l, y_l, u, v, 2, 3, 1, 1);
+            i += 4;
+        }
+        for (; count < len_count; count++) {
+            int Y1 = (buf0[count * 2    ] + 64) >> 7;
+            int Y2 = (buf0[count * 2 + 1] + 64) >> 7;
+            int U  = (ubuf0[count]        + 64) >> 7;
+            int V  = (vbuf0[count]        + 64) >> 7;
+
+            r =  c->table_rV[V + YUVRGB_TABLE_HEADROOM],
+            g = (c->table_gU[U + YUVRGB_TABLE_HEADROOM] +
+                 c->table_gV[V + YUVRGB_TABLE_HEADROOM]),
+            b =  c->table_bU[U + YUVRGB_TABLE_HEADROOM];
+
+            yuv2rgb_write(dest, count, Y1, Y2, 0, 0,
+                          r, g, b, y, target, 0);
+        }
+    } else {
+        const int16_t *ubuf1 = ubuf[1], *vbuf1 = vbuf[1];
+        int count = 0;
+        int HEADROOM = YUVRGB_TABLE_HEADROOM;
+        __m128i headroom    = __lsx_vreplgr2vr_w(HEADROOM);
+
+        for (i = 0; i < len; i += 8) {
+            int Y1, Y2, U, V;
+            int i_dex = i << 1;
+            int c_dex = count << 1;
+            __m128i src_y, src_u0, src_v0, src_u1, src_v1;
+            __m128i y_l, y_h, u1, u2, v1, v2;
+
+            DUP4_ARG2(__lsx_vldx, buf0, i_dex, ubuf0, c_dex, vbuf0, c_dex,
+                      ubuf1, c_dex, src_y, src_u0, src_v0, src_u1);
+            src_v1 = __lsx_vldx(vbuf1, c_dex);
+            src_y  = __lsx_vsrari_h(src_y, 7);
+            u1      = __lsx_vaddwev_w_h(src_u0, src_u1);
+            v1      = __lsx_vaddwod_w_h(src_u0, src_u1);
+            u2      = __lsx_vaddwev_w_h(src_v0, src_v1);
+            v2      = __lsx_vaddwod_w_h(src_v0, src_v1);
+            y_l     = __lsx_vsllwil_w_h(src_y, 0);
+            y_h     = __lsx_vexth_w_h(src_y);
+            u1      = __lsx_vsrari_w(u1, 8);
+            v1      = __lsx_vsrari_w(v1, 8);
+            u2      = __lsx_vsrari_w(u2, 8);
+            v2      = __lsx_vsrari_w(v2, 8);
+            u1      = __lsx_vadd_w(u1, headroom);
+            v1      = __lsx_vadd_w(v1, headroom);
+            u2      = __lsx_vadd_w(u2, headroom);
+            v2      = __lsx_vadd_w(v2, headroom);
+            WRITE_YUV2RGB_LSX(y_l, y_l, u1, v1, 0, 1, 0, 0);
+            WRITE_YUV2RGB_LSX(y_l, y_l, u2, v2, 2, 3, 0, 0);
+            WRITE_YUV2RGB_LSX(y_h, y_h, u1, v1, 0, 1, 1, 1);
+            WRITE_YUV2RGB_LSX(y_h, y_h, u2, v2, 2, 3, 1, 1);
+        }
+        if (dstW - i >= 4) {
+            int Y1, Y2, U, V;
+            int i_dex = i << 1;
+            __m128i src_y, src_u0, src_v0, src_u1, src_v1;
+            __m128i uv;
+
+            src_y  = __lsx_vldx(buf0, i_dex);
+            src_u0 = __lsx_vldrepl_d((ubuf0 + count), 0);
+            src_v0 = __lsx_vldrepl_d((vbuf0 + count), 0);
+            src_u1 = __lsx_vldrepl_d((ubuf1 + count), 0);
+            src_v1 = __lsx_vldrepl_d((vbuf1 + count), 0);
+
+            src_u0 = __lsx_vilvl_h(src_u1, src_u0);
+            src_v0 = __lsx_vilvl_h(src_v1, src_v0);
+            src_y  = __lsx_vsrari_h(src_y, 7);
+            src_y  = __lsx_vsllwil_w_h(src_y, 0);
+            uv     = __lsx_vilvl_h(src_v0, src_u0);
+            uv     = __lsx_vhaddw_w_h(uv, uv);
+            uv     = __lsx_vsrari_w(uv, 8);
+            uv     = __lsx_vadd_w(uv, headroom);
+            WRITE_YUV2RGB_LSX(src_y, src_y, uv, uv, 0, 1, 0, 1);
+            WRITE_YUV2RGB_LSX(src_y, src_y, uv, uv, 2, 3, 2, 3);
+            i += 4;
+        }
+        for (; count < len_count; count++) {
+            int Y1 = (buf0[count * 2    ]         +  64) >> 7;
+            int Y2 = (buf0[count * 2 + 1]         +  64) >> 7;
+            int U  = (ubuf0[count] + ubuf1[count] + 128) >> 8;
+            int V  = (vbuf0[count] + vbuf1[count] + 128) >> 8;
+
+            r =  c->table_rV[V + YUVRGB_TABLE_HEADROOM],
+            g = (c->table_gU[U + YUVRGB_TABLE_HEADROOM] +
+                 c->table_gV[V + YUVRGB_TABLE_HEADROOM]),
+            b =  c->table_bU[U + YUVRGB_TABLE_HEADROOM];
+
+            yuv2rgb_write(dest, count, Y1, Y2, 0, 0,
+                          r, g, b, y, target, 0);
+        }
+    }
+}
+
+#define YUV2RGBWRAPPERX(name, base, ext, fmt, hasAlpha)                               \
+static void name ## ext ## _X_lsx(SwsContext *c, const int16_t *lumFilter,            \
+                                  const int16_t **lumSrc, int lumFilterSize,          \
+                                  const int16_t *chrFilter, const int16_t **chrUSrc,  \
+                                  const int16_t **chrVSrc, int chrFilterSize,         \
+                                  const int16_t **alpSrc, uint8_t *dest, int dstW,    \
+                                  int y)                                              \
+{                                                                                     \
+    name ## base ## _X_template_lsx(c, lumFilter, lumSrc, lumFilterSize,              \
+                                    chrFilter, chrUSrc, chrVSrc, chrFilterSize,       \
+                                    alpSrc, dest, dstW, y, fmt, hasAlpha);            \
+}
+
+#define YUV2RGBWRAPPERX2(name, base, ext, fmt, hasAlpha)                              \
+YUV2RGBWRAPPERX(name, base, ext, fmt, hasAlpha)                                       \
+static void name ## ext ## _2_lsx(SwsContext *c, const int16_t *buf[2],               \
+                                  const int16_t *ubuf[2], const int16_t *vbuf[2],     \
+                                  const int16_t *abuf[2], uint8_t *dest, int dstW,    \
+                                  int yalpha, int uvalpha, int y)                     \
+{                                                                                     \
+    name ## base ## _2_template_lsx(c, buf, ubuf, vbuf, abuf, dest,                   \
+                                    dstW, yalpha, uvalpha, y, fmt, hasAlpha);         \
+}
+
+#define YUV2RGBWRAPPER(name, base, ext, fmt, hasAlpha)                                \
+YUV2RGBWRAPPERX2(name, base, ext, fmt, hasAlpha)                                      \
+static void name ## ext ## _1_lsx(SwsContext *c, const int16_t *buf0,                 \
+                                  const int16_t *ubuf[2], const int16_t *vbuf[2],     \
+                                  const int16_t *abuf0, uint8_t *dest, int dstW,      \
+                                  int uvalpha, int y)                                 \
+{                                                                                     \
+    name ## base ## _1_template_lsx(c, buf0, ubuf, vbuf, abuf0, dest,                 \
+                                    dstW, uvalpha, y, fmt, hasAlpha);                 \
+}
+
+#if CONFIG_SMALL
+#else
+#if CONFIG_SWSCALE_ALPHA
+#endif
+YUV2RGBWRAPPER(yuv2rgb,, x32_1,  AV_PIX_FMT_RGB32_1, 0)
+YUV2RGBWRAPPER(yuv2rgb,, x32,    AV_PIX_FMT_RGB32,   0)
+#endif
+YUV2RGBWRAPPER(yuv2, rgb, rgb24, AV_PIX_FMT_RGB24,     0)
+YUV2RGBWRAPPER(yuv2, rgb, bgr24, AV_PIX_FMT_BGR24,     0)
+YUV2RGBWRAPPER(yuv2rgb,,  16,    AV_PIX_FMT_RGB565,    0)
+YUV2RGBWRAPPER(yuv2rgb,,  15,    AV_PIX_FMT_RGB555,    0)
+YUV2RGBWRAPPER(yuv2rgb,,  12,    AV_PIX_FMT_RGB444,    0)
+YUV2RGBWRAPPER(yuv2rgb,,  8,     AV_PIX_FMT_RGB8,      0)
+YUV2RGBWRAPPER(yuv2rgb,,  4,     AV_PIX_FMT_RGB4,      0)
+YUV2RGBWRAPPER(yuv2rgb,,  4b,    AV_PIX_FMT_RGB4_BYTE, 0)
+
+// This function is copied from libswscale/output.c
+static av_always_inline void yuv2rgb_write_full(SwsContext *c,
+    uint8_t *dest, int i, int R, int A, int G, int B,
+    int y, enum AVPixelFormat target, int hasAlpha, int err[4])
+{
+    int isrgb8 = target == AV_PIX_FMT_BGR8 || target == AV_PIX_FMT_RGB8;
+
+    if ((R | G | B) & 0xC0000000) {
+        R = av_clip_uintp2(R, 30);
+        G = av_clip_uintp2(G, 30);
+        B = av_clip_uintp2(B, 30);
+    }
+
+    switch(target) {
+    case AV_PIX_FMT_ARGB:
+        dest[0] = hasAlpha ? A : 255;
+        dest[1] = R >> 22;
+        dest[2] = G >> 22;
+        dest[3] = B >> 22;
+        break;
+    case AV_PIX_FMT_RGB24:
+        dest[0] = R >> 22;
+        dest[1] = G >> 22;
+        dest[2] = B >> 22;
+        break;
+    case AV_PIX_FMT_RGBA:
+        dest[0] = R >> 22;
+        dest[1] = G >> 22;
+        dest[2] = B >> 22;
+        dest[3] = hasAlpha ? A : 255;
+        break;
+    case AV_PIX_FMT_ABGR:
+        dest[0] = hasAlpha ? A : 255;
+        dest[1] = B >> 22;
+        dest[2] = G >> 22;
+        dest[3] = R >> 22;
+        break;
+    case AV_PIX_FMT_BGR24:
+        dest[0] = B >> 22;
+        dest[1] = G >> 22;
+        dest[2] = R >> 22;
+        break;
+    case AV_PIX_FMT_BGRA:
+        dest[0] = B >> 22;
+        dest[1] = G >> 22;
+        dest[2] = R >> 22;
+        dest[3] = hasAlpha ? A : 255;
+        break;
+    case AV_PIX_FMT_BGR4_BYTE:
+    case AV_PIX_FMT_RGB4_BYTE:
+    case AV_PIX_FMT_BGR8:
+    case AV_PIX_FMT_RGB8:
+    {
+        int r,g,b;
+
+        switch (c->dither) {
+        default:
+        case SWS_DITHER_AUTO:
+        case SWS_DITHER_ED:
+            R >>= 22;
+            G >>= 22;
+            B >>= 22;
+            R += (7*err[0] + 1*c->dither_error[0][i] + 5*c->dither_error[0][i+1] + 3*c->dither_error[0][i+2])>>4;
+            G += (7*err[1] + 1*c->dither_error[1][i] + 5*c->dither_error[1][i+1] + 3*c->dither_error[1][i+2])>>4;
+            B += (7*err[2] + 1*c->dither_error[2][i] + 5*c->dither_error[2][i+1] + 3*c->dither_error[2][i+2])>>4;
+            c->dither_error[0][i] = err[0];
+            c->dither_error[1][i] = err[1];
+            c->dither_error[2][i] = err[2];
+            r = R >> (isrgb8 ? 5 : 7);
+            g = G >> (isrgb8 ? 5 : 6);
+            b = B >> (isrgb8 ? 6 : 7);
+            r = av_clip(r, 0, isrgb8 ? 7 : 1);
+            g = av_clip(g, 0, isrgb8 ? 7 : 3);
+            b = av_clip(b, 0, isrgb8 ? 3 : 1);
+            err[0] = R - r*(isrgb8 ? 36 : 255);
+            err[1] = G - g*(isrgb8 ? 36 : 85);
+            err[2] = B - b*(isrgb8 ? 85 : 255);
+            break;
+        case SWS_DITHER_A_DITHER:
+            if (isrgb8) {
+  /* see http://pippin.gimp.org/a_dither/ for details/origin */
+#define A_DITHER(u,v)   (((((u)+((v)*236))*119)&0xff))
+                r = (((R >> 19) + A_DITHER(i,y)  -96)>>8);
+                g = (((G >> 19) + A_DITHER(i + 17,y) - 96)>>8);
+                b = (((B >> 20) + A_DITHER(i + 17*2,y) -96)>>8);
+                r = av_clip_uintp2(r, 3);
+                g = av_clip_uintp2(g, 3);
+                b = av_clip_uintp2(b, 2);
+            } else {
+                r = (((R >> 21) + A_DITHER(i,y)-256)>>8);
+                g = (((G >> 19) + A_DITHER(i + 17,y)-256)>>8);
+                b = (((B >> 21) + A_DITHER(i + 17*2,y)-256)>>8);
+                r = av_clip_uintp2(r, 1);
+                g = av_clip_uintp2(g, 2);
+                b = av_clip_uintp2(b, 1);
+            }
+            break;
+        case SWS_DITHER_X_DITHER:
+            if (isrgb8) {
+  /* see http://pippin.gimp.org/a_dither/ for details/origin */
+#define X_DITHER(u,v)   (((((u)^((v)*237))*181)&0x1ff)/2)
+                r = (((R >> 19) + X_DITHER(i,y) - 96)>>8);
+                g = (((G >> 19) + X_DITHER(i + 17,y) - 96)>>8);
+                b = (((B >> 20) + X_DITHER(i + 17*2,y) - 96)>>8);
+                r = av_clip_uintp2(r, 3);
+                g = av_clip_uintp2(g, 3);
+                b = av_clip_uintp2(b, 2);
+            } else {
+                r = (((R >> 21) + X_DITHER(i,y)-256)>>8);
+                g = (((G >> 19) + X_DITHER(i + 17,y)-256)>>8);
+                b = (((B >> 21) + X_DITHER(i + 17*2,y)-256)>>8);
+                r = av_clip_uintp2(r, 1);
+                g = av_clip_uintp2(g, 2);
+                b = av_clip_uintp2(b, 1);
+            }
+
+            break;
+        }
+
+        if(target == AV_PIX_FMT_BGR4_BYTE) {
+            dest[0] = r + 2*g + 8*b;
+        } else if(target == AV_PIX_FMT_RGB4_BYTE) {
+            dest[0] = b + 2*g + 8*r;
+        } else if(target == AV_PIX_FMT_BGR8) {
+            dest[0] = r + 8*g + 64*b;
+        } else if(target == AV_PIX_FMT_RGB8) {
+            dest[0] = b + 4*g + 32*r;
+        } else
+            av_assert2(0);
+        break; }
+    }
+}
+
+#define YUVTORGB_SETUP_LSX                                   \
+    int y_offset   = c->yuv2rgb_y_offset;                    \
+    int y_coeff    = c->yuv2rgb_y_coeff;                     \
+    int v2r_coe    = c->yuv2rgb_v2r_coeff;                   \
+    int v2g_coe    = c->yuv2rgb_v2g_coeff;                   \
+    int u2g_coe    = c->yuv2rgb_u2g_coeff;                   \
+    int u2b_coe    = c->yuv2rgb_u2b_coeff;                   \
+    __m128i offset = __lsx_vreplgr2vr_w(y_offset);           \
+    __m128i coeff  = __lsx_vreplgr2vr_w(y_coeff);            \
+    __m128i v2r    = __lsx_vreplgr2vr_w(v2r_coe);            \
+    __m128i v2g    = __lsx_vreplgr2vr_w(v2g_coe);            \
+    __m128i u2g    = __lsx_vreplgr2vr_w(u2g_coe);            \
+    __m128i u2b    = __lsx_vreplgr2vr_w(u2b_coe);            \
+
+#define YUVTORGB_LSX(y, u, v, R, G, B, offset, coeff,        \
+                     y_temp, v2r, v2g, u2g, u2b)             \
+{                                                            \
+     y = __lsx_vsub_w(y, offset);                            \
+     y = __lsx_vmul_w(y, coeff);                             \
+     y = __lsx_vadd_w(y, y_temp);                            \
+     R = __lsx_vmadd_w(y, v, v2r);                           \
+     v = __lsx_vmadd_w(y, v, v2g);                           \
+     G = __lsx_vmadd_w(v, u, u2g);                           \
+     B = __lsx_vmadd_w(y, u, u2b);                           \
+}
+
+#define WRITE_FULL_A_LSX(r, g, b, a, t1, s)                                  \
+{                                                                            \
+    R = __lsx_vpickve2gr_w(r, t1);                                           \
+    G = __lsx_vpickve2gr_w(g, t1);                                           \
+    B = __lsx_vpickve2gr_w(b, t1);                                           \
+    A = __lsx_vpickve2gr_w(a, t1);                                           \
+    if (A & 0x100)                                                           \
+        A = av_clip_uint8(A);                                                \
+    yuv2rgb_write_full(c, dest, i + s, R, A, G, B, y, target, hasAlpha, err);\
+    dest += step;                                                            \
+}
+
+#define WRITE_FULL_LSX(r, g, b, t1, s)                                        \
+{                                                                             \
+    R = __lsx_vpickve2gr_w(r, t1);                                            \
+    G = __lsx_vpickve2gr_w(g, t1);                                            \
+    B = __lsx_vpickve2gr_w(b, t1);                                            \
+    yuv2rgb_write_full(c, dest, i + s, R, 0, G, B, y, target, hasAlpha, err); \
+    dest += step;                                                             \
+}
+
+static void
+yuv2rgb_full_X_template_lsx(SwsContext *c, const int16_t *lumFilter,
+                            const int16_t **lumSrc, int lumFilterSize,
+                            const int16_t *chrFilter, const int16_t **chrUSrc,
+                            const int16_t **chrVSrc, int chrFilterSize,
+                            const int16_t **alpSrc, uint8_t *dest,
+                            int dstW, int y, enum AVPixelFormat target,
+                            int hasAlpha)
+{
+    int i, j, B, G, R, A;
+    int step       = (target == AV_PIX_FMT_RGB24 ||
+                      target == AV_PIX_FMT_BGR24) ? 3 : 4;
+    int err[4]     = {0};
+    int a_temp     = 1 << 18;
+    int templ      = 1 << 9;
+    int tempc      = templ - (128 << 19);
+    int ytemp      = 1 << 21;
+    int len        = dstW - 7;
+    __m128i y_temp = __lsx_vreplgr2vr_w(ytemp);
+    YUVTORGB_SETUP_LSX
+
+    if(   target == AV_PIX_FMT_BGR4_BYTE || target == AV_PIX_FMT_RGB4_BYTE
+       || target == AV_PIX_FMT_BGR8      || target == AV_PIX_FMT_RGB8)
+        step = 1;
+
+    for (i = 0; i < len; i += 8) {
+        __m128i l_src, u_src, v_src;
+        __m128i y_ev, y_od, u_ev, u_od, v_ev, v_od, temp;
+        __m128i R_ev, R_od, G_ev, G_od, B_ev, B_od;
+        int n = i << 1;
+
+        y_ev = y_od = __lsx_vreplgr2vr_w(templ);
+        u_ev = u_od = v_ev = v_od = __lsx_vreplgr2vr_w(tempc);
+        for (j = 0; j < lumFilterSize; j++) {
+            temp  = __lsx_vldrepl_h((lumFilter + j), 0);
+            l_src = __lsx_vldx(lumSrc[j], n);
+            y_ev  = __lsx_vmaddwev_w_h(y_ev, l_src, temp);
+            y_od  = __lsx_vmaddwod_w_h(y_od, l_src, temp);
+        }
+        for (j = 0; j < chrFilterSize; j++) {
+            temp  = __lsx_vldrepl_h((chrFilter + j), 0);
+            DUP2_ARG2(__lsx_vldx, chrUSrc[j], n, chrVSrc[j], n,
+                      u_src, v_src);
+            DUP2_ARG3(__lsx_vmaddwev_w_h, u_ev, u_src, temp, v_ev,
+                      v_src, temp, u_ev, v_ev);
+            DUP2_ARG3(__lsx_vmaddwod_w_h, u_od, u_src, temp, v_od,
+                      v_src, temp, u_od, v_od);
+        }
+        y_ev = __lsx_vsrai_w(y_ev, 10);
+        y_od = __lsx_vsrai_w(y_od, 10);
+        u_ev = __lsx_vsrai_w(u_ev, 10);
+        u_od = __lsx_vsrai_w(u_od, 10);
+        v_ev = __lsx_vsrai_w(v_ev, 10);
+        v_od = __lsx_vsrai_w(v_od, 10);
+        YUVTORGB_LSX(y_ev, u_ev, v_ev, R_ev, G_ev, B_ev, offset, coeff,
+                     y_temp, v2r, v2g, u2g, u2b);
+        YUVTORGB_LSX(y_od, u_od, v_od, R_od, G_od, B_od, offset, coeff,
+                     y_temp, v2r, v2g, u2g, u2b);
+
+        if (hasAlpha) {
+            __m128i a_src, a_ev, a_od;
+
+            a_ev = a_od = __lsx_vreplgr2vr_w(a_temp);
+            for (j = 0; j < lumFilterSize; j++) {
+                temp  = __lsx_vldrepl_h(lumFilter + j, 0);
+                a_src = __lsx_vldx(alpSrc[j], n);
+                a_ev  = __lsx_vmaddwev_w_h(a_ev, a_src, temp);
+                a_od  = __lsx_vmaddwod_w_h(a_od, a_src, temp);
+            }
+            a_ev = __lsx_vsrai_w(a_ev, 19);
+            a_od = __lsx_vsrai_w(a_od, 19);
+            WRITE_FULL_A_LSX(R_ev, G_ev, B_ev, a_ev, 0, 0);
+            WRITE_FULL_A_LSX(R_od, G_od, B_od, a_od, 0, 1);
+            WRITE_FULL_A_LSX(R_ev, G_ev, B_ev, a_ev, 1, 2);
+            WRITE_FULL_A_LSX(R_od, G_od, B_od, a_od, 1, 3);
+            WRITE_FULL_A_LSX(R_ev, G_ev, B_ev, a_ev, 2, 4);
+            WRITE_FULL_A_LSX(R_od, G_od, B_od, a_od, 2, 5);
+            WRITE_FULL_A_LSX(R_ev, G_ev, B_ev, a_ev, 3, 6);
+            WRITE_FULL_A_LSX(R_od, G_od, B_od, a_od, 3, 7);
+        } else {
+            WRITE_FULL_LSX(R_ev, G_ev, B_ev, 0, 0);
+            WRITE_FULL_LSX(R_od, G_od, B_od, 0, 1);
+            WRITE_FULL_LSX(R_ev, G_ev, B_ev, 1, 2);
+            WRITE_FULL_LSX(R_od, G_od, B_od, 1, 3);
+            WRITE_FULL_LSX(R_ev, G_ev, B_ev, 2, 4);
+            WRITE_FULL_LSX(R_od, G_od, B_od, 2, 5);
+            WRITE_FULL_LSX(R_ev, G_ev, B_ev, 3, 6);
+            WRITE_FULL_LSX(R_od, G_od, B_od, 3, 7);
+        }
+    }
+    if (dstW - i >= 4) {
+        __m128i l_src, u_src, v_src;
+        __m128i y_ev, u_ev, v_ev, uv, temp;
+        __m128i R_ev, G_ev, B_ev;
+        int n = i << 1;
+
+        y_ev = __lsx_vreplgr2vr_w(templ);
+        u_ev = v_ev = __lsx_vreplgr2vr_w(tempc);
+        for (j = 0; j < lumFilterSize; j++) {
+            temp  = __lsx_vldrepl_h((lumFilter + j), 0);
+            l_src = __lsx_vldx(lumSrc[j], n);
+            l_src = __lsx_vilvl_h(l_src, l_src);
+            y_ev  = __lsx_vmaddwev_w_h(y_ev, l_src, temp);
+        }
+        for (j = 0; j < chrFilterSize; j++) {
+            temp  = __lsx_vldrepl_h((chrFilter + j), 0);
+            DUP2_ARG2(__lsx_vldx, chrUSrc[j], n, chrVSrc[j], n, u_src, v_src);
+            uv    = __lsx_vilvl_h(v_src, u_src);
+            u_ev  = __lsx_vmaddwev_w_h(u_ev, uv, temp);
+            v_ev  = __lsx_vmaddwod_w_h(v_ev, uv, temp);
+        }
+        y_ev = __lsx_vsrai_w(y_ev, 10);
+        u_ev = __lsx_vsrai_w(u_ev, 10);
+        v_ev = __lsx_vsrai_w(v_ev, 10);
+        YUVTORGB_LSX(y_ev, u_ev, v_ev, R_ev, G_ev, B_ev, offset, coeff,
+                     y_temp, v2r, v2g, u2g, u2b);
+
+        if (hasAlpha) {
+            __m128i a_src, a_ev;
+
+            a_ev = __lsx_vreplgr2vr_w(a_temp);
+            for (j = 0; j < lumFilterSize; j++) {
+                temp  = __lsx_vldrepl_h(lumFilter + j, 0);
+                a_src = __lsx_vldx(alpSrc[j], n);
+                a_src = __lsx_vilvl_h(a_src, a_src);
+                a_ev  =  __lsx_vmaddwev_w_h(a_ev, a_src, temp);
+            }
+            a_ev = __lsx_vsrai_w(a_ev, 19);
+            WRITE_FULL_A_LSX(R_ev, G_ev, B_ev, a_ev, 0, 0);
+            WRITE_FULL_A_LSX(R_ev, G_ev, B_ev, a_ev, 1, 1);
+            WRITE_FULL_A_LSX(R_ev, G_ev, B_ev, a_ev, 2, 2);
+            WRITE_FULL_A_LSX(R_ev, G_ev, B_ev, a_ev, 3, 3);
+        } else {
+            WRITE_FULL_LSX(R_ev, G_ev, B_ev, 0, 0);
+            WRITE_FULL_LSX(R_ev, G_ev, B_ev, 1, 1);
+            WRITE_FULL_LSX(R_ev, G_ev, B_ev, 2, 2);
+            WRITE_FULL_LSX(R_ev, G_ev, B_ev, 3, 3);
+        }
+        i += 4;
+    }
+    for (; i < dstW; i++) {
+        int Y = templ;
+        int V, U = V = tempc;
+
+        A = 0;
+        for (j = 0; j < lumFilterSize; j++) {
+            Y += lumSrc[j][i] * lumFilter[j];
+        }
+        for (j = 0; j < chrFilterSize; j++) {
+            U += chrUSrc[j][i] * chrFilter[j];
+            V += chrVSrc[j][i] * chrFilter[j];
+
+        }
+        Y >>= 10;
+        U >>= 10;
+        V >>= 10;
+        if (hasAlpha) {
+            A = 1 << 18;
+            for (j = 0; j < lumFilterSize; j++) {
+                A += alpSrc[j][i] * lumFilter[j];
+            }
+            A >>= 19;
+            if (A & 0x100)
+                A = av_clip_uint8(A);
+        }
+        Y -= y_offset;
+        Y *= y_coeff;
+        Y += ytemp;
+        R  = (unsigned)Y + V * v2r_coe;
+        G  = (unsigned)Y + V * v2g_coe + U * u2g_coe;
+        B  = (unsigned)Y + U * u2b_coe;
+        yuv2rgb_write_full(c, dest, i, R, A, G, B, y, target, hasAlpha, err);
+        dest += step;
+    }
+    c->dither_error[0][i] = err[0];
+    c->dither_error[1][i] = err[1];
+    c->dither_error[2][i] = err[2];
+}
+
+static void
+yuv2rgb_full_2_template_lsx(SwsContext *c, const int16_t *buf[2],
+                            const int16_t *ubuf[2], const int16_t *vbuf[2],
+                            const int16_t *abuf[2], uint8_t *dest, int dstW,
+                            int yalpha, int uvalpha, int y,
+                            enum AVPixelFormat target, int hasAlpha)
+{
+    const int16_t *buf0  = buf[0],  *buf1  = buf[1],
+                  *ubuf0 = ubuf[0], *ubuf1 = ubuf[1],
+                  *vbuf0 = vbuf[0], *vbuf1 = vbuf[1],
+                  *abuf0 = hasAlpha ? abuf[0] : NULL,
+                  *abuf1 = hasAlpha ? abuf[1] : NULL;
+    int yalpha1  = 4096 - yalpha;
+    int uvalpha1 = 4096 - uvalpha;
+    int uvtemp   = 128 << 19;
+    int atemp    = 1 << 18;
+    int err[4]   = {0};
+    int ytemp    = 1 << 21;
+    int len      = dstW - 7;
+    int i, R, G, B, A;
+    int step = (target == AV_PIX_FMT_RGB24 ||
+                target == AV_PIX_FMT_BGR24) ? 3 : 4;
+    __m128i v_uvalpha1 = __lsx_vreplgr2vr_w(uvalpha1);
+    __m128i v_yalpha1  = __lsx_vreplgr2vr_w(yalpha1);
+    __m128i v_uvalpha  = __lsx_vreplgr2vr_w(uvalpha);
+    __m128i v_yalpha   = __lsx_vreplgr2vr_w(yalpha);
+    __m128i uv         = __lsx_vreplgr2vr_w(uvtemp);
+    __m128i a_bias     = __lsx_vreplgr2vr_w(atemp);
+    __m128i y_temp     = __lsx_vreplgr2vr_w(ytemp);
+    YUVTORGB_SETUP_LSX
+
+    av_assert2(yalpha  <= 4096U);
+    av_assert2(uvalpha <= 4096U);
+
+    if(   target == AV_PIX_FMT_BGR4_BYTE || target == AV_PIX_FMT_RGB4_BYTE
+       || target == AV_PIX_FMT_BGR8      || target == AV_PIX_FMT_RGB8)
+        step = 1;
+
+    for (i = 0; i < len; i += 8) {
+        __m128i b0, b1, ub0, ub1, vb0, vb1;
+        __m128i y0_l, y0_h, y1_l, y1_h, u0_l, u0_h;
+        __m128i v0_l, v0_h, u1_l, u1_h, v1_l, v1_h;
+        __m128i y_l, y_h, v_l, v_h, u_l, u_h;
+        __m128i R_l, R_h, G_l, G_h, B_l, B_h;
+        int n = i << 1;
+
+        DUP4_ARG2(__lsx_vldx, buf0, n, buf1, n, ubuf0,
+                  n, ubuf1, n, b0, b1, ub0, ub1);
+        DUP2_ARG2(__lsx_vldx, vbuf0, n, vbuf1, n, vb0 , vb1);
+        DUP2_ARG2(__lsx_vsllwil_w_h, b0, 0, b1, 0, y0_l, y1_l);
+        DUP4_ARG2(__lsx_vsllwil_w_h, ub0, 0, ub1, 0, vb0, 0, vb1, 0,
+                  u0_l, u1_l, v0_l, v1_l);
+        DUP2_ARG1(__lsx_vexth_w_h, b0, b1, y0_h, y1_h);
+        DUP4_ARG1(__lsx_vexth_w_h, ub0, ub1, vb0, vb1,
+                  u0_h, u1_h, v0_h, v1_h);
+        y0_l = __lsx_vmul_w(y0_l, v_yalpha1);
+        y0_h = __lsx_vmul_w(y0_h, v_yalpha1);
+        u0_l = __lsx_vmul_w(u0_l, v_uvalpha1);
+        u0_h = __lsx_vmul_w(u0_h, v_uvalpha1);
+        v0_l = __lsx_vmul_w(v0_l, v_uvalpha1);
+        v0_h = __lsx_vmul_w(v0_h, v_uvalpha1);
+        y_l  = __lsx_vmadd_w(y0_l, v_yalpha, y1_l);
+        y_h  = __lsx_vmadd_w(y0_h, v_yalpha, y1_h);
+        u_l  = __lsx_vmadd_w(u0_l, v_uvalpha, u1_l);
+        u_h  = __lsx_vmadd_w(u0_h, v_uvalpha, u1_h);
+        v_l  = __lsx_vmadd_w(v0_l, v_uvalpha, v1_l);
+        v_h  = __lsx_vmadd_w(v0_h, v_uvalpha, v1_h);
+        u_l  = __lsx_vsub_w(u_l, uv);
+        u_h  = __lsx_vsub_w(u_h, uv);
+        v_l  = __lsx_vsub_w(v_l, uv);
+        v_h  = __lsx_vsub_w(v_h, uv);
+        y_l  = __lsx_vsrai_w(y_l, 10);
+        y_h  = __lsx_vsrai_w(y_h, 10);
+        u_l  = __lsx_vsrai_w(u_l, 10);
+        u_h  = __lsx_vsrai_w(u_h, 10);
+        v_l  = __lsx_vsrai_w(v_l, 10);
+        v_h  = __lsx_vsrai_w(v_h, 10);
+        YUVTORGB_LSX(y_l, u_l, v_l, R_l, G_l, B_l, offset, coeff,
+                     y_temp, v2r, v2g, u2g, u2b);
+        YUVTORGB_LSX(y_h, u_h, v_h, R_h, G_h, B_h, offset, coeff,
+                     y_temp, v2r, v2g, u2g, u2b);
+
+        if (hasAlpha) {
+            __m128i a0, a1, a0_l, a0_h;
+            __m128i a_l, a_h, a1_l, a1_h;
+
+            DUP2_ARG2(__lsx_vldx, abuf0, n, abuf1, n, a0, a1);
+            DUP2_ARG2(__lsx_vsllwil_w_h, a0, 0, a1, 0, a0_l, a1_l);
+            DUP2_ARG1(__lsx_vexth_w_h, a0, a1, a0_h, a1_h);
+            a_l = __lsx_vmadd_w(a_bias, a0_l, v_yalpha1);
+            a_h = __lsx_vmadd_w(a_bias, a0_h, v_yalpha1);
+            a_l = __lsx_vmadd_w(a_l, v_yalpha, a1_l);
+            a_h = __lsx_vmadd_w(a_h, v_yalpha, a1_h);
+            a_l = __lsx_vsrai_w(a_l, 19);
+            a_h = __lsx_vsrai_w(a_h, 19);
+            WRITE_FULL_A_LSX(R_l, G_l, B_l, a_l, 0, 0);
+            WRITE_FULL_A_LSX(R_l, G_l, B_l, a_l, 1, 1);
+            WRITE_FULL_A_LSX(R_l, G_l, B_l, a_l, 2, 2);
+            WRITE_FULL_A_LSX(R_l, G_l, B_l, a_l, 3, 3);
+            WRITE_FULL_A_LSX(R_h, G_h, B_h, a_h, 0, 4);
+            WRITE_FULL_A_LSX(R_h, G_h, B_h, a_h, 1, 5);
+            WRITE_FULL_A_LSX(R_h, G_h, B_h, a_h, 2, 6);
+            WRITE_FULL_A_LSX(R_h, G_h, B_h, a_h, 3, 7);
+        } else {
+            WRITE_FULL_LSX(R_l, G_l, B_l, 0, 0);
+            WRITE_FULL_LSX(R_l, G_l, B_l, 1, 1);
+            WRITE_FULL_LSX(R_l, G_l, B_l, 2, 2);
+            WRITE_FULL_LSX(R_l, G_l, B_l, 3, 3);
+            WRITE_FULL_LSX(R_h, G_h, B_h, 0, 4);
+            WRITE_FULL_LSX(R_h, G_h, B_h, 1, 5);
+            WRITE_FULL_LSX(R_h, G_h, B_h, 2, 6);
+            WRITE_FULL_LSX(R_h, G_h, B_h, 3, 7);
+        }
+    }
+    if (dstW - i >= 4) {
+        __m128i b0, b1, ub0, ub1, vb0, vb1;
+        __m128i y0_l, y1_l, u0_l;
+        __m128i v0_l, u1_l, v1_l;
+        __m128i y_l, u_l, v_l;
+        __m128i R_l, G_l, B_l;
+        int n = i << 1;
+
+        DUP4_ARG2(__lsx_vldx, buf0, n, buf1, n, ubuf0, n,
+                  ubuf1, n, b0, b1, ub0, ub1);
+        DUP2_ARG2(__lsx_vldx, vbuf0, n, vbuf1, n, vb0, vb1);
+        DUP2_ARG2(__lsx_vsllwil_w_h, b0, 0, b1, 0, y0_l, y1_l);
+        DUP4_ARG2(__lsx_vsllwil_w_h, ub0, 0, ub1, 0, vb0, 0, vb1, 0,
+                  u0_l, u1_l, v0_l, v1_l);
+        y0_l = __lsx_vmul_w(y0_l, v_yalpha1);
+        u0_l = __lsx_vmul_w(u0_l, v_uvalpha1);
+        v0_l = __lsx_vmul_w(v0_l, v_uvalpha1);
+        y_l  = __lsx_vmadd_w(y0_l, v_yalpha, y1_l);
+        u_l  = __lsx_vmadd_w(u0_l, v_uvalpha, u1_l);
+        v_l  = __lsx_vmadd_w(v0_l, v_uvalpha, v1_l);
+        u_l  = __lsx_vsub_w(u_l, uv);
+        v_l  = __lsx_vsub_w(v_l, uv);
+        y_l  = __lsx_vsrai_w(y_l, 10);
+        u_l  = __lsx_vsrai_w(u_l, 10);
+        v_l  = __lsx_vsrai_w(v_l, 10);
+        YUVTORGB_LSX(y_l, u_l, v_l, R_l, G_l, B_l, offset, coeff,
+                     y_temp, v2r, v2g, u2g, u2b);
+
+        if (hasAlpha) {
+            __m128i a0, a1, a0_l;
+            __m128i a_l, a1_l;
+
+            DUP2_ARG2(__lsx_vldx, abuf0, n, abuf1, n, a0, a1);
+            DUP2_ARG2(__lsx_vsllwil_w_h, a0, 0, a1, 0, a0_l, a1_l);
+            a_l = __lsx_vmadd_w(a_bias, a0_l, v_yalpha1);
+            a_l = __lsx_vmadd_w(a_l, v_yalpha, a1_l);
+            a_l = __lsx_vsrai_w(a_l, 19);
+            WRITE_FULL_A_LSX(R_l, G_l, B_l, a_l, 0, 0);
+            WRITE_FULL_A_LSX(R_l, G_l, B_l, a_l, 1, 1);
+            WRITE_FULL_A_LSX(R_l, G_l, B_l, a_l, 2, 2);
+            WRITE_FULL_A_LSX(R_l, G_l, B_l, a_l, 3, 3);
+        } else {
+            WRITE_FULL_LSX(R_l, G_l, B_l, 0, 0);
+            WRITE_FULL_LSX(R_l, G_l, B_l, 1, 1);
+            WRITE_FULL_LSX(R_l, G_l, B_l, 2, 2);
+            WRITE_FULL_LSX(R_l, G_l, B_l, 3, 3);
+        }
+        i += 4;
+    }
+    for (; i < dstW; i++){
+        int Y = ( buf0[i] * yalpha1  +  buf1[i] * yalpha         ) >> 10;
+        int U = (ubuf0[i] * uvalpha1 + ubuf1[i] * uvalpha- uvtemp) >> 10;
+        int V = (vbuf0[i] * uvalpha1 + vbuf1[i] * uvalpha- uvtemp) >> 10;
+
+        A = 0;
+        if (hasAlpha){
+            A = (abuf0[i] * yalpha1 + abuf1[i] * yalpha + atemp) >> 19;
+            if (A & 0x100)
+                A = av_clip_uint8(A);
+        }
+
+        Y -= y_offset;
+        Y *= y_coeff;
+        Y += ytemp;
+        R  = (unsigned)Y + V * v2r_coe;
+        G  = (unsigned)Y + V * v2g_coe + U * u2g_coe;
+        B  = (unsigned)Y + U * u2b_coe;
+        yuv2rgb_write_full(c, dest, i, R, A, G, B, y, target, hasAlpha, err);
+        dest += step;
+    }
+    c->dither_error[0][i] = err[0];
+    c->dither_error[1][i] = err[1];
+    c->dither_error[2][i] = err[2];
+}
+
+static void
+yuv2rgb_full_1_template_lsx(SwsContext *c, const int16_t *buf0,
+                            const int16_t *ubuf[2], const int16_t *vbuf[2],
+                            const int16_t *abuf0, uint8_t *dest, int dstW,
+                            int uvalpha, int y, enum AVPixelFormat target,
+                            int hasAlpha)
+{
+    const int16_t *ubuf0 = ubuf[0], *vbuf0 = vbuf[0];
+    int i, B, G, R, A;
+    int step = (target == AV_PIX_FMT_RGB24 || target == AV_PIX_FMT_BGR24) ? 3 : 4;
+    int err[4]     = {0};
+    int ytemp      = 1 << 21;
+    int bias_int   = 64;
+    int len        = dstW - 7;
+    __m128i y_temp = __lsx_vreplgr2vr_w(ytemp);
+    YUVTORGB_SETUP_LSX
+
+    if(   target == AV_PIX_FMT_BGR4_BYTE || target == AV_PIX_FMT_RGB4_BYTE
+       || target == AV_PIX_FMT_BGR8      || target == AV_PIX_FMT_RGB8)
+        step = 1;
+    if (uvalpha < 2048) {
+        int uvtemp   = 128 << 7;
+        __m128i uv   = __lsx_vreplgr2vr_w(uvtemp);
+        __m128i bias = __lsx_vreplgr2vr_w(bias_int);
+
+        for (i = 0; i < len; i += 8) {
+            __m128i b, ub, vb, ub_l, ub_h, vb_l, vb_h;
+            __m128i y_l, y_h, u_l, u_h, v_l, v_h;
+            __m128i R_l, R_h, G_l, G_h, B_l, B_h;
+            int n = i << 1;
+
+            DUP2_ARG2(__lsx_vldx, buf0, n, ubuf0, n, b, ub);
+            vb  = __lsx_vldx(vbuf0, n);
+            y_l = __lsx_vsllwil_w_h(b, 2);
+            y_h = __lsx_vexth_w_h(b);
+            DUP2_ARG2(__lsx_vsllwil_w_h, ub, 0, vb, 0, ub_l, vb_l);
+            DUP2_ARG1(__lsx_vexth_w_h, ub, vb, ub_h, vb_h);
+            y_h = __lsx_vslli_w(y_h, 2);
+            u_l = __lsx_vsub_w(ub_l, uv);
+            u_h = __lsx_vsub_w(ub_h, uv);
+            v_l = __lsx_vsub_w(vb_l, uv);
+            v_h = __lsx_vsub_w(vb_h, uv);
+            u_l = __lsx_vslli_w(u_l, 2);
+            u_h = __lsx_vslli_w(u_h, 2);
+            v_l = __lsx_vslli_w(v_l, 2);
+            v_h = __lsx_vslli_w(v_h, 2);
+            YUVTORGB_LSX(y_l, u_l, v_l, R_l, G_l, B_l, offset, coeff,
+                         y_temp, v2r, v2g, u2g, u2b);
+            YUVTORGB_LSX(y_h, u_h, v_h, R_h, G_h, B_h, offset, coeff,
+                         y_temp, v2r, v2g, u2g, u2b);
+
+            if(hasAlpha) {
+                __m128i a_src;
+                __m128i a_l, a_h;
+
+                a_src = __lsx_vld(abuf0 + i, 0);
+                a_l   = __lsx_vsllwil_w_h(a_src, 0);
+                a_h   = __lsx_vexth_w_h(a_src);
+                a_l   = __lsx_vadd_w(a_l, bias);
+                a_h   = __lsx_vadd_w(a_h, bias);
+                a_l   = __lsx_vsrai_w(a_l, 7);
+                a_h   = __lsx_vsrai_w(a_h, 7);
+                WRITE_FULL_A_LSX(R_l, G_l, B_l, a_l, 0, 0);
+                WRITE_FULL_A_LSX(R_l, G_l, B_l, a_l, 1, 1);
+                WRITE_FULL_A_LSX(R_l, G_l, B_l, a_l, 2, 2);
+                WRITE_FULL_A_LSX(R_l, G_l, B_l, a_l, 3, 3);
+                WRITE_FULL_A_LSX(R_h, G_h, B_h, a_h, 0, 4);
+                WRITE_FULL_A_LSX(R_h, G_h, B_h, a_h, 1, 5);
+                WRITE_FULL_A_LSX(R_h, G_h, B_h, a_h, 2, 6);
+                WRITE_FULL_A_LSX(R_h, G_h, B_h, a_h, 3, 7);
+            } else {
+                WRITE_FULL_LSX(R_l, G_l, B_l, 0, 0);
+                WRITE_FULL_LSX(R_l, G_l, B_l, 1, 1);
+                WRITE_FULL_LSX(R_l, G_l, B_l, 2, 2);
+                WRITE_FULL_LSX(R_l, G_l, B_l, 3, 3);
+                WRITE_FULL_LSX(R_h, G_h, B_h, 0, 4);
+                WRITE_FULL_LSX(R_h, G_h, B_h, 1, 5);
+                WRITE_FULL_LSX(R_h, G_h, B_h, 2, 6);
+                WRITE_FULL_LSX(R_h, G_h, B_h, 3, 7);
+            }
+        }
+        if (dstW - i >= 4) {
+            __m128i b, ub, vb, ub_l, vb_l;
+            __m128i y_l, u_l, v_l;
+            __m128i R_l, G_l, B_l;
+            int n = i << 1;
+
+            DUP2_ARG2(__lsx_vldx, buf0, n, ubuf0, n, b, ub);
+            vb  = __lsx_vldx(vbuf0, n);
+            y_l = __lsx_vsllwil_w_h(b, 0);
+            DUP2_ARG2(__lsx_vsllwil_w_h, ub, 0, vb, 0, ub_l, vb_l);
+            y_l = __lsx_vslli_w(y_l, 2);
+            u_l = __lsx_vsub_w(ub_l, uv);
+            v_l = __lsx_vsub_w(vb_l, uv);
+            u_l = __lsx_vslli_w(u_l, 2);
+            v_l = __lsx_vslli_w(v_l, 2);
+            YUVTORGB_LSX(y_l, u_l, v_l, R_l, G_l, B_l, offset, coeff,
+                         y_temp, v2r, v2g, u2g, u2b);
+
+            if(hasAlpha) {
+                __m128i a_src, a_l;
+
+                a_src = __lsx_vldx(abuf0, n);
+                a_src = __lsx_vsllwil_w_h(a_src, 0);
+                a_l   = __lsx_vadd_w(bias, a_src);
+                a_l   = __lsx_vsrai_w(a_l, 7);
+                WRITE_FULL_A_LSX(R_l, G_l, B_l, a_l, 0, 0);
+                WRITE_FULL_A_LSX(R_l, G_l, B_l, a_l, 1, 1);
+                WRITE_FULL_A_LSX(R_l, G_l, B_l, a_l, 2, 2);
+                WRITE_FULL_A_LSX(R_l, G_l, B_l, a_l, 3, 3);
+            } else {
+                WRITE_FULL_LSX(R_l, G_l, B_l, 0, 0);
+                WRITE_FULL_LSX(R_l, G_l, B_l, 1, 1);
+                WRITE_FULL_LSX(R_l, G_l, B_l, 2, 2);
+                WRITE_FULL_LSX(R_l, G_l, B_l, 3, 3);
+            }
+            i += 4;
+        }
+        for (; i < dstW; i++) {
+            int Y = buf0[i] << 2;
+            int U = (ubuf0[i] - uvtemp) << 2;
+            int V = (vbuf0[i] - uvtemp) << 2;
+
+            A = 0;
+            if(hasAlpha) {
+                A = (abuf0[i] + 64) >> 7;
+                if (A & 0x100)
+                    A = av_clip_uint8(A);
+            }
+            Y -= y_offset;
+            Y *= y_coeff;
+            Y += ytemp;
+            R  = (unsigned)Y + V * v2r_coe;
+            G  = (unsigned)Y + V * v2g_coe + U * u2g_coe;
+            B  = (unsigned)Y + U * u2b_coe;
+            yuv2rgb_write_full(c, dest, i, R, A, G, B, y, target, hasAlpha, err);
+            dest += step;
+        }
+    } else {
+        const int16_t *ubuf1 = ubuf[1], *vbuf1 = vbuf[1];
+        int uvtemp   = 128 << 8;
+        __m128i uv   = __lsx_vreplgr2vr_w(uvtemp);
+        __m128i zero = __lsx_vldi(0);
+        __m128i bias = __lsx_vreplgr2vr_h(bias_int);
+
+        for (i = 0; i < len; i += 8) {
+            __m128i b, ub0, ub1, vb0, vb1;
+            __m128i y_ev, y_od, u_ev, u_od, v_ev, v_od;
+            __m128i R_ev, R_od, G_ev, G_od, B_ev, B_od;
+            int n = i << 1;
+
+            DUP4_ARG2(__lsx_vldx, buf0, n, ubuf0, n, vbuf0, n,
+                      ubuf1, n, b, ub0, vb0, ub1);
+            vb1 = __lsx_vldx(vbuf, n);
+            y_ev = __lsx_vaddwev_w_h(b, zero);
+            y_od = __lsx_vaddwod_w_h(b, zero);
+            DUP2_ARG2(__lsx_vaddwev_w_h, ub0, vb0, ub1, vb1, u_ev, v_ev);
+            DUP2_ARG2(__lsx_vaddwod_w_h, ub0, vb0, ub1, vb1, u_od, v_od);
+            DUP2_ARG2(__lsx_vslli_w, y_ev, 2, y_od, 2, y_ev, y_od);
+            DUP4_ARG2(__lsx_vsub_w, u_ev, uv, u_od, uv, v_ev, uv, v_od, uv,
+                      u_ev, u_od, v_ev, v_od);
+            DUP4_ARG2(__lsx_vslli_w, u_ev, 1, u_od, 1, v_ev, 1, v_od, 1,
+                      u_ev, u_od, v_ev, v_od);
+            YUVTORGB_LSX(y_ev, u_ev, v_ev, R_ev, G_ev, B_ev, offset, coeff,
+                         y_temp, v2r, v2g, u2g, u2b);
+            YUVTORGB_LSX(y_od, u_od, v_od, R_od, G_od, B_od, offset, coeff,
+                         y_temp, v2r, v2g, u2g, u2b);
+
+            if(hasAlpha) {
+                __m128i a_src;
+                __m128i a_ev, a_od;
+
+                a_src = __lsx_vld(abuf0 + i, 0);
+                a_ev  = __lsx_vaddwev_w_h(bias, a_src);
+                a_od  = __lsx_vaddwod_w_h(bias, a_src);
+                a_ev  = __lsx_vsrai_w(a_ev, 7);
+                a_od  = __lsx_vsrai_w(a_od, 7);
+                WRITE_FULL_A_LSX(R_ev, G_ev, B_ev, a_ev, 0, 0);
+                WRITE_FULL_A_LSX(R_od, G_od, B_od, a_od, 0, 1);
+                WRITE_FULL_A_LSX(R_ev, G_ev, B_ev, a_ev, 1, 2);
+                WRITE_FULL_A_LSX(R_od, G_od, B_od, a_od, 1, 3);
+                WRITE_FULL_A_LSX(R_ev, G_ev, B_ev, a_ev, 2, 4);
+                WRITE_FULL_A_LSX(R_od, G_od, B_od, a_od, 2, 5);
+                WRITE_FULL_A_LSX(R_ev, G_ev, B_ev, a_ev, 3, 6);
+                WRITE_FULL_A_LSX(R_od, G_od, B_od, a_od, 3, 7);
+            } else {
+                WRITE_FULL_LSX(R_ev, G_ev, B_ev, 0, 0);
+                WRITE_FULL_LSX(R_od, G_od, B_od, 0, 1);
+                WRITE_FULL_LSX(R_ev, G_ev, B_ev, 1, 2);
+                WRITE_FULL_LSX(R_od, G_od, B_od, 1, 3);
+                WRITE_FULL_LSX(R_ev, G_ev, B_ev, 2, 4);
+                WRITE_FULL_LSX(R_od, G_od, B_od, 2, 5);
+                WRITE_FULL_LSX(R_ev, G_ev, B_ev, 3, 6);
+                WRITE_FULL_LSX(R_od, G_od, B_od, 3, 7);
+            }
+        }
+        if (dstW - i >= 4) {
+            __m128i b, ub0, ub1, vb0, vb1;
+            __m128i y_l, u_l, v_l;
+            __m128i R_l, G_l, B_l;
+            int n = i << 1;
+
+            DUP4_ARG2(__lsx_vldx, buf0, n, ubuf0, n, vbuf0, n,
+                      ubuf1, n, b, ub0, vb0, ub1);
+            vb1 = __lsx_vldx(vbuf1, n);
+            y_l = __lsx_vsllwil_w_h(b, 0);
+            y_l = __lsx_vslli_w(y_l, 2);
+            DUP4_ARG2(__lsx_vsllwil_w_h, ub0, 0, vb0, 0, ub1, 0, vb1, 0,
+                      ub0, vb0, ub1, vb1);
+            DUP2_ARG2(__lsx_vadd_w, ub0, ub1, vb0, vb1, u_l, v_l);
+            u_l = __lsx_vsub_w(u_l, uv);
+            v_l = __lsx_vsub_w(v_l, uv);
+            u_l = __lsx_vslli_w(u_l, 1);
+            v_l = __lsx_vslli_w(v_l, 1);
+            YUVTORGB_LSX(y_l, u_l, v_l, R_l, G_l, B_l, offset, coeff,
+                         y_temp, v2r, v2g, u2g, u2b);
+
+            if(hasAlpha) {
+                __m128i a_src;
+                __m128i a_l;
+
+                a_src  = __lsx_vld(abuf0 + i, 0);
+                a_src  = __lsx_vilvl_h(a_src, a_src);
+                a_l    = __lsx_vaddwev_w_h(bias, a_l);
+                a_l   = __lsx_vsrai_w(a_l, 7);
+                WRITE_FULL_A_LSX(R_l, G_l, B_l, a_l, 0, 0);
+                WRITE_FULL_A_LSX(R_l, G_l, B_l, a_l, 1, 1);
+                WRITE_FULL_A_LSX(R_l, G_l, B_l, a_l, 2, 2);
+                WRITE_FULL_A_LSX(R_l, G_l, B_l, a_l, 3, 3);
+            } else {
+                WRITE_FULL_LSX(R_l, G_l, B_l, 0, 0);
+                WRITE_FULL_LSX(R_l, G_l, B_l, 1, 1);
+                WRITE_FULL_LSX(R_l, G_l, B_l, 2, 2);
+                WRITE_FULL_LSX(R_l, G_l, B_l, 3, 3);
+            }
+            i += 4;
+        }
+        for (; i < dstW; i++) {
+            int Y = buf0[i] << 2;
+            int U = (ubuf0[i] + ubuf1[i] - uvtemp) << 1;
+            int V = (vbuf0[i] + vbuf1[i] - uvtemp) << 1;
+
+            A = 0;
+            if(hasAlpha) {
+                A = (abuf0[i] + 64) >> 7;
+                if (A & 0x100)
+                    A = av_clip_uint8(A);
+            }
+            Y -= y_offset;
+            Y *= y_coeff;
+            Y += ytemp;
+            R  = (unsigned)Y + V * v2r_coe;
+            G  = (unsigned)Y + V * v2g_coe + U * u2g_coe;
+            B  = (unsigned)Y + U * u2b_coe;
+            yuv2rgb_write_full(c, dest, i, R, A, G, B, y, target, hasAlpha, err);
+            dest += step;
+        }
+    }
+    c->dither_error[0][i] = err[0];
+    c->dither_error[1][i] = err[1];
+    c->dither_error[2][i] = err[2];
+}
+
+#if CONFIG_SMALL
+YUV2RGBWRAPPER(yuv2, rgb_full, bgra32_full, AV_PIX_FMT_BGRA,
+               CONFIG_SWSCALE_ALPHA && c->needAlpha)
+YUV2RGBWRAPPER(yuv2, rgb_full, abgr32_full, AV_PIX_FMT_ABGR,
+               CONFIG_SWSCALE_ALPHA && c->needAlpha)
+YUV2RGBWRAPPER(yuv2, rgb_full, rgba32_full, AV_PIX_FMT_RGBA,
+               CONFIG_SWSCALE_ALPHA && c->needAlpha)
+YUV2RGBWRAPPER(yuv2, rgb_full, argb32_full, AV_PIX_FMT_ARGB,
+               CONFIG_SWSCALE_ALPHA && c->needAlpha)
+#else
+#if CONFIG_SWSCALE_ALPHA
+YUV2RGBWRAPPER(yuv2, rgb_full, bgra32_full, AV_PIX_FMT_BGRA,  1)
+YUV2RGBWRAPPER(yuv2, rgb_full, abgr32_full, AV_PIX_FMT_ABGR,  1)
+YUV2RGBWRAPPER(yuv2, rgb_full, rgba32_full, AV_PIX_FMT_RGBA,  1)
+YUV2RGBWRAPPER(yuv2, rgb_full, argb32_full, AV_PIX_FMT_ARGB,  1)
+#endif
+YUV2RGBWRAPPER(yuv2, rgb_full, bgrx32_full, AV_PIX_FMT_BGRA,  0)
+YUV2RGBWRAPPER(yuv2, rgb_full, xbgr32_full, AV_PIX_FMT_ABGR,  0)
+YUV2RGBWRAPPER(yuv2, rgb_full, rgbx32_full, AV_PIX_FMT_RGBA,  0)
+YUV2RGBWRAPPER(yuv2, rgb_full, xrgb32_full, AV_PIX_FMT_ARGB,  0)
+#endif
+YUV2RGBWRAPPER(yuv2, rgb_full, bgr24_full,  AV_PIX_FMT_BGR24, 0)
+YUV2RGBWRAPPER(yuv2, rgb_full, rgb24_full,  AV_PIX_FMT_RGB24, 0)
+
+YUV2RGBWRAPPER(yuv2, rgb_full, bgr4_byte_full,  AV_PIX_FMT_BGR4_BYTE, 0)
+YUV2RGBWRAPPER(yuv2, rgb_full, rgb4_byte_full,  AV_PIX_FMT_RGB4_BYTE, 0)
+YUV2RGBWRAPPER(yuv2, rgb_full, bgr8_full,   AV_PIX_FMT_BGR8,  0)
+YUV2RGBWRAPPER(yuv2, rgb_full, rgb8_full,   AV_PIX_FMT_RGB8,  0)
+
+
+av_cold void ff_sws_init_output_lsx(SwsContext *c)
+{
+    if(c->flags & SWS_FULL_CHR_H_INT) {
+        switch (c->dstFormat) {
+        case AV_PIX_FMT_RGBA:
+#if CONFIG_SMALL
+            c->yuv2packedX = yuv2rgba32_full_X_lsx;
+            c->yuv2packed2 = yuv2rgba32_full_2_lsx;
+            c->yuv2packed1 = yuv2rgba32_full_1_lsx;
+#else
+#if CONFIG_SWSCALE_ALPHA
+            if (c->needAlpha) {
+                c->yuv2packedX = yuv2rgba32_full_X_lsx;
+                c->yuv2packed2 = yuv2rgba32_full_2_lsx;
+                c->yuv2packed1 = yuv2rgba32_full_1_lsx;
+            } else
+#endif /* CONFIG_SWSCALE_ALPHA */
+            {
+                c->yuv2packedX = yuv2rgbx32_full_X_lsx;
+                c->yuv2packed2 = yuv2rgbx32_full_2_lsx;
+                c->yuv2packed1 = yuv2rgbx32_full_1_lsx;
+            }
+#endif /* !CONFIG_SMALL */
+            break;
+        case AV_PIX_FMT_ARGB:
+#if CONFIG_SMALL
+            c->yuv2packedX = yuv2argb32_full_X_lsx;
+            c->yuv2packed2 = yuv2argb32_full_2_lsx;
+            c->yuv2packed1 = yuv2argb32_full_1_lsx;
+#else
+#if CONFIG_SWSCALE_ALPHA
+            if (c->needAlpha) {
+                c->yuv2packedX = yuv2argb32_full_X_lsx;
+                c->yuv2packed2 = yuv2argb32_full_2_lsx;
+                c->yuv2packed1 = yuv2argb32_full_1_lsx;
+            } else
+#endif /* CONFIG_SWSCALE_ALPHA */
+            {
+                c->yuv2packedX = yuv2xrgb32_full_X_lsx;
+                c->yuv2packed2 = yuv2xrgb32_full_2_lsx;
+                c->yuv2packed1 = yuv2xrgb32_full_1_lsx;
+            }
+#endif /* !CONFIG_SMALL */
+            break;
+        case AV_PIX_FMT_BGRA:
+#if CONFIG_SMALL
+            c->yuv2packedX = yuv2bgra32_full_X_lsx;
+            c->yuv2packed2 = yuv2bgra32_full_2_lsx;
+            c->yuv2packed1 = yuv2bgra32_full_1_lsx;
+#else
+#if CONFIG_SWSCALE_ALPHA
+            if (c->needAlpha) {
+                c->yuv2packedX = yuv2bgra32_full_X_lsx;
+                c->yuv2packed2 = yuv2bgra32_full_2_lsx;
+                c->yuv2packed1 = yuv2bgra32_full_1_lsx;
+            } else
+#endif /* CONFIG_SWSCALE_ALPHA */
+            {
+                c->yuv2packedX = yuv2bgrx32_full_X_lsx;
+                c->yuv2packed2 = yuv2bgrx32_full_2_lsx;
+                c->yuv2packed1 = yuv2bgrx32_full_1_lsx;
+            }
+#endif /* !CONFIG_SMALL */
+            break;
+        case AV_PIX_FMT_ABGR:
+#if CONFIG_SMALL
+            c->yuv2packedX = yuv2abgr32_full_X_lsx;
+            c->yuv2packed2 = yuv2abgr32_full_2_lsx;
+            c->yuv2packed1 = yuv2abgr32_full_1_lsx;
+#else
+#if CONFIG_SWSCALE_ALPHA
+            if (c->needAlpha) {
+                c->yuv2packedX = yuv2abgr32_full_X_lsx;
+                c->yuv2packed2 = yuv2abgr32_full_2_lsx;
+                c->yuv2packed1 = yuv2abgr32_full_1_lsx;
+            } else
+#endif /* CONFIG_SWSCALE_ALPHA */
+            {
+                c->yuv2packedX = yuv2xbgr32_full_X_lsx;
+                c->yuv2packed2 = yuv2xbgr32_full_2_lsx;
+                c->yuv2packed1 = yuv2xbgr32_full_1_lsx;
+            }
+#endif /* !CONFIG_SMALL */
+            break;
+        case AV_PIX_FMT_RGB24:
+            c->yuv2packedX = yuv2rgb24_full_X_lsx;
+            c->yuv2packed2 = yuv2rgb24_full_2_lsx;
+            c->yuv2packed1 = yuv2rgb24_full_1_lsx;
+            break;
+        case AV_PIX_FMT_BGR24:
+            c->yuv2packedX = yuv2bgr24_full_X_lsx;
+            c->yuv2packed2 = yuv2bgr24_full_2_lsx;
+            c->yuv2packed1 = yuv2bgr24_full_1_lsx;
+            break;
+        case AV_PIX_FMT_BGR4_BYTE:
+            c->yuv2packedX = yuv2bgr4_byte_full_X_lsx;
+            c->yuv2packed2 = yuv2bgr4_byte_full_2_lsx;
+            c->yuv2packed1 = yuv2bgr4_byte_full_1_lsx;
+            break;
+        case AV_PIX_FMT_RGB4_BYTE:
+            c->yuv2packedX = yuv2rgb4_byte_full_X_lsx;
+            c->yuv2packed2 = yuv2rgb4_byte_full_2_lsx;
+            c->yuv2packed1 = yuv2rgb4_byte_full_1_lsx;
+            break;
+        case AV_PIX_FMT_BGR8:
+            c->yuv2packedX = yuv2bgr8_full_X_lsx;
+            c->yuv2packed2 = yuv2bgr8_full_2_lsx;
+            c->yuv2packed1 = yuv2bgr8_full_1_lsx;
+            break;
+        case AV_PIX_FMT_RGB8:
+            c->yuv2packedX = yuv2rgb8_full_X_lsx;
+            c->yuv2packed2 = yuv2rgb8_full_2_lsx;
+            c->yuv2packed1 = yuv2rgb8_full_1_lsx;
+            break;
+    }
+    } else {
+        switch (c->dstFormat) {
+        case AV_PIX_FMT_RGB32:
+        case AV_PIX_FMT_BGR32:
+#if CONFIG_SMALL
+#else
+#if CONFIG_SWSCALE_ALPHA
+            if (c->needAlpha) {
+            } else
+#endif /* CONFIG_SWSCALE_ALPHA */
+            {
+                c->yuv2packed1 = yuv2rgbx32_1_lsx;
+                c->yuv2packed2 = yuv2rgbx32_2_lsx;
+                c->yuv2packedX = yuv2rgbx32_X_lsx;
+            }
+#endif /* !CONFIG_SMALL */
+            break;
+        case AV_PIX_FMT_RGB32_1:
+        case AV_PIX_FMT_BGR32_1:
+#if CONFIG_SMALL
+#else
+#if CONFIG_SWSCALE_ALPHA
+            if (c->needAlpha) {
+            } else
+#endif /* CONFIG_SWSCALE_ALPHA */
+            {
+                c->yuv2packed1 = yuv2rgbx32_1_1_lsx;
+                c->yuv2packed2 = yuv2rgbx32_1_2_lsx;
+                c->yuv2packedX = yuv2rgbx32_1_X_lsx;
+            }
+#endif /* !CONFIG_SMALL */
+            break;
+        case AV_PIX_FMT_RGB24:
+            c->yuv2packed1 = yuv2rgb24_1_lsx;
+            c->yuv2packed2 = yuv2rgb24_2_lsx;
+            c->yuv2packedX = yuv2rgb24_X_lsx;
+            break;
+        case AV_PIX_FMT_BGR24:
+            c->yuv2packed1 = yuv2bgr24_1_lsx;
+            c->yuv2packed2 = yuv2bgr24_2_lsx;
+            c->yuv2packedX = yuv2bgr24_X_lsx;
+            break;
+        case AV_PIX_FMT_RGB565LE:
+        case AV_PIX_FMT_RGB565BE:
+        case AV_PIX_FMT_BGR565LE:
+        case AV_PIX_FMT_BGR565BE:
+            c->yuv2packed1 = yuv2rgb16_1_lsx;
+            c->yuv2packed2 = yuv2rgb16_2_lsx;
+            c->yuv2packedX = yuv2rgb16_X_lsx;
+            break;
+        case AV_PIX_FMT_RGB555LE:
+        case AV_PIX_FMT_RGB555BE:
+        case AV_PIX_FMT_BGR555LE:
+        case AV_PIX_FMT_BGR555BE:
+            c->yuv2packed1 = yuv2rgb15_1_lsx;
+            c->yuv2packed2 = yuv2rgb15_2_lsx;
+            c->yuv2packedX = yuv2rgb15_X_lsx;
+            break;
+        case AV_PIX_FMT_RGB444LE:
+        case AV_PIX_FMT_RGB444BE:
+        case AV_PIX_FMT_BGR444LE:
+        case AV_PIX_FMT_BGR444BE:
+            c->yuv2packed1 = yuv2rgb12_1_lsx;
+            c->yuv2packed2 = yuv2rgb12_2_lsx;
+            c->yuv2packedX = yuv2rgb12_X_lsx;
+            break;
+        case AV_PIX_FMT_RGB8:
+        case AV_PIX_FMT_BGR8:
+            c->yuv2packed1 = yuv2rgb8_1_lsx;
+            c->yuv2packed2 = yuv2rgb8_2_lsx;
+            c->yuv2packedX = yuv2rgb8_X_lsx;
+            break;
+        case AV_PIX_FMT_RGB4:
+        case AV_PIX_FMT_BGR4:
+            c->yuv2packed1 = yuv2rgb4_1_lsx;
+            c->yuv2packed2 = yuv2rgb4_2_lsx;
+            c->yuv2packedX = yuv2rgb4_X_lsx;
+            break;
+        case AV_PIX_FMT_RGB4_BYTE:
+        case AV_PIX_FMT_BGR4_BYTE:
+            c->yuv2packed1 = yuv2rgb4b_1_lsx;
+            c->yuv2packed2 = yuv2rgb4b_2_lsx;
+            c->yuv2packedX = yuv2rgb4b_X_lsx;
+            break;
+        }
+    }
+}
diff --git a/libswscale/loongarch/rgb2rgb_lasx.c b/libswscale/loongarch/rgb2rgb_lasx.c
new file mode 100644
index 0000000000..78b5a03bb4
--- /dev/null
+++ b/libswscale/loongarch/rgb2rgb_lasx.c
@@ -0,0 +1,53 @@
+/*
+ * Copyright (C) 2021 Loongson Technology Co. Ltd.
+ * Contributed by Hao Chen(chenhao@loongson.cn)
+ * All rights reserved.
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "swscale_loongarch.h"
+#include "libavutil/loongarch/loongson_intrinsics.h"
+
+void ff_interleave_bytes_lasx(const uint8_t *src1, const uint8_t *src2,
+                              uint8_t *dest, int width, int height,
+                              int src1Stride, int src2Stride, int dstStride)
+{
+    int h;
+    int len = width & (0xFFFFFFF0);
+
+    for (h = 0; h < height; h++) {
+        int w, index = 0;
+        __m256i src_1, src_2, dst;
+
+        for (w = 0; w < len; w += 16) {
+            DUP2_ARG2(__lasx_xvld, src1 + w, 0, src2 + w, 0, src_1, src_2);
+            src_1 = __lasx_xvpermi_d(src_1, 0xD8);
+            src_2 = __lasx_xvpermi_d(src_2, 0xD8);
+            dst   = __lasx_xvilvl_b(src_2, src_1);
+            __lasx_xvst(dst, dest + index, 0);
+            index  += 32;
+        }
+        for (w = 0; w < width; w++) {
+            dest[(w << 1) + 0] = src1[w];
+            dest[(w << 1) + 1] = src2[w];
+        }
+        dest += dstStride;
+        src1 += src1Stride;
+        src2 += src2Stride;
+    }
+}
diff --git a/libswscale/loongarch/swscale.S b/libswscale/loongarch/swscale.S
new file mode 100644
index 0000000000..aa4c5cbe28
--- /dev/null
+++ b/libswscale/loongarch/swscale.S
@@ -0,0 +1,1868 @@
+/*
+ * Loongson LSX optimized swscale
+ *
+ * Copyright (c) 2023 Loongson Technology Corporation Limited
+ * Contributed by Lu Wang <wanglu@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "libavcodec/loongarch/loongson_asm.S"
+
+/* void ff_hscale_8_to_15_lsx(SwsContext *c, int16_t *dst, int dstW,
+ *                            const uint8_t *src, const int16_t *filter,
+ *                            const int32_t *filterPos, int filterSize)
+ */
+function ff_hscale_8_to_15_lsx
+    addi.d           sp,      sp,     -72
+    st.d             s0,      sp,     0
+    st.d             s1,      sp,     8
+    st.d             s2,      sp,     16
+    st.d             s3,      sp,     24
+    st.d             s4,      sp,     32
+    st.d             s5,      sp,     40
+    st.d             s6,      sp,     48
+    st.d             s7,      sp,     56
+    st.d             s8,      sp,     64
+    li.w             t0,      32767
+    li.w             t8,      8
+    li.w             t7,      4
+    vldi             vr0,     0
+    vreplgr2vr.w     vr20,    t0
+    beq              a6,      t7,     .LOOP_DSTW4
+    beq              a6,      t8,     .LOOP_DSTW8
+    blt              t8,      a6,     .LOOP_START
+    b                .END_DSTW4
+
+.LOOP_START:
+    li.w             t1,      0
+    li.w             s1,      0
+    li.w             s2,      0
+    li.w             s3,      0
+    li.w             s4,      0
+    li.w             s5,      0
+    vldi             vr22,    0
+    addi.w           s0,      a6,     -7
+    slli.w           s7,      a6,     1
+    slli.w           s8,      a6,     2
+    add.w            t6,      s7,     s8
+.LOOP_DSTW:
+    ld.w             t2,      a5,     0
+    ld.w             t3,      a5,     4
+    ld.w             t4,      a5,     8
+    ld.w             t5,      a5,     12
+    fldx.d           f1,      a3,     t2
+    fldx.d           f2,      a3,     t3
+    fldx.d           f3,      a3,     t4
+    fldx.d           f4,      a3,     t5
+    vld              vr9,     a4,     0
+    vldx             vr10,    a4,     s7
+    vldx             vr11,    a4,     s8
+    vldx             vr12,    a4,     t6
+    vilvl.b          vr1,     vr0,    vr1
+    vilvl.b          vr2,     vr0,    vr2
+    vilvl.b          vr3,     vr0,    vr3
+    vilvl.b          vr4,     vr0,    vr4
+    vdp2.w.h         vr17,    vr1,    vr9
+    vdp2.w.h         vr18,    vr2,    vr10
+    vdp2.w.h         vr19,    vr3,    vr11
+    vdp2.w.h         vr21,    vr4,    vr12
+    vhaddw.d.w       vr1,     vr17,   vr17
+    vhaddw.d.w       vr2,     vr18,   vr18
+    vhaddw.d.w       vr3,     vr19,   vr19
+    vhaddw.d.w       vr4,     vr21,   vr21
+    vhaddw.q.d       vr1,     vr1,    vr1
+    vhaddw.q.d       vr2,     vr2,    vr2
+    vhaddw.q.d       vr3,     vr3,    vr3
+    vhaddw.q.d       vr4,     vr4,    vr4
+    vilvl.w          vr1,     vr2,    vr1
+    vilvl.w          vr3,     vr4,    vr3
+    vilvl.d          vr1,     vr3,    vr1
+    vadd.w           vr22,    vr22,   vr1
+    addi.w           s1,      s1,     8
+    addi.d           a3,      a3,     8
+    addi.d           a4,      a4,     16
+    blt              s1,      s0,     .LOOP_DSTW
+    blt              s1,      a6,     .DSTWA
+    b                .END_FILTER
+.DSTWA:
+    ld.w             t2,      a5,     0
+    li.w             t3,      0
+    move             s6,      s1
+.FILTERSIZEA:
+    add.w            t4,      t2,     t3
+    ldx.bu           t5,      a3,     t4
+    mul.w            t6,      a6,     t1
+    add.w            t6,      t6,     t3
+    slli.w           t6,      t6,     1
+    ldx.h            t6,      a4,     t6
+    mul.w            t6,      t5,     t6
+    add.w            s2,      s2,     t6
+    addi.w           t3,      t3,     1
+    addi.w           s6,      s6,     1
+    blt              s6,      a6,     .FILTERSIZEA
+
+    ld.w             t2,      a5,     4
+    li.w             t3,      0
+    move             s6,      s1
+    addi.w           t1,      t1,     1
+.FILTERSIZEB:
+    add.w            t4,      t2,     t3
+    ldx.bu           t5,      a3,     t4
+    mul.w            t6,      a6,     t1
+    add.w            t6,      t6,     t3
+    slli.w           t6,      t6,     1
+    ldx.h            t6,      a4,     t6
+    mul.w            t6,      t5,     t6
+    add.w            s3,      s3,     t6
+    addi.w           t3,      t3,     1
+    addi.w           s6,      s6,     1
+    blt              s6,      a6,     .FILTERSIZEB
+    ld.w             t2,      a5,     8
+    addi.w           t1,      t1,     1
+    li.w             t3,      0
+    move             s6,      s1
+.FILTERSIZEC:
+    add.w            t4,      t2,     t3
+    ldx.bu           t5,      a3,     t4
+    mul.w            t6,      a6,     t1
+    add.w            t6,      t6,     t3
+    slli.w           t6,      t6,     1
+    ldx.h            t6,      a4,     t6
+    mul.w            t6,      t5,     t6
+    add.w            s4,      s4,     t6
+    addi.w           t3,      t3,     1
+    addi.w           s6,      s6,     1
+    blt              s6,      a6,     .FILTERSIZEC
+    ld.w             t2,      a5,     12
+    addi.w           t1,      t1,     1
+    move             s6,      s1
+    li.w             t3,      0
+.FILTERSIZED:
+    add.w            t4,      t2,     t3
+    ldx.bu           t5,      a3,     t4
+    mul.w            t6,      a6,     t1
+    add.w            t6,      t6,     t3
+    slli.w           t6,      t6,     1
+    ldx.h            t6,      a4,     t6
+    mul.w            t6,      t5,     t6
+    add.w            s5,      s5,     t6
+    addi.w           t3,      t3,     1
+    addi.w           s6,      s6,     1
+    blt              s6,      a6,     .FILTERSIZED
+.END_FILTER:
+    vpickve2gr.w     t1,      vr22,   0
+    vpickve2gr.w     t2,      vr22,   1
+    vpickve2gr.w     t3,      vr22,   2
+    vpickve2gr.w     t4,      vr22,   3
+    add.w            s2,      s2,     t1
+    add.w            s3,      s3,     t2
+    add.w            s4,      s4,     t3
+    add.w            s5,      s5,     t4
+    srai.w           s2,      s2,     7
+    srai.w           s3,      s3,     7
+    srai.w           s4,      s4,     7
+    srai.w           s5,      s5,     7
+    slt              t1,      s2,     t0
+    slt              t2,      s3,     t0
+    slt              t3,      s4,     t0
+    slt              t4,      s5,     t0
+    maskeqz          s2,      s2,     t1
+    maskeqz          s3,      s3,     t2
+    maskeqz          s4,      s4,     t3
+    maskeqz          s5,      s5,     t4
+    masknez          t1,      t0,     t1
+    masknez          t2,      t0,     t2
+    masknez          t3,      t0,     t3
+    masknez          t4,      t0,     t4
+    or               s2,      s2,     t1
+    or               s3,      s3,     t2
+    or               s4,      s4,     t3
+    or               s5,      s5,     t4
+    st.h             s2,      a1,     0
+    st.h             s3,      a1,     2
+    st.h             s4,      a1,     4
+    st.h             s5,      a1,     6
+
+    addi.d           a1,      a1,     8
+    sub.d            a3,      a3,     s1
+    addi.d           a5,      a5,     16
+    slli.d           t3,      a6,     3
+    add.d            a4,      a4,     t3
+    sub.d            a4,      a4,     s1
+    sub.d            a4,      a4,     s1
+    addi.d           a2,      a2,     -4
+    bge              a2,      t7,     .LOOP_START
+    blt              zero,    a2,     .RES
+    b                .END_LOOP
+.RES:
+    li.w             t1,      0
+.DSTW:
+    slli.w           t2,      t1,     2
+    ldx.w            t2,      a5,     t2
+    li.w             t3,      0
+    li.w             t8,      0
+.FILTERSIZE:
+    add.w            t4,      t2,     t3
+    ldx.bu           t5,      a3,     t4
+    mul.w            t6,      a6,     t1
+    add.w            t6,      t6,     t3
+    slli.w           t7,      t6,     1
+    ldx.h            t7,      a4,     t7
+    mul.w            t7,      t5,     t7
+    add.w            t8,      t8,     t7
+    addi.w           t3,      t3,     1
+    blt              t3,      a6,     .FILTERSIZE
+    srai.w           t8,      t8,     7
+    slt              t5,      t8,     t0
+    maskeqz          t8,      t8,     t5
+    masknez          t5,      t0,     t5
+    or               t8,      t8,     t5
+    slli.w           t4,      t1,     1
+    stx.h            t8,      a1,     t4
+    addi.w           t1,      t1,     1
+    blt              t1,      a2,     .DSTW
+    b                .END_LOOP
+
+.LOOP_DSTW8:
+    ld.w             t1,      a5,     0
+    ld.w             t2,      a5,     4
+    ld.w             t3,      a5,     8
+    ld.w             t4,      a5,     12
+    fldx.d           f1,      a3,     t1
+    fldx.d           f2,      a3,     t2
+    fldx.d           f3,      a3,     t3
+    fldx.d           f4,      a3,     t4
+    ld.w             t1,      a5,     16
+    ld.w             t2,      a5,     20
+    ld.w             t3,      a5,     24
+    ld.w             t4,      a5,     28
+    fldx.d           f5,      a3,     t1
+    fldx.d           f6,      a3,     t2
+    fldx.d           f7,      a3,     t3
+    fldx.d           f8,      a3,     t4
+    vld              vr9,     a4,     0
+    vld              vr10,    a4,     16
+    vld              vr11,    a4,     32
+    vld              vr12,    a4,     48
+    vld              vr13,    a4,     64
+    vld              vr14,    a4,     80
+    vld              vr15,    a4,     96
+    vld              vr16,    a4,     112
+    vilvl.b          vr1,     vr0,    vr1
+    vilvl.b          vr2,     vr0,    vr2
+    vilvl.b          vr3,     vr0,    vr3
+    vilvl.b          vr4,     vr0,    vr4
+    vilvl.b          vr5,     vr0,    vr5
+    vilvl.b          vr6,     vr0,    vr6
+    vilvl.b          vr7,     vr0,    vr7
+    vilvl.b          vr8,     vr0,    vr8
+
+    vdp2.w.h         vr17,    vr1,    vr9
+    vdp2.w.h         vr18,    vr2,    vr10
+    vdp2.w.h         vr19,    vr3,    vr11
+    vdp2.w.h         vr21,    vr4,    vr12
+    vdp2.w.h         vr1,     vr5,    vr13
+    vdp2.w.h         vr2,     vr6,    vr14
+    vdp2.w.h         vr3,     vr7,    vr15
+    vdp2.w.h         vr4,     vr8,    vr16
+    vhaddw.d.w       vr5,     vr1,    vr1
+    vhaddw.d.w       vr6,     vr2,    vr2
+    vhaddw.d.w       vr7,     vr3,    vr3
+    vhaddw.d.w       vr8,     vr4,    vr4
+    vhaddw.d.w       vr1,     vr17,   vr17
+    vhaddw.d.w       vr2,     vr18,   vr18
+    vhaddw.d.w       vr3,     vr19,   vr19
+    vhaddw.d.w       vr4,     vr21,   vr21
+    vhaddw.q.d       vr1,     vr1,    vr1
+    vhaddw.q.d       vr2,     vr2,    vr2
+    vhaddw.q.d       vr3,     vr3,    vr3
+    vhaddw.q.d       vr4,     vr4,    vr4
+    vhaddw.q.d       vr5,     vr5,    vr5
+    vhaddw.q.d       vr6,     vr6,    vr6
+    vhaddw.q.d       vr7,     vr7,    vr7
+    vhaddw.q.d       vr8,     vr8,    vr8
+    vilvl.w          vr1,     vr2,    vr1
+    vilvl.w          vr3,     vr4,    vr3
+    vilvl.w          vr5,     vr6,    vr5
+    vilvl.w          vr7,     vr8,    vr7
+    vilvl.d          vr1,     vr3,    vr1
+    vilvl.d          vr5,     vr7,    vr5
+    vsrai.w          vr1,     vr1,    7
+    vsrai.w          vr5,     vr5,    7
+    vmin.w           vr1,     vr1,    vr20
+    vmin.w           vr5,     vr5,    vr20
+
+    vpickev.h        vr1,     vr5,    vr1
+    vst              vr1,     a1,     0
+    addi.d           a1,      a1,     16
+    addi.d           a5,      a5,     32
+    addi.d           a4,      a4,     128
+    addi.d           a2,      a2,     -8
+    bge              a2,      t8,     .LOOP_DSTW8
+    blt              zero,    a2,     .RES8
+    b                .END_LOOP
+.RES8:
+    li.w             t1,      0
+.DSTW8:
+    slli.w           t2,      t1,     2
+    ldx.w            t2,      a5,     t2
+    li.w             t3,      0
+    li.w             t8,      0
+.FILTERSIZE8:
+    add.w            t4,      t2,     t3
+    ldx.bu           t5,      a3,     t4
+    mul.w            t6,      a6,     t1
+    add.w            t6,      t6,     t3
+    slli.w           t7,      t6,     1
+    ldx.h            t7,      a4,     t7
+    mul.w            t7,      t5,     t7
+    add.w            t8,      t8,     t7
+    addi.w           t3,      t3,     1
+    blt              t3,      a6,     .FILTERSIZE8
+    srai.w           t8,      t8,     7
+    slt              t5,      t8,     t0
+    maskeqz          t8,      t8,     t5
+    masknez          t5,      t0,     t5
+    or               t8,      t8,     t5
+    slli.w           t4,      t1,     1
+    stx.h            t8,      a1,     t4
+    addi.w           t1,      t1,     1
+    blt              t1,      a2,     .DSTW8
+    b                .END_LOOP
+
+.LOOP_DSTW4:
+    ld.w             t1,      a5,     0
+    ld.w             t2,      a5,     4
+    ld.w             t3,      a5,     8
+    ld.w             t4,      a5,     12
+    fldx.s           f1,      a3,     t1
+    fldx.s           f2,      a3,     t2
+    fldx.s           f3,      a3,     t3
+    fldx.s           f4,      a3,     t4
+    ld.w             t1,      a5,     16
+    ld.w             t2,      a5,     20
+    ld.w             t3,      a5,     24
+    ld.w             t4,      a5,     28
+    fldx.s           f5,      a3,     t1
+    fldx.s           f6,      a3,     t2
+    fldx.s           f7,      a3,     t3
+    fldx.s           f8,      a3,     t4
+    vld              vr9,     a4,     0
+    vld              vr10,    a4,     16
+    vld              vr11,    a4,     32
+    vld              vr12,    a4,     48
+    vilvl.w          vr1,     vr2,    vr1
+    vilvl.w          vr3,     vr4,    vr3
+    vilvl.w          vr5,     vr6,    vr5
+    vilvl.w          vr7,     vr8,    vr7
+    vilvl.b          vr1,     vr0,    vr1
+    vilvl.b          vr3,     vr0,    vr3
+    vilvl.b          vr5,     vr0,    vr5
+    vilvl.b          vr7,     vr0,    vr7
+
+    vdp2.w.h         vr13,    vr1,    vr9
+    vdp2.w.h         vr14,    vr3,    vr10
+    vdp2.w.h         vr15,    vr5,    vr11
+    vdp2.w.h         vr16,    vr7,    vr12
+    vhaddw.d.w       vr13,    vr13,   vr13
+    vhaddw.d.w       vr14,    vr14,   vr14
+    vhaddw.d.w       vr15,    vr15,   vr15
+    vhaddw.d.w       vr16,    vr16,   vr16
+    vpickev.w        vr13,    vr14,   vr13
+    vpickev.w        vr15,    vr16,   vr15
+    vsrai.w          vr13,    vr13,   7
+    vsrai.w          vr15,    vr15,   7
+    vmin.w           vr13,    vr13,   vr20
+    vmin.w           vr15,    vr15,   vr20
+
+    vpickev.h        vr13,    vr15,   vr13
+    vst              vr13,    a1,     0
+    addi.d           a1,      a1,     16
+    addi.d           a5,      a5,     32
+    addi.d           a4,      a4,     64
+    addi.d           a2,      a2,     -8
+    bge              a2,      t8,     .LOOP_DSTW4
+    blt              zero,    a2,     .RES4
+    b                .END_LOOP
+.RES4:
+    li.w             t1,      0
+.DSTW4:
+    slli.w           t2,      t1,     2
+    ldx.w            t2,      a5,     t2
+    li.w             t3,      0
+    li.w             t8,      0
+.FILTERSIZE4:
+    add.w            t4,      t2,     t3
+    ldx.bu           t5,      a3,     t4
+    mul.w            t6,      a6,     t1
+    add.w            t6,      t6,     t3
+    slli.w           t7,      t6,     1
+    ldx.h            t7,      a4,     t7
+    mul.w            t7,      t5,     t7
+    add.w            t8,      t8,     t7
+    addi.w           t3,      t3,     1
+    blt              t3,      a6,     .FILTERSIZE4
+    srai.w           t8,      t8,     7
+    slt              t5,      t8,     t0
+    maskeqz          t8,      t8,     t5
+    masknez          t5,      t0,     t5
+    or               t8,      t8,     t5
+    slli.w           t4,      t1,     1
+    stx.h            t8,      a1,     t4
+    addi.w           t1,      t1,     1
+    blt              t1,      a2,     .DSTW4
+    b                .END_LOOP
+.END_DSTW4:
+
+    li.w             t1,      0
+.LOOP_DSTW1:
+    slli.w           t2,      t1,     2
+    ldx.w            t2,      a5,     t2
+    li.w             t3,      0
+    li.w             t8,      0
+.FILTERSIZE1:
+    add.w            t4,      t2,     t3
+    ldx.bu           t5,      a3,     t4
+    mul.w            t6,      a6,     t1
+    add.w            t6,      t6,     t3
+    slli.w           t7,      t6,     1
+    ldx.h            t7,      a4,     t7
+    mul.w            t7,      t5,     t7
+    add.w            t8,      t8,     t7
+    addi.w           t3,      t3,     1
+    blt              t3,      a6,     .FILTERSIZE1
+    srai.w           t8,      t8,     7
+    slt              t5,      t8,     t0
+    maskeqz          t8,      t8,     t5
+    masknez          t5,      t0,     t5
+    or               t8,      t8,     t5
+    slli.w           t4,      t1,     1
+    stx.h            t8,      a1,     t4
+    addi.w           t1,      t1,     1
+    blt              t1,      a2,     .LOOP_DSTW1
+    b                .END_LOOP
+.END_LOOP:
+
+    ld.d             s0,      sp,     0
+    ld.d             s1,      sp,     8
+    ld.d             s2,      sp,     16
+    ld.d             s3,      sp,     24
+    ld.d             s4,      sp,     32
+    ld.d             s5,      sp,     40
+    ld.d             s6,      sp,     48
+    ld.d             s7,      sp,     56
+    ld.d             s8,      sp,     64
+    addi.d           sp,      sp,     72
+endfunc
+
+/* void ff_hscale_8_to_19_lsx(SwsContext *c, int16_t *dst, int dstW,
+ *                            const uint8_t *src, const int16_t *filter,
+ *                            const int32_t *filterPos, int filterSize)
+ */
+function ff_hscale_8_to_19_lsx
+    addi.d           sp,      sp,     -72
+    st.d             s0,      sp,     0
+    st.d             s1,      sp,     8
+    st.d             s2,      sp,     16
+    st.d             s3,      sp,     24
+    st.d             s4,      sp,     32
+    st.d             s5,      sp,     40
+    st.d             s6,      sp,     48
+    st.d             s7,      sp,     56
+    st.d             s8,      sp,     64
+    li.w             t0,      524287
+    li.w             t8,      8
+    li.w             t7,      4
+    vldi             vr0,     0
+    vreplgr2vr.w     vr20,    t0
+    beq              a6,      t7,     .LOOP_DST4
+    beq              a6,      t8,     .LOOP_DST8
+    blt              t8,      a6,     .LOOP
+    b                .END_DST4
+
+.LOOP:
+    li.w             t1,      0
+    li.w             s1,      0
+    li.w             s2,      0
+    li.w             s3,      0
+    li.w             s4,      0
+    li.w             s5,      0
+    vldi             vr22,    0
+    addi.w           s0,      a6,     -7
+    slli.w           s7,      a6,     1
+    slli.w           s8,      a6,     2
+    add.w            t6,      s7,     s8
+.LOOP_DST:
+    ld.w             t2,      a5,     0
+    ld.w             t3,      a5,     4
+    ld.w             t4,      a5,     8
+    ld.w             t5,      a5,     12
+    fldx.d           f1,      a3,     t2
+    fldx.d           f2,      a3,     t3
+    fldx.d           f3,      a3,     t4
+    fldx.d           f4,      a3,     t5
+    vld              vr9,     a4,     0
+    vldx             vr10,    a4,     s7
+    vldx             vr11,    a4,     s8
+    vldx             vr12,    a4,     t6
+    vilvl.b          vr1,     vr0,    vr1
+    vilvl.b          vr2,     vr0,    vr2
+    vilvl.b          vr3,     vr0,    vr3
+    vilvl.b          vr4,     vr0,    vr4
+    vdp2.w.h         vr17,    vr1,    vr9
+    vdp2.w.h         vr18,    vr2,    vr10
+    vdp2.w.h         vr19,    vr3,    vr11
+    vdp2.w.h         vr21,    vr4,    vr12
+    vhaddw.d.w       vr1,     vr17,   vr17
+    vhaddw.d.w       vr2,     vr18,   vr18
+    vhaddw.d.w       vr3,     vr19,   vr19
+    vhaddw.d.w       vr4,     vr21,   vr21
+    vhaddw.q.d       vr1,     vr1,    vr1
+    vhaddw.q.d       vr2,     vr2,    vr2
+    vhaddw.q.d       vr3,     vr3,    vr3
+    vhaddw.q.d       vr4,     vr4,    vr4
+    vilvl.w          vr1,     vr2,    vr1
+    vilvl.w          vr3,     vr4,    vr3
+    vilvl.d          vr1,     vr3,    vr1
+    vadd.w           vr22,    vr22,   vr1
+    addi.w           s1,      s1,     8
+    addi.d           a3,      a3,     8
+    addi.d           a4,      a4,     16
+    blt              s1,      s0,     .LOOP_DST
+    blt              s1,      a6,     .DSTA
+    b                .END_FILTERA
+.DSTA:
+    ld.w             t2,      a5,     0
+    li.w             t3,      0
+    move             s6,      s1
+.FILTERA:
+    add.w            t4,      t2,     t3
+    ldx.bu           t5,      a3,     t4
+    mul.w            t6,      a6,     t1
+    add.w            t6,      t6,     t3
+    slli.w           t6,      t6,     1
+    ldx.h            t6,      a4,     t6
+    mul.w            t6,      t5,     t6
+    add.w            s2,      s2,     t6
+    addi.w           t3,      t3,     1
+    addi.w           s6,      s6,     1
+    blt              s6,      a6,     .FILTERA
+
+    ld.w             t2,      a5,     4
+    li.w             t3,      0
+    move             s6,      s1
+    addi.w           t1,      t1,     1
+.FILTERB:
+    add.w            t4,      t2,     t3
+    ldx.bu           t5,      a3,     t4
+    mul.w            t6,      a6,     t1
+    add.w            t6,      t6,     t3
+    slli.w           t6,      t6,     1
+    ldx.h            t6,      a4,     t6
+    mul.w            t6,      t5,     t6
+    add.w            s3,      s3,     t6
+    addi.w           t3,      t3,     1
+    addi.w           s6,      s6,     1
+    blt              s6,      a6,     .FILTERB
+    ld.w             t2,      a5,     8
+    addi.w           t1,      t1,     1
+    li.w             t3,      0
+    move             s6,      s1
+.FILTERC:
+    add.w            t4,      t2,     t3
+    ldx.bu           t5,      a3,     t4
+    mul.w            t6,      a6,     t1
+    add.w            t6,      t6,     t3
+    slli.w           t6,      t6,     1
+    ldx.h            t6,      a4,     t6
+    mul.w            t6,      t5,     t6
+    add.w            s4,      s4,     t6
+    addi.w           t3,      t3,     1
+    addi.w           s6,      s6,     1
+    blt              s6,      a6,     .FILTERC
+    ld.w             t2,      a5,     12
+    addi.w           t1,      t1,     1
+    move             s6,      s1
+    li.w             t3,      0
+.FILTERD:
+    add.w            t4,      t2,     t3
+    ldx.bu           t5,      a3,     t4
+    mul.w            t6,      a6,     t1
+    add.w            t6,      t6,     t3
+    slli.w           t6,      t6,     1
+    ldx.h            t6,      a4,     t6
+    mul.w            t6,      t5,     t6
+    add.w            s5,      s5,     t6
+    addi.w           t3,      t3,     1
+    addi.w           s6,      s6,     1
+    blt              s6,      a6,     .FILTERD
+.END_FILTERA:
+    vpickve2gr.w     t1,      vr22,   0
+    vpickve2gr.w     t2,      vr22,   1
+    vpickve2gr.w     t3,      vr22,   2
+    vpickve2gr.w     t4,      vr22,   3
+    add.w            s2,      s2,     t1
+    add.w            s3,      s3,     t2
+    add.w            s4,      s4,     t3
+    add.w            s5,      s5,     t4
+    srai.w           s2,      s2,     3
+    srai.w           s3,      s3,     3
+    srai.w           s4,      s4,     3
+    srai.w           s5,      s5,     3
+    slt              t1,      s2,     t0
+    slt              t2,      s3,     t0
+    slt              t3,      s4,     t0
+    slt              t4,      s5,     t0
+    maskeqz          s2,      s2,     t1
+    maskeqz          s3,      s3,     t2
+    maskeqz          s4,      s4,     t3
+    maskeqz          s5,      s5,     t4
+    masknez          t1,      t0,     t1
+    masknez          t2,      t0,     t2
+    masknez          t3,      t0,     t3
+    masknez          t4,      t0,     t4
+    or               s2,      s2,     t1
+    or               s3,      s3,     t2
+    or               s4,      s4,     t3
+    or               s5,      s5,     t4
+    st.w             s2,      a1,     0
+    st.w             s3,      a1,     4
+    st.w             s4,      a1,     8
+    st.w             s5,      a1,     12
+
+    addi.d           a1,      a1,     16
+    sub.d            a3,      a3,     s1
+    addi.d           a5,      a5,     16
+    slli.d           t3,      a6,     3
+    add.d            a4,      a4,     t3
+    sub.d            a4,      a4,     s1
+    sub.d            a4,      a4,     s1
+    addi.d           a2,      a2,     -4
+    bge              a2,      t7,     .LOOP
+    blt              zero,    a2,     .RESA
+    b                .END
+.RESA:
+    li.w             t1,      0
+.DST:
+    slli.w           t2,      t1,     2
+    ldx.w            t2,      a5,     t2
+    li.w             t3,      0
+    li.w             t8,      0
+.FILTER:
+    add.w            t4,      t2,     t3
+    ldx.bu           t5,      a3,     t4
+    mul.w            t6,      a6,     t1
+    add.w            t6,      t6,     t3
+    slli.w           t7,      t6,     1
+    ldx.h            t7,      a4,     t7
+    mul.w            t7,      t5,     t7
+    add.w            t8,      t8,     t7
+    addi.w           t3,      t3,     1
+    blt              t3,      a6,     .FILTER
+    srai.w           t8,      t8,     3
+    slt              t5,      t8,     t0
+    maskeqz          t8,      t8,     t5
+    masknez          t5,      t0,     t5
+    or               t8,      t8,     t5
+    slli.w           t4,      t1,     2
+    stx.w            t8,      a1,     t4
+    addi.w           t1,      t1,     1
+    blt              t1,      a2,     .DST
+    b                .END
+
+.LOOP_DST8:
+    ld.w             t1,      a5,     0
+    ld.w             t2,      a5,     4
+    ld.w             t3,      a5,     8
+    ld.w             t4,      a5,     12
+    fldx.d           f1,      a3,     t1
+    fldx.d           f2,      a3,     t2
+    fldx.d           f3,      a3,     t3
+    fldx.d           f4,      a3,     t4
+    ld.w             t1,      a5,     16
+    ld.w             t2,      a5,     20
+    ld.w             t3,      a5,     24
+    ld.w             t4,      a5,     28
+    fldx.d           f5,      a3,     t1
+    fldx.d           f6,      a3,     t2
+    fldx.d           f7,      a3,     t3
+    fldx.d           f8,      a3,     t4
+    vld              vr9,     a4,     0
+    vld              vr10,    a4,     16
+    vld              vr11,    a4,     32
+    vld              vr12,    a4,     48
+    vld              vr13,    a4,     64
+    vld              vr14,    a4,     80
+    vld              vr15,    a4,     96
+    vld              vr16,    a4,     112
+    vilvl.b          vr1,     vr0,    vr1
+    vilvl.b          vr2,     vr0,    vr2
+    vilvl.b          vr3,     vr0,    vr3
+    vilvl.b          vr4,     vr0,    vr4
+    vilvl.b          vr5,     vr0,    vr5
+    vilvl.b          vr6,     vr0,    vr6
+    vilvl.b          vr7,     vr0,    vr7
+    vilvl.b          vr8,     vr0,    vr8
+
+    vdp2.w.h         vr17,    vr1,    vr9
+    vdp2.w.h         vr18,    vr2,    vr10
+    vdp2.w.h         vr19,    vr3,    vr11
+    vdp2.w.h         vr21,    vr4,    vr12
+    vdp2.w.h         vr1,     vr5,    vr13
+    vdp2.w.h         vr2,     vr6,    vr14
+    vdp2.w.h         vr3,     vr7,    vr15
+    vdp2.w.h         vr4,     vr8,    vr16
+    vhaddw.d.w       vr5,     vr1,    vr1
+    vhaddw.d.w       vr6,     vr2,    vr2
+    vhaddw.d.w       vr7,     vr3,    vr3
+    vhaddw.d.w       vr8,     vr4,    vr4
+    vhaddw.d.w       vr1,     vr17,   vr17
+    vhaddw.d.w       vr2,     vr18,   vr18
+    vhaddw.d.w       vr3,     vr19,   vr19
+    vhaddw.d.w       vr4,     vr21,   vr21
+    vhaddw.q.d       vr1,     vr1,    vr1
+    vhaddw.q.d       vr2,     vr2,    vr2
+    vhaddw.q.d       vr3,     vr3,    vr3
+    vhaddw.q.d       vr4,     vr4,    vr4
+    vhaddw.q.d       vr5,     vr5,    vr5
+    vhaddw.q.d       vr6,     vr6,    vr6
+    vhaddw.q.d       vr7,     vr7,    vr7
+    vhaddw.q.d       vr8,     vr8,    vr8
+    vilvl.w          vr1,     vr2,    vr1
+    vilvl.w          vr3,     vr4,    vr3
+    vilvl.w          vr5,     vr6,    vr5
+    vilvl.w          vr7,     vr8,    vr7
+    vilvl.d          vr1,     vr3,    vr1
+    vilvl.d          vr5,     vr7,    vr5
+    vsrai.w          vr1,     vr1,    3
+    vsrai.w          vr5,     vr5,    3
+    vmin.w           vr1,     vr1,    vr20
+    vmin.w           vr5,     vr5,    vr20
+
+    vst              vr1,     a1,     0
+    vst              vr5,     a1,     16
+    addi.d           a1,      a1,     32
+    addi.d           a5,      a5,     32
+    addi.d           a4,      a4,     128
+    addi.d           a2,      a2,     -8
+    bge              a2,      t8,     .LOOP_DST8
+    blt              zero,    a2,     .REST8
+    b                .END
+.REST8:
+    li.w             t1,      0
+.DST8:
+    slli.w           t2,      t1,     2
+    ldx.w            t2,      a5,     t2
+    li.w             t3,      0
+    li.w             t8,      0
+.FILTER8:
+    add.w            t4,      t2,     t3
+    ldx.bu           t5,      a3,     t4
+    mul.w            t6,      a6,     t1
+    add.w            t6,      t6,     t3
+    slli.w           t7,      t6,     1
+    ldx.h            t7,      a4,     t7
+    mul.w            t7,      t5,     t7
+    add.w            t8,      t8,     t7
+    addi.w           t3,      t3,     1
+    blt              t3,      a6,     .FILTER8
+    srai.w           t8,      t8,     3
+    slt              t5,      t8,     t0
+    maskeqz          t8,      t8,     t5
+    masknez          t5,      t0,     t5
+    or               t8,      t8,     t5
+    slli.w           t4,      t1,     2
+    stx.w            t8,      a1,     t4
+    addi.w           t1,      t1,     1
+    blt              t1,      a2,     .DST8
+    b                .END
+
+.LOOP_DST4:
+    ld.w             t1,      a5,     0
+    ld.w             t2,      a5,     4
+    ld.w             t3,      a5,     8
+    ld.w             t4,      a5,     12
+    fldx.s           f1,      a3,     t1
+    fldx.s           f2,      a3,     t2
+    fldx.s           f3,      a3,     t3
+    fldx.s           f4,      a3,     t4
+    ld.w             t1,      a5,     16
+    ld.w             t2,      a5,     20
+    ld.w             t3,      a5,     24
+    ld.w             t4,      a5,     28
+    fldx.s           f5,      a3,     t1
+    fldx.s           f6,      a3,     t2
+    fldx.s           f7,      a3,     t3
+    fldx.s           f8,      a3,     t4
+    vld              vr9,     a4,     0
+    vld              vr10,    a4,     16
+    vld              vr11,    a4,     32
+    vld              vr12,    a4,     48
+    vilvl.w          vr1,     vr2,    vr1
+    vilvl.w          vr3,     vr4,    vr3
+    vilvl.w          vr5,     vr6,    vr5
+    vilvl.w          vr7,     vr8,    vr7
+    vilvl.b          vr1,     vr0,    vr1
+    vilvl.b          vr3,     vr0,    vr3
+    vilvl.b          vr5,     vr0,    vr5
+    vilvl.b          vr7,     vr0,    vr7
+
+    vdp2.w.h         vr13,    vr1,    vr9
+    vdp2.w.h         vr14,    vr3,    vr10
+    vdp2.w.h         vr15,    vr5,    vr11
+    vdp2.w.h         vr16,    vr7,    vr12
+    vhaddw.d.w       vr13,    vr13,   vr13
+    vhaddw.d.w       vr14,    vr14,   vr14
+    vhaddw.d.w       vr15,    vr15,   vr15
+    vhaddw.d.w       vr16,    vr16,   vr16
+    vpickev.w        vr13,    vr14,   vr13
+    vpickev.w        vr15,    vr16,   vr15
+    vsrai.w          vr13,    vr13,   3
+    vsrai.w          vr15,    vr15,   3
+    vmin.w           vr13,    vr13,   vr20
+    vmin.w           vr15,    vr15,   vr20
+
+    vst              vr13,    a1,     0
+    vst              vr15,    a1,     16
+    addi.d           a1,      a1,     32
+    addi.d           a5,      a5,     32
+    addi.d           a4,      a4,     64
+    addi.d           a2,      a2,     -8
+    bge              a2,      t8,     .LOOP_DST4
+    blt              zero,    a2,     .REST4
+    b                .END
+.REST4:
+    li.w             t1,      0
+.DST4:
+    slli.w           t2,      t1,     2
+    ldx.w            t2,      a5,     t2
+    li.w             t3,      0
+    li.w             t8,      0
+.FILTER4:
+    add.w            t4,      t2,     t3
+    ldx.bu           t5,      a3,     t4
+    mul.w            t6,      a6,     t1
+    add.w            t6,      t6,     t3
+    slli.w           t7,      t6,     1
+    ldx.h            t7,      a4,     t7
+    mul.w            t7,      t5,     t7
+    add.w            t8,      t8,     t7
+    addi.w           t3,      t3,     1
+    blt              t3,      a6,     .FILTER4
+    srai.w           t8,      t8,     3
+    slt              t5,      t8,     t0
+    maskeqz          t8,      t8,     t5
+    masknez          t5,      t0,     t5
+    or               t8,      t8,     t5
+    slli.w           t4,      t1,     2
+    stx.w            t8,      a1,     t4
+    addi.w           t1,      t1,     1
+    blt              t1,      a2,     .DST4
+    b                .END
+.END_DST4:
+
+    li.w             t1,      0
+.LOOP_DST1:
+    slli.w           t2,      t1,     2
+    ldx.w            t2,      a5,     t2
+    li.w             t3,      0
+    li.w             t8,      0
+.FILTER1:
+    add.w            t4,      t2,     t3
+    ldx.bu           t5,      a3,     t4
+    mul.w            t6,      a6,     t1
+    add.w            t6,      t6,     t3
+    slli.w           t7,      t6,     1
+    ldx.h            t7,      a4,     t7
+    mul.w            t7,      t5,     t7
+    add.w            t8,      t8,     t7
+    addi.w           t3,      t3,     1
+    blt              t3,      a6,     .FILTER1
+    srai.w           t8,      t8,     3
+    slt              t5,      t8,     t0
+    maskeqz          t8,      t8,     t5
+    masknez          t5,      t0,     t5
+    or               t8,      t8,     t5
+    slli.w           t4,      t1,     2
+    stx.w            t8,      a1,     t4
+    addi.w           t1,      t1,     1
+    blt              t1,      a2,     .LOOP_DST1
+    b                .END
+.END:
+
+    ld.d             s0,      sp,     0
+    ld.d             s1,      sp,     8
+    ld.d             s2,      sp,     16
+    ld.d             s3,      sp,     24
+    ld.d             s4,      sp,     32
+    ld.d             s5,      sp,     40
+    ld.d             s6,      sp,     48
+    ld.d             s7,      sp,     56
+    ld.d             s8,      sp,     64
+    addi.d           sp,      sp,     72
+endfunc
+
+/* void ff_hscale_16_to_15_sub_lsx(SwsContext *c, int16_t *dst, int dstW,
+ *                                 const uint8_t *src, const int16_t *filter,
+ *                                 const int32_t *filterPos, int filterSize, int sh)
+ */
+function ff_hscale_16_to_15_sub_lsx
+    addi.d           sp,      sp,     -72
+    st.d             s0,      sp,     0
+    st.d             s1,      sp,     8
+    st.d             s2,      sp,     16
+    st.d             s3,      sp,     24
+    st.d             s4,      sp,     32
+    st.d             s5,      sp,     40
+    st.d             s6,      sp,     48
+    st.d             s7,      sp,     56
+    st.d             s8,      sp,     64
+    li.w             t0,      32767
+    li.w             t8,      8
+    li.w             t7,      4
+    vreplgr2vr.w     vr20,    t0
+    vreplgr2vr.w     vr0,     a7
+    beq              a6,      t7,     .LOOP_HS15_DST4
+    beq              a6,      t8,     .LOOP_HS15_DST8
+    blt              t8,      a6,     .LOOP_HS15
+    b                .END_HS15_DST4
+
+.LOOP_HS15:
+    li.w             t1,      0
+    li.w             s1,      0
+    li.w             s2,      0
+    li.w             s3,      0
+    li.w             s4,      0
+    li.w             s5,      0
+    vldi             vr22,    0
+    addi.w           s0,      a6,     -7
+    slli.w           s7,      a6,     1
+    slli.w           s8,      a6,     2
+    add.w            t6,      s7,     s8
+.LOOP_HS15_DST:
+    ld.w             t2,      a5,     0
+    ld.w             t3,      a5,     4
+    ld.w             t4,      a5,     8
+    ld.w             t5,      a5,     12
+    slli.w           t2,      t2,     1
+    slli.w           t3,      t3,     1
+    slli.w           t4,      t4,     1
+    slli.w           t5,      t5,     1
+    vldx             vr1,     a3,     t2
+    vldx             vr2,     a3,     t3
+    vldx             vr3,     a3,     t4
+    vldx             vr4,     a3,     t5
+    vld              vr9,     a4,     0
+    vldx             vr10,    a4,     s7
+    vldx             vr11,    a4,     s8
+    vldx             vr12,    a4,     t6
+    vmulwev.w.hu.h   vr17,    vr1,    vr9
+    vmulwev.w.hu.h   vr18,    vr2,    vr10
+    vmulwev.w.hu.h   vr19,    vr3,    vr11
+    vmulwev.w.hu.h   vr21,    vr4,    vr12
+    vmaddwod.w.hu.h  vr17,    vr1,    vr9
+    vmaddwod.w.hu.h  vr18,    vr2,    vr10
+    vmaddwod.w.hu.h  vr19,    vr3,    vr11
+    vmaddwod.w.hu.h  vr21,    vr4,    vr12
+    vhaddw.d.w       vr1,     vr17,   vr17
+    vhaddw.d.w       vr2,     vr18,   vr18
+    vhaddw.d.w       vr3,     vr19,   vr19
+    vhaddw.d.w       vr4,     vr21,   vr21
+    vhaddw.q.d       vr1,     vr1,    vr1
+    vhaddw.q.d       vr2,     vr2,    vr2
+    vhaddw.q.d       vr3,     vr3,    vr3
+    vhaddw.q.d       vr4,     vr4,    vr4
+    vilvl.w          vr1,     vr2,    vr1
+    vilvl.w          vr3,     vr4,    vr3
+    vilvl.d          vr1,     vr3,    vr1
+    vadd.w           vr22,    vr22,   vr1
+    addi.w           s1,      s1,     8
+    addi.d           a3,      a3,     16
+    addi.d           a4,      a4,     16
+    blt              s1,      s0,     .LOOP_HS15_DST
+    blt              s1,      a6,     .HS15_DSTA
+    b                .END_HS15_FILTERA
+.HS15_DSTA:
+    ld.w             t2,      a5,     0
+    li.w             t3,      0
+    move             s6,      s1
+.HS15_FILTERA:
+    add.w            t4,      t2,     t3
+    slli.w           t4,      t4,     1
+    ldx.hu           t5,      a3,     t4
+    mul.w            t6,      a6,     t1
+    add.w            t6,      t6,     t3
+    slli.w           t6,      t6,     1
+    ldx.h            t6,      a4,     t6
+    mul.w            t6,      t5,     t6
+    add.w            s2,      s2,     t6
+    addi.w           t3,      t3,     1
+    addi.w           s6,      s6,     1
+    blt              s6,      a6,     .HS15_FILTERA
+
+    ld.w             t2,      a5,     4
+    li.w             t3,      0
+    move             s6,      s1
+    addi.w           t1,      t1,     1
+.HS15_FILTERB:
+    add.w            t4,      t2,     t3
+    slli.w           t4,      t4,     1
+    ldx.hu           t5,      a3,     t4
+    mul.w            t6,      a6,     t1
+    add.w            t6,      t6,     t3
+    slli.w           t6,      t6,     1
+    ldx.h            t6,      a4,     t6
+    mul.w            t6,      t5,     t6
+    add.w            s3,      s3,     t6
+    addi.w           t3,      t3,     1
+    addi.w           s6,      s6,     1
+    blt              s6,      a6,     .HS15_FILTERB
+    ld.w             t2,      a5,     8
+    addi.w           t1,      t1,     1
+    li.w             t3,      0
+    move             s6,      s1
+.HS15_FILTERC:
+    add.w            t4,      t2,     t3
+    slli.w           t4,      t4,     1
+    ldx.hu           t5,      a3,     t4
+    mul.w            t6,      a6,     t1
+    add.w            t6,      t6,     t3
+    slli.w           t6,      t6,     1
+    ldx.h            t6,      a4,     t6
+    mul.w            t6,      t5,     t6
+    add.w            s4,      s4,     t6
+    addi.w           t3,      t3,     1
+    addi.w           s6,      s6,     1
+    blt              s6,      a6,     .HS15_FILTERC
+    ld.w             t2,      a5,     12
+    addi.w           t1,      t1,     1
+    move             s6,      s1
+    li.w             t3,      0
+.HS15_FILTERD:
+    add.w            t4,      t2,     t3
+    slli.w           t4,      t4,     1
+    ldx.hu           t5,      a3,     t4
+    mul.w            t6,      a6,     t1
+    add.w            t6,      t6,     t3
+    slli.w           t6,      t6,     1
+    ldx.h            t6,      a4,     t6
+    mul.w            t6,      t5,     t6
+    add.w            s5,      s5,     t6
+    addi.w           t3,      t3,     1
+    addi.w           s6,      s6,     1
+    blt              s6,      a6,     .HS15_FILTERD
+.END_HS15_FILTERA:
+    vpickve2gr.w     t1,      vr22,   0
+    vpickve2gr.w     t2,      vr22,   1
+    vpickve2gr.w     t3,      vr22,   2
+    vpickve2gr.w     t4,      vr22,   3
+    add.w            s2,      s2,     t1
+    add.w            s3,      s3,     t2
+    add.w            s4,      s4,     t3
+    add.w            s5,      s5,     t4
+    sra.w            s2,      s2,     a7
+    sra.w            s3,      s3,     a7
+    sra.w            s4,      s4,     a7
+    sra.w            s5,      s5,     a7
+    slt              t1,      s2,     t0
+    slt              t2,      s3,     t0
+    slt              t3,      s4,     t0
+    slt              t4,      s5,     t0
+    maskeqz          s2,      s2,     t1
+    maskeqz          s3,      s3,     t2
+    maskeqz          s4,      s4,     t3
+    maskeqz          s5,      s5,     t4
+    masknez          t1,      t0,     t1
+    masknez          t2,      t0,     t2
+    masknez          t3,      t0,     t3
+    masknez          t4,      t0,     t4
+    or               s2,      s2,     t1
+    or               s3,      s3,     t2
+    or               s4,      s4,     t3
+    or               s5,      s5,     t4
+    st.h             s2,      a1,     0
+    st.h             s3,      a1,     2
+    st.h             s4,      a1,     4
+    st.h             s5,      a1,     6
+
+    addi.d           a1,      a1,     8
+    sub.d            a3,      a3,     s1
+    sub.d            a3,      a3,     s1
+    addi.d           a5,      a5,     16
+    slli.d           t3,      a6,     3
+    add.d            a4,      a4,     t3
+    sub.d            a4,      a4,     s1
+    sub.d            a4,      a4,     s1
+    addi.d           a2,      a2,     -4
+    bge              a2,      t7,     .LOOP_HS15
+    blt              zero,    a2,     .HS15_RESA
+    b                .HS15_END
+.HS15_RESA:
+    li.w             t1,      0
+.HS15_DST:
+    slli.w           t2,      t1,     2
+    ldx.w            t2,      a5,     t2
+    li.w             t3,      0
+    li.w             t8,      0
+.HS15_FILTER:
+    add.w            t4,      t2,     t3
+    slli.w           t4,      t4,     1
+    ldx.hu           t5,      a3,     t4
+    mul.w            t6,      a6,     t1
+    add.w            t6,      t6,     t3
+    slli.w           t7,      t6,     1
+    ldx.h            t7,      a4,     t7
+    mul.w            t7,      t5,     t7
+    add.w            t8,      t8,     t7
+    addi.w           t3,      t3,     1
+    blt              t3,      a6,     .HS15_FILTER
+    sra.w            t8,      t8,     a7
+    slt              t5,      t8,     t0
+    maskeqz          t8,      t8,     t5
+    masknez          t5,      t0,     t5
+    or               t8,      t8,     t5
+    slli.w           t4,      t1,     1
+    stx.h            t8,      a1,     t4
+    addi.w           t1,      t1,     1
+    blt              t1,      a2,     .HS15_DST
+    b                .HS15_END
+
+.LOOP_HS15_DST8:
+    ld.w             t1,      a5,     0
+    ld.w             t2,      a5,     4
+    ld.w             t3,      a5,     8
+    ld.w             t4,      a5,     12
+    slli.w           t1,      t1,     1
+    slli.w           t2,      t2,     1
+    slli.w           t3,      t3,     1
+    slli.w           t4,      t4,     1
+    vldx             vr1,     a3,     t1
+    vldx             vr2,     a3,     t2
+    vldx             vr3,     a3,     t3
+    vldx             vr4,     a3,     t4
+    ld.w             t1,      a5,     16
+    ld.w             t2,      a5,     20
+    ld.w             t3,      a5,     24
+    ld.w             t4,      a5,     28
+    slli.w           t1,      t1,     1
+    slli.w           t2,      t2,     1
+    slli.w           t3,      t3,     1
+    slli.w           t4,      t4,     1
+    vldx             vr5,     a3,     t1
+    vldx             vr6,     a3,     t2
+    vldx             vr7,     a3,     t3
+    vldx             vr8,     a3,     t4
+    vld              vr9,     a4,     0
+    vld              vr10,    a4,     16
+    vld              vr11,    a4,     32
+    vld              vr12,    a4,     48
+    vld              vr13,    a4,     64
+    vld              vr14,    a4,     80
+    vld              vr15,    a4,     96
+    vld              vr16,    a4,     112
+
+    vmulwev.w.hu.h   vr17,    vr1,    vr9
+    vmulwev.w.hu.h   vr18,    vr2,    vr10
+    vmulwev.w.hu.h   vr19,    vr3,    vr11
+    vmulwev.w.hu.h   vr21,    vr4,    vr12
+    vmaddwod.w.hu.h  vr17,    vr1,    vr9
+    vmaddwod.w.hu.h  vr18,    vr2,    vr10
+    vmaddwod.w.hu.h  vr19,    vr3,    vr11
+    vmaddwod.w.hu.h  vr21,    vr4,    vr12
+    vmulwev.w.hu.h   vr1,     vr5,    vr13
+    vmulwev.w.hu.h   vr2,     vr6,    vr14
+    vmulwev.w.hu.h   vr3,     vr7,    vr15
+    vmulwev.w.hu.h   vr4,     vr8,    vr16
+    vmaddwod.w.hu.h  vr1,     vr5,    vr13
+    vmaddwod.w.hu.h  vr2,     vr6,    vr14
+    vmaddwod.w.hu.h  vr3,     vr7,    vr15
+    vmaddwod.w.hu.h  vr4,     vr8,    vr16
+    vhaddw.d.w       vr5,     vr1,    vr1
+    vhaddw.d.w       vr6,     vr2,    vr2
+    vhaddw.d.w       vr7,     vr3,    vr3
+    vhaddw.d.w       vr8,     vr4,    vr4
+    vhaddw.d.w       vr1,     vr17,   vr17
+    vhaddw.d.w       vr2,     vr18,   vr18
+    vhaddw.d.w       vr3,     vr19,   vr19
+    vhaddw.d.w       vr4,     vr21,   vr21
+    vhaddw.q.d       vr1,     vr1,    vr1
+    vhaddw.q.d       vr2,     vr2,    vr2
+    vhaddw.q.d       vr3,     vr3,    vr3
+    vhaddw.q.d       vr4,     vr4,    vr4
+    vhaddw.q.d       vr5,     vr5,    vr5
+    vhaddw.q.d       vr6,     vr6,    vr6
+    vhaddw.q.d       vr7,     vr7,    vr7
+    vhaddw.q.d       vr8,     vr8,    vr8
+    vilvl.w          vr1,     vr2,    vr1
+    vilvl.w          vr3,     vr4,    vr3
+    vilvl.w          vr5,     vr6,    vr5
+    vilvl.w          vr7,     vr8,    vr7
+    vilvl.d          vr1,     vr3,    vr1
+    vilvl.d          vr5,     vr7,    vr5
+    vsra.w           vr1,     vr1,    vr0
+    vsra.w           vr5,     vr5,    vr0
+    vmin.w           vr1,     vr1,    vr20
+    vmin.w           vr5,     vr5,    vr20
+
+    vpickev.h        vr1,     vr5,    vr1
+    vst              vr1,     a1,     0
+    addi.d           a1,      a1,     16
+    addi.d           a5,      a5,     32
+    addi.d           a4,      a4,     128
+    addi.d           a2,      a2,     -8
+    bge              a2,      t8,     .LOOP_HS15_DST8
+    blt              zero,    a2,     .HS15_REST8
+    b                .HS15_END
+.HS15_REST8:
+    li.w             t1,      0
+.HS15_DST8:
+    slli.w           t2,      t1,     2
+    ldx.w            t2,      a5,     t2
+    li.w             t3,      0
+    li.w             t8,      0
+.HS15_FILTER8:
+    add.w            t4,      t2,     t3
+    slli.w           t4,      t4,     1
+    ldx.hu           t5,      a3,     t4
+    mul.w            t6,      a6,     t1
+    add.w            t6,      t6,     t3
+    slli.w           t7,      t6,     1
+    ldx.h            t7,      a4,     t7
+    mul.w            t7,      t5,     t7
+    add.w            t8,      t8,     t7
+    addi.w           t3,      t3,     1
+    blt              t3,      a6,     .HS15_FILTER8
+    sra.w            t8,      t8,     a7
+    slt              t5,      t8,     t0
+    maskeqz          t8,      t8,     t5
+    masknez          t5,      t0,     t5
+    or               t8,      t8,     t5
+    slli.w           t4,      t1,     1
+    stx.h            t8,      a1,     t4
+    addi.w           t1,      t1,     1
+    blt              t1,      a2,     .HS15_DST8
+    b                .HS15_END
+
+.LOOP_HS15_DST4:
+    ld.w             t1,      a5,     0
+    ld.w             t2,      a5,     4
+    ld.w             t3,      a5,     8
+    ld.w             t4,      a5,     12
+    slli.w           t1,      t1,     1
+    slli.w           t2,      t2,     1
+    slli.w           t3,      t3,     1
+    slli.w           t4,      t4,     1
+    fldx.d           f1,      a3,     t1
+    fldx.d           f2,      a3,     t2
+    fldx.d           f3,      a3,     t3
+    fldx.d           f4,      a3,     t4
+    ld.w             t1,      a5,     16
+    ld.w             t2,      a5,     20
+    ld.w             t3,      a5,     24
+    ld.w             t4,      a5,     28
+    slli.w           t1,      t1,     1
+    slli.w           t2,      t2,     1
+    slli.w           t3,      t3,     1
+    slli.w           t4,      t4,     1
+    fldx.d           f5,      a3,     t1
+    fldx.d           f6,      a3,     t2
+    fldx.d           f7,      a3,     t3
+    fldx.d           f8,      a3,     t4
+    vld              vr9,     a4,     0
+    vld              vr10,    a4,     16
+    vld              vr11,    a4,     32
+    vld              vr12,    a4,     48
+    vilvl.d          vr1,     vr2,    vr1
+    vilvl.d          vr3,     vr4,    vr3
+    vilvl.d          vr5,     vr6,    vr5
+    vilvl.d          vr7,     vr8,    vr7
+    vmulwev.w.hu.h   vr13,    vr1,    vr9
+    vmulwev.w.hu.h   vr14,    vr3,    vr10
+    vmulwev.w.hu.h   vr15,    vr5,    vr11
+    vmulwev.w.hu.h   vr16,    vr7,    vr12
+    vmaddwod.w.hu.h  vr13,    vr1,    vr9
+    vmaddwod.w.hu.h  vr14,    vr3,    vr10
+    vmaddwod.w.hu.h  vr15,    vr5,    vr11
+    vmaddwod.w.hu.h  vr16,    vr7,    vr12
+    vhaddw.d.w       vr13,    vr13,   vr13
+    vhaddw.d.w       vr14,    vr14,   vr14
+    vhaddw.d.w       vr15,    vr15,   vr15
+    vhaddw.d.w       vr16,    vr16,   vr16
+    vpickev.w        vr13,    vr14,   vr13
+    vpickev.w        vr15,    vr16,   vr15
+    vsra.w           vr13,    vr13,   vr0
+    vsra.w           vr15,    vr15,   vr0
+    vmin.w           vr13,    vr13,   vr20
+    vmin.w           vr15,    vr15,   vr20
+
+    vpickev.h        vr13,    vr15,   vr13
+    vst              vr13,    a1,     0
+    addi.d           a1,      a1,     16
+    addi.d           a5,      a5,     32
+    addi.d           a4,      a4,     64
+    addi.d           a2,      a2,     -8
+    bge              a2,      t8,     .LOOP_HS15_DST4
+    blt              zero,    a2,     .HS15_REST4
+    b                .HS15_END
+.HS15_REST4:
+    li.w             t1,      0
+.HS15_DST4:
+    slli.w           t2,      t1,     2
+    ldx.w            t2,      a5,     t2
+    li.w             t3,      0
+    li.w             t8,      0
+.HS15_FILTER4:
+    add.w            t4,      t2,     t3
+    slli.w           t4,      t4,     1
+    ldx.hu           t5,      a3,     t4
+    mul.w            t6,      a6,     t1
+    add.w            t6,      t6,     t3
+    slli.w           t7,      t6,     1
+    ldx.h            t7,      a4,     t7
+    mul.w            t7,      t5,     t7
+    add.w            t8,      t8,     t7
+    addi.w           t3,      t3,     1
+    blt              t3,      a6,     .HS15_FILTER4
+    sra.w            t8,      t8,     a7
+    slt              t5,      t8,     t0
+    maskeqz          t8,      t8,     t5
+    masknez          t5,      t0,     t5
+    or               t8,      t8,     t5
+    slli.w           t4,      t1,     1
+    stx.h            t8,      a1,     t4
+    addi.w           t1,      t1,     1
+    blt              t1,      a2,     .HS15_DST4
+    b                .HS15_END
+.END_HS15_DST4:
+
+    li.w             t1,      0
+.LOOP_HS15_DST1:
+    slli.w           t2,      t1,     2
+    ldx.w            t2,      a5,     t2
+    li.w             t3,      0
+    li.w             t8,      0
+.HS15_FILTER1:
+    add.w            t4,      t2,     t3
+    slli.w           t4,      t4,     1
+    ldx.hu           t5,      a3,     t4
+    mul.w            t6,      a6,     t1
+    add.w            t6,      t6,     t3
+    slli.w           t7,      t6,     1
+    ldx.h            t7,      a4,     t7
+    mul.w            t7,      t5,     t7
+    add.w            t8,      t8,     t7
+    addi.w           t3,      t3,     1
+    blt              t3,      a6,     .HS15_FILTER1
+    sra.w            t8,      t8,     a7
+    slt              t5,      t8,     t0
+    maskeqz          t8,      t8,     t5
+    masknez          t5,      t0,     t5
+    or               t8,      t8,     t5
+    slli.w           t4,      t1,     1
+    stx.h            t8,      a1,     t4
+    addi.w           t1,      t1,     1
+    blt              t1,      a2,     .LOOP_HS15_DST1
+    b                .HS15_END
+.HS15_END:
+
+    ld.d             s0,      sp,     0
+    ld.d             s1,      sp,     8
+    ld.d             s2,      sp,     16
+    ld.d             s3,      sp,     24
+    ld.d             s4,      sp,     32
+    ld.d             s5,      sp,     40
+    ld.d             s6,      sp,     48
+    ld.d             s7,      sp,     56
+    ld.d             s8,      sp,     64
+    addi.d           sp,      sp,     72
+endfunc
+
+/* void ff_hscale_16_to_19_sub_lsx(SwsContext *c, int16_t *dst, int dstW,
+ *                                 const uint8_t *src, const int16_t *filter,
+ *                                 const int32_t *filterPos, int filterSize, int sh)
+ */
+function ff_hscale_16_to_19_sub_lsx
+    addi.d           sp,      sp,     -72
+    st.d             s0,      sp,     0
+    st.d             s1,      sp,     8
+    st.d             s2,      sp,     16
+    st.d             s3,      sp,     24
+    st.d             s4,      sp,     32
+    st.d             s5,      sp,     40
+    st.d             s6,      sp,     48
+    st.d             s7,      sp,     56
+    st.d             s8,      sp,     64
+
+    li.w             t0,      524287
+    li.w             t8,      8
+    li.w             t7,      4
+    vreplgr2vr.w     vr20,    t0
+    vreplgr2vr.w     vr0,     a7
+    beq              a6,      t7,     .LOOP_HS19_DST4
+    beq              a6,      t8,     .LOOP_HS19_DST8
+    blt              t8,      a6,     .LOOP_HS19
+    b                .END_HS19_DST4
+
+.LOOP_HS19:
+    li.w             t1,      0
+    li.w             s1,      0
+    li.w             s2,      0
+    li.w             s3,      0
+    li.w             s4,      0
+    li.w             s5,      0
+    vldi             vr22,    0
+    addi.w           s0,      a6,     -7
+    slli.w           s7,      a6,     1
+    slli.w           s8,      a6,     2
+    add.w            t6,      s7,     s8
+.LOOP_HS19_DST:
+    ld.w             t2,      a5,     0
+    ld.w             t3,      a5,     4
+    ld.w             t4,      a5,     8
+    ld.w             t5,      a5,     12
+    slli.w           t2,      t2,     1
+    slli.w           t3,      t3,     1
+    slli.w           t4,      t4,     1
+    slli.w           t5,      t5,     1
+    vldx             vr1,     a3,     t2
+    vldx             vr2,     a3,     t3
+    vldx             vr3,     a3,     t4
+    vldx             vr4,     a3,     t5
+    vld              vr9,     a4,     0
+    vldx             vr10,    a4,     s7
+    vldx             vr11,    a4,     s8
+    vldx             vr12,    a4,     t6
+    vmulwev.w.hu.h   vr17,    vr1,    vr9
+    vmulwev.w.hu.h   vr18,    vr2,    vr10
+    vmulwev.w.hu.h   vr19,    vr3,    vr11
+    vmulwev.w.hu.h   vr21,    vr4,    vr12
+    vmaddwod.w.hu.h  vr17,    vr1,    vr9
+    vmaddwod.w.hu.h  vr18,    vr2,    vr10
+    vmaddwod.w.hu.h  vr19,    vr3,    vr11
+    vmaddwod.w.hu.h  vr21,    vr4,    vr12
+    vhaddw.d.w       vr1,     vr17,   vr17
+    vhaddw.d.w       vr2,     vr18,   vr18
+    vhaddw.d.w       vr3,     vr19,   vr19
+    vhaddw.d.w       vr4,     vr21,   vr21
+    vhaddw.q.d       vr1,     vr1,    vr1
+    vhaddw.q.d       vr2,     vr2,    vr2
+    vhaddw.q.d       vr3,     vr3,    vr3
+    vhaddw.q.d       vr4,     vr4,    vr4
+    vilvl.w          vr1,     vr2,    vr1
+    vilvl.w          vr3,     vr4,    vr3
+    vilvl.d          vr1,     vr3,    vr1
+    vadd.w           vr22,    vr22,   vr1
+    addi.w           s1,      s1,     8
+    addi.d           a3,      a3,     16
+    addi.d           a4,      a4,     16
+    blt              s1,      s0,     .LOOP_HS19_DST
+    blt              s1,      a6,     .HS19_DSTA
+    b                .END_HS19_FILTERA
+.HS19_DSTA:
+    ld.w             t2,      a5,     0
+    li.w             t3,      0
+    move             s6,      s1
+.HS19_FILTERA:
+    add.w            t4,      t2,     t3
+    slli.w           t4,      t4,     1
+    ldx.hu           t5,      a3,     t4
+    mul.w            t6,      a6,     t1
+    add.w            t6,      t6,     t3
+    slli.w           t6,      t6,     1
+    ldx.h            t6,      a4,     t6
+    mul.w            t6,      t5,     t6
+    add.w            s2,      s2,     t6
+    addi.w           t3,      t3,     1
+    addi.w           s6,      s6,     1
+    blt              s6,      a6,     .HS19_FILTERA
+
+    ld.w             t2,      a5,     4
+    li.w             t3,      0
+    move             s6,      s1
+    addi.w           t1,      t1,     1
+.HS19_FILTERB:
+    add.w            t4,      t2,     t3
+    slli.w           t4,      t4,     1
+    ldx.hu           t5,      a3,     t4
+    mul.w            t6,      a6,     t1
+    add.w            t6,      t6,     t3
+    slli.w           t6,      t6,     1
+    ldx.h            t6,      a4,     t6
+    mul.w            t6,      t5,     t6
+    add.w            s3,      s3,     t6
+    addi.w           t3,      t3,     1
+    addi.w           s6,      s6,     1
+    blt              s6,      a6,     .HS19_FILTERB
+    ld.w             t2,      a5,     8
+    addi.w           t1,      t1,     1
+    li.w             t3,      0
+    move             s6,      s1
+.HS19_FILTERC:
+    add.w            t4,      t2,     t3
+    slli.w           t4,      t4,     1
+    ldx.hu           t5,      a3,     t4
+    mul.w            t6,      a6,     t1
+    add.w            t6,      t6,     t3
+    slli.w           t6,      t6,     1
+    ldx.h            t6,      a4,     t6
+    mul.w            t6,      t5,     t6
+    add.w            s4,      s4,     t6
+    addi.w           t3,      t3,     1
+    addi.w           s6,      s6,     1
+    blt              s6,      a6,     .HS19_FILTERC
+    ld.w             t2,      a5,     12
+    addi.w           t1,      t1,     1
+    move             s6,      s1
+    li.w             t3,      0
+.HS19_FILTERD:
+    add.w            t4,      t2,     t3
+    slli.w           t4,      t4,     1
+    ldx.hu           t5,      a3,     t4
+    mul.w            t6,      a6,     t1
+    add.w            t6,      t6,     t3
+    slli.w           t6,      t6,     1
+    ldx.h            t6,      a4,     t6
+    mul.w            t6,      t5,     t6
+    add.w            s5,      s5,     t6
+    addi.w           t3,      t3,     1
+    addi.w           s6,      s6,     1
+    blt              s6,      a6,     .HS19_FILTERD
+.END_HS19_FILTERA:
+    vpickve2gr.w     t1,      vr22,   0
+    vpickve2gr.w     t2,      vr22,   1
+    vpickve2gr.w     t3,      vr22,   2
+    vpickve2gr.w     t4,      vr22,   3
+    add.w            s2,      s2,     t1
+    add.w            s3,      s3,     t2
+    add.w            s4,      s4,     t3
+    add.w            s5,      s5,     t4
+    sra.w            s2,      s2,     a7
+    sra.w            s3,      s3,     a7
+    sra.w            s4,      s4,     a7
+    sra.w            s5,      s5,     a7
+    slt              t1,      s2,     t0
+    slt              t2,      s3,     t0
+    slt              t3,      s4,     t0
+    slt              t4,      s5,     t0
+    maskeqz          s2,      s2,     t1
+    maskeqz          s3,      s3,     t2
+    maskeqz          s4,      s4,     t3
+    maskeqz          s5,      s5,     t4
+    masknez          t1,      t0,     t1
+    masknez          t2,      t0,     t2
+    masknez          t3,      t0,     t3
+    masknez          t4,      t0,     t4
+    or               s2,      s2,     t1
+    or               s3,      s3,     t2
+    or               s4,      s4,     t3
+    or               s5,      s5,     t4
+    st.w             s2,      a1,     0
+    st.w             s3,      a1,     4
+    st.w             s4,      a1,     8
+    st.w             s5,      a1,     12
+
+    addi.d           a1,      a1,     16
+    sub.d            a3,      a3,     s1
+    sub.d            a3,      a3,     s1
+    addi.d           a5,      a5,     16
+    slli.d           t3,      a6,     3
+    add.d            a4,      a4,     t3
+    sub.d            a4,      a4,     s1
+    sub.d            a4,      a4,     s1
+    addi.d           a2,      a2,     -4
+    bge              a2,      t7,     .LOOP_HS19
+    blt              zero,    a2,     .HS19_RESA
+    b                .HS19_END
+.HS19_RESA:
+    li.w             t1,      0
+.HS19_DST:
+    slli.w           t2,      t1,     2
+    ldx.w            t2,      a5,     t2
+    li.w             t3,      0
+    li.w             t8,      0
+.HS19_FILTER:
+    add.w            t4,      t2,     t3
+    slli.w           t4,      t4,     1
+    ldx.hu           t5,      a3,     t4
+    mul.w            t6,      a6,     t1
+    add.w            t6,      t6,     t3
+    slli.w           t7,      t6,     1
+    ldx.h            t7,      a4,     t7
+    mul.w            t7,      t5,     t7
+    add.w            t8,      t8,     t7
+    addi.w           t3,      t3,     1
+    blt              t3,      a6,     .HS19_FILTER
+    sra.w            t8,      t8,     a7
+    slt              t5,      t8,     t0
+    maskeqz          t8,      t8,     t5
+    masknez          t5,      t0,     t5
+    or               t8,      t8,     t5
+    slli.w           t4,      t1,     2
+    stx.w            t8,      a1,     t4
+    addi.w           t1,      t1,     1
+    blt              t1,      a2,     .HS19_DST
+    b                .HS19_END
+
+.LOOP_HS19_DST8:
+    ld.w             t1,      a5,     0
+    ld.w             t2,      a5,     4
+    ld.w             t3,      a5,     8
+    ld.w             t4,      a5,     12
+    slli.w           t1,      t1,     1
+    slli.w           t2,      t2,     1
+    slli.w           t3,      t3,     1
+    slli.w           t4,      t4,     1
+    vldx             vr1,     a3,     t1
+    vldx             vr2,     a3,     t2
+    vldx             vr3,     a3,     t3
+    vldx             vr4,     a3,     t4
+    ld.w             t1,      a5,     16
+    ld.w             t2,      a5,     20
+    ld.w             t3,      a5,     24
+    ld.w             t4,      a5,     28
+    slli.w           t1,      t1,     1
+    slli.w           t2,      t2,     1
+    slli.w           t3,      t3,     1
+    slli.w           t4,      t4,     1
+    vldx             vr5,     a3,     t1
+    vldx             vr6,     a3,     t2
+    vldx             vr7,     a3,     t3
+    vldx             vr8,     a3,     t4
+    vld              vr9,     a4,     0
+    vld              vr10,    a4,     16
+    vld              vr11,    a4,     32
+    vld              vr12,    a4,     48
+    vld              vr13,    a4,     64
+    vld              vr14,    a4,     80
+    vld              vr15,    a4,     96
+    vld              vr16,    a4,     112
+    vmulwev.w.hu.h   vr17,    vr1,    vr9
+    vmulwev.w.hu.h   vr18,    vr2,    vr10
+    vmulwev.w.hu.h   vr19,    vr3,    vr11
+    vmulwev.w.hu.h   vr21,    vr4,    vr12
+    vmaddwod.w.hu.h  vr17,    vr1,    vr9
+    vmaddwod.w.hu.h  vr18,    vr2,    vr10
+    vmaddwod.w.hu.h  vr19,    vr3,    vr11
+    vmaddwod.w.hu.h  vr21,    vr4,    vr12
+    vmulwev.w.hu.h   vr1,     vr5,    vr13
+    vmulwev.w.hu.h   vr2,     vr6,    vr14
+    vmulwev.w.hu.h   vr3,     vr7,    vr15
+    vmulwev.w.hu.h   vr4,     vr8,    vr16
+    vmaddwod.w.hu.h  vr1,     vr5,    vr13
+    vmaddwod.w.hu.h  vr2,     vr6,    vr14
+    vmaddwod.w.hu.h  vr3,     vr7,    vr15
+    vmaddwod.w.hu.h  vr4,     vr8,    vr16
+    vhaddw.d.w       vr5,     vr1,    vr1
+    vhaddw.d.w       vr6,     vr2,    vr2
+    vhaddw.d.w       vr7,     vr3,    vr3
+    vhaddw.d.w       vr8,     vr4,    vr4
+    vhaddw.d.w       vr1,     vr17,   vr17
+    vhaddw.d.w       vr2,     vr18,   vr18
+    vhaddw.d.w       vr3,     vr19,   vr19
+    vhaddw.d.w       vr4,     vr21,   vr21
+    vhaddw.q.d       vr1,     vr1,    vr1
+    vhaddw.q.d       vr2,     vr2,    vr2
+    vhaddw.q.d       vr3,     vr3,    vr3
+    vhaddw.q.d       vr4,     vr4,    vr4
+    vhaddw.q.d       vr5,     vr5,    vr5
+    vhaddw.q.d       vr6,     vr6,    vr6
+    vhaddw.q.d       vr7,     vr7,    vr7
+    vhaddw.q.d       vr8,     vr8,    vr8
+    vilvl.w          vr1,     vr2,    vr1
+    vilvl.w          vr3,     vr4,    vr3
+    vilvl.w          vr5,     vr6,    vr5
+    vilvl.w          vr7,     vr8,    vr7
+    vilvl.d          vr1,     vr3,    vr1
+    vilvl.d          vr5,     vr7,    vr5
+    vsra.w           vr1,     vr1,    vr0
+    vsra.w           vr5,     vr5,    vr0
+    vmin.w           vr1,     vr1,    vr20
+    vmin.w           vr5,     vr5,    vr20
+
+    vst              vr1,     a1,     0
+    vst              vr5,     a1,     16
+    addi.d           a1,      a1,     32
+    addi.d           a5,      a5,     32
+    addi.d           a4,      a4,     128
+    addi.d           a2,      a2,     -8
+    bge              a2,      t8,     .LOOP_HS19_DST8
+    blt              zero,    a2,     .HS19_REST8
+    b                .HS19_END
+.HS19_REST8:
+    li.w             t1,      0
+.HS19_DST8:
+    slli.w           t2,      t1,     2
+    ldx.w            t2,      a5,     t2
+    li.w             t3,      0
+    li.w             t8,      0
+.HS19_FILTER8:
+    add.w            t4,      t2,     t3
+    slli.w           t4,      t4,     1
+    ldx.hu           t5,      a3,     t4
+    mul.w            t6,      a6,     t1
+    add.w            t6,      t6,     t3
+    slli.w           t7,      t6,     1
+    ldx.h            t7,      a4,     t7
+    mul.w            t7,      t5,     t7
+    add.w            t8,      t8,     t7
+    addi.w           t3,      t3,     1
+    blt              t3,      a6,     .HS19_FILTER8
+    sra.w            t8,      t8,     a7
+    slt              t5,      t8,     t0
+    maskeqz          t8,      t8,     t5
+    masknez          t5,      t0,     t5
+    or               t8,      t8,     t5
+    slli.w           t4,      t1,     2
+    stx.w            t8,      a1,     t4
+    addi.w           t1,      t1,     1
+    blt              t1,      a2,     .HS19_DST8
+    b                .HS19_END
+
+.LOOP_HS19_DST4:
+    ld.w             t1,      a5,     0
+    ld.w             t2,      a5,     4
+    ld.w             t3,      a5,     8
+    ld.w             t4,      a5,     12
+    slli.w           t1,      t1,     1
+    slli.w           t2,      t2,     1
+    slli.w           t3,      t3,     1
+    slli.w           t4,      t4,     1
+    fldx.d           f1,      a3,     t1
+    fldx.d           f2,      a3,     t2
+    fldx.d           f3,      a3,     t3
+    fldx.d           f4,      a3,     t4
+    ld.w             t1,      a5,     16
+    ld.w             t2,      a5,     20
+    ld.w             t3,      a5,     24
+    ld.w             t4,      a5,     28
+    slli.w           t1,      t1,     1
+    slli.w           t2,      t2,     1
+    slli.w           t3,      t3,     1
+    slli.w           t4,      t4,     1
+    fldx.d           f5,      a3,     t1
+    fldx.d           f6,      a3,     t2
+    fldx.d           f7,      a3,     t3
+    fldx.d           f8,      a3,     t4
+    vld              vr9,     a4,     0
+    vld              vr10,    a4,     16
+    vld              vr11,    a4,     32
+    vld              vr12,    a4,     48
+    vilvl.d          vr1,     vr2,    vr1
+    vilvl.d          vr3,     vr4,    vr3
+    vilvl.d          vr5,     vr6,    vr5
+    vilvl.d          vr7,     vr8,    vr7
+    vmulwev.w.hu.h   vr13,    vr1,    vr9
+    vmulwev.w.hu.h   vr14,    vr3,    vr10
+    vmulwev.w.hu.h   vr15,    vr5,    vr11
+    vmulwev.w.hu.h   vr16,    vr7,    vr12
+    vmaddwod.w.hu.h  vr13,    vr1,    vr9
+    vmaddwod.w.hu.h  vr14,    vr3,    vr10
+    vmaddwod.w.hu.h  vr15,    vr5,    vr11
+    vmaddwod.w.hu.h  vr16,    vr7,    vr12
+    vhaddw.d.w       vr13,    vr13,   vr13
+    vhaddw.d.w       vr14,    vr14,   vr14
+    vhaddw.d.w       vr15,    vr15,   vr15
+    vhaddw.d.w       vr16,    vr16,   vr16
+    vpickev.w        vr13,    vr14,   vr13
+    vpickev.w        vr15,    vr16,   vr15
+    vsra.w           vr13,    vr13,   vr0
+    vsra.w           vr15,    vr15,   vr0
+    vmin.w           vr13,    vr13,   vr20
+    vmin.w           vr15,    vr15,   vr20
+
+    vst              vr13,    a1,     0
+    vst              vr15,    a1,     16
+    addi.d           a1,      a1,     32
+    addi.d           a5,      a5,     32
+    addi.d           a4,      a4,     64
+    addi.d           a2,      a2,     -8
+    bge              a2,      t8,     .LOOP_HS19_DST4
+    blt              zero,    a2,     .HS19_REST4
+    b                .HS19_END
+.HS19_REST4:
+    li.w             t1,      0
+.HS19_DST4:
+    slli.w           t2,      t1,     2
+    ldx.w            t2,      a5,     t2
+    li.w             t3,      0
+    li.w             t8,      0
+.HS19_FILTER4:
+    add.w            t4,      t2,     t3
+    slli.w           t4,      t4,     1
+    ldx.hu           t5,      a3,     t4
+    mul.w            t6,      a6,     t1
+    add.w            t6,      t6,     t3
+    slli.w           t7,      t6,     1
+    ldx.h            t7,      a4,     t7
+    mul.w            t7,      t5,     t7
+    add.w            t8,      t8,     t7
+    addi.w           t3,      t3,     1
+    blt              t3,      a6,     .HS19_FILTER4
+    sra.w            t8,      t8,     a7
+    slt              t5,      t8,     t0
+    maskeqz          t8,      t8,     t5
+    masknez          t5,      t0,     t5
+    or               t8,      t8,     t5
+    slli.w           t4,      t1,     2
+    stx.w            t8,      a1,     t4
+    addi.w           t1,      t1,     1
+    blt              t1,      a2,     .HS19_DST4
+    b                .HS19_END
+.END_HS19_DST4:
+
+    li.w             t1,      0
+.LOOP_HS19_DST1:
+    slli.w           t2,      t1,     2
+    ldx.w            t2,      a5,     t2
+    li.w             t3,      0
+    li.w             t8,      0
+.HS19_FILTER1:
+    add.w            t4,      t2,     t3
+    slli.w           t4,      t4,     1
+    ldx.hu           t5,      a3,     t4
+    mul.w            t6,      a6,     t1
+    add.w            t6,      t6,     t3
+    slli.w           t7,      t6,     1
+    ldx.h            t7,      a4,     t7
+    mul.w            t7,      t5,     t7
+    add.w            t8,      t8,     t7
+    addi.w           t3,      t3,     1
+    blt              t3,      a6,     .HS19_FILTER1
+    sra.w            t8,      t8,     a7
+    slt              t5,      t8,     t0
+    maskeqz          t8,      t8,     t5
+    masknez          t5,      t0,     t5
+    or               t8,      t8,     t5
+    slli.w           t4,      t1,     2
+    stx.w            t8,      a1,     t4
+    addi.w           t1,      t1,     1
+    blt              t1,      a2,     .LOOP_HS19_DST1
+    b                .HS19_END
+.HS19_END:
+
+    ld.d             s0,      sp,     0
+    ld.d             s1,      sp,     8
+    ld.d             s2,      sp,     16
+    ld.d             s3,      sp,     24
+    ld.d             s4,      sp,     32
+    ld.d             s5,      sp,     40
+    ld.d             s6,      sp,     48
+    ld.d             s7,      sp,     56
+    ld.d             s8,      sp,     64
+    addi.d           sp,      sp,     72
+endfunc
diff --git a/libswscale/loongarch/swscale_init_loongarch.c b/libswscale/loongarch/swscale_init_loongarch.c
new file mode 100644
index 0000000000..e08f68f7ba
--- /dev/null
+++ b/libswscale/loongarch/swscale_init_loongarch.c
@@ -0,0 +1,147 @@
+/*
+ * Copyright (C) 2021 Loongson Technology Corporation Limited
+ * Contributed by Hao Chen(chenhao@loongson.cn)
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "swscale_loongarch.h"
+#include "libswscale/swscale_internal.h"
+#include "libswscale/rgb2rgb.h"
+#include "libavutil/loongarch/cpu.h"
+
+av_cold void ff_sws_init_swscale_loongarch(SwsContext *c)
+{
+    int cpu_flags = av_get_cpu_flags();
+    if (have_lsx(cpu_flags)) {
+        ff_sws_init_output_lsx(c);
+        if (c->srcBpc == 8) {
+            if (c->dstBpc <= 14) {
+                c->hyScale = c->hcScale = ff_hscale_8_to_15_lsx;
+            } else {
+                c->hyScale = c->hcScale = ff_hscale_8_to_19_lsx;
+            }
+        } else {
+            c->hyScale = c->hcScale = c->dstBpc > 14 ? ff_hscale_16_to_19_lsx
+                                                     : ff_hscale_16_to_15_lsx;
+        }
+        switch (c->srcFormat) {
+        case AV_PIX_FMT_GBRAP:
+        case AV_PIX_FMT_GBRP:
+            {
+                c->readChrPlanar = planar_rgb_to_uv_lsx;
+                c->readLumPlanar = planar_rgb_to_y_lsx;
+            }
+            break;
+        }
+        if (c->dstBpc == 8)
+            c->yuv2planeX = ff_yuv2planeX_8_lsx;
+    }
+    if (have_lasx(cpu_flags)) {
+        ff_sws_init_output_loongarch(c);
+        if (c->srcBpc == 8) {
+            if (c->dstBpc <= 14) {
+                c->hyScale = c->hcScale = ff_hscale_8_to_15_lasx;
+            } else {
+                c->hyScale = c->hcScale = ff_hscale_8_to_19_lasx;
+            }
+        } else {
+            c->hyScale = c->hcScale = c->dstBpc > 14 ? ff_hscale_16_to_19_lasx
+                                                     : ff_hscale_16_to_15_lasx;
+        }
+        switch (c->srcFormat) {
+        case AV_PIX_FMT_GBRAP:
+        case AV_PIX_FMT_GBRP:
+            {
+                c->readChrPlanar = planar_rgb_to_uv_lasx;
+                c->readLumPlanar = planar_rgb_to_y_lasx;
+            }
+            break;
+        }
+        if (c->dstBpc == 8)
+            c->yuv2planeX = ff_yuv2planeX_8_lasx;
+    }
+}
+
+av_cold void rgb2rgb_init_loongarch(void)
+{
+    int cpu_flags = av_get_cpu_flags();
+    if (have_lasx(cpu_flags))
+        interleaveBytes = ff_interleave_bytes_lasx;
+}
+
+av_cold SwsFunc ff_yuv2rgb_init_loongarch(SwsContext *c)
+{
+    int cpu_flags = av_get_cpu_flags();
+    if (have_lasx(cpu_flags)) {
+        switch (c->dstFormat) {
+            case AV_PIX_FMT_RGB24:
+                return yuv420_rgb24_lasx;
+            case AV_PIX_FMT_BGR24:
+                return yuv420_bgr24_lasx;
+            case AV_PIX_FMT_RGBA:
+                if (CONFIG_SWSCALE_ALPHA && isALPHA(c->srcFormat)) {
+                    break;
+                } else
+                    return yuv420_rgba32_lasx;
+            case AV_PIX_FMT_ARGB:
+                if (CONFIG_SWSCALE_ALPHA && isALPHA(c->srcFormat)) {
+                    break;
+                } else
+                    return yuv420_argb32_lasx;
+            case AV_PIX_FMT_BGRA:
+                if (CONFIG_SWSCALE_ALPHA && isALPHA(c->srcFormat)) {
+                    break;
+                } else
+                    return yuv420_bgra32_lasx;
+            case AV_PIX_FMT_ABGR:
+                if (CONFIG_SWSCALE_ALPHA && isALPHA(c->srcFormat)) {
+                    break;
+                } else
+                    return yuv420_abgr32_lasx;
+        }
+    }
+    if (have_lsx(cpu_flags)) {
+        switch (c->dstFormat) {
+            case AV_PIX_FMT_RGB24:
+                return yuv420_rgb24_lsx;
+            case AV_PIX_FMT_BGR24:
+                return yuv420_bgr24_lsx;
+            case AV_PIX_FMT_RGBA:
+                if (CONFIG_SWSCALE_ALPHA && isALPHA(c->srcFormat)) {
+                    break;
+                } else
+                    return yuv420_rgba32_lsx;
+            case AV_PIX_FMT_ARGB:
+                if (CONFIG_SWSCALE_ALPHA && isALPHA(c->srcFormat)) {
+                    break;
+                } else
+                    return yuv420_argb32_lsx;
+            case AV_PIX_FMT_BGRA:
+                if (CONFIG_SWSCALE_ALPHA && isALPHA(c->srcFormat)) {
+                    break;
+                } else
+                    return yuv420_bgra32_lsx;
+            case AV_PIX_FMT_ABGR:
+                if (CONFIG_SWSCALE_ALPHA && isALPHA(c->srcFormat)) {
+                    break;
+                } else
+                    return yuv420_abgr32_lsx;
+        }
+    }
+    return NULL;
+}
diff --git a/libswscale/loongarch/swscale_lasx.c b/libswscale/loongarch/swscale_lasx.c
new file mode 100644
index 0000000000..fb03d1291f
--- /dev/null
+++ b/libswscale/loongarch/swscale_lasx.c
@@ -0,0 +1,949 @@
+/*
+ * Copyright (C) 2021 Loongson Technology Corporation Limited
+ * Contributed by Hao Chen(chenhao@loongson.cn)
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "swscale_loongarch.h"
+#include "libavutil/loongarch/loongson_intrinsics.h"
+#include "libavutil/intreadwrite.h"
+
+#define SCALE_8_16(_sh)                                               \
+{                                                                     \
+    src0    = __lasx_xvldrepl_d(src + filterPos[0], 0);               \
+    src1    = __lasx_xvldrepl_d(src + filterPos[1], 0);               \
+    src2    = __lasx_xvldrepl_d(src + filterPos[2], 0);               \
+    src3    = __lasx_xvldrepl_d(src + filterPos[3], 0);               \
+    src4    = __lasx_xvldrepl_d(src + filterPos[4], 0);               \
+    src5    = __lasx_xvldrepl_d(src + filterPos[5], 0);               \
+    src6    = __lasx_xvldrepl_d(src + filterPos[6], 0);               \
+    src7    = __lasx_xvldrepl_d(src + filterPos[7], 0);               \
+    src8    = __lasx_xvldrepl_d(src + filterPos[8], 0);               \
+    src9    = __lasx_xvldrepl_d(src + filterPos[9], 0);               \
+    src10   = __lasx_xvldrepl_d(src + filterPos[10], 0);              \
+    src11   = __lasx_xvldrepl_d(src + filterPos[11], 0);              \
+    src12   = __lasx_xvldrepl_d(src + filterPos[12], 0);              \
+    src13   = __lasx_xvldrepl_d(src + filterPos[13], 0);              \
+    src14   = __lasx_xvldrepl_d(src + filterPos[14], 0);              \
+    src15   = __lasx_xvldrepl_d(src + filterPos[15], 0);              \
+    DUP4_ARG2(__lasx_xvld, filter, 0, filter + 16, 0, filter + 32, 0, \
+              filter + 48, 0, filter0, filter1, filter2, filter3);    \
+    DUP4_ARG2(__lasx_xvld, filter + 64, 0, filter + 80, 0,            \
+              filter + 96, 0, filter + 112, 0, filter4,               \
+              filter5, filter6, filter7);                             \
+    DUP4_ARG2(__lasx_xvpickev_d, src1, src0, src3, src2,              \
+              src5, src4, src7, src6, src0, src2, src4, src6);        \
+    DUP4_ARG2(__lasx_xvpickev_d, src9, src8, src11, src10,            \
+              src13, src12, src15, src14, src8, src10, src12, src14); \
+    DUP4_ARG1(__lasx_vext2xv_hu_bu, src0, src2, src4, src6,           \
+              src0, src2, src4, src6);                                \
+    DUP4_ARG1(__lasx_vext2xv_hu_bu, src8, src10, src12,               \
+              src14, src8, src10, src12, src14);                      \
+    DUP4_ARG2(__lasx_xvdp2_w_h, filter0, src0, filter1, src2,         \
+              filter2, src4, filter3, src6, src0, src1, src2, src3);  \
+    DUP4_ARG2(__lasx_xvdp2_w_h, filter4, src8, filter5, src10,        \
+              filter6, src12, filter7, src14, src4, src5, src6, src7);\
+    src0 = __lasx_xvhaddw_d_w(src0, src0);                            \
+    src1 = __lasx_xvhaddw_d_w(src1, src1);                            \
+    src2 = __lasx_xvhaddw_d_w(src2, src2);                            \
+    src3 = __lasx_xvhaddw_d_w(src3, src3);                            \
+    src4 = __lasx_xvhaddw_d_w(src4, src4);                            \
+    src5 = __lasx_xvhaddw_d_w(src5, src5);                            \
+    src6 = __lasx_xvhaddw_d_w(src6, src6);                            \
+    src7 = __lasx_xvhaddw_d_w(src7, src7);                            \
+    DUP4_ARG2(__lasx_xvpickev_w, src1, src0, src3, src2,              \
+              src5, src4, src7, src6, src0, src1, src2, src3);        \
+    src0 = __lasx_xvhaddw_d_w(src0, src0);                            \
+    src1 = __lasx_xvhaddw_d_w(src1, src1);                            \
+    src2 = __lasx_xvhaddw_d_w(src2, src2);                            \
+    src3 = __lasx_xvhaddw_d_w(src3, src3);                            \
+    DUP2_ARG2(__lasx_xvpickev_w, src1, src0, src3, src2, src0, src1); \
+    DUP2_ARG2(__lasx_xvsrai_w, src0, _sh, src1, _sh, src0, src1);     \
+    src0 = __lasx_xvmin_w(src0, vmax);                                \
+    src1 = __lasx_xvmin_w(src1, vmax);                                \
+    src0 = __lasx_xvperm_w(src0, shuf);                               \
+    src1 = __lasx_xvperm_w(src1, shuf);                               \
+    src0 = __lasx_xvpickev_h(src1, src0);                             \
+    src0 = __lasx_xvpermi_d(src0, 0xd8);                              \
+    __lasx_xvst(src0, dst, 0);                                        \
+    filterPos += 16;                                                  \
+    filter    += 128;                                                 \
+    dst       += 16;                                                  \
+}
+
+#define SCALE_8_8(_sh)                                                \
+{                                                                     \
+    src0    = __lasx_xvldrepl_d(src + filterPos[0], 0);               \
+    src1    = __lasx_xvldrepl_d(src + filterPos[1], 0);               \
+    src2    = __lasx_xvldrepl_d(src + filterPos[2], 0);               \
+    src3    = __lasx_xvldrepl_d(src + filterPos[3], 0);               \
+    src4    = __lasx_xvldrepl_d(src + filterPos[4], 0);               \
+    src5    = __lasx_xvldrepl_d(src + filterPos[5], 0);               \
+    src6    = __lasx_xvldrepl_d(src + filterPos[6], 0);               \
+    src7    = __lasx_xvldrepl_d(src + filterPos[7], 0);               \
+    DUP4_ARG2(__lasx_xvld, filter, 0, filter + 16, 0, filter + 32,    \
+              0, filter + 48, 0, filter0, filter1, filter2, filter3); \
+    DUP4_ARG2(__lasx_xvpickev_d, src1, src0, src3, src2,              \
+              src5, src4, src7, src6, src0, src2, src4, src6);        \
+    DUP4_ARG1(__lasx_vext2xv_hu_bu, src0, src2, src4, src6,           \
+              src0, src2, src4, src6);                                \
+    DUP4_ARG2(__lasx_xvdp2_w_h, filter0, src0, filter1, src2,         \
+              filter2, src4, filter3, src6, src0, src1, src2,src3);   \
+    src0 = __lasx_xvhaddw_d_w(src0, src0);                            \
+    src1 = __lasx_xvhaddw_d_w(src1, src1);                            \
+    src2 = __lasx_xvhaddw_d_w(src2, src2);                            \
+    src3 = __lasx_xvhaddw_d_w(src3, src3);                            \
+    DUP2_ARG2(__lasx_xvpickev_w, src1, src0, src3, src2, src0, src1); \
+    src0 = __lasx_xvhaddw_d_w(src0, src0);                            \
+    src1 = __lasx_xvhaddw_d_w(src1, src1);                            \
+    src0 = __lasx_xvpickev_w(src1, src0);                             \
+    src0 = __lasx_xvsrai_w(src0, _sh);                                \
+    src0 = __lasx_xvmin_w(src0, vmax);                                \
+    src0 = __lasx_xvperm_w(src0, shuf);                               \
+}
+
+#define SCALE_8_4(_sh)                                                    \
+{                                                                         \
+    src0    = __lasx_xvldrepl_d(src + filterPos[0], 0);                   \
+    src1    = __lasx_xvldrepl_d(src + filterPos[1], 0);                   \
+    src2    = __lasx_xvldrepl_d(src + filterPos[2], 0);                   \
+    src3    = __lasx_xvldrepl_d(src + filterPos[3], 0);                   \
+    DUP2_ARG2(__lasx_xvld, filter, 0, filter + 16, 0, filter0, filter1);  \
+    DUP2_ARG2(__lasx_xvpickev_d, src1, src0, src3, src2, src0, src2);     \
+    DUP2_ARG1(__lasx_vext2xv_hu_bu, src0, src2, src0, src2);              \
+    DUP2_ARG2(__lasx_xvdp2_w_h, filter0, src0, filter1, src2, src0, src1);\
+    src0 = __lasx_xvhaddw_d_w(src0, src0);                                \
+    src1 = __lasx_xvhaddw_d_w(src1, src1);                                \
+    src0 = __lasx_xvpickev_w(src1, src0);                                 \
+    src0 = __lasx_xvhaddw_d_w(src0, src0);                                \
+    src0 = __lasx_xvpickev_w(src0, src0);                                 \
+    src0 = __lasx_xvsrai_w(src0, _sh);                                    \
+    src0 = __lasx_xvmin_w(src0, vmax);                                    \
+    src0 = __lasx_xvperm_w(src0, shuf);                                   \
+}
+
+#define SCALE_8_2(_sh)                                                \
+{                                                                     \
+    src0    = __lasx_xvldrepl_d(src + filterPos[0], 0);               \
+    src1    = __lasx_xvldrepl_d(src + filterPos[1], 0);               \
+    filter0 = __lasx_xvld(filter, 0);                                 \
+    src0 = __lasx_xvpickev_d(src1, src0);                             \
+    src0 = __lasx_vext2xv_hu_bu(src0);                                \
+    src0 = __lasx_xvdp2_w_h(filter0, src0);                           \
+    src0 = __lasx_xvhaddw_d_w(src0, src0);                            \
+    src0 = __lasx_xvhaddw_q_d(src0, src0);                            \
+    src0 = __lasx_xvsrai_w(src0, _sh);                                \
+    src0 = __lasx_xvmin_w(src0, vmax);                                \
+    dst[0] = __lasx_xvpickve2gr_w(src0, 0);                           \
+    dst[1] = __lasx_xvpickve2gr_w(src0, 4);                           \
+    filterPos += 2;                                                   \
+    filter    += 16;                                                  \
+    dst       += 2;                                                   \
+}
+
+#define SCALE_4_16(_sh)                                               \
+{                                                                     \
+    src0    = __lasx_xvldrepl_w(src + filterPos[0], 0);               \
+    src1    = __lasx_xvldrepl_w(src + filterPos[1], 0);               \
+    src2    = __lasx_xvldrepl_w(src + filterPos[2], 0);               \
+    src3    = __lasx_xvldrepl_w(src + filterPos[3], 0);               \
+    src4    = __lasx_xvldrepl_w(src + filterPos[4], 0);               \
+    src5    = __lasx_xvldrepl_w(src + filterPos[5], 0);               \
+    src6    = __lasx_xvldrepl_w(src + filterPos[6], 0);               \
+    src7    = __lasx_xvldrepl_w(src + filterPos[7], 0);               \
+    src8    = __lasx_xvldrepl_w(src + filterPos[8], 0);               \
+    src9    = __lasx_xvldrepl_w(src + filterPos[9], 0);               \
+    src10   = __lasx_xvldrepl_w(src + filterPos[10], 0);              \
+    src11   = __lasx_xvldrepl_w(src + filterPos[11], 0);              \
+    src12   = __lasx_xvldrepl_w(src + filterPos[12], 0);              \
+    src13   = __lasx_xvldrepl_w(src + filterPos[13], 0);              \
+    src14   = __lasx_xvldrepl_w(src + filterPos[14], 0);              \
+    src15   = __lasx_xvldrepl_w(src + filterPos[15], 0);              \
+    DUP4_ARG2(__lasx_xvld, filter, 0, filter + 16, 0, filter + 32, 0, \
+              filter + 48, 0, filter0, filter1, filter2, filter3);    \
+    DUP4_ARG2(__lasx_xvilvl_w, src1, src0, src3, src2, src5,          \
+              src4, src7, src6, src0, src2, src4, src6);              \
+    DUP4_ARG2(__lasx_xvilvl_w, src9, src8, src11, src10, src13,       \
+              src12, src15, src14, src8, src10, src12, src14);        \
+    DUP4_ARG2(__lasx_xvilvl_d, src2, src0, src6, src4, src10,         \
+                   src8, src14, src12, src0, src1, src2, src3);       \
+    DUP4_ARG1(__lasx_vext2xv_hu_bu, src0, src1, src2, src3,           \
+              src0, src1, src2, src3);                                \
+    DUP4_ARG2(__lasx_xvdp2_w_h, filter0, src0, filter1, src1,         \
+              filter2, src2, filter3, src3, src0, src1, src2, src3);  \
+    src0 = __lasx_xvhaddw_d_w(src0, src0);                            \
+    src1 = __lasx_xvhaddw_d_w(src1, src1);                            \
+    src2 = __lasx_xvhaddw_d_w(src2, src2);                            \
+    src3 = __lasx_xvhaddw_d_w(src3, src3);                            \
+    DUP2_ARG2(__lasx_xvpickev_w, src1, src0, src3, src2, src0, src1); \
+    DUP2_ARG2(__lasx_xvsrai_w, src0, _sh, src1, _sh, src0, src1);     \
+    src0 = __lasx_xvmin_w(src0, vmax);                                \
+    src1 = __lasx_xvmin_w(src1, vmax);                                \
+    src0 = __lasx_xvpickev_h(src1, src0);                             \
+    src0 = __lasx_xvperm_w(src0, shuf);                               \
+    __lasx_xvst(src0, dst, 0);                                        \
+    filterPos += 16;                                                  \
+    filter    += 64;                                                  \
+    dst       += 16;                                                  \
+}
+
+#define SCALE_4_8(_sh)                                                    \
+{                                                                         \
+    src0    = __lasx_xvldrepl_w(src + filterPos[0], 0);                   \
+    src1    = __lasx_xvldrepl_w(src + filterPos[1], 0);                   \
+    src2    = __lasx_xvldrepl_w(src + filterPos[2], 0);                   \
+    src3    = __lasx_xvldrepl_w(src + filterPos[3], 0);                   \
+    src4    = __lasx_xvldrepl_w(src + filterPos[4], 0);                   \
+    src5    = __lasx_xvldrepl_w(src + filterPos[5], 0);                   \
+    src6    = __lasx_xvldrepl_w(src + filterPos[6], 0);                   \
+    src7    = __lasx_xvldrepl_w(src + filterPos[7], 0);                   \
+    DUP2_ARG2(__lasx_xvld, filter, 0, filter + 16, 0, filter0, filter1);  \
+    DUP4_ARG2(__lasx_xvilvl_w, src1, src0, src3, src2, src5,              \
+              src4, src7, src6, src0, src2, src4, src6);                  \
+    DUP2_ARG2(__lasx_xvilvl_d, src2, src0, src6, src4, src0, src1);       \
+                                                                          \
+    DUP2_ARG1(__lasx_vext2xv_hu_bu, src0, src1, src0, src1);              \
+    DUP2_ARG2(__lasx_xvdp2_w_h, filter0, src0, filter1, src1, src0, src1);\
+    src0 = __lasx_xvhaddw_d_w(src0, src0);                                \
+    src1 = __lasx_xvhaddw_d_w(src1, src1);                                \
+    src0 = __lasx_xvpickev_w(src1, src0);                                 \
+    src0 = __lasx_xvsrai_w(src0, _sh);                                    \
+    src0 = __lasx_xvmin_w(src0, vmax);                                    \
+}
+
+#define SCALE_4_4(_sh)                                                \
+{                                                                     \
+    src0    = __lasx_xvldrepl_w(src + filterPos[0], 0);               \
+    src1    = __lasx_xvldrepl_w(src + filterPos[1], 0);               \
+    src2    = __lasx_xvldrepl_w(src + filterPos[2], 0);               \
+    src3    = __lasx_xvldrepl_w(src + filterPos[3], 0);               \
+    filter0 = __lasx_xvld(filter, 0);                                 \
+    DUP2_ARG2(__lasx_xvilvl_w, src1, src0, src3, src2, src0, src1);   \
+                                                                      \
+    src0 = __lasx_xvilvl_d(src1, src0);                               \
+    src0 = __lasx_vext2xv_hu_bu(src0);                                \
+    src0 = __lasx_xvdp2_w_h(filter0, src0);                           \
+    src0 = __lasx_xvhaddw_d_w(src0, src0);                            \
+    src0 = __lasx_xvsrai_w(src0, _sh);                                \
+    src0 = __lasx_xvmin_w(src0, vmax);                                \
+    src0 = __lasx_xvpickev_w(src0, src0);                             \
+    src0 = __lasx_xvpermi_d(src0, 0xd8);                              \
+}
+
+#define SCALE_4_2(_sh)                                                \
+{                                                                     \
+    src0    = __lasx_xvldrepl_w(src + filterPos[0], 0);               \
+    src1    = __lasx_xvldrepl_w(src + filterPos[1], 0);               \
+    filter0 = __lasx_xvld(filter, 0);                                 \
+    src0 = __lasx_xvilvl_w(src1, src0);                               \
+    src0 = __lasx_vext2xv_hu_bu(src0);                                \
+    src0 = __lasx_xvdp2_w_h(filter0, src0);                           \
+    src0 = __lasx_xvhaddw_d_w(src0, src0);                            \
+    src0 = __lasx_xvsrai_w(src0, _sh);                                \
+    src0 = __lasx_xvmin_w(src0, vmax);                                \
+    dst[0] = __lasx_xvpickve2gr_w(src0, 0);                           \
+    dst[1] = __lasx_xvpickve2gr_w(src0, 2);                           \
+    filterPos += 2;                                                   \
+    filter    += 8;                                                   \
+    dst       += 2;                                                   \
+}
+
+#define SCALE_16                                                      \
+{                                                                     \
+    src0     = __lasx_xvldrepl_d((srcPos1 + j), 0);                   \
+    src1     = __lasx_xvldrepl_d((srcPos2 + j), 0);                   \
+    src2     = __lasx_xvldrepl_d((srcPos3 + j), 0);                   \
+    src3     = __lasx_xvldrepl_d((srcPos4 + j), 0);                   \
+    DUP4_ARG2(__lasx_xvld, filterStart1 + j, 0, filterStart2 + j, 0,  \
+              filterStart3 + j, 0, filterStart4 + j, 0, filter0,      \
+              filter1, filter2, filter3);                             \
+    src0     = __lasx_xvpermi_q(src0, src1, 0x02);                    \
+    src1     = __lasx_xvpermi_q(src2, src3, 0x02);                    \
+    filter0  = __lasx_xvpermi_q(filter0, filter1, 0x02);              \
+    filter1  = __lasx_xvpermi_q(filter2, filter3, 0x02);              \
+    DUP2_ARG2(__lasx_xvilvl_b, zero, src0, zero, src1, src0, src1);   \
+    DUP2_ARG2(__lasx_xvdp2_w_h, filter0, src0, filter1, src1,         \
+              out0, out1);                                            \
+    src0     = __lasx_xvhaddw_d_w(out0, out0);                        \
+    src1     = __lasx_xvhaddw_d_w(out1, out1);                        \
+    out0     = __lasx_xvpackev_d(src1, src0);                         \
+    out1     = __lasx_xvpackod_d(src1, src0);                         \
+    out0     = __lasx_xvadd_w(out0, out1);                            \
+    out      = __lasx_xvadd_w(out, out0);                             \
+}
+
+void ff_hscale_8_to_15_lasx(SwsContext *c, int16_t *dst, int dstW,
+                            const uint8_t *src, const int16_t *filter,
+                            const int32_t *filterPos, int filterSize)
+{
+    int i;
+    int max = (1 << 15) - 1;
+
+    if (filterSize == 8) {
+        __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+        __m256i src8, src9, src10, src11, src12, src13, src14, src15;
+        __m256i filter0, filter1, filter2, filter3;
+        __m256i filter4, filter5, filter6, filter7;
+        __m256i vmax = __lasx_xvreplgr2vr_w(max);
+        __m256i shuf = {0x0000000400000000, 0x0000000500000001, 0x0000000600000002, 0x0000000700000003};
+        int len = dstW >> 4;
+        int res = dstW & 15;
+        while (len--) {
+            SCALE_8_16(7);
+        }
+        if (res & 8) {
+            SCALE_8_8(7);
+            src0 = __lasx_xvpickev_h(src0, src0);
+            __lasx_xvstelm_d(src0, dst, 0, 0);
+            __lasx_xvstelm_d(src0, dst, 8, 2);
+            filterPos += 8;
+            filter    += 64;
+            dst       += 8;
+        }
+        if (res & 4) {
+            SCALE_8_4(7);
+            src0 = __lasx_xvpickev_h(src0, src0);
+            __lasx_xvstelm_d(src0, dst, 0, 0);
+            filterPos += 4;
+            filter    += 32;
+            dst       += 4;
+        }
+        if (res & 2) {
+            SCALE_8_2(7);
+        }
+        if (res & 1) {
+            int val = 0;
+            src0    = __lasx_xvldrepl_d(src + filterPos[0], 0);
+            filter0 = __lasx_xvld(filter, 0);
+            src0 = __lasx_vext2xv_hu_bu(src0);
+            src0 = __lasx_xvdp2_w_h(filter0, src0);
+            src0    = __lasx_xvhaddw_d_w(src0, src0);
+            src0    = __lasx_xvhaddw_q_d(src0, src0);
+            val     = __lasx_xvpickve2gr_w(src0, 0);
+            dst[0]  = FFMIN(val >> 7, max);
+        }
+    } else if (filterSize == 4) {
+        __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+        __m256i src8, src9, src10, src11, src12, src13, src14, src15;
+        __m256i filter0, filter1, filter2, filter3;
+        __m256i vmax = __lasx_xvreplgr2vr_w(max);
+        __m256i shuf = {0x0000000400000000, 0x0000000500000001, 0x0000000600000002, 0x0000000700000003};
+        int len = dstW >> 4;
+        int res = dstW & 15;
+        while (len--) {
+            SCALE_4_16(7);
+        }
+        if (res & 8) {
+            SCALE_4_8(7);
+            src0 = __lasx_xvpickev_h(src1, src0);
+            src0 = __lasx_xvperm_w(src0, shuf);
+            __lasx_xvstelm_d(src0, dst, 0, 0);
+            __lasx_xvstelm_d(src0, dst, 8, 1);
+            filterPos += 8;
+            filter    += 32;
+            dst       += 8;
+        }
+        if (res & 4) {
+            SCALE_4_4(7);
+            src0 = __lasx_xvpickev_h(src0, src0);
+            __lasx_xvstelm_d(src0, dst, 0, 0);
+            filterPos += 4;
+            filter    += 16;
+            dst       += 4;
+        }
+        if (res & 2) {
+            SCALE_4_2(7);
+        }
+        if (res & 1) {
+            int val = 0;
+            const uint8_t *srcPos = src + filterPos[0];
+
+            for (int j = 0; j < filterSize; j++) {
+                val += ((int)srcPos[j]) * filter[j];
+            }
+            dst[0] = FFMIN(val >> 7, max);
+        }
+    } else if (filterSize > 8) {
+        int filterlen = filterSize - 7;
+        int len = dstW >> 2;
+        int res = dstW & 3;
+        __m256i zero = __lasx_xvldi(0);
+
+        while (len--) {
+            __m256i src0, src1, src2, src3;
+            __m256i filter0, filter1, filter2, filter3, out0, out1;
+            __m256i out = zero;
+            const uint8_t *srcPos1 = src + filterPos[0];
+            const uint8_t *srcPos2 = src + filterPos[1];
+            const uint8_t *srcPos3 = src + filterPos[2];
+            const uint8_t *srcPos4 = src + filterPos[3];
+            const int16_t *filterStart1 = filter;
+            const int16_t *filterStart2 = filterStart1 + filterSize;
+            const int16_t *filterStart3 = filterStart2 + filterSize;
+            const int16_t *filterStart4 = filterStart3 + filterSize;
+            int j, val1 = 0, val2 = 0, val3 = 0, val4 = 0;
+
+            for (j = 0; j < filterlen; j += 8) {
+                SCALE_16
+            }
+            val1 = __lasx_xvpickve2gr_w(out, 0);
+            val2 = __lasx_xvpickve2gr_w(out, 4);
+            val3 = __lasx_xvpickve2gr_w(out, 2);
+            val4 = __lasx_xvpickve2gr_w(out, 6);
+            for (; j < filterSize; j++) {
+                val1 += ((int)srcPos1[j]) * filterStart1[j];
+                val2 += ((int)srcPos2[j]) * filterStart2[j];
+                val3 += ((int)srcPos3[j]) * filterStart3[j];
+                val4 += ((int)srcPos4[j]) * filterStart4[j];
+            }
+            dst[0] = FFMIN(val1 >> 7, max);
+            dst[1] = FFMIN(val2 >> 7, max);
+            dst[2] = FFMIN(val3 >> 7, max);
+            dst[3] = FFMIN(val4 >> 7, max);
+            dst       += 4;
+            filterPos += 4;
+            filter     = filterStart4 + filterSize;
+        }
+        for(i = 0; i < res; i++) {
+            int j, val = 0;
+            const uint8_t *srcPos = src + filterPos[i];
+            __m256i src1, filter0, out0;
+
+            for (j = 0; j < filterlen; j += 8) {
+                src1   = __lasx_xvldrepl_d((srcPos + j), 0);
+                filter0 = __lasx_xvld(filter + j, 0);
+                src1 = __lasx_xvilvl_b(zero, src1);
+                out0 = __lasx_xvdp2_w_h(filter0, src1);
+                out0 = __lasx_xvhaddw_d_w(out0, out0);
+                out0 = __lasx_xvhaddw_q_d(out0, out0);
+                val += __lasx_xvpickve2gr_w(out0, 0);
+            }
+            for (; j < filterSize; j++) {
+                val += ((int)srcPos[j]) * filter[j];
+            }
+            dst[i]  = FFMIN(val >> 7, max);
+            filter += filterSize;
+        }
+    } else {
+        for (i = 0; i < dstW; i++) {
+            int val = 0;
+            const uint8_t *srcPos = src + filterPos[i];
+
+            for (int j = 0; j < filterSize; j++) {
+                val += ((int)srcPos[j]) * filter[j];
+            }
+            dst[i]  = FFMIN(val >> 7, max);
+            filter += filterSize;
+        }
+    }
+}
+
+void ff_hscale_8_to_19_lasx(SwsContext *c, int16_t *_dst, int dstW,
+                            const uint8_t *src, const int16_t *filter,
+                            const int32_t *filterPos, int filterSize)
+{
+    int i;
+    int max = (1 << 19) - 1;
+    int32_t *dst = (int32_t *) _dst;
+
+    if (filterSize == 8) {
+        __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+        __m256i filter0, filter1, filter2, filter3;
+        __m256i vmax = __lasx_xvreplgr2vr_w(max);
+        __m256i shuf = {0x0000000400000000, 0x0000000500000001, 0x0000000600000002, 0x0000000700000003};
+        int len = dstW >> 3;
+        int res = dstW & 7;
+        while (len--) {
+            SCALE_8_8(3);
+            __lasx_xvst(src0, dst, 0);
+            filterPos += 8;
+            filter    += 64;
+            dst       += 8;
+        }
+        if (res & 4) {
+            SCALE_8_4(3);
+            __lasx_xvstelm_d(src0, dst, 0, 0);
+            __lasx_xvstelm_d(src0, dst, 8, 1);
+            filterPos += 4;
+            filter    += 32;
+            dst       += 4;
+        }
+        if (res & 2) {
+            SCALE_8_2(3);
+        }
+        if (res & 1) {
+            int val = 0;
+            __m256i src0, filter0, out0;
+
+            src0    = __lasx_xvldrepl_d(src + filterPos[0], 0);
+            filter0 = __lasx_xvld(filter, 0);
+            src0 = __lasx_vext2xv_hu_bu(src0);
+            out0 = __lasx_xvdp2_w_h(filter0, src0);
+            out0    = __lasx_xvhaddw_d_w(out0, out0);
+            out0    = __lasx_xvhaddw_q_d(out0, out0);
+            val     = __lasx_xvpickve2gr_w(out0, 0);
+            dst[0]  = FFMIN(val >> 3, max);
+        }
+    } else if (filterSize == 4) {
+        __m256i src0, src1, src2, src3, src4, src5, src6, src7;
+        __m256i filter0, filter1;
+        __m256i vmax = __lasx_xvreplgr2vr_w(max);
+        __m256i shuf = {0x0000000100000000, 0x0000000500000004, 0x0000000300000002, 0x0000000700000006};
+        int len = dstW >> 3;
+        int res = dstW & 7;
+        while (len--) {
+            SCALE_4_8(3);
+            src0 = __lasx_xvperm_w(src0, shuf);
+            __lasx_xvst(src0, dst, 0);
+            filterPos += 8;
+            filter    += 32;
+            dst       += 8;
+        }
+        if (res & 4) {
+            SCALE_4_4(3);
+            __lasx_xvstelm_d(src0, dst, 0, 0);
+            __lasx_xvstelm_d(src0, dst, 8, 1);
+            filterPos += 4;
+            filter    += 16;
+            dst       += 4;
+        }
+        if (res & 2) {
+            SCALE_4_2(3);
+        }
+        if (res & 1) {
+            int val = 0;
+            const uint8_t *srcPos = src + filterPos[0];
+
+            for (int j = 0; j < filterSize; j++) {
+                val += ((int)srcPos[j]) * filter[j];
+            }
+            dst[0] = FFMIN(val >> 3, max);
+        }
+    } else if (filterSize > 8) {
+        int len = dstW >> 2;
+        int res = dstW & 3;
+        int filterlen = filterSize - 7;
+        __m256i zero = __lasx_xvldi(0);
+
+        while (len--) {
+            __m256i src0, src1, src2, src3;
+            __m256i filter0, filter1, filter2, filter3, out0, out1;
+            __m256i out = zero;
+            const uint8_t *srcPos1 = src + filterPos[0];
+            const uint8_t *srcPos2 = src + filterPos[1];
+            const uint8_t *srcPos3 = src + filterPos[2];
+            const uint8_t *srcPos4 = src + filterPos[3];
+            const int16_t *filterStart1 = filter;
+            const int16_t *filterStart2 = filterStart1 + filterSize;
+            const int16_t *filterStart3 = filterStart2 + filterSize;
+            const int16_t *filterStart4 = filterStart3 + filterSize;
+            int j, val1 = 0, val2 = 0, val3 = 0, val4 = 0;
+
+            for (j = 0; j < filterlen; j += 8) {
+                SCALE_16
+            }
+            val1 = __lasx_xvpickve2gr_w(out, 0);
+            val2 = __lasx_xvpickve2gr_w(out, 4);
+            val3 = __lasx_xvpickve2gr_w(out, 2);
+            val4 = __lasx_xvpickve2gr_w(out, 6);
+            for (; j < filterSize; j++) {
+                val1 += ((int)srcPos1[j]) * filterStart1[j];
+                val2 += ((int)srcPos2[j]) * filterStart2[j];
+                val3 += ((int)srcPos3[j]) * filterStart3[j];
+                val4 += ((int)srcPos4[j]) * filterStart4[j];
+            }
+            dst[0] = FFMIN(val1 >> 3, max);
+            dst[1] = FFMIN(val2 >> 3, max);
+            dst[2] = FFMIN(val3 >> 3, max);
+            dst[3] = FFMIN(val4 >> 3, max);
+            dst       += 4;
+            filterPos += 4;
+            filter     = filterStart4 + filterSize;
+        }
+        for (i = 0; i < res; i++) {
+            int j, val = 0;
+            const uint8_t *srcPos = src + filterPos[i];
+            __m256i src1, filter0, out0;
+
+            for (j = 0; j < filterlen; j += 8) {
+                src1   = __lasx_xvldrepl_d((srcPos + j), 0);
+                filter0 = __lasx_xvld(filter + j, 0);
+                src1 = __lasx_xvilvl_b(zero, src1);
+                out0 = __lasx_xvdp2_w_h(filter0, src1);
+                out0 = __lasx_xvhaddw_d_w(out0, out0);
+                out0 = __lasx_xvhaddw_q_d(out0, out0);
+                val += __lasx_xvpickve2gr_w(out0, 0);
+            }
+            for (; j < filterSize; j++) {
+                val += ((int)srcPos[j]) * filter[j];
+            }
+            dst[i] = FFMIN(val >> 3, max);
+            filter += filterSize;
+        }
+    } else {
+        for (i = 0; i < dstW; i++) {
+            int val = 0;
+            const uint8_t *srcPos = src + filterPos[i];
+
+            for (int j = 0; j < filterSize; j++) {
+                val += ((int)srcPos[j]) * filter[j];
+            }
+            dst[i]  = FFMIN(val >> 3, max);
+            filter += filterSize;
+        }
+    }
+}
+
+#undef SCALE_16
+
+#define SCALE_8                                                              \
+{                                                                            \
+    int val1, val2, val3, val4;                                              \
+    __m256i src0, src1, src2, src3, filter0, filter1, out0, out1;            \
+    DUP4_ARG2(__lasx_xvld, src + filterPos[0], 0, src + filterPos[1], 0,     \
+              src + filterPos[2], 0, src + filterPos[3], 0, src0, src1, src2,\
+              src3);                                                         \
+    DUP2_ARG2(__lasx_xvld, filter, 0, filter + 16, 0, filter0, filter1);     \
+    src0    = __lasx_xvpermi_q(src0, src1, 0x02);                            \
+    src2    = __lasx_xvpermi_q(src2, src3, 0x02);                            \
+    DUP2_ARG2(__lasx_xvdp2_w_hu_h, src0, filter0, src2, filter1, out0, out1);\
+    src0    = __lasx_xvhaddw_d_w(out0, out0);                                \
+    src1    = __lasx_xvhaddw_d_w(out1, out1);                                \
+    out0    = __lasx_xvpackev_d(src1, src0);                                 \
+    out1    = __lasx_xvpackod_d(src1, src0);                                 \
+    out0    = __lasx_xvadd_w(out0, out1);                                    \
+    out0    = __lasx_xvsra_w(out0, shift);                                   \
+    val1    = __lasx_xvpickve2gr_w(out0, 0);                                 \
+    val2    = __lasx_xvpickve2gr_w(out0, 4);                                 \
+    val3    = __lasx_xvpickve2gr_w(out0, 2);                                 \
+    val4    = __lasx_xvpickve2gr_w(out0, 6);                                 \
+    dst[0]  = FFMIN(val1, max);                                              \
+    dst[1]  = FFMIN(val2, max);                                              \
+    dst[2]  = FFMIN(val3, max);                                              \
+    dst[3]  = FFMIN(val4, max);                                              \
+    filterPos += 4;                                                          \
+    filter += 32;                                                            \
+    dst += 4;                                                                \
+}
+
+#define SCALE_16                                                             \
+{                                                                            \
+    DUP4_ARG2(__lasx_xvld, srcPos1 + j, 0, srcPos2 + j, 0, srcPos3 + j,      \
+              0, srcPos4 + j, 0, src0, src1, src2, src3);                    \
+    DUP4_ARG2(__lasx_xvld, filterStart1 + j, 0, filterStart2 + j, 0,         \
+              filterStart3 + j, 0, filterStart4 + j, 0, filter0,             \
+              filter1, filter2, filter3);                                    \
+    src0     = __lasx_xvpermi_q(src0, src1, 0x02);                           \
+    src1     = __lasx_xvpermi_q(src2, src3, 0x02);                           \
+    filter0  = __lasx_xvpermi_q(filter0, filter1, 0x02);                     \
+    filter1  = __lasx_xvpermi_q(filter2, filter3, 0x02);                     \
+    DUP2_ARG2(__lasx_xvdp2_w_hu_h, src0, filter0, src1, filter1, out0, out1);\
+    src0     = __lasx_xvhaddw_d_w(out0, out0);                               \
+    src1     = __lasx_xvhaddw_d_w(out1, out1);                               \
+    out0     = __lasx_xvpackev_d(src1, src0);                                \
+    out1     = __lasx_xvpackod_d(src1, src0);                                \
+    out0     = __lasx_xvadd_w(out0, out1);                                   \
+    out      = __lasx_xvadd_w(out, out0);                                    \
+}
+
+void ff_hscale_16_to_15_lasx(SwsContext *c, int16_t *dst, int dstW,
+                             const uint8_t *_src, const int16_t *filter,
+                             const int32_t *filterPos, int filterSize)
+{
+    const AVPixFmtDescriptor *desc = av_pix_fmt_desc_get(c->srcFormat);
+    int i;
+    const uint16_t *src = (const uint16_t *) _src;
+    int sh              = desc->comp[0].depth - 1;
+    int max = (1 << 15) - 1;
+    int len = dstW >> 2;
+    int res = dstW & 3;
+    __m256i shift;
+    __m256i zero = __lasx_xvldi(0);
+
+    if (sh < 15) {
+        sh = isAnyRGB(c->srcFormat) || c->srcFormat==AV_PIX_FMT_PAL8 ? 13 :
+                      (desc->comp[0].depth - 1);
+    } else if (desc->flags && AV_PIX_FMT_FLAG_FLOAT) {
+        sh = 15;
+    }
+    shift = __lasx_xvreplgr2vr_w(sh);
+
+    if (filterSize == 8) {
+        for (i = 0; i < len; i++) {
+            SCALE_8
+        }
+        for (i = 0; i < res; i++) {
+            int val = 0;
+            __m256i src0, filter0, out0;
+
+            src0    = __lasx_xvld(src + filterPos[i], 0);
+            filter0 = __lasx_xvld(filter, 0);
+            out0 = __lasx_xvdp2_w_hu_h(src0, filter0);
+            out0    = __lasx_xvhaddw_d_w(out0, out0);
+            out0    = __lasx_xvhaddw_q_d(out0, out0);
+            val     = __lasx_xvpickve2gr_w(out0, 0);
+            dst[i]  = FFMIN(val >> sh, max);
+            filter += 8;
+        }
+    } else if (filterSize == 4) {
+        for (i = 0; i < len; i++) {
+            __m256i src1, src2, src3, src4, src0, filter0, out0;
+
+            src1 = __lasx_xvldrepl_d(src + filterPos[0], 0);
+            src2 = __lasx_xvldrepl_d(src + filterPos[1], 0);
+            src3 = __lasx_xvldrepl_d(src + filterPos[2], 0);
+            src4 = __lasx_xvldrepl_d(src + filterPos[3], 0);
+            filter0 = __lasx_xvld(filter, 0);
+            src1 = __lasx_xvextrins_d(src1, src2, 0x10);
+            src3 = __lasx_xvextrins_d(src3, src4, 0x10);
+            src0 = __lasx_xvpermi_q(src1, src3, 0x02);
+            out0 = __lasx_xvdp2_w_hu_h(src0, filter0);
+            out0 = __lasx_xvhaddw_d_w(out0, out0);
+            out0 = __lasx_xvsra_w(out0, shift);
+            dst[0] = FFMIN((__lasx_xvpickve2gr_w(out0, 0)), max);
+            dst[1] = FFMIN((__lasx_xvpickve2gr_w(out0, 2)), max);
+            dst[2] = FFMIN((__lasx_xvpickve2gr_w(out0, 4)), max);
+            dst[3] = FFMIN((__lasx_xvpickve2gr_w(out0, 6)), max);
+            dst       += 4;
+            filterPos += 4;
+            filter    += 16;
+        }
+        for (i = 0; i < res; i++) {
+            int val = 0;
+            const uint16_t *srcPos = src + filterPos[i];
+
+            for (int j = 0; j < filterSize; j++) {
+                val += ((int)srcPos[j]) * filter[j];
+            }
+            dst[i]  = FFMIN(val >> sh, max);
+            filter += 4;
+        }
+    } else if (filterSize > 8) {
+        int filterlen = filterSize - 7;
+
+        for (i = 0; i < len; i++) {
+            __m256i src0, src1, src2, src3;
+            __m256i filter0, filter1, filter2, filter3, out0, out1;
+            __m256i out = zero;
+            const uint16_t *srcPos1 = src + filterPos[0];
+            const uint16_t *srcPos2 = src + filterPos[1];
+            const uint16_t *srcPos3 = src + filterPos[2];
+            const uint16_t *srcPos4 = src + filterPos[3];
+            const int16_t *filterStart1 = filter;
+            const int16_t *filterStart2 = filterStart1 + filterSize;
+            const int16_t *filterStart3 = filterStart2 + filterSize;
+            const int16_t *filterStart4 = filterStart3 + filterSize;
+            int j, val1 = 0, val2 = 0, val3 = 0, val4 = 0;
+
+            for (j = 0; j < filterlen; j += 8) {
+                SCALE_16
+            }
+            val1 = __lasx_xvpickve2gr_w(out, 0);
+            val2 = __lasx_xvpickve2gr_w(out, 4);
+            val3 = __lasx_xvpickve2gr_w(out, 2);
+            val4 = __lasx_xvpickve2gr_w(out, 6);
+            for (; j < filterSize; j++) {
+                val1 += ((int)srcPos1[j]) * filterStart1[j];
+                val2 += ((int)srcPos2[j]) * filterStart2[j];
+                val3 += ((int)srcPos3[j]) * filterStart3[j];
+                val4 += ((int)srcPos4[j]) * filterStart4[j];
+            }
+            dst[0] = FFMIN(val1 >> sh, max);
+            dst[1] = FFMIN(val2 >> sh, max);
+            dst[2] = FFMIN(val3 >> sh, max);
+            dst[3] = FFMIN(val4 >> sh, max);
+            dst       += 4;
+            filterPos += 4;
+            filter     = filterStart4 + filterSize;
+        }
+        for (i = 0; i < res; i++) {
+            int j, val = 0;
+            const uint16_t *srcPos      = src + filterPos[i];
+            __m256i src0, filter0, out0;
+
+            for (j = 0; j < filterlen; j += 8) {
+                DUP2_ARG2(__lasx_xvld, srcPos + j, 0, filter + j, 0, src0, filter0);
+                out0 = __lasx_xvdp2_w_hu_h(src0, filter0);
+                out0    = __lasx_xvhaddw_d_w(out0, out0);
+                out0    = __lasx_xvhaddw_q_d(out0, out0);
+                val    += __lasx_xvpickve2gr_w(out0, 0);
+            }
+            for (; j < filterSize; j++) {
+                val += ((int)srcPos[j]) * filter[j];
+            }
+            dst[i]  = FFMIN(val >> sh, max);
+            filter += filterSize;
+        }
+    } else {
+        for (i = 0; i < dstW; i++) {
+            int val = 0;
+            const uint16_t *srcPos = src + filterPos[i];
+
+            for (int j = 0; j < filterSize; j++) {
+                val += ((int)srcPos[j]) * filter[j];
+            }
+            dst[i]  = FFMIN(val >> sh, max);
+            filter += filterSize;
+        }
+    }
+}
+
+void ff_hscale_16_to_19_lasx(SwsContext *c, int16_t *_dst, int dstW,
+                             const uint8_t *_src, const int16_t *filter,
+                             const int32_t *filterPos, int filterSize)
+{
+    const AVPixFmtDescriptor *desc = av_pix_fmt_desc_get(c->srcFormat);
+    int i;
+    int32_t *dst        = (int32_t *) _dst;
+    const uint16_t *src = (const uint16_t *) _src;
+    int sh              = desc->comp[0].depth - 5;
+    int max = (1 << 19) - 1;
+    int len = dstW >> 2;
+    int res = dstW & 3;
+    __m256i shift;
+    __m256i zero = __lasx_xvldi(0);
+
+    if ((isAnyRGB(c->srcFormat) || c->srcFormat == AV_PIX_FMT_PAL8)
+         && desc->comp[0].depth<16) {
+        sh = 9;
+    } else if (desc->flags & AV_PIX_FMT_FLAG_FLOAT) {
+        sh = 11;
+    }
+    shift = __lasx_xvreplgr2vr_w(sh);
+
+    if (filterSize == 8) {
+        for (i = 0; i < len; i++) {
+            SCALE_8
+        }
+        for (i = 0; i < res; i++) {
+            int val = 0;
+            __m256i src0, filter0, out0;
+
+            DUP2_ARG2(__lasx_xvld, src + filterPos[i], 0, filter, 0, src0, filter0);
+            out0 = __lasx_xvdp2_w_hu_h(src0, filter0);
+            out0 = __lasx_xvhaddw_d_w(out0, out0);
+            out0 = __lasx_xvhaddw_q_d(out0, out0);
+            val  = __lasx_xvpickve2gr_w(out0, 0);
+            dst[i] = FFMIN(val >> sh, max);
+            filter += 8;
+        }
+    } else if (filterSize == 4) {
+        for (i = 0; i < len; i++) {
+            __m256i src1, src2, src3, src4, src0, filter0, out0;
+
+            src1 = __lasx_xvldrepl_d(src + filterPos[0], 0);
+            src2 = __lasx_xvldrepl_d(src + filterPos[1], 0);
+            src3 = __lasx_xvldrepl_d(src + filterPos[2], 0);
+            src4 = __lasx_xvldrepl_d(src + filterPos[3], 0);
+            filter0 = __lasx_xvld(filter, 0);
+            src1 = __lasx_xvextrins_d(src1, src2, 0x10);
+            src3 = __lasx_xvextrins_d(src3, src4, 0x10);
+            src0 = __lasx_xvpermi_q(src1, src3, 0x02);
+            out0 = __lasx_xvdp2_w_hu_h(src0, filter0);
+            out0 = __lasx_xvhaddw_d_w(out0, out0);
+            out0 = __lasx_xvsra_w(out0, shift);
+            dst[0] = FFMIN((__lasx_xvpickve2gr_w(out0, 0)), max);
+            dst[1] = FFMIN((__lasx_xvpickve2gr_w(out0, 2)), max);
+            dst[2] = FFMIN((__lasx_xvpickve2gr_w(out0, 4)), max);
+            dst[3] = FFMIN((__lasx_xvpickve2gr_w(out0, 6)), max);
+            dst       += 4;
+            filterPos += 4;
+            filter    += 16;
+        }
+        for (i = 0; i < res; i++) {
+            int val = 0;
+            const uint16_t *srcPos = src + filterPos[i];
+
+            for (int j = 0; j < filterSize; j++) {
+                val += ((int)srcPos[j]) * filter[j];
+            }
+            dst[i]  = FFMIN(val >> sh, max);
+            filter += 4;
+        }
+    } else if (filterSize > 8) {
+        int filterlen = filterSize - 7;
+
+        for (i = 0; i < len; i ++) {
+            __m256i src0, src1, src2, src3;
+            __m256i filter0, filter1, filter2, filter3, out0, out1;
+            __m256i out = zero;
+            const uint16_t *srcPos1 = src + filterPos[0];
+            const uint16_t *srcPos2 = src + filterPos[1];
+            const uint16_t *srcPos3 = src + filterPos[2];
+            const uint16_t *srcPos4 = src + filterPos[3];
+            const int16_t *filterStart1 = filter;
+            const int16_t *filterStart2 = filterStart1 + filterSize;
+            const int16_t *filterStart3 = filterStart2 + filterSize;
+            const int16_t *filterStart4 = filterStart3 + filterSize;
+            int j, val1 = 0, val2 = 0, val3 = 0, val4 = 0;
+
+            for (j = 0; j < filterlen; j += 8) {
+                SCALE_16
+            }
+            val1 = __lasx_xvpickve2gr_w(out, 0);
+            val2 = __lasx_xvpickve2gr_w(out, 4);
+            val3 = __lasx_xvpickve2gr_w(out, 2);
+            val4 = __lasx_xvpickve2gr_w(out, 6);
+            for (; j < filterSize; j++) {
+                val1 += ((int)srcPos1[j]) * filterStart1[j];
+                val2 += ((int)srcPos2[j]) * filterStart2[j];
+                val3 += ((int)srcPos3[j]) * filterStart3[j];
+                val4 += ((int)srcPos4[j]) * filterStart4[j];
+            }
+            dst[0] = FFMIN(val1 >> sh, max);
+            dst[1] = FFMIN(val2 >> sh, max);
+            dst[2] = FFMIN(val3 >> sh, max);
+            dst[3] = FFMIN(val4 >> sh, max);
+            dst       += 4;
+            filterPos += 4;
+            filter     = filterStart4 + filterSize;
+        }
+        for (i = 0; i < res; i++) {
+            int j, val = 0;
+            const uint16_t *srcPos      = src + filterPos[i];
+            __m256i src0, filter0, out0;
+
+            for (j = 0; j < filterlen; j += 8) {
+                DUP2_ARG2(__lasx_xvld, srcPos + j, 0, filter + j, 0, src0, filter0);
+                out0 = __lasx_xvdp2_w_hu_h(src0, filter0);
+                out0    = __lasx_xvhaddw_d_w(out0, out0);
+                out0    = __lasx_xvhaddw_q_d(out0, out0);
+                val    += __lasx_xvpickve2gr_w(out0, 0);
+            }
+            for (; j < filterSize; j++) {
+                val += ((int)srcPos[j]) * filter[j];
+            }
+            dst[i]  = FFMIN(val >> sh, max);
+            filter += filterSize;
+        }
+    } else {
+        for (i = 0; i < dstW; i++) {
+            int val = 0;
+            const uint16_t *srcPos = src + filterPos[i];
+
+            for (int j = 0; j < filterSize; j++) {
+                val += ((int)srcPos[j]) * filter[j];
+            }
+            dst[i]  = FFMIN(val >> sh, max);
+            filter += filterSize;
+        }
+    }
+}
+
+#undef SCALE_8
+#undef SCALE_16
diff --git a/libswscale/loongarch/swscale_loongarch.h b/libswscale/loongarch/swscale_loongarch.h
new file mode 100644
index 0000000000..7e6cc87ddf
--- /dev/null
+++ b/libswscale/loongarch/swscale_loongarch.h
@@ -0,0 +1,132 @@
+/*
+ * Copyright (C) 2021 Loongson Technology Corporation Limited
+ * Contributed by Hao Chen(chenhao@loongson.cn)
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef SWSCALE_LOONGARCH_SWSCALE_LOONGARCH_H
+#define SWSCALE_LOONGARCH_SWSCALE_LOONGARCH_H
+
+#include "libswscale/swscale.h"
+#include "libswscale/swscale_internal.h"
+
+void ff_hscale_8_to_15_lsx(SwsContext *c, int16_t *dst, int dstW,
+                           const uint8_t *src, const int16_t *filter,
+                           const int32_t *filterPos, int filterSize);
+
+void ff_hscale_8_to_19_lsx(SwsContext *c, int16_t *_dst, int dstW,
+                           const uint8_t *src, const int16_t *filter,
+                           const int32_t *filterPos, int filterSize);
+
+void ff_hscale_16_to_15_lsx(SwsContext *c, int16_t *_dst, int dstW,
+                            const uint8_t *_src, const int16_t *filter,
+                            const int32_t *filterPos, int filterSize);
+
+void ff_hscale_16_to_15_sub_lsx(SwsContext *c, int16_t *_dst, int dstW,
+                                const uint8_t *_src, const int16_t *filter,
+                                const int32_t *filterPos, int filterSize, int sh);
+
+void ff_hscale_16_to_19_lsx(SwsContext *c, int16_t *_dst, int dstW,
+                            const uint8_t *_src, const int16_t *filter,
+                            const int32_t *filterPos, int filterSize);
+
+void ff_hscale_16_to_19_sub_lsx(SwsContext *c, int16_t *_dst, int dstW,
+                                const uint8_t *_src, const int16_t *filter,
+                                const int32_t *filterPos, int filterSize, int sh);
+
+void planar_rgb_to_uv_lsx(uint8_t *_dstU, uint8_t *_dstV, const uint8_t *src[4],
+                          int width, int32_t *rgb2yuv);
+
+void planar_rgb_to_y_lsx(uint8_t *_dst, const uint8_t *src[4], int width,
+                         int32_t *rgb2yuv);
+
+void ff_yuv2planeX_8_lsx(const int16_t *filter, int filterSize,
+                         const int16_t **src, uint8_t *dest, int dstW,
+                         const uint8_t *dither, int offset);
+
+int yuv420_rgb24_lsx(SwsContext *c, const uint8_t *src[], int srcStride[],
+                     int srcSliceY, int srcSliceH, uint8_t *dst[], int dstStride[]);
+
+int yuv420_bgr24_lsx(SwsContext *c, const uint8_t *src[], int srcStride[],
+                     int srcSliceY, int srcSliceH, uint8_t *dst[], int dstStride[]);
+
+int yuv420_rgba32_lsx(SwsContext *c, const uint8_t *src[], int srcStride[],
+                      int srcSliceY, int srcSliceH, uint8_t *dst[], int dstStride[]);
+
+int yuv420_bgra32_lsx(SwsContext *c, const uint8_t *src[], int srcStride[],
+                      int srcSliceY, int srcSliceH, uint8_t *dst[], int dstStride[]);
+
+int yuv420_argb32_lsx(SwsContext *c, const uint8_t *src[], int srcStride[],
+                      int srcSliceY, int srcSliceH, uint8_t *dst[], int dstStride[]);
+
+int yuv420_abgr32_lsx(SwsContext *c, const uint8_t *src[], int srcStride[],
+                      int srcSliceY, int srcSliceH, uint8_t *dst[], int dstStride[]);
+
+av_cold void ff_sws_init_output_lsx(SwsContext *c);
+
+void ff_hscale_8_to_15_lasx(SwsContext *c, int16_t *dst, int dstW,
+                            const uint8_t *src, const int16_t *filter,
+                            const int32_t *filterPos, int filterSize);
+
+void ff_hscale_8_to_19_lasx(SwsContext *c, int16_t *_dst, int dstW,
+                            const uint8_t *src, const int16_t *filter,
+                            const int32_t *filterPos, int filterSize);
+
+void ff_hscale_16_to_19_lasx(SwsContext *c, int16_t *_dst, int dstW,
+                             const uint8_t *_src, const int16_t *filter,
+                             const int32_t *filterPos, int filterSize);
+
+void ff_hscale_16_to_15_lasx(SwsContext *c, int16_t *dst, int dstW,
+                             const uint8_t *_src, const int16_t *filter,
+                             const int32_t *filterPos, int filterSize);
+
+void ff_yuv2planeX_8_lasx(const int16_t *filter, int filterSize,
+                          const int16_t **src, uint8_t *dest, int dstW,
+                          const uint8_t *dither, int offset);
+
+int yuv420_rgb24_lasx(SwsContext *c, const uint8_t *src[], int srcStride[],
+                      int srcSliceY, int srcSliceH, uint8_t *dst[], int dstStride[]);
+
+int yuv420_bgr24_lasx(SwsContext *c, const uint8_t *src[], int srcStride[],
+                      int srcSliceY, int srcSliceH, uint8_t *dst[], int dstStride[]);
+
+int yuv420_rgba32_lasx(SwsContext *c, const uint8_t *src[], int srcStride[],
+                       int srcSliceY, int srcSliceH, uint8_t *dst[], int dstStride[]);
+
+int yuv420_bgra32_lasx(SwsContext *c, const uint8_t *src[], int srcStride[],
+                       int srcSliceY, int srcSliceH, uint8_t *dst[], int dstStride[]);
+
+int yuv420_argb32_lasx(SwsContext *c, const uint8_t *src[], int srcStride[],
+                       int srcSliceY, int srcSliceH, uint8_t *dst[], int dstStride[]);
+
+int yuv420_abgr32_lasx(SwsContext *c, const uint8_t *src[], int srcStride[],
+                       int srcSliceY, int srcSliceH, uint8_t *dst[], int dstStride[]);
+
+void ff_interleave_bytes_lasx(const uint8_t *src1, const uint8_t *src2,
+                              uint8_t *dest, int width, int height,
+                              int src1Stride, int src2Stride, int dstStride);
+
+av_cold void ff_sws_init_output_loongarch(SwsContext *c);
+
+void planar_rgb_to_uv_lasx(uint8_t *_dstU, uint8_t *_dstV, const uint8_t *src[4],
+                           int width, int32_t *rgb2yuv);
+
+void planar_rgb_to_y_lasx(uint8_t *_dst, const uint8_t *src[4], int width,
+                          int32_t *rgb2yuv);
+
+#endif /* SWSCALE_LOONGARCH_SWSCALE_LOONGARCH_H */
diff --git a/libswscale/loongarch/swscale_lsx.c b/libswscale/loongarch/swscale_lsx.c
new file mode 100644
index 0000000000..da8eabfca3
--- /dev/null
+++ b/libswscale/loongarch/swscale_lsx.c
@@ -0,0 +1,57 @@
+/*
+ * Loongson LSX optimized swscale
+ *
+ * Copyright (c) 2023 Loongson Technology Corporation Limited
+ * Contributed by Lu Wang <wanglu@loongson.cn>
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "swscale_loongarch.h"
+
+void ff_hscale_16_to_15_lsx(SwsContext *c, int16_t *_dst, int dstW,
+                            const uint8_t *_src, const int16_t *filter,
+                            const int32_t *filterPos, int filterSize)
+{
+    const AVPixFmtDescriptor *desc = av_pix_fmt_desc_get(c->srcFormat);
+    int sh              = desc->comp[0].depth - 1;
+
+    if (sh < 15) {
+        sh = isAnyRGB(c->srcFormat) || c->srcFormat==AV_PIX_FMT_PAL8 ? 13 :
+                      (desc->comp[0].depth - 1);
+    } else if (desc->flags && AV_PIX_FMT_FLAG_FLOAT) {
+        sh = 15;
+    }
+    ff_hscale_16_to_15_sub_lsx(c, _dst, dstW, _src, filter, filterPos, filterSize, sh);
+}
+
+void ff_hscale_16_to_19_lsx(SwsContext *c, int16_t *_dst, int dstW,
+                            const uint8_t *_src, const int16_t *filter,
+                            const int32_t *filterPos, int filterSize)
+{
+    const AVPixFmtDescriptor *desc = av_pix_fmt_desc_get(c->srcFormat);
+    int bits            = desc->comp[0].depth - 1;
+    int sh              = bits - 4;
+
+    if ((isAnyRGB(c->srcFormat) || c->srcFormat==AV_PIX_FMT_PAL8) && desc->comp[0].depth<16) {
+
+        sh = 9;
+    } else if (desc->flags & AV_PIX_FMT_FLAG_FLOAT) { /* float input are process like uint 16bpc */
+        sh = 16 - 1 - 4;
+    }
+    ff_hscale_16_to_19_sub_lsx(c, _dst, dstW, _src, filter, filterPos, filterSize, sh);
+}
diff --git a/libswscale/loongarch/yuv2rgb_lasx.c b/libswscale/loongarch/yuv2rgb_lasx.c
new file mode 100644
index 0000000000..4fd7873aea
--- /dev/null
+++ b/libswscale/loongarch/yuv2rgb_lasx.c
@@ -0,0 +1,330 @@
+/*
+ * Copyright (C) 2021 Loongson Technology Co. Ltd.
+ * Contributed by Hao Chen(chenhao@loongson.cn)
+ * All rights reserved.
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "swscale_loongarch.h"
+#include "libavutil/loongarch/loongson_intrinsics.h"
+
+#define YUV2RGB_LOAD_COE                                     \
+    /* Load x_offset */                                      \
+    __m256i y_offset = __lasx_xvreplgr2vr_d(c->yOffset);     \
+    __m256i u_offset = __lasx_xvreplgr2vr_d(c->uOffset);     \
+    __m256i v_offset = __lasx_xvreplgr2vr_d(c->vOffset);     \
+    /* Load x_coeff  */                                      \
+    __m256i ug_coeff = __lasx_xvreplgr2vr_d(c->ugCoeff);     \
+    __m256i vg_coeff = __lasx_xvreplgr2vr_d(c->vgCoeff);     \
+    __m256i y_coeff  = __lasx_xvreplgr2vr_d(c->yCoeff);      \
+    __m256i ub_coeff = __lasx_xvreplgr2vr_d(c->ubCoeff);     \
+    __m256i vr_coeff = __lasx_xvreplgr2vr_d(c->vrCoeff);     \
+
+#define LOAD_YUV_16                                          \
+    m_y1 = __lasx_xvld(py_1, 0);                             \
+    m_y2 = __lasx_xvld(py_2, 0);                             \
+    m_u  = __lasx_xvldrepl_d(pu, 0);                         \
+    m_v  = __lasx_xvldrepl_d(pv, 0);                         \
+    DUP2_ARG2(__lasx_xvilvl_b, m_u, m_u, m_v, m_v, m_u, m_v);\
+    DUP4_ARG1(__lasx_vext2xv_hu_bu, m_y1, m_y2, m_u, m_v,    \
+              m_y1, m_y2, m_u, m_v);                         \
+
+/* YUV2RGB method
+ * The conversion method is as follows:
+ * R = Y' * y_coeff + V' * vr_coeff
+ * G = Y' * y_coeff + V' * vg_coeff + U' * ug_coeff
+ * B = Y' * y_coeff + U' * ub_coeff
+ *
+ * where X' = X * 8 - x_offset
+ *
+ */
+
+#define YUV2RGB                                                            \
+    m_y1 = __lasx_xvslli_h(m_y1, 3);                                       \
+    m_y2 = __lasx_xvslli_h(m_y2, 3);                                       \
+    m_u  = __lasx_xvslli_h(m_u, 3);                                        \
+    m_v  = __lasx_xvslli_h(m_v, 3);                                        \
+    m_y1 = __lasx_xvsub_h(m_y1, y_offset);                                 \
+    m_y2 = __lasx_xvsub_h(m_y2, y_offset);                                 \
+    m_u  = __lasx_xvsub_h(m_u, u_offset);                                  \
+    m_v  = __lasx_xvsub_h(m_v, v_offset);                                  \
+    y_1  = __lasx_xvmuh_h(m_y1, y_coeff);                                  \
+    y_2  = __lasx_xvmuh_h(m_y2, y_coeff);                                  \
+    u2g  = __lasx_xvmuh_h(m_u, ug_coeff);                                  \
+    u2b  = __lasx_xvmuh_h(m_u, ub_coeff);                                  \
+    v2r  = __lasx_xvmuh_h(m_v, vr_coeff);                                  \
+    v2g  = __lasx_xvmuh_h(m_v, vg_coeff);                                  \
+    r1   = __lasx_xvsadd_h(y_1, v2r);                                      \
+    v2g  = __lasx_xvsadd_h(v2g, u2g);                                      \
+    g1   = __lasx_xvsadd_h(y_1, v2g);                                      \
+    b1   = __lasx_xvsadd_h(y_1, u2b);                                      \
+    r2   = __lasx_xvsadd_h(y_2, v2r);                                      \
+    g2   = __lasx_xvsadd_h(y_2, v2g);                                      \
+    b2   = __lasx_xvsadd_h(y_2, u2b);                                      \
+    DUP4_ARG1(__lasx_xvclip255_h, r1, g1, b1, r2, r1, g1, b1, r2);         \
+    DUP2_ARG1(__lasx_xvclip255_h, g2, b2, g2, b2);                         \
+
+#define RGB_PACK(r, g, b, rgb_l, rgb_h)                                    \
+{                                                                          \
+    __m256i rg;                                                            \
+    rg = __lasx_xvpackev_b(g, r);                                          \
+    DUP2_ARG3(__lasx_xvshuf_b, b, rg, shuf2, b, rg, shuf3, rgb_l, rgb_h);  \
+}
+
+#define RGB32_PACK(a, r, g, b, rgb_l, rgb_h)                               \
+{                                                                          \
+    __m256i ra, bg, tmp0, tmp1;                                            \
+    ra    = __lasx_xvpackev_b(r, a);                                       \
+    bg    = __lasx_xvpackev_b(b, g);                                       \
+    tmp0  = __lasx_xvilvl_h(bg, ra);                                       \
+    tmp1  = __lasx_xvilvh_h(bg, ra);                                       \
+    rgb_l = __lasx_xvpermi_q(tmp1, tmp0, 0x20);                            \
+    rgb_h = __lasx_xvpermi_q(tmp1, tmp0, 0x31);                            \
+}
+
+#define RGB_STORE(rgb_l, rgb_h, image)                                       \
+{                                                                            \
+    __lasx_xvstelm_d(rgb_l, image, 0,  0);                                   \
+    __lasx_xvstelm_d(rgb_l, image, 8,  1);                                   \
+    __lasx_xvstelm_d(rgb_h, image, 16, 0);                                   \
+    __lasx_xvstelm_d(rgb_l, image, 24, 2);                                   \
+    __lasx_xvstelm_d(rgb_l, image, 32, 3);                                   \
+    __lasx_xvstelm_d(rgb_h, image, 40, 2);                                   \
+}
+
+#define RGB32_STORE(rgb_l, rgb_h, image)                                     \
+{                                                                            \
+    __lasx_xvst(rgb_l, image, 0);                                            \
+    __lasx_xvst(rgb_h, image, 32);                                           \
+}
+
+#define YUV2RGBFUNC(func_name, dst_type, alpha)                                     \
+           int func_name(SwsContext *c, const uint8_t *src[],                       \
+                         int srcStride[], int srcSliceY, int srcSliceH,             \
+                         uint8_t *dst[], int dstStride[])                           \
+{                                                                                   \
+    int x, y, h_size, vshift, res;                                                  \
+    __m256i m_y1, m_y2, m_u, m_v;                                                   \
+    __m256i y_1, y_2, u2g, v2g, u2b, v2r, rgb1_l, rgb1_h;                           \
+    __m256i rgb2_l, rgb2_h, r1, g1, b1, r2, g2, b2;                                 \
+    __m256i shuf2 = {0x0504120302100100, 0x0A18090816070614,                        \
+                     0x0504120302100100, 0x0A18090816070614};                       \
+    __m256i shuf3 = {0x1E0F0E1C0D0C1A0B, 0x0101010101010101,                        \
+                     0x1E0F0E1C0D0C1A0B, 0x0101010101010101};                       \
+    YUV2RGB_LOAD_COE                                                                \
+                                                                                    \
+    h_size = c->dstW >> 4;                                                          \
+    res = (c->dstW & 15) >> 1;                                                      \
+    vshift = c->srcFormat != AV_PIX_FMT_YUV422P;                                    \
+    for (y = 0; y < srcSliceH; y += 2) {                                            \
+        dst_type av_unused *r, *g, *b;                                              \
+        dst_type *image1    = (dst_type *)(dst[0] + (y + srcSliceY) * dstStride[0]);\
+        dst_type *image2    = (dst_type *)(image1 +                   dstStride[0]);\
+        const uint8_t *py_1 = src[0] +               y * srcStride[0];              \
+        const uint8_t *py_2 = py_1   +                   srcStride[0];              \
+        const uint8_t *pu   = src[1] +   (y >> vshift) * srcStride[1];              \
+        const uint8_t *pv   = src[2] +   (y >> vshift) * srcStride[2];              \
+        for(x = 0; x < h_size; x++) {                                               \
+
+#define YUV2RGBFUNC32(func_name, dst_type, alpha)                                   \
+           int func_name(SwsContext *c, const uint8_t *src[],                       \
+                         int srcStride[], int srcSliceY, int srcSliceH,             \
+                         uint8_t *dst[], int dstStride[])                           \
+{                                                                                   \
+    int x, y, h_size, vshift, res;                                                  \
+    __m256i m_y1, m_y2, m_u, m_v;                                                   \
+    __m256i y_1, y_2, u2g, v2g, u2b, v2r, rgb1_l, rgb1_h;                           \
+    __m256i rgb2_l, rgb2_h, r1, g1, b1, r2, g2, b2;                                 \
+    __m256i a = __lasx_xvldi(0xFF);                                                 \
+                                                                                    \
+    YUV2RGB_LOAD_COE                                                                \
+                                                                                    \
+    h_size = c->dstW >> 4;                                                          \
+    res = (c->dstW & 15) >> 1;                                                      \
+    vshift = c->srcFormat != AV_PIX_FMT_YUV422P;                                    \
+    for (y = 0; y < srcSliceH; y += 2) {                                            \
+        int yd = y + srcSliceY;                                                     \
+        dst_type av_unused *r, *g, *b;                                              \
+        dst_type *image1    = (dst_type *)(dst[0] + (yd)     * dstStride[0]);       \
+        dst_type *image2    = (dst_type *)(dst[0] + (yd + 1) * dstStride[0]);       \
+        const uint8_t *py_1 = src[0] +               y * srcStride[0];              \
+        const uint8_t *py_2 = py_1   +                   srcStride[0];              \
+        const uint8_t *pu   = src[1] +   (y >> vshift) * srcStride[1];              \
+        const uint8_t *pv   = src[2] +   (y >> vshift) * srcStride[2];              \
+        for(x = 0; x < h_size; x++) {                                               \
+
+#define DEALYUV2RGBREMAIN                                                           \
+            py_1 += 16;                                                             \
+            py_2 += 16;                                                             \
+            pu += 8;                                                                \
+            pv += 8;                                                                \
+            image1 += 48;                                                           \
+            image2 += 48;                                                           \
+        }                                                                           \
+        for (x = 0; x < res; x++) {                                                 \
+            int av_unused U, V, Y;                                                  \
+            U = pu[0];                                                              \
+            V = pv[0];                                                              \
+            r = (void *)c->table_rV[V+YUVRGB_TABLE_HEADROOM];                       \
+            g = (void *)(c->table_gU[U+YUVRGB_TABLE_HEADROOM]                       \
+                       + c->table_gV[V+YUVRGB_TABLE_HEADROOM]);                     \
+            b = (void *)c->table_bU[U+YUVRGB_TABLE_HEADROOM];                       \
+
+#define DEALYUV2RGBREMAIN32                                                         \
+            py_1 += 16;                                                             \
+            py_2 += 16;                                                             \
+            pu += 8;                                                                \
+            pv += 8;                                                                \
+            image1 += 16;                                                           \
+            image2 += 16;                                                           \
+        }                                                                           \
+        for (x = 0; x < res; x++) {                                                 \
+            int av_unused U, V, Y;                                                  \
+            U = pu[0];                                                              \
+            V = pv[0];                                                              \
+            r = (void *)c->table_rV[V+YUVRGB_TABLE_HEADROOM];                       \
+            g = (void *)(c->table_gU[U+YUVRGB_TABLE_HEADROOM]                       \
+                       + c->table_gV[V+YUVRGB_TABLE_HEADROOM]);                     \
+            b = (void *)c->table_bU[U+YUVRGB_TABLE_HEADROOM];                       \
+
+
+#define PUTRGB24(dst, src)                  \
+    Y      = src[0];                        \
+    dst[0] = r[Y];                          \
+    dst[1] = g[Y];                          \
+    dst[2] = b[Y];                          \
+    Y      = src[1];                        \
+    dst[3] = r[Y];                          \
+    dst[4] = g[Y];                          \
+    dst[5] = b[Y];
+
+#define PUTBGR24(dst, src)                  \
+    Y      = src[0];                        \
+    dst[0] = b[Y];                          \
+    dst[1] = g[Y];                          \
+    dst[2] = r[Y];                          \
+    Y      = src[1];                        \
+    dst[3] = b[Y];                          \
+    dst[4] = g[Y];                          \
+    dst[5] = r[Y];
+
+#define PUTRGB(dst, src)                    \
+    Y      = src[0];                        \
+    dst[0] = r[Y] + g[Y] + b[Y];            \
+    Y      = src[1];                        \
+    dst[1] = r[Y] + g[Y] + b[Y];            \
+
+#define ENDRES                              \
+    pu += 1;                                \
+    pv += 1;                                \
+    py_1 += 2;                              \
+    py_2 += 2;                              \
+    image1 += 6;                            \
+    image2 += 6;                            \
+
+#define ENDRES32                            \
+    pu += 1;                                \
+    pv += 1;                                \
+    py_1 += 2;                              \
+    py_2 += 2;                              \
+    image1 += 2;                            \
+    image2 += 2;                            \
+
+#define END_FUNC()                          \
+        }                                   \
+    }                                       \
+    return srcSliceH;                       \
+}
+
+YUV2RGBFUNC(yuv420_rgb24_lasx, uint8_t, 0)
+    LOAD_YUV_16
+    YUV2RGB
+    RGB_PACK(r1, g1, b1, rgb1_l, rgb1_h);
+    RGB_PACK(r2, g2, b2, rgb2_l, rgb2_h);
+    RGB_STORE(rgb1_l, rgb1_h, image1);
+    RGB_STORE(rgb2_l, rgb2_h, image2);
+    DEALYUV2RGBREMAIN
+    PUTRGB24(image1, py_1);
+    PUTRGB24(image2, py_2);
+    ENDRES
+    END_FUNC()
+
+YUV2RGBFUNC(yuv420_bgr24_lasx, uint8_t, 0)
+    LOAD_YUV_16
+    YUV2RGB
+    RGB_PACK(b1, g1, r1, rgb1_l, rgb1_h);
+    RGB_PACK(b2, g2, r2, rgb2_l, rgb2_h);
+    RGB_STORE(rgb1_l, rgb1_h, image1);
+    RGB_STORE(rgb2_l, rgb2_h, image2);
+    DEALYUV2RGBREMAIN
+    PUTBGR24(image1, py_1);
+    PUTBGR24(image2, py_2);
+    ENDRES
+    END_FUNC()
+
+YUV2RGBFUNC32(yuv420_rgba32_lasx, uint32_t, 0)
+    LOAD_YUV_16
+    YUV2RGB
+    RGB32_PACK(r1, g1, b1, a, rgb1_l, rgb1_h);
+    RGB32_PACK(r2, g2, b2, a, rgb2_l, rgb2_h);
+    RGB32_STORE(rgb1_l, rgb1_h, image1);
+    RGB32_STORE(rgb2_l, rgb2_h, image2);
+    DEALYUV2RGBREMAIN32
+    PUTRGB(image1, py_1);
+    PUTRGB(image2, py_2);
+    ENDRES32
+    END_FUNC()
+
+YUV2RGBFUNC32(yuv420_bgra32_lasx, uint32_t, 0)
+    LOAD_YUV_16
+    YUV2RGB
+    RGB32_PACK(b1, g1, r1, a, rgb1_l, rgb1_h);
+    RGB32_PACK(b2, g2, r2, a, rgb2_l, rgb2_h);
+    RGB32_STORE(rgb1_l, rgb1_h, image1);
+    RGB32_STORE(rgb2_l, rgb2_h, image2);
+    DEALYUV2RGBREMAIN32
+    PUTRGB(image1, py_1);
+    PUTRGB(image2, py_2);
+    ENDRES32
+    END_FUNC()
+
+YUV2RGBFUNC32(yuv420_argb32_lasx, uint32_t, 0)
+    LOAD_YUV_16
+    YUV2RGB
+    RGB32_PACK(a, r1, g1, b1, rgb1_l, rgb1_h);
+    RGB32_PACK(a, r2, g2, b2, rgb2_l, rgb2_h);
+    RGB32_STORE(rgb1_l, rgb1_h, image1);
+    RGB32_STORE(rgb2_l, rgb2_h, image2);
+    DEALYUV2RGBREMAIN32
+    PUTRGB(image1, py_1);
+    PUTRGB(image2, py_2);
+    ENDRES32
+    END_FUNC()
+
+YUV2RGBFUNC32(yuv420_abgr32_lasx, uint32_t, 0)
+    LOAD_YUV_16
+    YUV2RGB
+    RGB32_PACK(a, b1, g1, r1, rgb1_l, rgb1_h);
+    RGB32_PACK(a, b2, g2, r2, rgb2_l, rgb2_h);
+    RGB32_STORE(rgb1_l, rgb1_h, image1);
+    RGB32_STORE(rgb2_l, rgb2_h, image2);
+    DEALYUV2RGBREMAIN32
+    PUTRGB(image1, py_1);
+    PUTRGB(image2, py_2);
+    ENDRES32
+    END_FUNC()
diff --git a/libswscale/loongarch/yuv2rgb_lsx.c b/libswscale/loongarch/yuv2rgb_lsx.c
new file mode 100644
index 0000000000..254d143c96
--- /dev/null
+++ b/libswscale/loongarch/yuv2rgb_lsx.c
@@ -0,0 +1,361 @@
+/*
+ * Copyright (C) 2021 Loongson Technology Co. Ltd.
+ * Contributed by Hao Chen(chenhao@loongson.cn)
+ * All rights reserved.
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "swscale_loongarch.h"
+#include "libavutil/loongarch/loongson_intrinsics.h"
+
+#define YUV2RGB_LOAD_COE                               \
+    /* Load x_offset */                                \
+    __m128i y_offset = __lsx_vreplgr2vr_d(c->yOffset); \
+    __m128i u_offset = __lsx_vreplgr2vr_d(c->uOffset); \
+    __m128i v_offset = __lsx_vreplgr2vr_d(c->vOffset); \
+    /* Load x_coeff  */                                \
+    __m128i ug_coeff = __lsx_vreplgr2vr_d(c->ugCoeff); \
+    __m128i vg_coeff = __lsx_vreplgr2vr_d(c->vgCoeff); \
+    __m128i y_coeff  = __lsx_vreplgr2vr_d(c->yCoeff);  \
+    __m128i ub_coeff = __lsx_vreplgr2vr_d(c->ubCoeff); \
+    __m128i vr_coeff = __lsx_vreplgr2vr_d(c->vrCoeff); \
+
+#define LOAD_YUV_16                                                   \
+    m_y1 = __lsx_vld(py_1, 0);                                        \
+    m_y2 = __lsx_vld(py_2, 0);                                        \
+    m_u  = __lsx_vldrepl_d(pu, 0);                                    \
+    m_v  = __lsx_vldrepl_d(pv, 0);                                    \
+    DUP2_ARG2(__lsx_vilvl_b, m_u, m_u, m_v, m_v, m_u, m_v);           \
+    DUP2_ARG2(__lsx_vilvh_b, zero, m_u, zero, m_v, m_u_h, m_v_h);     \
+    DUP2_ARG2(__lsx_vilvl_b, zero, m_u, zero, m_v, m_u, m_v);         \
+    DUP2_ARG2(__lsx_vilvh_b, zero, m_y1, zero, m_y2, m_y1_h, m_y2_h); \
+    DUP2_ARG2(__lsx_vilvl_b, zero, m_y1, zero, m_y2, m_y1, m_y2);     \
+
+/* YUV2RGB method
+ * The conversion method is as follows:
+ * R = Y' * y_coeff + V' * vr_coeff
+ * G = Y' * y_coeff + V' * vg_coeff + U' * ug_coeff
+ * B = Y' * y_coeff + U' * ub_coeff
+ *
+ * where X' = X * 8 - x_offset
+ *
+ */
+
+#define YUV2RGB(y1, y2, u, v, r1, g1, b1, r2, g2, b2)               \
+{                                                                   \
+    y1  = __lsx_vslli_h(y1, 3);                                     \
+    y2  = __lsx_vslli_h(y2, 3);                                     \
+    u   = __lsx_vslli_h(u, 3);                                      \
+    v   = __lsx_vslli_h(v, 3);                                      \
+    y1  = __lsx_vsub_h(y1, y_offset);                               \
+    y2  = __lsx_vsub_h(y2, y_offset);                               \
+    u   = __lsx_vsub_h(u, u_offset);                                \
+    v   = __lsx_vsub_h(v, v_offset);                                \
+    y_1 = __lsx_vmuh_h(y1, y_coeff);                                \
+    y_2 = __lsx_vmuh_h(y2, y_coeff);                                \
+    u2g = __lsx_vmuh_h(u, ug_coeff);                                \
+    u2b = __lsx_vmuh_h(u, ub_coeff);                                \
+    v2r = __lsx_vmuh_h(v, vr_coeff);                                \
+    v2g = __lsx_vmuh_h(v, vg_coeff);                                \
+    r1  = __lsx_vsadd_h(y_1, v2r);                                  \
+    v2g = __lsx_vsadd_h(v2g, u2g);                                  \
+    g1  = __lsx_vsadd_h(y_1, v2g);                                  \
+    b1  = __lsx_vsadd_h(y_1, u2b);                                  \
+    r2  = __lsx_vsadd_h(y_2, v2r);                                  \
+    g2  = __lsx_vsadd_h(y_2, v2g);                                  \
+    b2  = __lsx_vsadd_h(y_2, u2b);                                  \
+    DUP4_ARG1(__lsx_vclip255_h, r1, g1, b1, r2, r1, g1, b1, r2);    \
+    DUP2_ARG1(__lsx_vclip255_h, g2, b2, g2, b2);                    \
+}
+
+#define RGB_PACK(r, g, b, rgb_l, rgb_h)                                 \
+{                                                                       \
+    __m128i rg;                                                         \
+    rg = __lsx_vpackev_b(g, r);                                         \
+    DUP2_ARG3(__lsx_vshuf_b, b, rg, shuf2, b, rg, shuf3, rgb_l, rgb_h); \
+}
+
+#define RGB32_PACK(a, r, g, b, rgb_l, rgb_h)                         \
+{                                                                    \
+    __m128i ra, bg;                                                  \
+    ra    = __lsx_vpackev_b(r, a);                                   \
+    bg    = __lsx_vpackev_b(b, g);                                   \
+    rgb_l = __lsx_vilvl_h(bg, ra);                                   \
+    rgb_h = __lsx_vilvh_h(bg, ra);                                   \
+}
+
+#define RGB_STORE(rgb_l, rgb_h, image)                               \
+{                                                                    \
+    __lsx_vstelm_d(rgb_l, image, 0,  0);                             \
+    __lsx_vstelm_d(rgb_l, image, 8,  1);                             \
+    __lsx_vstelm_d(rgb_h, image, 16, 0);                             \
+}
+
+#define RGB32_STORE(rgb_l, rgb_h, image)                             \
+{                                                                    \
+    __lsx_vst(rgb_l, image, 0);                                      \
+    __lsx_vst(rgb_h, image, 16);                                     \
+}
+
+#define YUV2RGBFUNC(func_name, dst_type, alpha)                                     \
+           int func_name(SwsContext *c, const uint8_t *src[],                       \
+                         int srcStride[], int srcSliceY, int srcSliceH,             \
+                         uint8_t *dst[], int dstStride[])                           \
+{                                                                                   \
+    int x, y, h_size, vshift, res;                                                  \
+    __m128i m_y1, m_y2, m_u, m_v;                                                   \
+    __m128i m_y1_h, m_y2_h, m_u_h, m_v_h;                                           \
+    __m128i y_1, y_2, u2g, v2g, u2b, v2r, rgb1_l, rgb1_h;                           \
+    __m128i rgb2_l, rgb2_h, r1, g1, b1, r2, g2, b2;                                 \
+    __m128i shuf2 = {0x0504120302100100, 0x0A18090816070614};                       \
+    __m128i shuf3 = {0x1E0F0E1C0D0C1A0B, 0x0101010101010101};                       \
+    __m128i zero = __lsx_vldi(0);                                                   \
+                                                                                    \
+    YUV2RGB_LOAD_COE                                                                \
+                                                                                    \
+    h_size = c->dstW >> 4;                                                          \
+    res = (c->dstW & 15) >> 1;                                                      \
+    vshift = c->srcFormat != AV_PIX_FMT_YUV422P;                                    \
+    for (y = 0; y < srcSliceH; y += 2) {                                            \
+        dst_type av_unused *r, *g, *b;                                              \
+        dst_type *image1    = (dst_type *)(dst[0] + (y + srcSliceY) * dstStride[0]);\
+        dst_type *image2    = (dst_type *)(image1 +                   dstStride[0]);\
+        const uint8_t *py_1 = src[0] +               y * srcStride[0];              \
+        const uint8_t *py_2 = py_1   +                   srcStride[0];              \
+        const uint8_t *pu   = src[1] +   (y >> vshift) * srcStride[1];              \
+        const uint8_t *pv   = src[2] +   (y >> vshift) * srcStride[2];              \
+        for(x = 0; x < h_size; x++) {                                               \
+
+#define YUV2RGBFUNC32(func_name, dst_type, alpha)                                   \
+           int func_name(SwsContext *c, const uint8_t *src[],                       \
+                         int srcStride[], int srcSliceY, int srcSliceH,             \
+                         uint8_t *dst[], int dstStride[])                           \
+{                                                                                   \
+    int x, y, h_size, vshift, res;                                                  \
+    __m128i m_y1, m_y2, m_u, m_v;                                                   \
+    __m128i m_y1_h, m_y2_h, m_u_h, m_v_h;                                           \
+    __m128i y_1, y_2, u2g, v2g, u2b, v2r, rgb1_l, rgb1_h;                           \
+    __m128i rgb2_l, rgb2_h, r1, g1, b1, r2, g2, b2;                                 \
+    __m128i a = __lsx_vldi(0xFF);                                                   \
+    __m128i zero = __lsx_vldi(0);                                                   \
+                                                                                    \
+    YUV2RGB_LOAD_COE                                                                \
+                                                                                    \
+    h_size = c->dstW >> 4;                                                          \
+    res = (c->dstW & 15) >> 1;                                                      \
+    vshift = c->srcFormat != AV_PIX_FMT_YUV422P;                                    \
+    for (y = 0; y < srcSliceH; y += 2) {                                            \
+        int yd = y + srcSliceY;                                                     \
+        dst_type av_unused *r, *g, *b;                                              \
+        dst_type *image1    = (dst_type *)(dst[0] + (yd)     * dstStride[0]);       \
+        dst_type *image2    = (dst_type *)(dst[0] + (yd + 1) * dstStride[0]);       \
+        const uint8_t *py_1 = src[0] +               y * srcStride[0];              \
+        const uint8_t *py_2 = py_1   +                   srcStride[0];              \
+        const uint8_t *pu   = src[1] +   (y >> vshift) * srcStride[1];              \
+        const uint8_t *pv   = src[2] +   (y >> vshift) * srcStride[2];              \
+        for(x = 0; x < h_size; x++) {                                               \
+
+#define DEALYUV2RGBREMAIN                                                           \
+            py_1 += 16;                                                             \
+            py_2 += 16;                                                             \
+            pu += 8;                                                                \
+            pv += 8;                                                                \
+            image1 += 48;                                                           \
+            image2 += 48;                                                           \
+        }                                                                           \
+        for (x = 0; x < res; x++) {                                                 \
+            int av_unused U, V, Y;                                                  \
+            U = pu[0];                                                              \
+            V = pv[0];                                                              \
+            r = (void *)c->table_rV[V+YUVRGB_TABLE_HEADROOM];                       \
+            g = (void *)(c->table_gU[U+YUVRGB_TABLE_HEADROOM]                       \
+                       + c->table_gV[V+YUVRGB_TABLE_HEADROOM]);                     \
+            b = (void *)c->table_bU[U+YUVRGB_TABLE_HEADROOM];
+
+#define DEALYUV2RGBREMAIN32                                                         \
+            py_1 += 16;                                                             \
+            py_2 += 16;                                                             \
+            pu += 8;                                                                \
+            pv += 8;                                                                \
+            image1 += 16;                                                           \
+            image2 += 16;                                                           \
+        }                                                                           \
+        for (x = 0; x < res; x++) {                                                 \
+            int av_unused U, V, Y;                                                  \
+            U = pu[0];                                                              \
+            V = pv[0];                                                              \
+            r = (void *)c->table_rV[V+YUVRGB_TABLE_HEADROOM];                       \
+            g = (void *)(c->table_gU[U+YUVRGB_TABLE_HEADROOM]                       \
+                       + c->table_gV[V+YUVRGB_TABLE_HEADROOM]);                     \
+            b = (void *)c->table_bU[U+YUVRGB_TABLE_HEADROOM];                       \
+
+#define PUTRGB24(dst, src)                  \
+    Y      = src[0];                        \
+    dst[0] = r[Y];                          \
+    dst[1] = g[Y];                          \
+    dst[2] = b[Y];                          \
+    Y      = src[1];                        \
+    dst[3] = r[Y];                          \
+    dst[4] = g[Y];                          \
+    dst[5] = b[Y];
+
+#define PUTBGR24(dst, src)                  \
+    Y      = src[0];                        \
+    dst[0] = b[Y];                          \
+    dst[1] = g[Y];                          \
+    dst[2] = r[Y];                          \
+    Y      = src[1];                        \
+    dst[3] = b[Y];                          \
+    dst[4] = g[Y];                          \
+    dst[5] = r[Y];
+
+#define PUTRGB(dst, src)                    \
+    Y      = src[0];                        \
+    dst[0] = r[Y] + g[Y] + b[Y];            \
+    Y      = src[1];                        \
+    dst[1] = r[Y] + g[Y] + b[Y];            \
+
+#define ENDRES                              \
+    pu += 1;                                \
+    pv += 1;                                \
+    py_1 += 2;                              \
+    py_2 += 2;                              \
+    image1 += 6;                            \
+    image2 += 6;                            \
+
+#define ENDRES32                            \
+    pu += 1;                                \
+    pv += 1;                                \
+    py_1 += 2;                              \
+    py_2 += 2;                              \
+    image1 += 2;                            \
+    image2 += 2;                            \
+
+#define END_FUNC()                          \
+        }                                   \
+    }                                       \
+    return srcSliceH;                       \
+}
+
+YUV2RGBFUNC(yuv420_rgb24_lsx, uint8_t, 0)
+    LOAD_YUV_16
+    YUV2RGB(m_y1, m_y2, m_u, m_v, r1, g1, b1, r2, g2, b2);
+    RGB_PACK(r1, g1, b1, rgb1_l, rgb1_h);
+    RGB_PACK(r2, g2, b2, rgb2_l, rgb2_h);
+    RGB_STORE(rgb1_l, rgb1_h, image1);
+    RGB_STORE(rgb2_l, rgb2_h, image2);
+    YUV2RGB(m_y1_h, m_y2_h, m_u_h, m_v_h, r1, g1, b1, r2, g2, b2);
+    RGB_PACK(r1, g1, b1, rgb1_l, rgb1_h);
+    RGB_PACK(r2, g2, b2, rgb2_l, rgb2_h);
+    RGB_STORE(rgb1_l, rgb1_h, image1 + 24);
+    RGB_STORE(rgb2_l, rgb2_h, image2 + 24);
+    DEALYUV2RGBREMAIN
+    PUTRGB24(image1, py_1);
+    PUTRGB24(image2, py_2);
+    ENDRES
+    END_FUNC()
+
+YUV2RGBFUNC(yuv420_bgr24_lsx, uint8_t, 0)
+    LOAD_YUV_16
+    YUV2RGB(m_y1, m_y2, m_u, m_v, r1, g1, b1, r2, g2, b2);
+    RGB_PACK(b1, g1, r1, rgb1_l, rgb1_h);
+    RGB_PACK(b2, g2, r2, rgb2_l, rgb2_h);
+    RGB_STORE(rgb1_l, rgb1_h, image1);
+    RGB_STORE(rgb2_l, rgb2_h, image2);
+    YUV2RGB(m_y1_h, m_y2_h, m_u_h, m_v_h, r1, g1, b1, r2, g2, b2);
+    RGB_PACK(b1, g1, r1, rgb1_l, rgb1_h);
+    RGB_PACK(b2, g2, r2, rgb2_l, rgb2_h);
+    RGB_STORE(rgb1_l, rgb1_h, image1 + 24);
+    RGB_STORE(rgb2_l, rgb2_h, image2 + 24);
+    DEALYUV2RGBREMAIN
+    PUTBGR24(image1, py_1);
+    PUTBGR24(image2, py_2);
+    ENDRES
+    END_FUNC()
+
+YUV2RGBFUNC32(yuv420_rgba32_lsx, uint32_t, 0)
+    LOAD_YUV_16
+    YUV2RGB(m_y1, m_y2, m_u, m_v, r1, g1, b1, r2, g2, b2);
+    RGB32_PACK(r1, g1, b1, a, rgb1_l, rgb1_h);
+    RGB32_PACK(r2, g2, b2, a, rgb2_l, rgb2_h);
+    RGB32_STORE(rgb1_l, rgb1_h, image1);
+    RGB32_STORE(rgb2_l, rgb2_h, image2);
+    YUV2RGB(m_y1_h, m_y2_h, m_u_h, m_v_h, r1, g1, b1, r2, g2, b2);
+    RGB32_PACK(r1, g1, b1, a, rgb1_l, rgb1_h);
+    RGB32_PACK(r2, g2, b2, a, rgb2_l, rgb2_h);
+    RGB32_STORE(rgb1_l, rgb1_h, image1 + 8);
+    RGB32_STORE(rgb2_l, rgb2_h, image2 + 8);
+    DEALYUV2RGBREMAIN32
+    PUTRGB(image1, py_1);
+    PUTRGB(image2, py_2);
+    ENDRES32
+    END_FUNC()
+
+YUV2RGBFUNC32(yuv420_bgra32_lsx, uint32_t, 0)
+    LOAD_YUV_16
+    YUV2RGB(m_y1, m_y2, m_u, m_v, r1, g1, b1, r2, g2, b2);
+    RGB32_PACK(b1, g1, r1, a, rgb1_l, rgb1_h);
+    RGB32_PACK(b2, g2, r2, a, rgb2_l, rgb2_h);
+    RGB32_STORE(rgb1_l, rgb1_h, image1);
+    RGB32_STORE(rgb2_l, rgb2_h, image2);
+    YUV2RGB(m_y1_h, m_y2_h, m_u_h, m_v_h, r1, g1, b1, r2, g2, b2);
+    RGB32_PACK(b1, g1, r1, a, rgb1_l, rgb1_h);
+    RGB32_PACK(b2, g2, r2, a, rgb2_l, rgb2_h);
+    RGB32_STORE(rgb1_l, rgb1_h, image1 + 8);
+    RGB32_STORE(rgb2_l, rgb2_h, image2 + 8);
+    DEALYUV2RGBREMAIN32
+    PUTRGB(image1, py_1);
+    PUTRGB(image2, py_2);
+    ENDRES32
+    END_FUNC()
+
+YUV2RGBFUNC32(yuv420_argb32_lsx, uint32_t, 0)
+    LOAD_YUV_16
+    YUV2RGB(m_y1, m_y2, m_u, m_v, r1, g1, b1, r2, g2, b2);
+    RGB32_PACK(a, r1, g1, b1, rgb1_l, rgb1_h);
+    RGB32_PACK(a, r2, g2, b2, rgb2_l, rgb2_h);
+    RGB32_STORE(rgb1_l, rgb1_h, image1);
+    RGB32_STORE(rgb2_l, rgb2_h, image2);
+    YUV2RGB(m_y1_h, m_y2_h, m_u_h, m_v_h, r1, g1, b1, r2, g2, b2);
+    RGB32_PACK(a, r1, g1, b1, rgb1_l, rgb1_h);
+    RGB32_PACK(a, r2, g2, b2, rgb2_l, rgb2_h);
+    RGB32_STORE(rgb1_l, rgb1_h, image1 + 8);
+    RGB32_STORE(rgb2_l, rgb2_h, image2 + 8);
+    DEALYUV2RGBREMAIN32
+    PUTRGB(image1, py_1);
+    PUTRGB(image2, py_2);
+    ENDRES32
+    END_FUNC()
+
+YUV2RGBFUNC32(yuv420_abgr32_lsx, uint32_t, 0)
+    LOAD_YUV_16
+    YUV2RGB(m_y1, m_y2, m_u, m_v, r1, g1, b1, r2, g2, b2);
+    RGB32_PACK(a, b1, g1, r1, rgb1_l, rgb1_h);
+    RGB32_PACK(a, b2, g2, r2, rgb2_l, rgb2_h);
+    RGB32_STORE(rgb1_l, rgb1_h, image1);
+    RGB32_STORE(rgb2_l, rgb2_h, image2);
+    YUV2RGB(m_y1_h, m_y2_h, m_u_h, m_v_h, r1, g1, b1, r2, g2, b2);
+    RGB32_PACK(a, b1, g1, r1, rgb1_l, rgb1_h);
+    RGB32_PACK(a, b2, g2, r2, rgb2_l, rgb2_h);
+    RGB32_STORE(rgb1_l, rgb1_h, image1 + 8);
+    RGB32_STORE(rgb2_l, rgb2_h, image2 + 8);
+    DEALYUV2RGBREMAIN32
+    PUTRGB(image1, py_1);
+    PUTRGB(image2, py_2);
+    ENDRES32
+    END_FUNC()
diff --git a/libswscale/mips/Makefile b/libswscale/mips/Makefile
new file mode 100644
index 0000000000..8721561930
--- /dev/null
+++ b/libswscale/mips/Makefile
@@ -0,0 +1,5 @@
+OBJS     += mips/swscale_init_mips.o
+OBJS     += mips/rgb2rgb_init_mips.o
+MSA-OBJS += mips/swscale_msa.o
+MSA-OBJS += mips/rgb2rgb_msa.o
+MSA-OBJS += mips/yuv2rgb_msa.o
diff --git a/libswscale/mips/rgb2rgb_init_mips.c b/libswscale/mips/rgb2rgb_init_mips.c
new file mode 100644
index 0000000000..f1bcd6f726
--- /dev/null
+++ b/libswscale/mips/rgb2rgb_init_mips.c
@@ -0,0 +1,33 @@
+/*
+ * Copyright (C) 2020 Loongson Technology Co. Ltd.
+ * Contributed by Gu Xiwei(guxiwei-hf@loongson.cn)
+ * All rights reserved.
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "rgb2rgb_mips.h"
+#include "libavutil/mips/cpu.h"
+
+av_cold void rgb2rgb_init_mips(void)
+{
+    int cpu_flags = av_get_cpu_flags();
+#if HAVE_MSA
+    if (have_msa(cpu_flags))
+        interleaveBytes = ff_interleave_bytes_msa;
+#endif /* #if HAVE_MSA */
+}
diff --git a/libswscale/mips/rgb2rgb_mips.h b/libswscale/mips/rgb2rgb_mips.h
new file mode 100644
index 0000000000..5271d2f032
--- /dev/null
+++ b/libswscale/mips/rgb2rgb_mips.h
@@ -0,0 +1,32 @@
+/*
+ * Copyright (C) 2020 Loongson Technology Co. Ltd.
+ * Contributed by Gu Xiwei(guxiwei-hf@loongson.cn)
+ * All rights reserved.
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef SWSCALE_MIPS_RGB2RGB_MIPS_H
+#define SWSCALE_MIPS_RGB2RGB_MIPS_H
+
+#include "libswscale/rgb2rgb.h"
+
+void ff_interleave_bytes_msa(const uint8_t *src1, const uint8_t *src2,
+                             uint8_t *dest, int width, int height,
+                             int src1Stride, int src2Stride, int dstStride);
+
+#endif /* SWSCALE_MIPS_RGB2RGB_MIPS_H */
diff --git a/libswscale/mips/rgb2rgb_msa.c b/libswscale/mips/rgb2rgb_msa.c
new file mode 100644
index 0000000000..594a34c651
--- /dev/null
+++ b/libswscale/mips/rgb2rgb_msa.c
@@ -0,0 +1,51 @@
+/*
+ * Copyright (C) 2020 Loongson Technology Co. Ltd.
+ * Contributed by Gu Xiwei(guxiwei-hf@loongson.cn)
+ * All rights reserved.
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "rgb2rgb_mips.h"
+#include "libavutil/mips/generic_macros_msa.h"
+
+void ff_interleave_bytes_msa(const uint8_t *src1, const uint8_t *src2,
+                             uint8_t *dest, int width, int height,
+                             int src1Stride, int src2Stride, int dstStride)
+{
+    int h;
+    int len  = width >> 4;
+    int part = len << 4;
+    for (h = 0; h < height; h++) {
+        int w;
+        v16u8 src_0, src_1;
+        v16u8 dst_0, dst_1;
+        for (w = 0; w < len; w++) {
+            src_0 = LD_UB(src1 + w * 16);
+            src_1 = LD_UB(src2 + w * 16);
+            ILVRL_B2_UB(src_1, src_0, dst_0, dst_1);
+            ST_UB2(dst_0, dst_1, dest + w * 32, 16);
+        }
+        for (w = part; w < width; w++) {
+            dest[2 * w + 0] = src1[w];
+            dest[2 * w + 1] = src2[w];
+        }
+        dest += dstStride;
+        src1 += src1Stride;
+        src2 += src2Stride;
+    }
+}
diff --git a/libswscale/mips/swscale_init_mips.c b/libswscale/mips/swscale_init_mips.c
new file mode 100644
index 0000000000..8ee13bea21
--- /dev/null
+++ b/libswscale/mips/swscale_init_mips.c
@@ -0,0 +1,243 @@
+/*
+ * Copyright (C) 2020 Loongson Technology Co. Ltd.
+ * Contributed by Gu Xiwei(guxiwei-hf@loongson.cn)
+ * All rights reserved.
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "swscale_mips.h"
+#include "libavutil/mips/cpu.h"
+
+av_cold void ff_sws_init_swscale_mips(SwsContext *c)
+{
+    int cpu_flags = av_get_cpu_flags();
+#if HAVE_MSA
+    if (have_msa(cpu_flags)) {
+        const AVPixFmtDescriptor *desc = av_pix_fmt_desc_get(c->dstFormat);
+
+        if (c->srcBpc == 8) {
+            if (c->dstBpc <= 14) {
+                c->hyScale = c->hcScale = ff_hscale_8_to_15_msa;
+            } else {
+                c->hyScale = c->hcScale = ff_hscale_8_to_19_msa;
+            }
+        } else {
+            c->hyScale = c->hcScale = c->dstBpc > 14 ? ff_hscale_16_to_19_msa
+                                                     : ff_hscale_16_to_15_msa;
+        }
+        switch (c->srcFormat) {
+        case AV_PIX_FMT_GBRAP:
+        case AV_PIX_FMT_GBRP:
+            {
+                 c->readChrPlanar = planar_rgb_to_uv_msa;
+                 c->readLumPlanar = planar_rgb_to_y_msa;
+            }
+            break;
+        }
+        if (c->dstBpc == 8)
+            c->yuv2planeX = ff_yuv2planeX_8_msa;
+        if (c->flags & SWS_FULL_CHR_H_INT) {
+            switch (c->dstFormat) {
+            case AV_PIX_FMT_RGBA:
+#if CONFIG_SMALL
+                c->yuv2packedX = yuv2rgba32_full_X_msa;
+                c->yuv2packed2 = yuv2rgba32_full_2_msa;
+                c->yuv2packed1 = yuv2rgba32_full_1_msa;
+#else
+#if CONFIG_SWSCALE_ALPHA
+                if (c->needAlpha) {
+                    c->yuv2packedX = yuv2rgba32_full_X_msa;
+                    c->yuv2packed2 = yuv2rgba32_full_2_msa;
+                    c->yuv2packed1 = yuv2rgba32_full_1_msa;
+                } else
+#endif /* CONFIG_SWSCALE_ALPHA */
+                {
+                    c->yuv2packedX = yuv2rgbx32_full_X_msa;
+                    c->yuv2packed2 = yuv2rgbx32_full_2_msa;
+                    c->yuv2packed1 = yuv2rgbx32_full_1_msa;
+                }
+#endif /* !CONFIG_SMALL */
+                break;
+            case AV_PIX_FMT_ARGB:
+#if CONFIG_SMALL
+                c->yuv2packedX = yuv2argb32_full_X_msa;
+                c->yuv2packed2 = yuv2argb32_full_2_msa;
+                c->yuv2packed1 = yuv2argb32_full_1_msa;
+#else
+#if CONFIG_SWSCALE_ALPHA
+                if (c->needAlpha) {
+                    c->yuv2packedX = yuv2argb32_full_X_msa;
+                    c->yuv2packed2 = yuv2argb32_full_2_msa;
+                    c->yuv2packed1 = yuv2argb32_full_1_msa;
+                } else
+#endif /* CONFIG_SWSCALE_ALPHA */
+                {
+                    c->yuv2packedX = yuv2xrgb32_full_X_msa;
+                    c->yuv2packed2 = yuv2xrgb32_full_2_msa;
+                    c->yuv2packed1 = yuv2xrgb32_full_1_msa;
+                }
+#endif /* !CONFIG_SMALL */
+                break;
+            case AV_PIX_FMT_BGRA:
+#if CONFIG_SMALL
+                c->yuv2packedX = yuv2bgra32_full_X_msa;
+                c->yuv2packed2 = yuv2bgra32_full_2_msa;
+                c->yuv2packed1 = yuv2bgra32_full_1_msa;
+#else
+#if CONFIG_SWSCALE_ALPHA
+                if (c->needAlpha) {
+                    c->yuv2packedX = yuv2bgra32_full_X_msa;
+                    c->yuv2packed2 = yuv2bgra32_full_2_msa;
+                    c->yuv2packed1 = yuv2bgra32_full_1_msa;
+                } else
+#endif /* CONFIG_SWSCALE_ALPHA */
+                {
+                    c->yuv2packedX = yuv2bgrx32_full_X_msa;
+                    c->yuv2packed2 = yuv2bgrx32_full_2_msa;
+                    c->yuv2packed1 = yuv2bgrx32_full_1_msa;
+                }
+#endif /* !CONFIG_SMALL */
+                break;
+            case AV_PIX_FMT_ABGR:
+#if CONFIG_SMALL
+                c->yuv2packedX = yuv2abgr32_full_X_msa;
+                c->yuv2packed2 = yuv2abgr32_full_2_msa;
+                c->yuv2packed1 = yuv2abgr32_full_1_msa;
+#else
+#if CONFIG_SWSCALE_ALPHA
+                if (c->needAlpha) {
+                    c->yuv2packedX = yuv2abgr32_full_X_msa;
+                    c->yuv2packed2 = yuv2abgr32_full_2_msa;
+                    c->yuv2packed1 = yuv2abgr32_full_1_msa;
+                } else
+#endif /* CONFIG_SWSCALE_ALPHA */
+                {
+                    c->yuv2packedX = yuv2xbgr32_full_X_msa;
+                    c->yuv2packed2 = yuv2xbgr32_full_2_msa;
+                    c->yuv2packed1 = yuv2xbgr32_full_1_msa;
+                }
+#endif /* !CONFIG_SMALL */
+                break;
+            case AV_PIX_FMT_RGB24:
+                c->yuv2packedX = yuv2rgb24_full_X_msa;
+                c->yuv2packed2 = yuv2rgb24_full_2_msa;
+                c->yuv2packed1 = yuv2rgb24_full_1_msa;
+                break;
+            case AV_PIX_FMT_BGR24:
+                c->yuv2packedX = yuv2bgr24_full_X_msa;
+                c->yuv2packed2 = yuv2bgr24_full_2_msa;
+                c->yuv2packed1 = yuv2bgr24_full_1_msa;
+                break;
+            case AV_PIX_FMT_BGR4_BYTE:
+                c->yuv2packedX = yuv2bgr4_byte_full_X_msa;
+                c->yuv2packed2 = yuv2bgr4_byte_full_2_msa;
+                c->yuv2packed1 = yuv2bgr4_byte_full_1_msa;
+                break;
+            case AV_PIX_FMT_RGB4_BYTE:
+                c->yuv2packedX = yuv2rgb4_byte_full_X_msa;
+                c->yuv2packed2 = yuv2rgb4_byte_full_2_msa;
+                c->yuv2packed1 = yuv2rgb4_byte_full_1_msa;
+                break;
+            case AV_PIX_FMT_BGR8:
+                c->yuv2packedX = yuv2bgr8_full_X_msa;
+                c->yuv2packed2 = yuv2bgr8_full_2_msa;
+                c->yuv2packed1 = yuv2bgr8_full_1_msa;
+                break;
+            case AV_PIX_FMT_RGB8:
+                c->yuv2packedX = yuv2rgb8_full_X_msa;
+                c->yuv2packed2 = yuv2rgb8_full_2_msa;
+                c->yuv2packed1 = yuv2rgb8_full_1_msa;
+                break;
+            }
+        } else {
+            switch (c->dstFormat) {
+            case AV_PIX_FMT_RGB32:
+            case AV_PIX_FMT_BGR32:
+#if CONFIG_SMALL
+#else
+#if CONFIG_SWSCALE_ALPHA
+                if (c->needAlpha) {
+                } else
+#endif /* CONFIG_SWSCALE_ALPHA */
+                {
+                    c->yuv2packed1 = yuv2rgbx32_1_msa;
+                    c->yuv2packed2 = yuv2rgbx32_2_msa;
+                    c->yuv2packedX = yuv2rgbx32_X_msa;
+                }
+#endif /* !CONFIG_SMALL */
+                break;
+            case AV_PIX_FMT_RGB32_1:
+            case AV_PIX_FMT_BGR32_1:
+#if CONFIG_SMALL
+#else
+#if CONFIG_SWSCALE_ALPHA
+                if (c->needAlpha) {
+                } else
+#endif /* CONFIG_SWSCALE_ALPHA */
+                {
+                    c->yuv2packed1 = yuv2rgbx32_1_1_msa;
+                    c->yuv2packed2 = yuv2rgbx32_1_2_msa;
+                    c->yuv2packedX = yuv2rgbx32_1_X_msa;
+                }
+#endif /* !CONFIG_SMALL */
+                break;
+            case AV_PIX_FMT_RGB565LE:
+            case AV_PIX_FMT_RGB565BE:
+            case AV_PIX_FMT_BGR565LE:
+            case AV_PIX_FMT_BGR565BE:
+                c->yuv2packed1 = yuv2rgb16_1_msa;
+                c->yuv2packed2 = yuv2rgb16_2_msa;
+                c->yuv2packedX = yuv2rgb16_X_msa;
+                break;
+            }
+        }
+    }
+#endif /* #if HAVE_MSA */
+}
+
+av_cold SwsFunc ff_yuv2rgb_init_mips(SwsContext *c)
+{
+    int cpu_flags = av_get_cpu_flags();
+#if HAVE_MSA
+    if (have_msa(cpu_flags)) {
+        switch (c->dstFormat) {
+            case AV_PIX_FMT_RGBA:
+                if (CONFIG_SWSCALE_ALPHA && isALPHA(c->srcFormat)) {
+                    break;
+                } else
+                    return yuv420_rgba32_msa;
+            case AV_PIX_FMT_ARGB:
+                if (CONFIG_SWSCALE_ALPHA && isALPHA(c->srcFormat)) {
+                    break;
+                } else
+                    return yuv420_argb32_msa;
+            case AV_PIX_FMT_BGRA:
+                if (CONFIG_SWSCALE_ALPHA && isALPHA(c->srcFormat)) {
+                    break;
+                } else
+                    return yuv420_bgra32_msa;
+            case AV_PIX_FMT_ABGR:
+                if (CONFIG_SWSCALE_ALPHA && isALPHA(c->srcFormat)) {
+                    break;
+                } else
+                    return yuv420_abgr32_msa;
+        }
+    }
+    return NULL;
+#endif
+}
diff --git a/libswscale/mips/swscale_mips.h b/libswscale/mips/swscale_mips.h
new file mode 100644
index 0000000000..1152f9a497
--- /dev/null
+++ b/libswscale/mips/swscale_mips.h
@@ -0,0 +1,417 @@
+/*
+ * Copyright (C) 2020 Loongson Technology Co. Ltd.
+ * Contributed by Gu Xiwei(guxiwei-hf@loongson.cn)
+ * All rights reserved.
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#ifndef SWSCALE_MIPS_SWSCALE_MIPS_H
+#define SWSCALE_MIPS_SWSCALE_MIPS_H
+
+#include "libswscale/swscale.h"
+#include "libswscale/swscale_internal.h"
+
+void ff_hscale_8_to_15_msa(SwsContext *c, int16_t *dst, int dstW,
+                           const uint8_t *src, const int16_t *filter,
+                           const int32_t *filterPos, int filterSize);
+
+void ff_hscale_8_to_19_msa(SwsContext *c, int16_t *_dst, int dstW,
+                           const uint8_t *src, const int16_t *filter,
+                           const int32_t *filterPos, int filterSize);
+
+void ff_hscale_16_to_19_msa(SwsContext *c, int16_t *_dst, int dstW,
+                            const uint8_t *_src, const int16_t *filter,
+                            const int32_t *filterPos, int filterSize);
+
+void ff_hscale_16_to_15_msa(SwsContext *c, int16_t *dst, int dstW,
+                            const uint8_t *_src, const int16_t *filter,
+                            const int32_t *filterPos, int filterSize);
+
+void ff_yuv2planeX_8_msa(const int16_t *filter, int filterSize,
+                         const int16_t **src, uint8_t *dest, int dstW,
+                         const uint8_t *dither, int offset);
+
+void yuv2rgbx32_1_X_msa(SwsContext *c, const int16_t *lumFilter,
+                        const int16_t **lumSrc, int lumFilterSize,
+                        const int16_t *chrFilter, const int16_t **chrUSrc,
+                        const int16_t **chrVSrc, int chrFilterSize,
+                        const int16_t **alpSrc, uint8_t *dest, int dstW,
+                        int y);
+
+void yuv2rgbx32_1_2_msa(SwsContext *c, const int16_t *buf[2],
+                        const int16_t *ubuf[2], const int16_t *vbuf[2],
+                        const int16_t *abuf[2], uint8_t *dest, int dstW,
+                        int yalpha, int uvalpha, int y);
+
+void yuv2rgbx32_1_1_msa(SwsContext *c, const int16_t *buf0,
+                        const int16_t *ubuf[2], const int16_t *vbuf[2],
+                        const int16_t *abuf0, uint8_t *dest, int dstW,
+                        int uvalpha, int y);
+
+void yuv2rgbx32_X_msa(SwsContext *c, const int16_t *lumFilter,
+                      const int16_t **lumSrc, int lumFilterSize,
+                      const int16_t *chrFilter, const int16_t **chrUSrc,
+                      const int16_t **chrVSrc, int chrFilterSize,
+                      const int16_t **alpSrc, uint8_t *dest, int dstW,
+                      int y);
+
+void yuv2rgbx32_2_msa(SwsContext *c, const int16_t *buf[2],
+                      const int16_t *ubuf[2], const int16_t *vbuf[2],
+                      const int16_t *abuf[2], uint8_t *dest, int dstW,
+                      int yalpha, int uvalpha, int y);
+
+void yuv2rgbx32_1_msa(SwsContext *c, const int16_t *buf0,
+                      const int16_t *ubuf[2], const int16_t *vbuf[2],
+                      const int16_t *abuf0, uint8_t *dest, int dstW,
+                      int uvalpha, int y);
+
+void yuv2rgb16_2_msa(SwsContext *c, const int16_t *buf[2],
+                     const int16_t *ubuf[2], const int16_t *vbuf[2],
+                     const int16_t *abuf[2], uint8_t *dest, int dstW,
+                     int yalpha, int uvalpha, int y);
+
+void yuv2rgb16_1_msa(SwsContext *c, const int16_t *buf0,
+                     const int16_t *ubuf[2], const int16_t *vbuf[2],
+                     const int16_t *abuf0, uint8_t *dest, int dstW,
+                     int uvalpha, int y);
+
+void yuv2rgb16_X_msa(SwsContext *c, const int16_t *lumFilter,
+                     const int16_t **lumSrc, int lumFilterSize,
+                     const int16_t *chrFilter, const int16_t **chrUSrc,
+                     const int16_t **chrVSrc, int chrFilterSize,
+                     const int16_t **alpSrc, uint8_t *dest, int dstW,
+                     int y);
+
+void yuv2bgra32_full_2_msa(SwsContext *c, const int16_t *buf[2],
+                           const int16_t *ubuf[2], const int16_t *vbuf[2],
+                           const int16_t *abuf[2], uint8_t *dest, int dstW,
+                           int yalpha, int uvalpha, int y);
+
+void yuv2bgra32_full_1_msa(SwsContext *c, const int16_t *buf0,
+                           const int16_t *ubuf[2], const int16_t *vbuf[2],
+                           const int16_t *abuf0, uint8_t *dest, int dstW,
+                           int uvalpha, int y);
+
+void yuv2bgra32_full_X_msa(SwsContext *c, const int16_t *lumFilter,
+                           const int16_t **lumSrc, int lumFilterSize,
+                           const int16_t *chrFilter, const int16_t **chrUSrc,
+                           const int16_t **chrVSrc, int chrFilterSize,
+                           const int16_t **alpSrc, uint8_t *dest, int dstW,
+                           int y);
+
+void yuv2abgr32_full_2_msa(SwsContext *c, const int16_t *buf[2],
+                           const int16_t *ubuf[2], const int16_t *vbuf[2],
+                           const int16_t *abuf[2], uint8_t *dest, int dstW,
+                           int yalpha, int uvalpha, int y);
+
+void yuv2abgr32_full_1_msa(SwsContext *c, const int16_t *buf0,
+                           const int16_t *ubuf[2], const int16_t *vbuf[2],
+                           const int16_t *abuf0, uint8_t *dest, int dstW,
+                           int uvalpha, int y);
+
+void yuv2abgr32_full_X_msa(SwsContext *c, const int16_t *lumFilter,
+                           const int16_t **lumSrc, int lumFilterSize,
+                           const int16_t *chrFilter, const int16_t **chrUSrc,
+                           const int16_t **chrVSrc, int chrFilterSize,
+                           const int16_t **alpSrc, uint8_t *dest, int dstW,
+                           int y);
+
+void yuv2rgba32_full_2_msa(SwsContext *c, const int16_t *buf[2],
+                           const int16_t *ubuf[2], const int16_t *vbuf[2],
+                           const int16_t *abuf[2], uint8_t *dest, int dstW,
+                           int yalpha, int uvalpha, int y);
+
+void yuv2rgba32_full_1_msa(SwsContext *c, const int16_t *buf0,
+                           const int16_t *ubuf[2], const int16_t *vbuf[2],
+                           const int16_t *abuf0, uint8_t *dest, int dstW,
+                           int uvalpha, int y);
+
+void yuv2rgba32_full_X_msa(SwsContext *c, const int16_t *lumFilter,
+                           const int16_t **lumSrc, int lumFilterSize,
+                           const int16_t *chrFilter, const int16_t **chrUSrc,
+                           const int16_t **chrVSrc, int chrFilterSize,
+                           const int16_t **alpSrc, uint8_t *dest, int dstW,
+                           int y);
+
+void yuv2argb32_full_2_msa(SwsContext *c, const int16_t *buf[2],
+                           const int16_t *ubuf[2], const int16_t *vbuf[2],
+                           const int16_t *abuf[2], uint8_t *dest, int dstW,
+                           int yalpha, int uvalpha, int y);
+
+void yuv2argb32_full_1_msa(SwsContext *c, const int16_t *buf0,
+                           const int16_t *ubuf[2], const int16_t *vbuf[2],
+                           const int16_t *abuf0, uint8_t *dest, int dstW,
+                           int uvalpha, int y);
+
+void yuv2argb32_full_X_msa(SwsContext *c, const int16_t *lumFilter,
+                           const int16_t **lumSrc, int lumFilterSize,
+                           const int16_t *chrFilter, const int16_t **chrUSrc,
+                           const int16_t **chrVSrc, int chrFilterSize,
+                           const int16_t **alpSrc, uint8_t *dest, int dstW,
+                           int y);
+
+void yuv2bgrx32_full_2_msa(SwsContext *c, const int16_t *buf[2],
+                           const int16_t *ubuf[2], const int16_t *vbuf[2],
+                           const int16_t *abuf[2], uint8_t *dest, int dstW,
+                           int yalpha, int uvalpha, int y);
+
+void yuv2bgrx32_full_1_msa(SwsContext *c, const int16_t *buf0,
+                           const int16_t *ubuf[2], const int16_t *vbuf[2],
+                           const int16_t *abuf0, uint8_t *dest, int dstW,
+                           int uvalpha, int y);
+
+void yuv2bgrx32_full_X_msa(SwsContext *c, const int16_t *lumFilter,
+                           const int16_t **lumSrc, int lumFilterSize,
+                           const int16_t *chrFilter, const int16_t **chrUSrc,
+                           const int16_t **chrVSrc, int chrFilterSize,
+                           const int16_t **alpSrc, uint8_t *dest, int dstW,
+                           int y);
+
+void yuv2xbgr32_full_2_msa(SwsContext *c, const int16_t *buf[2],
+                           const int16_t *ubuf[2], const int16_t *vbuf[2],
+                           const int16_t *abuf[2], uint8_t *dest, int dstW,
+                           int yalpha, int uvalpha, int y);
+
+void yuv2xbgr32_full_1_msa(SwsContext *c, const int16_t *buf0,
+                           const int16_t *ubuf[2], const int16_t *vbuf[2],
+                           const int16_t *abuf0, uint8_t *dest, int dstW,
+                           int uvalpha, int y);
+
+void yuv2xbgr32_full_X_msa(SwsContext *c, const int16_t *lumFilter,
+                           const int16_t **lumSrc, int lumFilterSize,
+                           const int16_t *chrFilter, const int16_t **chrUSrc,
+                           const int16_t **chrVSrc, int chrFilterSize,
+                           const int16_t **alpSrc, uint8_t *dest, int dstW,
+                           int y);
+
+void yuv2rgbx32_full_2_msa(SwsContext *c, const int16_t *buf[2],
+                           const int16_t *ubuf[2], const int16_t *vbuf[2],
+                           const int16_t *abuf[2], uint8_t *dest, int dstW,
+                           int yalpha, int uvalpha, int y);
+
+void yuv2rgbx32_full_1_msa(SwsContext *c, const int16_t *buf0,
+                           const int16_t *ubuf[2], const int16_t *vbuf[2],
+                           const int16_t *abuf0, uint8_t *dest, int dstW,
+                           int uvalpha, int y);
+
+void yuv2rgbx32_full_X_msa(SwsContext *c, const int16_t *lumFilter,
+                           const int16_t **lumSrc, int lumFilterSize,
+                           const int16_t *chrFilter, const int16_t **chrUSrc,
+                           const int16_t **chrVSrc, int chrFilterSize,
+                           const int16_t **alpSrc, uint8_t *dest, int dstW,
+                           int y);
+
+void yuv2xrgb32_full_2_msa(SwsContext *c, const int16_t *buf[2],
+                           const int16_t *ubuf[2], const int16_t *vbuf[2],
+                           const int16_t *abuf[2], uint8_t *dest, int dstW,
+                           int yalpha, int uvalpha, int y);
+
+void yuv2xrgb32_full_1_msa(SwsContext *c, const int16_t *buf0,
+                           const int16_t *ubuf[2], const int16_t *vbuf[2],
+                           const int16_t *abuf0, uint8_t *dest, int dstW,
+                           int uvalpha, int y);
+
+void yuv2xrgb32_full_X_msa(SwsContext *c, const int16_t *lumFilter,
+                           const int16_t **lumSrc, int lumFilterSize,
+                           const int16_t *chrFilter, const int16_t **chrUSrc,
+                           const int16_t **chrVSrc, int chrFilterSize,
+                           const int16_t **alpSrc, uint8_t *dest, int dstW,
+                           int y);
+
+void yuv2bgr24_full_2_msa(SwsContext *c, const int16_t *buf[2],
+                          const int16_t *ubuf[2], const int16_t *vbuf[2],
+                          const int16_t *abuf[2], uint8_t *dest, int dstW,
+                          int yalpha, int uvalpha, int y);
+
+void yuv2bgr24_full_1_msa(SwsContext *c, const int16_t *buf0,
+                          const int16_t *ubuf[2], const int16_t *vbuf[2],
+                          const int16_t *abuf0, uint8_t *dest, int dstW,
+                          int uvalpha, int y);
+
+void yuv2bgr24_full_X_msa(SwsContext *c, const int16_t *lumFilter,
+                          const int16_t **lumSrc, int lumFilterSize,
+                          const int16_t *chrFilter, const int16_t **chrUSrc,
+                          const int16_t **chrVSrc, int chrFilterSize,
+                          const int16_t **alpSrc, uint8_t *dest, int dstW,
+                          int y);
+
+void yuv2rgb24_full_2_msa(SwsContext *c, const int16_t *buf[2],
+                          const int16_t *ubuf[2], const int16_t *vbuf[2],
+                          const int16_t *abuf[2], uint8_t *dest, int dstW,
+                          int yalpha, int uvalpha, int y);
+
+void yuv2rgb24_full_1_msa(SwsContext *c, const int16_t *buf0,
+                          const int16_t *ubuf[2], const int16_t *vbuf[2],
+                          const int16_t *abuf0, uint8_t *dest, int dstW,
+                          int uvalpha, int y);
+
+void yuv2rgb24_full_X_msa(SwsContext *c, const int16_t *lumFilter,
+                          const int16_t **lumSrc, int lumFilterSize,
+                          const int16_t *chrFilter, const int16_t **chrUSrc,
+                          const int16_t **chrVSrc, int chrFilterSize,
+                          const int16_t **alpSrc, uint8_t *dest, int dstW,
+                          int y);
+
+void yuv2bgr4_byte_full_2_msa(SwsContext *c, const int16_t *buf[2],
+                              const int16_t *ubuf[2], const int16_t *vbuf[2],
+                              const int16_t *abuf[2], uint8_t *dest, int dstW,
+                              int yalpha, int uvalpha, int y);
+
+void yuv2bgr4_byte_full_1_msa(SwsContext *c, const int16_t *buf0,
+                              const int16_t *ubuf[2], const int16_t *vbuf[2],
+                              const int16_t *abuf0, uint8_t *dest, int dstW,
+                              int uvalpha, int y);
+
+void yuv2bgr4_byte_full_X_msa(SwsContext *c, const int16_t *lumFilter,
+                              const int16_t **lumSrc, int lumFilterSize,
+                              const int16_t *chrFilter, const int16_t **chrUSrc,
+                              const int16_t **chrVSrc, int chrFilterSize,
+                              const int16_t **alpSrc, uint8_t *dest, int dstW,
+                              int y);
+
+void yuv2rgb4_byte_full_2_msa(SwsContext *c, const int16_t *buf[2],
+                              const int16_t *ubuf[2], const int16_t *vbuf[2],
+                              const int16_t *abuf[2], uint8_t *dest, int dstW,
+                              int yalpha, int uvalpha, int y);
+
+void yuv2rgb4_byte_full_1_msa(SwsContext *c, const int16_t *buf0,
+                              const int16_t *ubuf[2], const int16_t *vbuf[2],
+                              const int16_t *abuf0, uint8_t *dest, int dstW,
+                              int uvalpha, int y);
+
+void yuv2rgb4_byte_full_X_msa(SwsContext *c, const int16_t *lumFilter,
+                              const int16_t **lumSrc, int lumFilterSize,
+                              const int16_t *chrFilter, const int16_t **chrUSrc,
+                              const int16_t **chrVSrc, int chrFilterSize,
+                              const int16_t **alpSrc, uint8_t *dest, int dstW,
+                              int y);
+
+void yuv2bgr8_full_2_msa(SwsContext *c, const int16_t *buf[2],
+                         const int16_t *ubuf[2], const int16_t *vbuf[2],
+                         const int16_t *abuf[2], uint8_t *dest, int dstW,
+                         int yalpha, int uvalpha, int y);
+
+void yuv2bgr8_full_1_msa(SwsContext *c, const int16_t *buf0,
+                         const int16_t *ubuf[2], const int16_t *vbuf[2],
+                         const int16_t *abuf0, uint8_t *dest, int dstW,
+                         int uvalpha, int y);
+
+void yuv2bgr8_full_X_msa(SwsContext *c, const int16_t *lumFilter,
+                         const int16_t **lumSrc, int lumFilterSize,
+                         const int16_t *chrFilter, const int16_t **chrUSrc,
+                         const int16_t **chrVSrc, int chrFilterSize,
+                         const int16_t **alpSrc, uint8_t *dest, int dstW,
+                         int y);
+
+void yuv2rgb8_full_2_msa(SwsContext *c, const int16_t *buf[2],
+                         const int16_t *ubuf[2], const int16_t *vbuf[2],
+                         const int16_t *abuf[2], uint8_t *dest, int dstW,
+                         int yalpha, int uvalpha, int y);
+
+void yuv2rgb8_full_1_msa(SwsContext *c, const int16_t *buf0,
+                         const int16_t *ubuf[2], const int16_t *vbuf[2],
+                         const int16_t *abuf0, uint8_t *dest, int dstW,
+                         int uvalpha, int y);
+
+void yuv2rgb8_full_X_msa(SwsContext *c, const int16_t *lumFilter,
+                          const int16_t **lumSrc, int lumFilterSize,
+                          const int16_t *chrFilter, const int16_t **chrUSrc,
+                          const int16_t **chrVSrc, int chrFilterSize,
+                          const int16_t **alpSrc, uint8_t *dest, int dstW,
+                          int y);
+
+void yuv2plane1_9BE_msa(const int16_t *src, uint8_t *dest, int dstW,
+                        const uint8_t *dither, int offset);
+
+void yuv2plane1_9LE_msa(const int16_t *src, uint8_t *dest, int dstW,
+                        const uint8_t *dither, int offset);
+
+void yuv2planeX_9BE_msa(const int16_t *filter, int filterSize, const int16_t **src,
+                        uint8_t *dest, int dstW, const uint8_t *ditherm, int offset);
+
+void yuv2planeX_9LE_msa(const int16_t *filter, int filterSize, const int16_t **src,
+                        uint8_t *dest, int dstW, const uint8_t *ditherm, int offset);
+
+void yuv2plane1_10BE_msa(const int16_t *src, uint8_t *dest, int dstW,
+                         const uint8_t *dither, int offset);
+
+void yuv2plane1_10LE_msa(const int16_t *src, uint8_t *dest, int dstW,
+                         const uint8_t *dither, int offset);
+
+void yuv2planeX_10BE_msa(const int16_t *filter, int filterSize, const int16_t **src,
+                         uint8_t *dest, int dstW, const uint8_t *ditherm, int offset);
+
+void yuv2planeX_10LE_msa(const int16_t *filter, int filterSize, const int16_t **src,
+                         uint8_t *dest, int dstW, const uint8_t *ditherm, int offset);
+
+void yuv2plane1_12BE_msa(const int16_t *src, uint8_t *dest, int dstW,
+                         const uint8_t *dither, int offset);
+
+void yuv2plane1_12LE_msa(const int16_t *src, uint8_t *dest, int dstW,
+                         const uint8_t *dither, int offset);
+
+void yuv2planeX_12BE_msa(const int16_t *filter, int filterSize, const int16_t **src,
+                         uint8_t *dest, int dstW, const uint8_t *ditherm, int offset);
+
+void yuv2planeX_12LE_msa(const int16_t *filter, int filterSize, const int16_t **src,
+                         uint8_t *dest, int dstW, const uint8_t *ditherm, int offset);
+
+void yuv2plane1_14BE_msa(const int16_t *src, uint8_t *dest, int dstW,
+                         const uint8_t *dither, int offset);
+
+void yuv2plane1_14LE_msa(const int16_t *src, uint8_t *dest, int dstW,
+                         const uint8_t *dither, int offset);
+
+void yuv2planeX_14BE_msa(const int16_t *filter, int filterSize, const int16_t **src,
+                         uint8_t *dest, int dstW, const uint8_t *ditherm, int offset);
+
+void yuv2planeX_14LE_msa(const int16_t *filter, int filterSize, const int16_t **src,
+                         uint8_t *dest, int dstW, const uint8_t *ditherm, int offset);
+
+void yuv2plane1_16BE_msa(const int16_t *src, uint8_t *dest, int dstW,
+                         const uint8_t *dither, int offset);
+
+void yuv2plane1_16LE_msa(const int16_t *src, uint8_t *dest, int dstW,
+                         const uint8_t *dither, int offset);
+
+void yuv2planeX_16BE_msa(const int16_t *filter, int filterSize, const int16_t **src,
+                         uint8_t *dest, int dstW, const uint8_t *ditherm, int offset);
+
+void yuv2planeX_16LE_msa(const int16_t *filter, int filterSize, const int16_t **src,
+                         uint8_t *dest, int dstW, const uint8_t *ditherm, int offset);
+
+void planar_rgb_to_uv_msa(uint8_t *_dstU, uint8_t *_dstV, const uint8_t *src[4],
+                          int width, int32_t *rgb2yuv);
+
+void planar_rgb_to_y_msa(uint8_t *_dst, const uint8_t *src[4], int width,
+                         int32_t *rgb2yuv);
+
+int yuv420_rgba32_msa(SwsContext *c, const uint8_t *src[], int srcStride[],
+                      int srcSliceY, int srcSliceH, uint8_t *dst[], int dstStride[]);
+
+int yuv420_bgra32_msa(SwsContext *c, const uint8_t *src[], int srcStride[],
+                      int srcSliceY, int srcSliceH, uint8_t *dst[], int dstStride[]);
+
+int yuv420_argb32_msa(SwsContext *c, const uint8_t *src[], int srcStride[],
+                      int srcSliceY, int srcSliceH, uint8_t *dst[], int dstStride[]);
+
+int yuv420_abgr32_msa(SwsContext *c, const uint8_t *src[], int srcStride[],
+                      int srcSliceY, int srcSliceH, uint8_t *dst[], int dstStride[]);
+
+
+#endif /* SWSCALE_MIPS_SWSCALE_MIPS_H */
diff --git a/libswscale/mips/swscale_msa.c b/libswscale/mips/swscale_msa.c
new file mode 100644
index 0000000000..5c8d3bfd21
--- /dev/null
+++ b/libswscale/mips/swscale_msa.c
@@ -0,0 +1,2164 @@
+/*
+ * Copyright (C) 2020 Loongson Technology Co. Ltd.
+ * Contributed by Gu Xiwei(guxiwei-hf@loongson.cn)
+ * All rights reserved.
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "swscale_mips.h"
+#include "libavutil/mips/generic_macros_msa.h"
+#include "libavutil/intreadwrite.h"
+
+#define SCALE_8_8(_sh)                                                             \
+{                                                                                  \
+    v8i16 src0, src1, src2, src3, src4, src5, src6, src7;                          \
+    v8i16 filter0, filter1, filter2, filter3, filter4, filter5, filter6, filter7;  \
+    v4i32 out0, out1, out2, out3, out4, out5, out6, out7;                          \
+                                                                                   \
+    src0 = LD_V(v8i16, src + filterPos[0]);                                        \
+    src1 = LD_V(v8i16, src + filterPos[1]);                                        \
+    src2 = LD_V(v8i16, src + filterPos[2]);                                        \
+    src3 = LD_V(v8i16, src + filterPos[3]);                                        \
+    src4 = LD_V(v8i16, src + filterPos[4]);                                        \
+    src5 = LD_V(v8i16, src + filterPos[5]);                                        \
+    src6 = LD_V(v8i16, src + filterPos[6]);                                        \
+    src7 = LD_V(v8i16, src + filterPos[7]);                                        \
+    filter0 = LD_V(v8i16, filter);                                                 \
+    filter1 = LD_V(v8i16, filter + 8);                                             \
+    filter2 = LD_V(v8i16, filter + 16);                                            \
+    filter3 = LD_V(v8i16, filter + 24);                                            \
+    filter4 = LD_V(v8i16, filter + 32);                                            \
+    filter5 = LD_V(v8i16, filter + 40);                                            \
+    filter6 = LD_V(v8i16, filter + 48);                                            \
+    filter7 = LD_V(v8i16, filter + 56);                                            \
+    ILVR_B8(v8i16, zero, src0, zero, src1, zero, src2, zero, src3,                 \
+            zero, src4, zero, src5, zero, src6, zero, src7,                        \
+            src0, src1, src2, src3, src4, src5, src6, src7);                       \
+    DOTP_SH4(v4i32, src0, src1, src2, src3, filter0,                               \
+             filter1, filter2, filter3, out0, out1, out2, out3);                   \
+    DOTP_SH4(v4i32, src4, src5, src6, src7, filter4,                               \
+             filter5, filter6, filter7, out4, out5, out6, out7);                   \
+    out0 = (v4i32)__msa_hadd_s_d(out0, out0);                                      \
+    out1 = (v4i32)__msa_hadd_s_d(out1, out1);                                      \
+    out2 = (v4i32)__msa_hadd_s_d(out2, out2);                                      \
+    out3 = (v4i32)__msa_hadd_s_d(out3, out3);                                      \
+    out4 = (v4i32)__msa_hadd_s_d(out4, out4);                                      \
+    out5 = (v4i32)__msa_hadd_s_d(out5, out5);                                      \
+    out6 = (v4i32)__msa_hadd_s_d(out6, out6);                                      \
+    out7 = (v4i32)__msa_hadd_s_d(out7, out7);                                      \
+                                                                                   \
+    out0 = (v4i32)__msa_pckev_w(out1, out0);                                       \
+    out1 = (v4i32)__msa_pckev_w(out3, out2);                                       \
+    out2 = (v4i32)__msa_pckev_w(out5, out4);                                       \
+    out3 = (v4i32)__msa_pckev_w(out7, out6);                                       \
+    out0 = (v4i32)__msa_hadd_s_d(out0, out0);                                      \
+    out1 = (v4i32)__msa_hadd_s_d(out1, out1);                                      \
+    out2 = (v4i32)__msa_hadd_s_d(out2, out2);                                      \
+    out3 = (v4i32)__msa_hadd_s_d(out3, out3);                                      \
+                                                                                   \
+    out0 = (v4i32)__msa_pckev_w(out1, out0);                                       \
+    out1 = (v4i32)__msa_pckev_w(out3, out2);                                       \
+    out0 = (v4i32)__msa_srai_w(out0, _sh);                                         \
+    out1 = (v4i32)__msa_srai_w(out1, _sh);                                         \
+    out0 = (v4i32)__msa_min_s_w(out0, vmax);                                       \
+    out1 = (v4i32)__msa_min_s_w(out1, vmax);                                       \
+    src0 = (v8i16)__msa_pckev_h((v8i16)out1, (v8i16)out0);                         \
+    ST_V(v8i16, src0, dst);                                                        \
+    filterPos += 8;                                                                \
+    filter    += 64;                                                               \
+    dst       += 8;                                                                \
+}
+
+#define SCALE_8_4(_sh, out0)                                                       \
+{                                                                                  \
+    v8i16 src0, src1, src2, src3;                                                  \
+    v8i16 filter0, filter1, filter2, filter3;                                      \
+    v4i32 out1, out2, out3;                                                        \
+                                                                                   \
+    src0 = LD_V(v8i16, src + filterPos[0]);                                        \
+    src1 = LD_V(v8i16, src + filterPos[1]);                                        \
+    src2 = LD_V(v8i16, src + filterPos[2]);                                        \
+    src3 = LD_V(v8i16, src + filterPos[3]);                                        \
+    filter0 = LD_V(v8i16, filter);                                                 \
+    filter1 = LD_V(v8i16, filter + 8);                                             \
+    filter2 = LD_V(v8i16, filter + 16);                                            \
+    filter3 = LD_V(v8i16, filter + 24);                                            \
+    ILVR_B4(v8i16, zero, src0, zero, src1, zero, src2, zero, src3,                 \
+            src0, src1, src2, src3);                                               \
+    DOTP_SH4(v4i32, src0, src1, src2, src3, filter0,                               \
+             filter1, filter2, filter3, out0, out1, out2, out3);                   \
+    out0 = (v4i32)__msa_hadd_s_d(out0, out0);                                      \
+    out1 = (v4i32)__msa_hadd_s_d(out1, out1);                                      \
+    out2 = (v4i32)__msa_hadd_s_d(out2, out2);                                      \
+    out3 = (v4i32)__msa_hadd_s_d(out3, out3);                                      \
+                                                                                   \
+    out0 = (v4i32)__msa_pckev_w(out1, out0);                                       \
+    out1 = (v4i32)__msa_pckev_w(out3, out2);                                       \
+    out0 = (v4i32)__msa_hadd_s_d(out0, out0);                                      \
+    out1 = (v4i32)__msa_hadd_s_d(out1, out1);                                      \
+                                                                                   \
+    out0 = (v4i32)__msa_pckev_w(out1, out0);                                       \
+    out0 = (v4i32)__msa_srai_w(out0, _sh);                                         \
+    out0 = (v4i32)__msa_min_s_w(out0, vmax);                                       \
+}
+
+#define SCALE_8_2(_sh, out0)                                        \
+{                                                                   \
+    v8i16 src0, src1, filter0, filter1;                             \
+    v4i32 out1;                                                     \
+                                                                    \
+    src0 = LD_V(v8i16, src + filterPos[0]);                         \
+    src1 = LD_V(v8i16, src + filterPos[1]);                         \
+    filter0 = LD_V(v8i16, filter);                                  \
+    filter1 = LD_V(v8i16, filter + 8);                              \
+    ILVR_B2(v8i16, zero, src0, zero, src1, src0, src1);             \
+    DOTP_SH2(v4i32, src0, src1, filter0, filter1, out0, out1);      \
+    out0 = (v4i32)__msa_hadd_s_d(out0, out0);                       \
+    out1 = (v4i32)__msa_hadd_s_d(out1, out1);                       \
+                                                                    \
+    out0 = (v4i32)__msa_pckev_w(out1, out0);                        \
+    out0 = (v4i32)__msa_hadd_s_d(out0, out0);                       \
+    out0 = (v4i32)__msa_srai_w(out0, _sh);                          \
+    out0 = (v4i32)__msa_min_s_w(out0, vmax);                        \
+}
+
+#define SCALE_4_8(_sh)                                                             \
+{                                                                                  \
+    v8i16 src0, src1, src2, src3, src4, src5, src6, src7;                          \
+    v8i16 filter0, filter1, filter2, filter3;                                      \
+    v4i32 out0, out1, out2, out3;                                                  \
+                                                                                   \
+    src0 = LD_V(v8i16, src + filterPos[0]);                                        \
+    src1 = LD_V(v8i16, src + filterPos[1]);                                        \
+    src2 = LD_V(v8i16, src + filterPos[2]);                                        \
+    src3 = LD_V(v8i16, src + filterPos[3]);                                        \
+    src4 = LD_V(v8i16, src + filterPos[4]);                                        \
+    src5 = LD_V(v8i16, src + filterPos[5]);                                        \
+    src6 = LD_V(v8i16, src + filterPos[6]);                                        \
+    src7 = LD_V(v8i16, src + filterPos[7]);                                        \
+    filter0 = LD_V(v8i16, filter);                                                 \
+    filter1 = LD_V(v8i16, filter + 8);                                             \
+    filter2 = LD_V(v8i16, filter + 16);                                            \
+    filter3 = LD_V(v8i16, filter + 24);                                            \
+    ILVR_B8(v8i16, zero, src0, zero, src1, zero, src2, zero, src3,                 \
+            zero, src4, zero, src5, zero, src6, zero, src7,                        \
+            src0, src1, src2, src3, src4, src5, src6, src7);                       \
+    src0 = (v8i16)__msa_ilvr_d((v2i64)src1, (v2i64)src0);                          \
+    src1 = (v8i16)__msa_ilvr_d((v2i64)src3, (v2i64)src2);                          \
+    src2 = (v8i16)__msa_ilvr_d((v2i64)src5, (v2i64)src4);                          \
+    src3 = (v8i16)__msa_ilvr_d((v2i64)src7, (v2i64)src6);                          \
+    DOTP_SH4(v4i32, src0, src1, src2, src3, filter0, filter1, filter2, filter3,    \
+             out0, out1, out2, out3);                                              \
+    out0 = (v4i32)__msa_hadd_s_d(out0, out0);                                      \
+    out1 = (v4i32)__msa_hadd_s_d(out1, out1);                                      \
+    out2 = (v4i32)__msa_hadd_s_d(out2, out2);                                      \
+    out3 = (v4i32)__msa_hadd_s_d(out3, out3);                                      \
+    out0 = (v4i32)__msa_pckev_w(out1, out0);                                       \
+    out1 = (v4i32)__msa_pckev_w(out3, out2);                                       \
+    out0 = (v4i32)__msa_srai_w(out0, _sh);                                         \
+    out1 = (v4i32)__msa_srai_w(out1, _sh);                                         \
+    out0 = (v4i32)__msa_min_s_w(out0, vmax);                                       \
+    out1 = (v4i32)__msa_min_s_w(out1, vmax);                                       \
+    src0 = (v8i16)__msa_pckev_h((v8i16)out1, (v8i16)out0);                         \
+    ST_V(v8i16, src0, dst);                                                        \
+    filterPos += 8;                                                                \
+    filter    += 32;                                                               \
+    dst       += 8;                                                                \
+}
+
+#define SCALE_4_4(_sh, out0)                                                       \
+{                                                                                  \
+    v8i16 src0, src1, src2, src3;                                                  \
+    v8i16 filter0, filter1;                                                        \
+    v4i32 out1;                                                                    \
+                                                                                   \
+    src0 = LD_V(v8i16, src + filterPos[0]);                                        \
+    src1 = LD_V(v8i16, src + filterPos[1]);                                        \
+    src2 = LD_V(v8i16, src + filterPos[2]);                                        \
+    src3 = LD_V(v8i16, src + filterPos[3]);                                        \
+    filter0 = LD_V(v8i16, filter);                                                 \
+    filter1 = LD_V(v8i16, filter + 8);                                             \
+    ILVR_B4(v8i16, zero, src0, zero, src1, zero, src2, zero, src3,                 \
+            src0, src1, src2, src3);                                               \
+    src0 = (v8i16)__msa_ilvr_d((v2i64)src1, (v2i64)src0);                          \
+    src1 = (v8i16)__msa_ilvr_d((v2i64)src3, (v2i64)src2);                          \
+    DOTP_SH2(v4i32, src0, src1, filter0, filter1, out0, out1);                     \
+    out0 = (v4i32)__msa_hadd_s_d(out0, out0);                                      \
+    out1 = (v4i32)__msa_hadd_s_d(out1, out1);                                      \
+    out0 = (v4i32)__msa_pckev_w(out1, out0);                                       \
+    out0 = (v4i32)__msa_srai_w(out0, _sh);                                         \
+    out0 = (v4i32)__msa_min_s_w(out0, vmax);                                       \
+}
+
+#define SCALE_4_2(_sh, out0)                                                       \
+{                                                                                  \
+    v8i16 src0, src1, filter0;                                                     \
+                                                                                   \
+    src0 = LD_V(v8i16, src + filterPos[0]);                                        \
+    src1 = LD_V(v8i16, src + filterPos[1]);                                        \
+    filter0 = LD_V(v8i16, filter);                                                 \
+    ILVR_B2(v8i16, zero, src0, zero, src1, src0, src1);                            \
+    src0 = (v8i16)__msa_ilvr_d((v2i64)src1, (v2i64)src0);                          \
+    out0 = (v4i32)__msa_dotp_s_w(src0, filter0);                                   \
+    out0 = (v4i32)__msa_hadd_s_d(out0, out0);                                      \
+    out0 = (v4i32)__msa_srai_w(out0, _sh);                                         \
+    out0 = (v4i32)__msa_min_s_w(out0, vmax);                                       \
+}
+
+#define SCALE_16_4                                                                 \
+{                                                                                  \
+    src0 = LD_V(v8i16, (srcPos0 + j));                                             \
+    src1 = LD_V(v8i16, (srcPos1 + j));                                             \
+    src2 = LD_V(v8i16, (srcPos2 + j));                                             \
+    src3 = LD_V(v8i16, (srcPos3 + j));                                             \
+    filter0 = LD_V(v8i16, (filterStart0 + j));                                     \
+    filter1 = LD_V(v8i16, (filterStart1 + j));                                     \
+    filter2 = LD_V(v8i16, (filterStart2 + j));                                     \
+    filter3 = LD_V(v8i16, (filterStart3 + j));                                     \
+    ILVR_B4(v8i16, zero, src0, zero, src1, zero, src2, zero, src3,                 \
+            src0, src1, src2, src3);                                               \
+    DOTP_SH4(v4i32, src0, src1, src2, src3, filter0,                               \
+             filter1, filter2, filter3, out0, out1, out2, out3);                   \
+    out0 = (v4i32)__msa_hadd_s_d(out0, out0);                                      \
+    out1 = (v4i32)__msa_hadd_s_d(out1, out1);                                      \
+    out2 = (v4i32)__msa_hadd_s_d(out2, out2);                                      \
+    out3 = (v4i32)__msa_hadd_s_d(out3, out3);                                      \
+                                                                                   \
+    out0 = (v4i32)__msa_pckev_w(out1, out0);                                       \
+    out1 = (v4i32)__msa_pckev_w(out3, out2);                                       \
+    out0 = (v4i32)__msa_hadd_s_d(out0, out0);                                      \
+    out1 = (v4i32)__msa_hadd_s_d(out1, out1);                                      \
+    out0 = (v4i32)__msa_pckev_w(out1, out0);                                       \
+    tmp0 = (v4i32)__msa_addv_w(tmp0, out0);                                        \
+}
+
+void ff_hscale_8_to_15_msa(SwsContext *c, int16_t *dst, int dstW,
+                           const uint8_t *src, const int16_t *filter,
+                           const int32_t *filterPos, int filterSize)
+{
+    int i;
+    int max = (1 << 15) - 1;
+    v16i8 zero = {0};
+    v4i32 vmax = (v4i32)__msa_fill_w(max);
+
+    if (filterSize == 8) {
+        int len = dstW >> 3;
+        int res = dstW & 7;
+
+        for (i = 0; i < len; i++) {
+            SCALE_8_8(7);
+        }
+        if (res & 4) {
+            v8i16 temp;
+            v4i32 out0;
+
+            SCALE_8_4(7, out0);
+            temp = (v8i16)__msa_pckev_h((v8i16)out0, (v8i16)out0);
+            ST_D1(temp, 0, dst);
+            filterPos += 4;
+            filter    += 32;
+            dst       += 4;
+        }
+        if (res & 2) {
+            v4i32 out0;
+
+            SCALE_8_2(7, out0);
+            dst[0] = __msa_copy_s_h((v8i16)out0, 0);
+            dst[1] = __msa_copy_s_h((v8i16)out0, 4);
+            filterPos += 2;
+            filter    += 16;
+            dst       += 2;
+        }
+        if (res & 1) {
+            int val;
+            v8i16 src0, filter0;
+            v4i32 out0;
+
+            src0 = LD_V(v8i16, src + filterPos[0]);
+            filter0 = LD_V(v8i16, filter);
+            src0 = (v8i16)__msa_ilvr_b((v16i8)zero, (v16i8)src0);
+            out0 = (v4i32)__msa_dotp_s_w(src0, filter0);
+            out0 = (v4i32)__msa_hadd_s_d(out0, out0);
+            val  = __msa_copy_s_w(out0, 0) + __msa_copy_s_w(out0, 2);
+            dst[0] = FFMIN(val >> 7, max);
+        }
+    } else if (filterSize == 4) {
+        int len = dstW >> 3;
+        int res = dstW & 7;
+
+        for (i = 0; i < len; i++) {
+            SCALE_4_8(7);
+        }
+        if (res & 4) {
+            v8i16 temp;
+            v4i32 out0;
+
+            SCALE_4_4(7, out0);
+            temp = (v8i16)__msa_pckev_h((v8i16)out0, (v8i16)out0);
+            ST_D1(temp, 0, dst);
+            filterPos += 4;
+            filter    += 16;
+            dst       += 4;
+        }
+        if (res & 2) {
+            v4i32 out0;
+
+            SCALE_4_2(7, out0);
+            dst[0] = __msa_copy_s_h((v8i16)out0, 0);
+            dst[1] = __msa_copy_s_h((v8i16)out0, 4);
+            filterPos += 2;
+            filter    += 8;
+            dst       += 2;
+        }
+        if (res & 1) {
+           int val = 0;
+           const uint8_t *srcPos = src + filterPos[0];
+
+           for (int j = 0; j < 4; j++) {
+               val += ((int)srcPos[j]) * filter[j];
+           }
+           dst[0] = FFMIN(val >> 7, (1 << 15) - 1);
+        }
+    } else if (filterSize > 8) {
+        int j, len = dstW >> 2;
+        int res = dstW & 3;
+        int filterlen = filterSize - 7;
+        v8i16 src0, src1, src2, src3;
+        v8i16 filter0, filter1, filter2, filter3;
+        v4i32 out0, out1, out2, out3;
+
+        for (i = 0; i < len; i++) {
+            const uint8_t *srcPos0 = src + filterPos[0];
+            const uint8_t *srcPos1 = src + filterPos[1];
+            const uint8_t *srcPos2 = src + filterPos[2];
+            const uint8_t *srcPos3 = src + filterPos[3];
+            const int16_t *filterStart0 = filter;
+            const int16_t *filterStart1 = filterStart0 + filterSize;
+            const int16_t *filterStart2 = filterStart1 + filterSize;
+            const int16_t *filterStart3 = filterStart2 + filterSize;
+            int val0 = 0, val1 = 0, val2 = 0, val3 = 0;
+            v4i32 tmp0 = {0};
+
+            for (j = 0; j < filterlen; j += 8) {
+                SCALE_16_4
+            }
+            val0 = __msa_copy_s_w(tmp0, 0);
+            val1 = __msa_copy_s_w(tmp0, 1);
+            val2 = __msa_copy_s_w(tmp0, 2);
+            val3 = __msa_copy_s_w(tmp0, 3);
+            for (; j < filterSize; j++) {
+                val0 += ((int)srcPos0[j]) * filterStart0[j];
+                val1 += ((int)srcPos1[j]) * filterStart1[j];
+                val2 += ((int)srcPos2[j]) * filterStart2[j];
+                val3 += ((int)srcPos3[j]) * filterStart3[j];
+            }
+            dst[0] = FFMIN(val0 >> 7, max);
+            dst[1] = FFMIN(val1 >> 7, max);
+            dst[2] = FFMIN(val2 >> 7, max);
+            dst[3] = FFMIN(val3 >> 7, max);
+            filterPos += 4;
+            filter     = filterStart3 + filterSize;
+            dst       += 4;
+        }
+        for (i = 0; i < res; i++) {
+            v8i16 src0, filter0;
+            v4i32 out0;
+            const uint8_t *srcPos = src + filterPos[i];
+            int val = 0;
+
+            for (j = 0; j < filterlen; j += 8) {
+                src0 = LD_V(v8i16, (srcPos + j));
+                filter0 = LD_V(v8i16, (filter + j));
+                src0 = (v8i16)__msa_ilvr_b(zero, (v16i8)src0);
+                out0 = (v4i32)__msa_dotp_s_w(src0, filter0);
+                out0 = (v4i32)__msa_hadd_s_d((v4i32)out0, (v4i32)out0);
+                val += (__msa_copy_s_w((v4i32)out0, 0) +
+                        __msa_copy_s_w((v4i32)out0, 2));
+            }
+            for (; j < filterSize; j++) {
+                val += ((int)srcPos[j]) * filter[j];
+            }
+            dst[i]  = FFMIN(val >> 7, max);
+            filter += filterSize;
+        }
+    } else {
+        for (i = 0; i < dstW; i++) {
+            int val = 0;
+            const uint8_t *srcPos = src + filterPos[i];
+            const int16_t *filterStart = filter + filterSize * i;
+
+            for (int j = 0; j < filterSize; j++) {
+                val += ((int)srcPos[j]) * filterStart[j];
+            }
+            dst[i] = FFMIN(val >> 7, max);
+        }
+    }
+}
+
+void ff_hscale_8_to_19_msa(SwsContext *c, int16_t *_dst, int dstW,
+                           const uint8_t *src, const int16_t *filter,
+                           const int32_t *filterPos, int filterSize)
+{
+    int i;
+    int32_t *dst = (int32_t *) _dst;
+    int max = (1 << 19) - 1;
+    v16i8 zero = {0};
+    v4i32 vmax = (v4i32)__msa_fill_w(max);
+
+    if (filterSize == 8) {
+        int len = dstW >> 2;
+        int res = dstW & 3;
+
+        for (i = 0; i < len; i++) {
+            v4i32 out0;
+
+            SCALE_8_4(3, out0);
+            ST_V(v4i32, out0, dst);
+            filterPos += 4;
+            filter    += 32;
+            dst       += 4;
+        }
+        if (res & 2) {
+            v4i32 out0;
+
+            SCALE_8_2(3, out0);
+            dst[0] = __msa_copy_s_w(out0, 0);
+            dst[1] = __msa_copy_s_w(out0, 2);
+            filterPos += 2;
+            filter    += 16;
+            dst       += 2;
+        }
+        if (res & 1) {
+            int val;
+            v8i16 src0, filter0;
+            v4i32 out0;
+
+            src0 = LD_V(v8i16, src + filterPos[0]);
+            filter0 = LD_V(v8i16, filter);
+            src0 = (v8i16)__msa_ilvr_b((v16i8)zero, (v16i8)src0);
+            out0 = (v4i32)__msa_dotp_s_w(src0, filter0);
+            out0 = (v4i32)__msa_hadd_s_d(out0, out0);
+            val  = __msa_copy_s_w(out0, 0) + __msa_copy_s_w(out0, 2);
+            dst[0] = FFMIN(val >> 3, max);
+        }
+    } else if (filterSize == 4) {
+        int len = dstW >> 2;
+        int res = dstW & 3;
+
+        for (i = 0; i < len; i++) {
+            v4i32 out0;
+
+            SCALE_4_4(3, out0);
+            ST_V(v4i32, out0, dst);
+            filterPos += 4;
+            filter    += 16;
+            dst       += 4;
+        }
+        if (res & 2) {
+            v4i32 out0;
+
+            SCALE_4_2(3, out0);
+            dst[0] = __msa_copy_s_w(out0, 0);
+            dst[1] = __msa_copy_s_w(out0, 2);
+            filterPos += 2;
+            filter    += 8;
+            dst       += 2;
+        }
+        if (res & 1) {
+           int val = 0;
+           const uint8_t *srcPos = src + filterPos[0];
+
+           for (int j = 0; j < 4; j++) {
+               val += ((int)srcPos[j]) * filter[j];
+           }
+           dst[0] = FFMIN(val >> 3, max);
+        }
+    } else if (filterSize > 8) {
+        int j, len = dstW >> 2;
+        int res = dstW & 3;
+        int filterlen = filterSize - 7;
+        v8i16 src0, src1, src2, src3;
+        v8i16 filter0, filter1, filter2, filter3;
+        v4i32 out0, out1, out2, out3;
+
+        for (i = 0; i < len; i++) {
+            const uint8_t *srcPos0 = src + filterPos[0];
+            const uint8_t *srcPos1 = src + filterPos[1];
+            const uint8_t *srcPos2 = src + filterPos[2];
+            const uint8_t *srcPos3 = src + filterPos[3];
+            const int16_t *filterStart0 = filter;
+            const int16_t *filterStart1 = filterStart0 + filterSize;
+            const int16_t *filterStart2 = filterStart1 + filterSize;
+            const int16_t *filterStart3 = filterStart2 + filterSize;
+            int val0 = 0, val1 = 0, val2 = 0, val3 = 0;
+            v4i32 tmp0 = {0};
+
+            for (j = 0; j < filterlen; j += 8) {
+                SCALE_16_4
+            }
+            val0 = __msa_copy_s_w(tmp0, 0);
+            val1 = __msa_copy_s_w(tmp0, 1);
+            val2 = __msa_copy_s_w(tmp0, 2);
+            val3 = __msa_copy_s_w(tmp0, 3);
+            for (; j < filterSize; j++) {
+                val0 += ((int)srcPos0[j]) * filterStart0[j];
+                val1 += ((int)srcPos1[j]) * filterStart1[j];
+                val2 += ((int)srcPos2[j]) * filterStart2[j];
+                val3 += ((int)srcPos3[j]) * filterStart3[j];
+            }
+            dst[0] = FFMIN(val0 >> 3, max);
+            dst[1] = FFMIN(val1 >> 3, max);
+            dst[2] = FFMIN(val2 >> 3, max);
+            dst[3] = FFMIN(val3 >> 3, max);
+            filterPos += 4;
+            filter     = filterStart3 + filterSize;
+            dst       += 4;
+        }
+        for (i = 0; i < res; i++) {
+            v8i16 src0, filter0;
+            v4i32 out0;
+            const uint8_t *srcPos = src + filterPos[i];
+            int val = 0;
+
+            for (j = 0; j < filterlen; j += 8) {
+                src0 = LD_V(v8i16, (srcPos + j));
+                filter0 = LD_V(v8i16, (filter + j));
+                src0 = (v8i16)__msa_ilvr_b(zero, (v16i8)src0);
+                out0 = (v4i32)__msa_dotp_s_w(src0, filter0);
+                out0 = (v4i32)__msa_hadd_s_d((v4i32)out0, (v4i32)out0);
+                val += (__msa_copy_s_w((v4i32)out0, 0) +
+                        __msa_copy_s_w((v4i32)out0, 2));
+            }
+            for (; j < filterSize; j++) {
+                val += ((int)srcPos[j]) * filter[j];
+            }
+            dst[i]  = FFMIN(val >> 3, max);
+            filter += filterSize;
+        }
+    } else {
+        for (i = 0; i < dstW; i++) {
+            int val = 0;
+            const uint8_t *srcPos = src + filterPos[i];
+            const int16_t *filterStart = filter + filterSize * i;
+
+            for (int j = 0; j < filterSize; j++) {
+                val += ((int)srcPos[j]) * filterStart[j];
+            }
+            dst[i] = FFMIN(val >> 3, max);
+        }
+    }
+}
+
+void ff_hscale_16_to_19_msa(SwsContext *c, int16_t *_dst, int dstW,
+                            const uint8_t *_src, const int16_t *filter,
+                            const int32_t *filterPos, int filterSize)
+{
+    const AVPixFmtDescriptor *desc = av_pix_fmt_desc_get(c->srcFormat);
+    int i;
+    int32_t *dst        = (int32_t *) _dst;
+    const uint16_t *src = (const uint16_t *) _src;
+    int bits            = desc->comp[0].depth - 1;
+    int sh              = bits - 4;
+    int max             = (1 << 19) - 1;
+    v8i16 zero = {0};
+
+    if ((isAnyRGB(c->srcFormat) || c->srcFormat == AV_PIX_FMT_PAL8)
+              && desc->comp[0].depth<16) {
+         sh = 9;
+    } else if (desc->flags & AV_PIX_FMT_FLAG_FLOAT) {
+         sh = 11;
+    }
+
+    if (filterSize == 8) {
+        for (i = 0; i < dstW; i++) {
+            int val = 0;
+            v8i16 src0, filter0;
+            v4i32 src_l, src_r, filter_l, filter_r, out_l, out_r, out;
+
+            src0 = LD_V(v8i16, src + filterPos[i]);
+            filter0 = LD_V(v8i16, filter + (i << 3));
+            src_r = (v4i32)__msa_ilvr_h(zero, (v8i16)src0);
+            src_l = (v4i32)__msa_ilvl_h(zero, (v8i16)src0);
+            UNPCK_SH_SW(filter0, filter_r, filter_l);
+            out_r = (v4i32)__msa_dotp_s_d(src_r, filter_r);
+            out_l = (v4i32)__msa_dotp_s_d(src_l, filter_l);
+            out   = (v4i32)__msa_addv_w(out_r, out_l);
+            val += (__msa_copy_s_w(out, 0) +
+                    __msa_copy_s_w(out, 2));
+            dst[i] = FFMIN(val >> sh, max);
+        }
+    } else if (filterSize == 4) {
+        int len = dstW & (~1);
+
+        for (i = 0; i < len; i += 2) {
+            int val1 = 0, val2 = 0;
+            v8i16 src1, src2, filter0;
+            v4i32 src1_r, src2_r, filter_r, filter_l;
+            v4i32 out1, out2;
+
+            src1     = LD_V(v8i16, src + filterPos[i]);
+            src2     = LD_V(v8i16, src + filterPos[i + 1]);
+            filter0  = LD_V(v8i16, filter + (i << 2));
+            src1_r   = (v4i32)__msa_ilvr_h(zero, src1);
+            src2_r   = (v4i32)__msa_ilvr_h(zero, src2);
+            UNPCK_SH_SW(filter0, filter_r, filter_l);
+            out1     = (v4i32)__msa_dotp_s_d(src1_r, filter_r);
+            out2     = (v4i32)__msa_dotp_s_d(src2_r, filter_l);
+            val1     = __msa_copy_s_w(out1, 0) + __msa_copy_s_w(out1, 2);
+            val2     = __msa_copy_s_w(out2, 0) + __msa_copy_s_w(out2, 2);
+            dst[i]   = FFMIN(val1 >> sh, max);
+            dst[i + 1] = FFMIN(val2 >> sh, max);
+        }
+        if (i < dstW) {
+           int val = 0;
+           const uint16_t *srcPos = src + filterPos[i];
+           const int16_t *filterStart = filter + (i << 2);
+
+           for (int j = 0; j < 4; j++) {
+               val += ((int)srcPos[j]) * filterStart[j];
+           }
+           dst[i] = FFMIN(val >> sh, max);
+        }
+    } else if (filterSize > 8) {
+        int len  = filterSize >> 3;
+        int part = len << 3;
+
+        for (i = 0; i < dstW; i++) {
+            v8i16 src0, filter0;
+            v4i32 src_r, src_l, filter_r, filter_l, out_r, out_l, out;
+            const uint16_t *srcPos = src + filterPos[i];
+            const int16_t  *filterStart = filter + filterSize * i;
+            int j, val = 0;
+
+            for (j = 0; j < len; j++) {
+                src0 = LD_V(v8i16, srcPos + (j << 3));
+                filter0 = LD_V(v8i16, filterStart + (j << 3));
+                src_r = (v4i32)__msa_ilvr_h(zero, (v8i16)src0);
+                src_l = (v4i32)__msa_ilvl_h(zero, (v8i16)src0);
+                UNPCK_SH_SW(filter0, filter_r, filter_l);
+                out_r = (v4i32)__msa_dotp_s_d(src_r, filter_r);
+                out_l = (v4i32)__msa_dotp_s_d(src_l, filter_l);
+                out   = (v4i32)__msa_addv_w(out_r, out_l);
+                val  += (__msa_copy_s_w(out, 0) +
+                         __msa_copy_s_w(out, 2));
+            }
+            for (j = part; j < filterSize; j++) {
+                val += ((int)srcPos[j]) * filterStart[j];
+            }
+            dst[i] = FFMIN(val >> sh, max);
+        }
+    } else {
+        for (i = 0; i < dstW; i++) {
+            int val = 0;
+            const uint16_t *srcPos = src + filterPos[i];
+            const int16_t  *filterStart = filter + filterSize * i;
+
+            for (int j = 0; j < filterSize; j++) {
+                val += ((int)srcPos[j]) * filterStart[j];
+            }
+            dst[i] = FFMIN(val >> sh, max);
+        }
+    }
+}
+
+void ff_hscale_16_to_15_msa(SwsContext *c, int16_t *dst, int dstW,
+                           const uint8_t *_src, const int16_t *filter,
+                           const int32_t *filterPos, int filterSize)
+{
+    const AVPixFmtDescriptor *desc = av_pix_fmt_desc_get(c->srcFormat);
+    int i;
+    const uint16_t *src = (const uint16_t *) _src;
+    int sh              = desc->comp[0].depth - 1;
+    int max = (1 << 15) - 1;
+    v8i16 zero = {0};
+
+    if (sh < 15) {
+        sh = isAnyRGB(c->srcFormat) || c->srcFormat==AV_PIX_FMT_PAL8 ? 13 :
+                     (desc->comp[0].depth - 1);
+    } else if (desc->flags && AV_PIX_FMT_FLAG_FLOAT) {
+        sh = 15;
+    }
+
+    if (filterSize == 8) {
+        for (i = 0; i < dstW; i++) {
+            int val = 0;
+            v8i16 src0, filter0;
+            v4i32 src_l, src_r, filter_l, filter_r, out_l, out_r, out;
+
+            src0 = LD_V(v8i16, src + filterPos[i]);
+            filter0 = LD_V(v8i16, filter + (i << 3));
+            src_r = (v4i32)__msa_ilvr_h(zero, (v8i16)src0);
+            src_l = (v4i32)__msa_ilvl_h(zero, (v8i16)src0);
+            UNPCK_SH_SW(filter0, filter_r, filter_l);
+            out_r = (v4i32)__msa_dotp_s_d(src_r, filter_r);
+            out_l = (v4i32)__msa_dotp_s_d(src_l, filter_l);
+            out   = (v4i32)__msa_addv_w(out_r, out_l);
+            val += (__msa_copy_s_w(out, 0) +
+                    __msa_copy_s_w(out, 2));
+            dst[i] = FFMIN(val >> sh, max);
+        }
+    } else if (filterSize == 4) {
+        int len = dstW & (~1);
+
+        for (i = 0; i < len; i += 2) {
+            v8i16 src1, src2, filter0;
+            v4i32 src1_r, src2_r, filter_r, filter_l;
+            v4i32 out1, out2;
+            int val1 = 0;
+            int val2 = 0;
+
+            src1     = LD_V(v8i16, src + filterPos[i]);
+            src2     = LD_V(v8i16, src + filterPos[i + 1]);
+            filter0  = LD_V(v8i16, filter + (i << 2));
+            src1_r   = (v4i32)__msa_ilvr_h(zero, src1);
+            src2_r   = (v4i32)__msa_ilvr_h(zero, src2);
+            UNPCK_SH_SW(filter0, filter_r, filter_l);
+            out1     = (v4i32)__msa_dotp_s_d(src1_r, filter_r);
+            out2     = (v4i32)__msa_dotp_s_d(src2_r, filter_l);
+            val1     = __msa_copy_s_w(out1, 0) + __msa_copy_s_w(out1, 2);
+            val2     = __msa_copy_s_w(out2, 0) + __msa_copy_s_w(out2, 2);
+            dst[i]   = FFMIN(val1 >> sh, max);
+            dst[i + 1] = FFMIN(val2 >> sh, max);
+        }
+        if (i < dstW) {
+           int val = 0;
+           const uint16_t *srcPos = src + filterPos[i];
+           const int16_t *filterStart = filter + (i << 2);
+
+           for (int j = 0; j < 4; j++) {
+               val += ((int)srcPos[j]) * filterStart[j];
+           }
+           dst[i] = FFMIN(val >> sh, max);
+        }
+    } else if (filterSize > 8) {
+        int len  = filterSize >> 3;
+        int part = len << 3;
+
+        for (i = 0; i < dstW; i++) {
+            v8i16 src0, filter0;
+            v4i32 src_r, src_l, filter_r, filter_l, out_r, out_l, out;
+            const uint16_t *srcPos = src + filterPos[i];
+            const int16_t  *filterStart = filter + filterSize * i;
+            int j, val = 0;
+
+            for (j = 0; j < len; j++) {
+                src0 = LD_V(v8i16, srcPos + (j << 3));
+                filter0 = LD_V(v8i16, filterStart + (j << 3));
+                src_r = (v4i32)__msa_ilvr_h(zero, (v8i16)src0);
+                src_l = (v4i32)__msa_ilvl_h(zero, (v8i16)src0);
+                UNPCK_SH_SW(filter0, filter_r, filter_l);
+                out_r = (v4i32)__msa_dotp_s_d(src_r, filter_r);
+                out_l = (v4i32)__msa_dotp_s_d(src_l, filter_l);
+                out   = (v4i32)__msa_addv_w(out_r, out_l);
+                val  += (__msa_copy_s_w(out, 0) +
+                         __msa_copy_s_w(out, 2));
+            }
+            for (j = part; j < filterSize; j++) {
+                val += ((int)srcPos[j]) * filterStart[j];
+            }
+            dst[i] = FFMIN(val >> sh, max);
+        }
+    } else {
+        for (i = 0; i < dstW; i++) {
+            int val = 0;
+            const uint16_t *srcPos = src + filterPos[i];
+            const int16_t  *filterStart = filter + filterSize * i;
+
+            for (int j = 0; j < filterSize; j++) {
+                val += ((int)srcPos[j]) * filterStart[j];
+            }
+            dst[i] = FFMIN(val >> sh, max);
+        }
+    }
+}
+
+void ff_yuv2planeX_8_msa(const int16_t *filter, int filterSize,
+                         const int16_t **src, uint8_t *dest, int dstW,
+                         const uint8_t *dither, int offset)
+{
+    int i;
+    int len  = dstW >> 3;
+    int part = len << 3;
+    uint8_t dither0 = dither[offset & 7];
+    uint8_t dither1 = dither[(offset + 1) & 7];
+    uint8_t dither2 = dither[(offset + 2) & 7];
+    uint8_t dither3 = dither[(offset + 3) & 7];
+    uint8_t dither4 = dither[(offset + 4) & 7];
+    uint8_t dither5 = dither[(offset + 5) & 7];
+    uint8_t dither6 = dither[(offset + 6) & 7];
+    uint8_t dither7 = dither[(offset + 7) & 7];
+    v4i32 val1 = {dither0, dither1, dither2, dither3};
+    v4i32 val2 = {dither4, dither5, dither6, dither7};
+    v8i16 zero = {0};
+
+    val1 = (v4i32)__msa_slli_w(val1, 12);
+    val2 = (v4i32)__msa_slli_w(val2, 12);
+
+    for (i = 0; i < len; i++) {
+        int j;
+        v8i16 src0, flags;
+        v4i32 src_l, src_r, filter0;
+        v4i32 val_r = val1;
+        v4i32 val_l = val2;
+
+        for (j = 0; j < filterSize; j++) {
+            src0 = LD_V(v8i16, &src[j][i * 8]);
+            filter0 = __msa_fill_w(filter[j]);
+            flags = __msa_clt_s_h(src0, zero);
+            ILVRL_H2_SW(flags, src0, src_r, src_l);
+            val_r += src_r * filter0;
+            val_l += src_l * filter0;
+        }
+        val_r >>= 19;
+        val_l >>= 19;
+        CLIP_SW2_0_255(val_r, val_l);
+        src0 = __msa_pckev_h((v8i16)val_l, (v8i16)val_r);
+        src0 = (v8i16)__msa_pckev_b((v16i8)src0, (v16i8)src0);
+        SD(__msa_copy_s_d((v2i64)src0, 0), dest + (i << 3));
+    }
+    for (i = part; i < dstW; i++) {
+        int val = dither[(i + offset) & 7] << 12;
+        int j;
+        for (j = 0; j< filterSize; j++)
+            val += src[j][i] * filter[j];
+
+        dest[i] = av_clip_uint8(val >> 19);
+    }
+}
+
+/*Copy from libswscale/output.c*/
+static av_always_inline void
+yuv2rgb_write(uint8_t *_dest, int i, int Y1, int Y2,
+              unsigned A1, unsigned A2,
+              const void *_r, const void *_g, const void *_b, int y,
+              enum AVPixelFormat target, int hasAlpha)
+{
+    if (target == AV_PIX_FMT_ARGB || target == AV_PIX_FMT_RGBA ||
+        target == AV_PIX_FMT_ABGR || target == AV_PIX_FMT_BGRA) {
+        uint32_t *dest = (uint32_t *) _dest;
+        const uint32_t *r = (const uint32_t *) _r;
+        const uint32_t *g = (const uint32_t *) _g;
+        const uint32_t *b = (const uint32_t *) _b;
+
+#if CONFIG_SMALL
+        int sh = hasAlpha ? ((target ==AV_PIX_FMT_RGB32_1 || target == AV_PIX_FMT_BGR32_1) ? 0 : 24) : 0;
+        dest[i * 2 + 0] = r[Y1] + g[Y1] + b[Y1];
+        dest[i * 2 + 1] = r[Y2] + g[Y2] + b[Y2];
+#else
+#if defined(ASSERT_LEVEL) && ASSERT_LEVEL > 1
+        int sh = (target == AV_PIX_FMT_RGB32_1 ||
+                  target == AV_PIX_FMT_BGR32_1) ? 0 : 24;
+        av_assert2((((r[Y1] + g[Y1] + b[Y1]) >> sh) & 0xFF) == 0xFF);
+#endif
+        dest[i * 2 + 0] = r[Y1] + g[Y1] + b[Y1];
+        dest[i * 2 + 1] = r[Y2] + g[Y2] + b[Y2];
+#endif
+
+    } else if (target == AV_PIX_FMT_RGB565 || target == AV_PIX_FMT_BGR565 ||
+               target == AV_PIX_FMT_RGB555 || target == AV_PIX_FMT_BGR555 ||
+               target == AV_PIX_FMT_RGB444 || target == AV_PIX_FMT_BGR444) {
+        uint16_t *dest = (uint16_t *) _dest;
+        const uint16_t *r = (const uint16_t *) _r;
+        const uint16_t *g = (const uint16_t *) _g;
+        const uint16_t *b = (const uint16_t *) _b;
+        int dr1, dg1, db1, dr2, dg2, db2;
+
+        if (target == AV_PIX_FMT_RGB565 || target == AV_PIX_FMT_BGR565) {
+            dr1 = ff_dither_2x2_8[ y & 1     ][0];
+            dg1 = ff_dither_2x2_4[ y & 1     ][0];
+            db1 = ff_dither_2x2_8[(y & 1) ^ 1][0];
+            dr2 = ff_dither_2x2_8[ y & 1     ][1];
+            dg2 = ff_dither_2x2_4[ y & 1     ][1];
+            db2 = ff_dither_2x2_8[(y & 1) ^ 1][1];
+        }
+
+        dest[i * 2 + 0] = r[Y1 + dr1] + g[Y1 + dg1] + b[Y1 + db1];
+        dest[i * 2 + 1] = r[Y2 + dr2] + g[Y2 + dg2] + b[Y2 + db2];
+    }
+}
+
+static av_always_inline void
+yuv2rgb_X_msa_template(SwsContext *c, const int16_t *lumFilter,
+                       const int16_t **lumSrc, int lumFilterSize,
+                       const int16_t *chrFilter, const int16_t **chrUSrc,
+                       const int16_t **chrVSrc, int chrFilterSize,
+                       const int16_t **alpSrc, uint8_t *dest, int dstW,
+                       int y, enum AVPixelFormat target, int hasAlpha)
+{
+    int i, j;
+    int count = 0;
+    int len = dstW & (~0x07);
+    int len_count = (dstW + 1) >> 1;
+    const void *r, *g, *b;
+    v4i32 headroom  = (v4i32)__msa_fill_w(YUVRGB_TABLE_HEADROOM);
+
+    for (i = 0; i < len; i += 8) {
+        int t = 1 << 18;
+        v8i16 l_src, u_src, v_src;
+        v4i32 lumsrc_r, lumsrc_l, usrc, vsrc, temp;
+        v4i32 y_r, y_l, u, v;
+
+        y_r = __msa_fill_w(t);
+        y_l = y_r;
+        u   = y_r;
+        v   = y_r;
+        for (j = 0; j < lumFilterSize; j++) {
+            temp     = __msa_fill_w(lumFilter[j]);
+            l_src    = LD_V(v8i16, (lumSrc[j] + i));
+            UNPCK_SH_SW(l_src, lumsrc_r, lumsrc_l);       /*can use lsx optimization*/
+            y_r      = __msa_maddv_w(lumsrc_r, temp, y_r);
+            y_l      = __msa_maddv_w(lumsrc_l, temp, y_l);
+        }
+        for (j = 0; j < chrFilterSize; j++) {
+            u_src = LD_V(v8i16, (chrUSrc[j] + count));
+            v_src = LD_V(v8i16, (chrVSrc[j] + count));
+            UNPCK_R_SH_SW(u_src, usrc);
+            UNPCK_R_SH_SW(v_src, vsrc);
+            temp  = __msa_fill_w(chrFilter[j]);
+            u     = __msa_maddv_w(usrc, temp, u);
+            v     = __msa_maddv_w(vsrc, temp, v);
+        }
+        y_r = __msa_srai_w(y_r, 19);
+        y_l = __msa_srai_w(y_l, 19);
+        u   = __msa_srai_w(u, 19);
+        v   = __msa_srai_w(v, 19);
+        u   = __msa_addv_w(u, headroom);
+        v   = __msa_addv_w(v, headroom);
+        for (j = 0; j < 2; j++) {
+            int Y1, Y2, U, V;
+            int m = j * 2;
+            int n = j + 2;
+
+            Y1 = y_r[m];
+            Y2 = y_r[m + 1];
+            U  = u[j];
+            V  = v[j];
+            r  =  c->table_rV[V];
+            g  = (c->table_gU[U] + c->table_gV[V]);
+            b  =  c->table_bU[U];
+
+            yuv2rgb_write(dest, count + j, Y1, Y2, 0, 0,
+                          r, g, b, y, target, 0);
+            Y1 = y_l[m];
+            Y2 = y_l[m + 1];
+            U  = u[n];
+            V  = v[n];
+            r  =  c->table_rV[V];
+            g  = (c->table_gU[U] + c->table_gV[V]);
+            b  =  c->table_bU[U];
+
+            yuv2rgb_write(dest, count + n, Y1, Y2, 0, 0,
+                          r, g, b, y, target, 0);
+        }
+        count += 4;
+    }
+    for (; count < len_count; count++) {
+        int Y1 = 1 << 18;
+        int Y2 = 1 << 18;
+        int U  = 1 << 18;
+        int V  = 1 << 18;
+
+        for (j = 0; j < lumFilterSize; j++) {
+            Y1 += lumSrc[j][count * 2]     * lumFilter[j];
+            Y2 += lumSrc[j][count * 2 + 1] * lumFilter[j];
+        }
+        for (j = 0; j < chrFilterSize; j++) {
+            U += chrUSrc[j][count] * chrFilter[j];
+            V += chrVSrc[j][count] * chrFilter[j];
+        }
+        Y1 >>= 19;
+        Y2 >>= 19;
+        U  >>= 19;
+        V  >>= 19;
+        r =  c->table_rV[V + YUVRGB_TABLE_HEADROOM];
+        g = (c->table_gU[U + YUVRGB_TABLE_HEADROOM] +
+             c->table_gV[V + YUVRGB_TABLE_HEADROOM]);
+        b =  c->table_bU[U + YUVRGB_TABLE_HEADROOM];
+
+        yuv2rgb_write(dest, count, Y1, Y2, 0, 0,
+                      r, g, b, y, target, 0);
+    }
+}
+
+static av_always_inline void
+yuv2rgb_2_msa_template(SwsContext *c, const int16_t *buf[2],
+                       const int16_t *ubuf[2], const int16_t *vbuf[2],
+                       const int16_t *abuf[2], uint8_t *dest, int dstW,
+                       int yalpha, int uvalpha, int y,
+                       enum AVPixelFormat target, int hasAlpha)
+{
+    const int16_t *buf0  = buf[0],  *buf1  = buf[1],
+                  *ubuf0 = ubuf[0], *ubuf1 = ubuf[1],
+                  *vbuf0 = vbuf[0], *vbuf1 = vbuf[1];
+    int yalpha1  = 4096 - yalpha;
+    int uvalpha1 = 4096 - uvalpha;
+    int i, count = 0;
+    int len = dstW & (~0x07);
+    int len_count = (dstW + 1) >> 1;
+    const void *r, *g, *b;
+    v4i32 v_yalpha1  = (v4i32)__msa_fill_w(yalpha1);
+    v4i32 v_uvalpha1 = (v4i32)__msa_fill_w(uvalpha1);
+    v4i32 v_yalpha   = (v4i32)__msa_fill_w(yalpha);
+    v4i32 v_uvalpha  = (v4i32)__msa_fill_w(uvalpha);
+    v4i32 headroom   = (v4i32)__msa_fill_w(YUVRGB_TABLE_HEADROOM);
+
+    for (i = 0; i < len; i += 8) {
+        v8i16 src_y, src_u, src_v;
+        v4i32 y0_r, y0_l, u0, v0;
+        v4i32 y1_r, y1_l, u1, v1;
+        v4i32 y_r, y_l, u, v;
+
+        src_y = LD_V(v8i16, buf0 + i);
+        src_u = LD_V(v8i16, ubuf0 + count);
+        src_v = LD_V(v8i16, vbuf0 + count);
+        UNPCK_SH_SW(src_y, y0_r, y0_l);
+        UNPCK_R_SH_SW(src_u, u0);
+        UNPCK_R_SH_SW(src_v, v0);
+        src_y = LD_V(v8i16, buf1  + i);
+        src_u = LD_V(v8i16, ubuf1 + count);
+        src_v = LD_V(v8i16, vbuf1 + count);
+        UNPCK_SH_SW(src_y, y1_r, y1_l);
+        UNPCK_R_SH_SW(src_u, u1);
+        UNPCK_R_SH_SW(src_v, v1);
+        y0_r = __msa_mulv_w(y0_r, v_yalpha1);
+        y0_l = __msa_mulv_w(y0_l, v_yalpha1);
+        u0   = __msa_mulv_w(u0, v_uvalpha1);
+        v0   = __msa_mulv_w(v0, v_uvalpha1);
+        y_r  = __msa_maddv_w(y1_r, v_yalpha, y0_r);
+        y_l  = __msa_maddv_w(y1_l, v_yalpha, y0_l);
+        u    = __msa_maddv_w(u1, v_uvalpha, u0);
+        v    = __msa_maddv_w(v1, v_uvalpha, v0);
+        y_r  = __msa_srai_w(y_r, 19);
+        y_l  = __msa_srai_w(y_l, 19);
+        u    = __msa_srai_w(u, 19);
+        v    = __msa_srai_w(v, 19);
+        u    = __msa_addv_w(u, headroom);
+        v    = __msa_addv_w(v, headroom);
+        for (int j = 0; j < 2; j++) {
+            int Y1, Y2, U, V;
+            int m = j * 2;
+            int n = j + 2;
+
+            Y1 = y_r[m];
+            Y2 = y_r[m + 1];
+            U  = u[j];
+            V  = v[j];
+            r  =  c->table_rV[V];
+            g  = (c->table_gU[U] + c->table_gV[V]);
+            b  =  c->table_bU[U];
+
+            yuv2rgb_write(dest, count + j, Y1, Y2, 0, 0,
+                          r, g, b, y, target, 0);
+            Y1 = y_l[m];
+            Y2 = y_l[m + 1];
+            U  = u[n];
+            V  = v[n];
+            r  =  c->table_rV[V];
+            g  = (c->table_gU[U] + c->table_gV[V]);
+            b  =  c->table_bU[U];
+
+            yuv2rgb_write(dest, count + n, Y1, Y2, 0, 0,
+                          r, g, b, y, target, 0);
+        }
+        count += 4;
+    }
+
+    for (; count < len_count; count++) {
+        int Y1 = (buf0[count * 2]     * yalpha1  +
+                  buf1[count * 2]     * yalpha)  >> 19;
+        int Y2 = (buf0[count * 2 + 1] * yalpha1  +
+                  buf1[count * 2 + 1] * yalpha) >> 19;
+        int U  = (ubuf0[count] * uvalpha1 + ubuf1[count] * uvalpha) >> 19;
+        int V  = (vbuf0[count] * uvalpha1 + vbuf1[count] * uvalpha) >> 19;
+
+        r =  c->table_rV[V + YUVRGB_TABLE_HEADROOM],
+        g = (c->table_gU[U + YUVRGB_TABLE_HEADROOM] +
+             c->table_gV[V + YUVRGB_TABLE_HEADROOM]),
+        b =  c->table_bU[U + YUVRGB_TABLE_HEADROOM];
+
+        yuv2rgb_write(dest, count, Y1, Y2, 0, 0,
+                      r, g, b, y, target, 0);
+    }
+}
+
+static av_always_inline void
+yuv2rgb_1_msa_template(SwsContext *c, const int16_t *buf0,
+                       const int16_t *ubuf[2], const int16_t *vbuf[2],
+                       const int16_t *abuf0, uint8_t *dest, int dstW,
+                       int uvalpha, int y, enum AVPixelFormat target,
+                       int hasAlpha)
+{
+    const int16_t *ubuf0 = ubuf[0], *vbuf0 = vbuf[0];
+    int i, j;
+    const void *r, *g, *b;
+    int len = dstW & (~0x07);
+    int len_count = (dstW + 1) >> 1;
+
+    if (uvalpha < 2048) {
+        int count = 0;
+        v4i32 headroom  = (v4i32)__msa_fill_w(YUVRGB_TABLE_HEADROOM);
+        v4i32 bias_64   = (v4i32)__msa_fill_w(64);
+
+        for (i = 0; i < len; i += 8) {
+            v8i16 src_y, src_u, src_v;
+            v4i32 y_r, y_l, u, v;
+
+            src_y = LD_V(v8i16, buf0 + i);
+            src_u = LD_V(v8i16, ubuf0 + count);
+            src_v = LD_V(v8i16, vbuf0 + count);
+            UNPCK_SH_SW(src_y, y_r, y_l);
+            UNPCK_R_SH_SW(src_u, u);
+            UNPCK_R_SH_SW(src_v, v);
+            y_r = __msa_addv_w(y_r, bias_64);
+            y_l = __msa_addv_w(y_l, bias_64);
+            u   = __msa_addv_w(u, bias_64);
+            v   = __msa_addv_w(v, bias_64);
+            y_r = __msa_srai_w(y_r, 7);
+            y_l = __msa_srai_w(y_l, 7);
+            u   = __msa_srai_w(u, 7);
+            v   = __msa_srai_w(v, 7);
+            u   = __msa_addv_w(u, headroom);
+            v   = __msa_addv_w(v, headroom);
+            for (j = 0; j < 2; j++) {
+                int Y1, Y2, U, V;
+                int m = j * 2;
+                int n = j + 2;
+
+                Y1 = y_r[m];
+                Y2 = y_r[m + 1];
+                U  = u[j];
+                V  = v[j];
+                r  =  c->table_rV[V];
+                g  = (c->table_gU[U] + c->table_gV[V]);
+                b  =  c->table_bU[U];
+
+                yuv2rgb_write(dest, count + j, Y1, Y2, 0, 0,
+                              r, g, b, y, target, 0);
+                Y1 = y_l[m];
+                Y2 = y_l[m + 1];
+                U  = u[n];
+                V  = v[n];
+                r  =  c->table_rV[V];
+                g  = (c->table_gU[U] + c->table_gV[V]);
+                b  =  c->table_bU[U];
+
+                yuv2rgb_write(dest, count + n, Y1, Y2, 0, 0,
+                              r, g, b, y, target, 0);
+            }
+            count += 4;
+        }
+        for (; count < len_count; count++) {
+            int Y1 = (buf0[count * 2    ] + 64) >> 7;
+            int Y2 = (buf0[count * 2 + 1] + 64) >> 7;
+            int U  = (ubuf0[count]        + 64) >> 7;
+            int V  = (vbuf0[count]        + 64) >> 7;
+
+            r =  c->table_rV[V + YUVRGB_TABLE_HEADROOM],
+            g = (c->table_gU[U + YUVRGB_TABLE_HEADROOM] +
+                 c->table_gV[V + YUVRGB_TABLE_HEADROOM]),
+            b =  c->table_bU[U + YUVRGB_TABLE_HEADROOM];
+
+            yuv2rgb_write(dest, count, Y1, Y2, 0, 0,
+                          r, g, b, y, target, 0);
+        }
+    } else {
+        const int16_t *ubuf1 = ubuf[1], *vbuf1 = vbuf[1];
+        int count = 0;
+        v4i32 headroom  = (v4i32)__msa_fill_w(YUVRGB_TABLE_HEADROOM);
+        v4i32 bias_64   = (v4i32)__msa_fill_w(64);
+        v4i32 bias_128  = (v4i32)__msa_fill_w(128);
+
+        for (i = 0; i < len; i += 8) {
+            v8i16 src_y, src_u, src_v;
+            v4i32 y_r, y_l, u0, v0, u1, v1, u, v;
+
+            src_y = LD_V(v8i16, buf0 + i);
+            src_u = LD_V(v8i16, ubuf0 + count);
+            src_v = LD_V(v8i16, vbuf0 + count);
+            UNPCK_SH_SW(src_y, y_r, y_l);
+            UNPCK_R_SH_SW(src_u, u0);
+            UNPCK_R_SH_SW(src_v, v0);
+            src_u = LD_V(v8i16, ubuf1 + count);
+            src_v = LD_V(v8i16, vbuf1 + count);
+            UNPCK_R_SH_SW(src_u, u1);
+            UNPCK_R_SH_SW(src_v, v1);
+
+            u   = __msa_addv_w(u0, u1);
+            v   = __msa_addv_w(v0, v1);
+            y_r = __msa_addv_w(y_r, bias_64);
+            y_l = __msa_addv_w(y_l, bias_64);
+            u   = __msa_addv_w(u, bias_128);
+            v   = __msa_addv_w(v, bias_128);
+            y_r = __msa_srai_w(y_r, 7);
+            y_l = __msa_srai_w(y_l, 7);
+            u   = __msa_srai_w(u, 8);
+            v   = __msa_srai_w(v, 8);
+            u   = __msa_addv_w(u, headroom);
+            v   = __msa_addv_w(v, headroom);
+            for (j = 0; j < 2; j++) {
+                int Y1, Y2, U, V;
+                int m = j * 2;
+                int n = j + 2;
+
+                Y1 = y_r[m];
+                Y2 = y_r[m + 1];
+                U  = u[j];
+                V  = v[j];
+                r  =  c->table_rV[V];
+                g  = (c->table_gU[U] + c->table_gV[V]);
+                b  =  c->table_bU[U];
+
+                yuv2rgb_write(dest, count + j, Y1, Y2, 0, 0,
+                              r, g, b, y, target, 0);
+                Y1 = y_l[m];
+                Y2 = y_l[m + 1];
+                U  = u[n];
+                V  = v[n];
+                r  =  c->table_rV[V];
+                g  = (c->table_gU[U] + c->table_gV[V]);
+                b  =  c->table_bU[U];
+
+                yuv2rgb_write(dest, count + n, Y1, Y2, 0, 0,
+                              r, g, b, y, target, 0);
+            }
+            count += 4;
+        }
+        for (; count < len_count; count++) {
+            int Y1 = (buf0[count * 2    ]         +  64) >> 7;
+            int Y2 = (buf0[count * 2 + 1]         +  64) >> 7;
+            int U  = (ubuf0[count] + ubuf1[count] + 128) >> 8;
+            int V  = (vbuf0[count] + vbuf1[count] + 128) >> 8;
+
+            r =  c->table_rV[V + YUVRGB_TABLE_HEADROOM],
+            g = (c->table_gU[U + YUVRGB_TABLE_HEADROOM] +
+                 c->table_gV[V + YUVRGB_TABLE_HEADROOM]),
+            b =  c->table_bU[U + YUVRGB_TABLE_HEADROOM];
+
+            yuv2rgb_write(dest, count, Y1, Y2, 0, 0,
+                          r, g, b, y, target, 0);
+        }
+    }
+}
+
+#define YUV2RGBWRAPPERX(name, base, ext, fmt, hasAlpha)                        \
+void name ## ext ## _X_msa(SwsContext *c, const int16_t *lumFilter,            \
+                           const int16_t **lumSrc, int lumFilterSize,          \
+                           const int16_t *chrFilter, const int16_t **chrUSrc,  \
+                           const int16_t **chrVSrc, int chrFilterSize,         \
+                           const int16_t **alpSrc, uint8_t *dest, int dstW,    \
+                           int y)                                              \
+{                                                                              \
+    name ## base ## _X_msa_template(c, lumFilter, lumSrc, lumFilterSize,       \
+                                    chrFilter, chrUSrc, chrVSrc, chrFilterSize,\
+                                    alpSrc, dest, dstW, y, fmt, hasAlpha);     \
+}
+
+#define YUV2RGBWRAPPERX2(name, base, ext, fmt, hasAlpha)                       \
+YUV2RGBWRAPPERX(name, base, ext, fmt, hasAlpha)                                \
+void name ## ext ## _2_msa(SwsContext *c, const int16_t *buf[2],               \
+                           const int16_t *ubuf[2], const int16_t *vbuf[2],     \
+                           const int16_t *abuf[2], uint8_t *dest, int dstW,    \
+                           int yalpha, int uvalpha, int y)                     \
+{                                                                              \
+    name ## base ## _2_msa_template(c, buf, ubuf, vbuf, abuf, dest,            \
+                                    dstW, yalpha, uvalpha, y, fmt, hasAlpha);  \
+}
+
+#define YUV2RGBWRAPPER(name, base, ext, fmt, hasAlpha)                         \
+YUV2RGBWRAPPERX2(name, base, ext, fmt, hasAlpha)                               \
+void name ## ext ## _1_msa(SwsContext *c, const int16_t *buf0,                 \
+                           const int16_t *ubuf[2], const int16_t *vbuf[2],     \
+                           const int16_t *abuf0, uint8_t *dest, int dstW,      \
+                           int uvalpha, int y)                                 \
+{                                                                              \
+    name ## base ## _1_msa_template(c, buf0, ubuf, vbuf, abuf0, dest,          \
+                                    dstW, uvalpha, y, fmt, hasAlpha);          \
+}
+
+
+#if CONFIG_SMALL
+#else
+#if CONFIG_SWSCALE_ALPHA
+#endif
+YUV2RGBWRAPPER(yuv2rgb,, x32_1,  AV_PIX_FMT_RGB32_1, 0)
+YUV2RGBWRAPPER(yuv2rgb,, x32,    AV_PIX_FMT_RGB32, 0)
+#endif
+YUV2RGBWRAPPER(yuv2rgb,,  16,    AV_PIX_FMT_RGB565,    0)
+
+// This function is copied from libswscale/output.c
+static av_always_inline void yuv2rgb_write_full(SwsContext *c,
+    uint8_t *dest, int i, int R, int A, int G, int B,
+    int y, enum AVPixelFormat target, int hasAlpha, int err[4])
+{
+    int isrgb8 = target == AV_PIX_FMT_BGR8 || target == AV_PIX_FMT_RGB8;
+
+    if ((R | G | B) & 0xC0000000) {
+        R = av_clip_uintp2(R, 30);
+        G = av_clip_uintp2(G, 30);
+        B = av_clip_uintp2(B, 30);
+    }
+
+    switch(target) {
+    case AV_PIX_FMT_ARGB:
+        dest[0] = hasAlpha ? A : 255;
+        dest[1] = R >> 22;
+        dest[2] = G >> 22;
+        dest[3] = B >> 22;
+        break;
+    case AV_PIX_FMT_RGB24:
+        dest[0] = R >> 22;
+        dest[1] = G >> 22;
+        dest[2] = B >> 22;
+        break;
+    case AV_PIX_FMT_RGBA:
+        dest[0] = R >> 22;
+        dest[1] = G >> 22;
+        dest[2] = B >> 22;
+        dest[3] = hasAlpha ? A : 255;
+        break;
+    case AV_PIX_FMT_ABGR:
+        dest[0] = hasAlpha ? A : 255;
+        dest[1] = B >> 22;
+        dest[2] = G >> 22;
+        dest[3] = R >> 22;
+        break;
+    case AV_PIX_FMT_BGR24:
+        dest[0] = B >> 22;
+        dest[1] = G >> 22;
+        dest[2] = R >> 22;
+        break;
+    case AV_PIX_FMT_BGRA:
+        dest[0] = B >> 22;
+        dest[1] = G >> 22;
+        dest[2] = R >> 22;
+        dest[3] = hasAlpha ? A : 255;
+        break;
+    case AV_PIX_FMT_BGR4_BYTE:
+    case AV_PIX_FMT_RGB4_BYTE:
+    case AV_PIX_FMT_BGR8:
+    case AV_PIX_FMT_RGB8:
+    {
+        int r,g,b;
+
+        switch (c->dither) {
+        default:
+        case SWS_DITHER_AUTO:
+        case SWS_DITHER_ED:
+            R >>= 22;
+            G >>= 22;
+            B >>= 22;
+            R += (7*err[0] + 1*c->dither_error[0][i] + 5*c->dither_error[0][i+1] + 3*c->dither_error[0][i+2])>>4;
+            G += (7*err[1] + 1*c->dither_error[1][i] + 5*c->dither_error[1][i+1] + 3*c->dither_error[1][i+2])>>4;
+            B += (7*err[2] + 1*c->dither_error[2][i] + 5*c->dither_error[2][i+1] + 3*c->dither_error[2][i+2])>>4;
+            c->dither_error[0][i] = err[0];
+            c->dither_error[1][i] = err[1];
+            c->dither_error[2][i] = err[2];
+            r = R >> (isrgb8 ? 5 : 7);
+            g = G >> (isrgb8 ? 5 : 6);
+            b = B >> (isrgb8 ? 6 : 7);
+            r = av_clip(r, 0, isrgb8 ? 7 : 1);
+            g = av_clip(g, 0, isrgb8 ? 7 : 3);
+            b = av_clip(b, 0, isrgb8 ? 3 : 1);
+            err[0] = R - r*(isrgb8 ? 36 : 255);
+            err[1] = G - g*(isrgb8 ? 36 : 85);
+            err[2] = B - b*(isrgb8 ? 85 : 255);
+            break;
+        case SWS_DITHER_A_DITHER:
+            if (isrgb8) {
+  /* see http://pippin.gimp.org/a_dither/ for details/origin */
+#define A_DITHER(u,v)   (((((u)+((v)*236))*119)&0xff))
+                r = (((R >> 19) + A_DITHER(i,y)  -96)>>8);
+                g = (((G >> 19) + A_DITHER(i + 17,y) - 96)>>8);
+                b = (((B >> 20) + A_DITHER(i + 17*2,y) -96)>>8);
+                r = av_clip_uintp2(r, 3);
+                g = av_clip_uintp2(g, 3);
+                b = av_clip_uintp2(b, 2);
+            } else {
+                r = (((R >> 21) + A_DITHER(i,y)-256)>>8);
+                g = (((G >> 19) + A_DITHER(i + 17,y)-256)>>8);
+                b = (((B >> 21) + A_DITHER(i + 17*2,y)-256)>>8);
+                r = av_clip_uintp2(r, 1);
+                g = av_clip_uintp2(g, 2);
+                b = av_clip_uintp2(b, 1);
+            }
+            break;
+        case SWS_DITHER_X_DITHER:
+            if (isrgb8) {
+  /* see http://pippin.gimp.org/a_dither/ for details/origin */
+#define X_DITHER(u,v)   (((((u)^((v)*237))*181)&0x1ff)/2)
+                r = (((R >> 19) + X_DITHER(i,y) - 96)>>8);
+                g = (((G >> 19) + X_DITHER(i + 17,y) - 96)>>8);
+                b = (((B >> 20) + X_DITHER(i + 17*2,y) - 96)>>8);
+                r = av_clip_uintp2(r, 3);
+                g = av_clip_uintp2(g, 3);
+                b = av_clip_uintp2(b, 2);
+            } else {
+                r = (((R >> 21) + X_DITHER(i,y)-256)>>8);
+                g = (((G >> 19) + X_DITHER(i + 17,y)-256)>>8);
+                b = (((B >> 21) + X_DITHER(i + 17*2,y)-256)>>8);
+                r = av_clip_uintp2(r, 1);
+                g = av_clip_uintp2(g, 2);
+                b = av_clip_uintp2(b, 1);
+            }
+
+            break;
+        }
+
+        if(target == AV_PIX_FMT_BGR4_BYTE) {
+            dest[0] = r + 2*g + 8*b;
+        } else if(target == AV_PIX_FMT_RGB4_BYTE) {
+            dest[0] = b + 2*g + 8*r;
+        } else if(target == AV_PIX_FMT_BGR8) {
+            dest[0] = r + 8*g + 64*b;
+        } else if(target == AV_PIX_FMT_RGB8) {
+            dest[0] = b + 4*g + 32*r;
+        } else
+            av_assert2(0);
+        break; }
+    }
+}
+
+#define yuvTorgb_setup                                           \
+    int y_offset = c->yuv2rgb_y_offset;                          \
+    int y_coeff  = c->yuv2rgb_y_coeff;                           \
+    int v2r_coe  = c->yuv2rgb_v2r_coeff;                         \
+    int v2g_coe  = c->yuv2rgb_v2g_coeff;                         \
+    int u2g_coe  = c->yuv2rgb_u2g_coeff;                         \
+    int u2b_coe  = c->yuv2rgb_u2b_coeff;                         \
+    v4i32 offset = __msa_fill_w(y_offset);                       \
+    v4i32 coeff  = __msa_fill_w(y_coeff);                        \
+    v4i32 v2r    = __msa_fill_w(v2r_coe);                        \
+    v4i32 v2g    = __msa_fill_w(v2g_coe);                        \
+    v4i32 u2g    = __msa_fill_w(u2g_coe);                        \
+    v4i32 u2b    = __msa_fill_w(u2b_coe);                        \
+
+
+#define yuvTorgb                                                 \
+     y_r -= offset;                                              \
+     y_l -= offset;                                              \
+     y_r *= coeff;                                               \
+     y_l *= coeff;                                               \
+     y_r += y_temp;                                              \
+     y_l += y_temp;                                              \
+     R_r = __msa_maddv_w(v_r, v2r, y_r);                         \
+     R_l = __msa_maddv_w(v_l, v2r, y_l);                         \
+     v_r = __msa_maddv_w(v_r, v2g, y_r);                         \
+     v_l = __msa_maddv_w(v_l, v2g, y_l);                         \
+     G_r = __msa_maddv_w(u_r, u2g, v_r);                         \
+     G_l = __msa_maddv_w(u_l, u2g, v_l);                         \
+     B_r = __msa_maddv_w(u_r, u2b, y_r);                         \
+     B_l = __msa_maddv_w(u_l, u2b, y_l);                         \
+
+
+static av_always_inline void
+yuv2rgb_full_X_msa_template(SwsContext *c, const int16_t *lumFilter,
+                          const int16_t **lumSrc, int lumFilterSize,
+                          const int16_t *chrFilter, const int16_t **chrUSrc,
+                          const int16_t **chrVSrc, int chrFilterSize,
+                          const int16_t **alpSrc, uint8_t *dest,
+                          int dstW, int y, enum AVPixelFormat target, int hasAlpha)
+{
+    int i, j, B, G, R, A;
+    int step     = (target == AV_PIX_FMT_RGB24 || target == AV_PIX_FMT_BGR24) ? 3 : 4;
+    int err[4]   = {0};
+    int a_temp   = 1 << 18; //init to silence warning
+    int templ    = 1 << 9;
+    int tempc    = templ - (128 << 19);
+    int len      = dstW & (~0x07);
+    int ytemp    = 1 << 21;
+    v4i32 y_temp = __msa_fill_w(ytemp);
+    yuvTorgb_setup
+
+    if(   target == AV_PIX_FMT_BGR4_BYTE || target == AV_PIX_FMT_RGB4_BYTE
+       || target == AV_PIX_FMT_BGR8      || target == AV_PIX_FMT_RGB8)
+        step = 1;
+
+    for (i = 0; i < len; i += 8) {
+        int m = i + 4;
+        v8i16 l_src, u_src, v_src;
+        v4i32 lum_r, lum_l, usrc_r, usrc_l, vsrc_r, vsrc_l;
+        v4i32 y_r, y_l, u_r, u_l, v_r, v_l, temp;
+        v4i32 R_r, R_l, G_r, G_l, B_r, B_l;
+
+        y_r = y_l = (v4i32)__msa_fill_w(templ);
+        u_r = u_l = v_r = v_l = (v4i32)__msa_fill_w(tempc);
+        for (j = 0; j < lumFilterSize; j++) {
+            temp  = __msa_fill_w(lumFilter[j]);
+            l_src = LD_V(v8i16, (lumSrc[j] + i));
+            UNPCK_SH_SW(l_src, lum_r, lum_l);
+            y_l   = __msa_maddv_w(lum_l, temp, y_l);
+            y_r   = __msa_maddv_w(lum_r, temp, y_r);
+        }
+        for (j = 0; j < chrFilterSize; j++) {
+            temp  = __msa_fill_w(chrFilter[j]);
+            u_src = LD_V(v8i16, (chrUSrc[j] + i));
+            v_src = LD_V(v8i16, (chrVSrc[j] + i));
+            UNPCK_SH_SW(u_src, usrc_r, usrc_l);
+            UNPCK_SH_SW(v_src, vsrc_r, vsrc_l);
+            u_l   = __msa_maddv_w(usrc_l, temp, u_l);
+            u_r   = __msa_maddv_w(usrc_r, temp, u_r);
+            v_l   = __msa_maddv_w(vsrc_l, temp, v_l);
+            v_r   = __msa_maddv_w(vsrc_r, temp, v_r);
+        }
+        y_r = __msa_srai_w(y_r, 10);
+        y_l = __msa_srai_w(y_l, 10);
+        u_r = __msa_srai_w(u_r, 10);
+        u_l = __msa_srai_w(u_l, 10);
+        v_r = __msa_srai_w(v_r, 10);
+        v_l = __msa_srai_w(v_l, 10);
+        yuvTorgb
+
+        if (hasAlpha) {
+            v8i16 a_src;
+            v4i32 asrc_r, asrc_l, a_r, a_l;
+
+            a_r = a_l = __msa_fill_w(a_temp);
+            for (j = 0; j < lumFilterSize; j++) {
+                temp  = __msa_fill_w(lumFilter[j]);
+                a_src = LD_V(v8i16, (alpSrc[j] + i));
+                UNPCK_SH_SW(a_src, asrc_r, asrc_l);
+                a_l   = __msa_maddv_w(asrc_l, temp, a_l);
+                a_r   = __msa_maddv_w(asrc_r, temp, a_r);
+            }
+            a_l = __msa_srai_w(a_l, 19);
+            a_r = __msa_srai_w(a_r, 19);
+            for (j = 0; j < 4; j++) {
+                R = R_r[j];
+                G = G_r[j];
+                B = B_r[j];
+                A = a_r[j];
+                if (A & 0x100)
+                    A = av_clip_uint8(A);
+                yuv2rgb_write_full(c, dest, i + j, R, A, G, B, y, target, hasAlpha, err);
+                dest += step;
+            }
+            for (j = 0; j < 4; j++) {
+                R = R_l[j];
+                G = G_l[j];
+                B = B_l[j];
+                A = a_l[j];
+                if (A & 0x100)
+                    A = av_clip_uint8(A);
+                yuv2rgb_write_full(c, dest, m + j, R, A, G, B, y, target, hasAlpha, err);
+                dest += step;
+            }
+        } else {
+            for (j = 0; j < 4; j++) {
+                R = R_r[j];
+                G = G_r[j];
+                B = B_r[j];
+                yuv2rgb_write_full(c, dest, i + j, R, 0, G, B, y, target, hasAlpha, err);
+                dest += step;
+            }
+            for (j = 0; j < 4; j++) {
+                R = R_l[j];
+                G = G_l[j];
+                B = B_l[j];
+                yuv2rgb_write_full(c, dest, m + j, R, 0, G, B, y, target, hasAlpha, err);
+                dest += step;
+            }
+        }
+    }
+    for (; i < dstW; i++) {
+        int Y = templ;
+        int V, U = V = tempc;
+
+        A = 0;
+        for (j = 0; j < lumFilterSize; j++) {
+            Y += lumSrc[j][i] * lumFilter[j];
+        }
+        for (j = 0; j < chrFilterSize; j++) {
+            U += chrUSrc[j][i] * chrFilter[j];
+            V += chrVSrc[j][i] * chrFilter[j];
+
+        }
+        Y >>= 10;
+        U >>= 10;
+        V >>= 10;
+        if (hasAlpha) {
+            A = 1 << 18;
+            for (j = 0; j < lumFilterSize; j++) {
+                A += alpSrc[j][i] * lumFilter[j];
+            }
+            A >>= 19;
+            if (A & 0x100)
+                A = av_clip_uint8(A);
+        }
+        Y -= y_offset;
+        Y *= y_coeff;
+        Y += ytemp;
+        R = (unsigned)Y + V * v2r_coe;
+        G = (unsigned)Y + V * v2g_coe + U * u2g_coe;
+        B = (unsigned)Y + U * u2b_coe;
+        yuv2rgb_write_full(c, dest, i, R, A, G, B, y, target, hasAlpha, err);
+        dest += step;
+    }
+    c->dither_error[0][i] = err[0];
+    c->dither_error[1][i] = err[1];
+    c->dither_error[2][i] = err[2];
+}
+
+static av_always_inline void
+yuv2rgb_full_2_msa_template(SwsContext *c, const int16_t *buf[2],
+                     const int16_t *ubuf[2], const int16_t *vbuf[2],
+                     const int16_t *abuf[2], uint8_t *dest, int dstW,
+                     int yalpha, int uvalpha, int y,
+                     enum AVPixelFormat target, int hasAlpha)
+{
+    const int16_t *buf0  = buf[0],  *buf1  = buf[1],
+                  *ubuf0 = ubuf[0], *ubuf1 = ubuf[1],
+                  *vbuf0 = vbuf[0], *vbuf1 = vbuf[1],
+                  *abuf0 = hasAlpha ? abuf[0] : NULL,
+                  *abuf1 = hasAlpha ? abuf[1] : NULL;
+    int yalpha1  = 4096 - yalpha;
+    int uvalpha1 = 4096 - uvalpha;
+    int uvtemp   = 128 << 19;
+    int atemp    = 1 << 18;
+    int i, j, R, G, B, A;
+    int step = (target == AV_PIX_FMT_RGB24 || target == AV_PIX_FMT_BGR24) ? 3 : 4;
+    int err[4] = {0};
+    int len = dstW & (~0x07);
+    int ytemp   = 1 << 21;
+    v4i32 uvalp1 = (v4i32)__msa_fill_w(uvalpha1);
+    v4i32 yalp1  = (v4i32)__msa_fill_w(yalpha1);
+    v4i32 uvalp  = (v4i32)__msa_fill_w(uvalpha);
+    v4i32 yalp   = (v4i32)__msa_fill_w(yalpha);
+    v4i32 uv     = (v4i32)__msa_fill_w(uvtemp);
+    v4i32 a_bias = (v4i32)__msa_fill_w(atemp);
+    v4i32 y_temp = __msa_fill_w(ytemp);
+    yuvTorgb_setup
+
+    av_assert2(yalpha  <= 4096U);
+    av_assert2(uvalpha <= 4096U);
+
+    if(   target == AV_PIX_FMT_BGR4_BYTE || target == AV_PIX_FMT_RGB4_BYTE
+       || target == AV_PIX_FMT_BGR8      || target == AV_PIX_FMT_RGB8)
+        step = 1;
+
+    for (i = 0; i < len; i += 8) {
+        int m = i + 4;
+        v8i16 b0, b1, ub0, ub1, vb0, vb1;
+        v4i32 b0_r, b0_l, b1_r, b1_l, ub0_r, ub0_l, ub1_r, ub1_l, vb0_r, vb0_l, vb1_r, vb1_l;
+        v4i32 y_r, y_l, u_r, u_l, v_r, v_l;
+        v4i32 R_r, R_l, G_r, G_l, B_r, B_l;
+
+        b0  = LD_V(v8i16, (buf0 + i));
+        b1  = LD_V(v8i16, (buf1 + i));
+        ub0 = LD_V(v8i16, (ubuf0 + i));
+        ub1 = LD_V(v8i16, (ubuf1 + i));
+        vb0 = LD_V(v8i16, (vbuf0 + i));
+        vb1 = LD_V(v8i16, (vbuf1 + i));
+        UNPCK_SH_SW(b0, b0_r, b0_l);
+        UNPCK_SH_SW(b1, b1_r, b1_l);
+        UNPCK_SH_SW(ub0, ub0_r, ub0_l);
+        UNPCK_SH_SW(ub1, ub1_r, ub1_l);
+        UNPCK_SH_SW(vb0, vb0_r, vb0_l);
+        UNPCK_SH_SW(vb1, vb1_r, vb1_l);
+        y_r = b0_r * yalp1;
+        y_l = b0_l * yalp1;
+        y_r = __msa_maddv_w(b1_r, yalp, y_r);
+        y_l = __msa_maddv_w(b1_l, yalp, y_l);
+        u_r = ub0_r * uvalp1;
+        u_l = ub0_l * uvalp1;
+        u_r = __msa_maddv_w(ub1_r, uvalp, u_r);
+        u_l = __msa_maddv_w(ub1_l, uvalp, u_l);
+        v_r = vb0_r * uvalp1;
+        v_l = vb0_l * uvalp1;
+        v_r = __msa_maddv_w(vb1_r, uvalp, v_r);
+        v_l = __msa_maddv_w(vb1_l, uvalp, v_l);
+        u_r -= uv;
+        u_l -= uv;
+        v_r -= uv;
+        v_l -= uv;
+        y_r = __msa_srai_w(y_r, 10);
+        y_l = __msa_srai_w(y_l, 10);
+        u_r = __msa_srai_w(u_r, 10);
+        u_l = __msa_srai_w(u_l, 10);
+        v_r = __msa_srai_w(v_r, 10);
+        v_l = __msa_srai_w(v_l, 10);
+        yuvTorgb
+
+        if (hasAlpha) {
+            v8i16 a0, a1;
+            v4i32 a_r, a_l, a1_r, a1_l;
+
+            a0 = LD_V(v8i16, (abuf0 + i));
+            a1 = LD_V(v8i16, (abuf1 + i));
+            UNPCK_SH_SW(a0, a_r, a_l);
+            UNPCK_SH_SW(a1, a1_r, a1_l);
+            a_r *= yalp1;
+            a_l *= yalp1;
+            a_r = __msa_maddv_w(a1_r, yalp, a_r);
+            a_l = __msa_maddv_w(a1_l, yalp, a_l);
+            a_r += a_bias;
+            a_l += a_bias;
+            a_r = __msa_srai_w(a_r, 19);
+            a_l = __msa_srai_w(a_l, 19);
+            for (j = 0; j < 4; j++) {
+                R = R_r[j];
+                G = G_r[j];
+                B = B_r[j];
+                A = a_r[j];
+                if (A & 0x100)
+                    A = av_clip_uint8(A);
+                yuv2rgb_write_full(c, dest, i + j, R, A, G, B, y, target, hasAlpha, err);
+                dest += step;
+            }
+            for (j = 0; j < 4; j++) {
+                R = R_l[j];
+                G = G_l[j];
+                B = B_l[j];
+                A = a_l[j];
+                if (A & 0x100)
+                    A = av_clip_uint8(A);
+                yuv2rgb_write_full(c, dest, m + j, R, A, G, B, y, target, hasAlpha, err);
+                dest += step;
+            }
+        } else {
+            for (j = 0; j < 4; j++) {
+                R = R_r[j];
+                G = G_r[j];
+                B = B_r[j];
+                yuv2rgb_write_full(c, dest, i + j, R, 0, G, B, y, target, hasAlpha, err);
+                dest += step;
+            }
+            for (j = 0; j < 4; j++) {
+                R = R_l[j];
+                G = G_l[j];
+                B = B_l[j];
+                yuv2rgb_write_full(c, dest, m + j, R, 0, G, B, y, target, hasAlpha, err);
+                dest += step;
+            }
+        }
+    }
+    for (; i < dstW; i++){
+        int Y = ( buf0[i] * yalpha1  +  buf1[i] * yalpha         ) >> 10; //FIXME rounding
+        int U = (ubuf0[i] * uvalpha1 + ubuf1[i] * uvalpha- uvtemp) >> 10;
+        int V = (vbuf0[i] * uvalpha1 + vbuf1[i] * uvalpha- uvtemp) >> 10;
+
+        A = 0;
+        if (hasAlpha){
+            A = (abuf0[i] * yalpha1 + abuf1[i] * yalpha + atemp) >> 19;
+            if (A & 0x100)
+                A = av_clip_uint8(A);
+        }
+
+        Y -= y_offset;
+        Y *= y_coeff;
+        Y += ytemp;
+        R = (unsigned)Y + V * v2r_coe;
+        G = (unsigned)Y + V * v2g_coe + U * u2g_coe;
+        B = (unsigned)Y + U * u2b_coe;
+        yuv2rgb_write_full(c, dest, i, R, A, G, B, y, target, hasAlpha, err);
+        dest += step;
+    }
+    c->dither_error[0][i] = err[0];
+    c->dither_error[1][i] = err[1];
+    c->dither_error[2][i] = err[2];
+}
+
+static av_always_inline void
+yuv2rgb_full_1_msa_template(SwsContext *c, const int16_t *buf0,
+                            const int16_t *ubuf[2], const int16_t *vbuf[2],
+                            const int16_t *abuf0, uint8_t *dest, int dstW,
+                            int uvalpha, int y, enum AVPixelFormat target,
+                            int hasAlpha)
+{
+    const int16_t *ubuf0 = ubuf[0], *vbuf0 = vbuf[0];
+    int i, j, B, G, R, A;
+    int step = (target == AV_PIX_FMT_RGB24 || target == AV_PIX_FMT_BGR24) ? 3 : 4;
+    int err[4] = {0};
+    int len = dstW & (~0x07);
+    int ytemp    = 1 << 21;
+    v4i32 bias   = __msa_fill_w(64);
+    v4i32 y_temp = __msa_fill_w(ytemp);
+    yuvTorgb_setup
+
+    if(   target == AV_PIX_FMT_BGR4_BYTE || target == AV_PIX_FMT_RGB4_BYTE
+       || target == AV_PIX_FMT_BGR8      || target == AV_PIX_FMT_RGB8)
+        step = 1;
+    if (uvalpha < 2048) {
+        int uvtemp = 128 << 7;
+        v4i32 uv   = __msa_fill_w(uvtemp);
+
+        for (i = 0; i < len; i += 8) {
+            int m = i + 4;
+            v8i16 b, ub, vb;
+            v4i32 y_r, y_l, u_r, u_l, v_r, v_l;
+            v4i32 R_r, R_l, G_r, G_l, B_r, B_l;
+
+            b   = LD_V(v8i16, (buf0 + i));
+            ub  = LD_V(v8i16, (ubuf0 + i));
+            vb  = LD_V(v8i16, (vbuf0 + i));
+            UNPCK_SH_SW(b, y_r, y_l);
+            UNPCK_SH_SW(ub, u_r, u_l);
+            UNPCK_SH_SW(vb, v_r, v_l);
+            y_r = __msa_slli_w(y_r, 2);
+            y_l = __msa_slli_w(y_l, 2);
+            u_r -= uv;
+            u_l -= uv;
+            v_r -= uv;
+            v_l -= uv;
+            u_r = __msa_slli_w(u_r, 2);
+            u_l = __msa_slli_w(u_l, 2);
+            v_r = __msa_slli_w(v_r, 2);
+            v_l = __msa_slli_w(v_l, 2);
+            yuvTorgb
+
+            if(hasAlpha) {
+                v8i16 a_src;
+                v4i32 a_r, a_l;
+
+                a_src = LD_V(v8i16, (abuf0 + i));
+                UNPCK_SH_SW(a_src, a_r, a_l);
+                a_r += bias;
+                a_l += bias;
+                a_r = __msa_srai_w(a_r, 7);
+                a_l = __msa_srai_w(a_l, 7);
+                for (j = 0; j < 4; j++) {
+                    R = R_r[j];
+                    G = G_r[j];
+                    B = B_r[j];
+                    A = a_r[j];
+                    if (A & 0x100)
+                        A = av_clip_uint8(A);
+                    yuv2rgb_write_full(c, dest, i + j, R, A, G, B, y, target, hasAlpha, err);
+                    dest += step;
+                }
+                for (j = 0; j < 4; j++) {
+                    R = R_l[j];
+                    G = G_l[j];
+                    B = B_l[j];
+                    A = a_l[j];
+                    if (A & 0x100)
+                        A = av_clip_uint8(A);
+                    yuv2rgb_write_full(c, dest, m + j, R, A, G, B, y, target, hasAlpha, err);
+                    dest += step;
+                }
+            } else {
+                for (j = 0; j < 4; j++) {
+                    R = R_r[j];
+                    G = G_r[j];
+                    B = B_r[j];
+                    yuv2rgb_write_full(c, dest, i + j, R, 0, G, B, y, target, hasAlpha, err);
+                    dest += step;
+                }
+                for (j = 0; j < 4; j++) {
+                    R = R_l[j];
+                    G = G_l[j];
+                    B = B_l[j];
+                    yuv2rgb_write_full(c, dest, m + j, R, 0, G, B, y, target, hasAlpha, err);
+                    dest += step;
+                }
+            }
+        }
+        for (; i < dstW; i++) {
+            int Y = buf0[i] << 2;
+            int U = (ubuf0[i] - uvtemp) << 2;
+            int V = (vbuf0[i] - uvtemp) << 2;
+
+            A = 0;
+            if(hasAlpha) {
+                A = (abuf0[i] + 64) >> 7;
+                if (A & 0x100)
+                    A = av_clip_uint8(A);
+            }
+            Y -= y_offset;
+            Y *= y_coeff;
+            Y += ytemp;
+            R = (unsigned)Y + V * v2r_coe;
+            G = (unsigned)Y + V * v2g_coe + U * u2g_coe;
+            B = (unsigned)Y + U * u2b_coe;
+            yuv2rgb_write_full(c, dest, i, R, A, G, B, y, target, hasAlpha, err);
+            dest += step;
+        }
+    } else {
+        const int16_t *ubuf1 = ubuf[1], *vbuf1 = vbuf[1];
+        int uvtemp = 128 << 8;
+        v4i32 uv   = __msa_fill_w(uvtemp);
+
+        for (i = 0; i < len; i += 8) {
+            v8i16 b, ub, vb;
+            v4i32 y_r, y_l, u_r, u_l, v_r, v_l;
+            v4i32 u1_r, u1_l, v1_r, v1_l;
+            v4i32 R_r, R_l, G_r, G_l, B_r, B_l;
+            int m = i + 4;
+
+            b   = LD_V(v8i16, (buf0 + i));
+            ub  = LD_V(v8i16, (ubuf0 + i));
+            vb  = LD_V(v8i16, (vbuf0 + i));
+            UNPCK_SH_SW(b, y_r, y_l);
+            UNPCK_SH_SW(ub, u_r, u_l);
+            UNPCK_SH_SW(vb, v_r, v_l);
+            y_r = __msa_slli_w(y_r, 2);
+            y_l = __msa_slli_w(y_l, 2);
+            u_r -= uv;
+            u_l -= uv;
+            v_r -= uv;
+            v_l -= uv;
+            ub  = LD_V(v8i16, (ubuf1 + i));
+            vb  = LD_V(v8i16, (vbuf1 + i));
+            UNPCK_SH_SW(ub, u1_r, u1_l);
+            UNPCK_SH_SW(vb, v1_r, v1_l);
+            u_r += u1_r;
+            u_l += u1_l;
+            v_r += v1_r;
+            v_l += v1_l;
+            u_r = __msa_slli_w(u_r, 1);
+            u_l = __msa_slli_w(u_l, 1);
+            yuvTorgb
+
+            if(hasAlpha) {
+                v8i16 a_src;
+                v4i32 a_r, a_l;
+
+                a_src = LD_V(v8i16, (abuf0 + i));
+                UNPCK_SH_SW(a_src, a_r, a_l);
+                a_r += bias;
+                a_l += bias;
+                a_r = __msa_srai_w(a_r, 7);
+                a_l = __msa_srai_w(a_l, 7);
+                for (j = 0; j < 4; j++) {
+                    R = R_r[j];
+                    G = G_r[j];
+                    B = B_r[j];
+                    A = a_r[j];
+                    if (A & 0x100)
+                        A = av_clip_uint8(A);
+                    yuv2rgb_write_full(c, dest, i + j, R, A, G, B, y, target, hasAlpha, err);
+                    dest += step;
+                }
+                for (j = 0; j < 4; j++) {
+                    R = R_l[j];
+                    G = G_l[j];
+                    B = B_l[j];
+                    A = a_l[j];
+                    if (A & 0x100)
+                        A = av_clip_uint8(A);
+                    yuv2rgb_write_full(c, dest, m + j, R, A, G, B, y, target, hasAlpha, err);
+                    dest += step;
+                }
+            } else {
+                for (j = 0; j < 4; j++) {
+                    R = R_r[j];
+                    G = G_r[j];
+                    B = B_r[j];
+                    yuv2rgb_write_full(c, dest, i + j, R, 0, G, B, y, target, hasAlpha, err);
+                    dest += step;
+                }
+                for (j = 0; j < 4; j++) {
+                    R = R_l[j];
+                    G = G_l[j];
+                    B = B_l[j];
+                    yuv2rgb_write_full(c, dest, m + j, R, 0, G, B, y, target, hasAlpha, err);
+                    dest += step;
+                }
+            }
+        }
+        for (; i < dstW; i++) {
+            int Y = buf0[i] << 2;
+            int U = (ubuf0[i] + ubuf1[i] - uvtemp) << 1;
+            int V = (vbuf0[i] + vbuf1[i] - uvtemp) << 1;
+
+            A = 0;
+            if(hasAlpha) {
+                A = (abuf0[i] + 64) >> 7;
+                if (A & 0x100)
+                    A = av_clip_uint8(A);
+            }
+            Y -= y_offset;
+            Y *= y_coeff;
+            Y += ytemp;
+            R = (unsigned)Y + V * v2r_coe;
+            G = (unsigned)Y + V * v2g_coe + U * u2g_coe;
+            B = (unsigned)Y + U * u2b_coe;
+            yuv2rgb_write_full(c, dest, i, R, A, G, B, y, target, hasAlpha, err);
+            dest += step;
+        }
+    }
+    c->dither_error[0][i] = err[0];
+    c->dither_error[1][i] = err[1];
+    c->dither_error[2][i] = err[2];
+}
+#if CONFIG_SMALL
+YUV2RGBWRAPPER(yuv2, rgb_full, bgra32_full, AV_PIX_FMT_BGRA,  CONFIG_SWSCALE_ALPHA && c->needAlpha)
+YUV2RGBWRAPPER(yuv2, rgb_full, abgr32_full, AV_PIX_FMT_ABGR,  CONFIG_SWSCALE_ALPHA && c->needAlpha)
+YUV2RGBWRAPPER(yuv2, rgb_full, rgba32_full, AV_PIX_FMT_RGBA,  CONFIG_SWSCALE_ALPHA && c->needAlpha)
+YUV2RGBWRAPPER(yuv2, rgb_full, argb32_full, AV_PIX_FMT_ARGB,  CONFIG_SWSCALE_ALPHA && c->needAlpha)
+#else
+#if CONFIG_SWSCALE_ALPHA
+YUV2RGBWRAPPER(yuv2, rgb_full, bgra32_full, AV_PIX_FMT_BGRA,  1)
+YUV2RGBWRAPPER(yuv2, rgb_full, abgr32_full, AV_PIX_FMT_ABGR,  1)
+YUV2RGBWRAPPER(yuv2, rgb_full, rgba32_full, AV_PIX_FMT_RGBA,  1)
+YUV2RGBWRAPPER(yuv2, rgb_full, argb32_full, AV_PIX_FMT_ARGB,  1)
+#endif
+YUV2RGBWRAPPER(yuv2, rgb_full, bgrx32_full, AV_PIX_FMT_BGRA,  0)
+YUV2RGBWRAPPER(yuv2, rgb_full, xbgr32_full, AV_PIX_FMT_ABGR,  0)
+YUV2RGBWRAPPER(yuv2, rgb_full, rgbx32_full, AV_PIX_FMT_RGBA,  0)
+YUV2RGBWRAPPER(yuv2, rgb_full, xrgb32_full, AV_PIX_FMT_ARGB,  0)
+#endif
+YUV2RGBWRAPPER(yuv2, rgb_full, bgr24_full,  AV_PIX_FMT_BGR24, 0)
+YUV2RGBWRAPPER(yuv2, rgb_full, rgb24_full,  AV_PIX_FMT_RGB24, 0)
+
+YUV2RGBWRAPPER(yuv2, rgb_full, bgr4_byte_full,  AV_PIX_FMT_BGR4_BYTE, 0)
+YUV2RGBWRAPPER(yuv2, rgb_full, rgb4_byte_full,  AV_PIX_FMT_RGB4_BYTE, 0)
+YUV2RGBWRAPPER(yuv2, rgb_full, bgr8_full,   AV_PIX_FMT_BGR8,  0)
+YUV2RGBWRAPPER(yuv2, rgb_full, rgb8_full,   AV_PIX_FMT_RGB8,  0)
+#undef yuvTorgb
+#undef yuvTorgb_setup
+
+void planar_rgb_to_uv_msa(uint8_t *_dstU, uint8_t *_dstV, const uint8_t *src[4],
+                          int width, int32_t *rgb2yuv)
+{
+    uint16_t *dstU = (uint16_t *)_dstU;
+    uint16_t *dstV = (uint16_t *)_dstV;
+    int i;
+    int len = width & (~0x07);
+    int set = 0x4001<<(RGB2YUV_SHIFT - 7);
+    int32_t tem_ru = rgb2yuv[RU_IDX], tem_gu = rgb2yuv[GU_IDX];
+    int32_t tem_bu = rgb2yuv[BU_IDX];
+    int32_t tem_rv = rgb2yuv[RV_IDX], tem_gv = rgb2yuv[GV_IDX];
+    int32_t tem_bv = rgb2yuv[BV_IDX];
+    int shift = RGB2YUV_SHIFT - 6;
+    v4i32 ru, gu, bu, rv, gv, bv;
+    v4i32 temp = __msa_fill_w(set);
+    v4i32 sra  = __msa_fill_w(shift);
+    v16i8 zero = {0};
+
+    ru = __msa_fill_w(tem_ru);
+    gu = __msa_fill_w(tem_gu);
+    bu = __msa_fill_w(tem_bu);
+    rv = __msa_fill_w(tem_rv);
+    gv = __msa_fill_w(tem_gv);
+    bv = __msa_fill_w(tem_bv);
+    for (i = 0; i < len; i += 8) {
+        v16i8 _g, _b, _r;
+        v8i16 t_g, t_b, t_r;
+        v4i32 g_r, g_l, b_r, b_l, r_r, r_l;
+        v4i32 v_r, v_l, u_r, u_l;
+
+        _g  = LD_V(v16i8, (src[0] + i));
+        _b  = LD_V(v16i8, (src[1] + i));
+        _r  = LD_V(v16i8, (src[2] + i));
+        t_g = (v8i16)__msa_ilvr_b((v16i8)zero, (v16i8)_g);
+        t_b = (v8i16)__msa_ilvr_b((v16i8)zero, (v16i8)_b);
+        t_r = (v8i16)__msa_ilvr_b((v16i8)zero, (v16i8)_r);
+        g_r = (v4i32)__msa_ilvr_h((v8i16)zero, (v8i16)t_g);
+        g_l = (v4i32)__msa_ilvl_h((v8i16)zero, (v8i16)t_g);
+        b_r = (v4i32)__msa_ilvr_h((v8i16)zero, (v8i16)t_b);
+        b_l = (v4i32)__msa_ilvl_h((v8i16)zero, (v8i16)t_b);
+        r_r = (v4i32)__msa_ilvr_h((v8i16)zero, (v8i16)t_r);
+        r_l = (v4i32)__msa_ilvl_h((v8i16)zero, (v8i16)t_r);
+        v_r = (v4i32)__msa_mulv_w(r_r, rv);
+        v_l = (v4i32)__msa_mulv_w(r_l, rv);
+        v_r = (v4i32)__msa_maddv_w(g_r, gv, v_r);
+        v_l = (v4i32)__msa_maddv_w(g_l, gv, v_l);
+        v_r = (v4i32)__msa_maddv_w(b_r, bv, v_r);
+        v_l = (v4i32)__msa_maddv_w(b_l, bv, v_l);
+        u_r = (v4i32)__msa_mulv_w(r_r, ru);
+        u_l = (v4i32)__msa_mulv_w(r_l, ru);
+        u_r = (v4i32)__msa_maddv_w(g_r, gu, u_r);
+        u_l = (v4i32)__msa_maddv_w(g_l, gu, u_l);
+        u_r = (v4i32)__msa_maddv_w(b_r, bu, u_r);
+        u_l = (v4i32)__msa_maddv_w(b_l, bu, u_l);
+        v_r = (v4i32)__msa_addv_w(v_r, temp);
+        v_l = (v4i32)__msa_addv_w(v_l, temp);
+        u_r = (v4i32)__msa_addv_w(u_r, temp);
+        u_l = (v4i32)__msa_addv_w(u_l, temp);
+        v_r = (v4i32)__msa_sra_w(v_r, sra);
+        v_l = (v4i32)__msa_sra_w(v_l, sra);
+        u_r = (v4i32)__msa_sra_w(u_r, sra);
+        u_l = (v4i32)__msa_sra_w(u_l, sra);
+        for (int j = 0; j < 4; j++) {
+            int m = i + j;
+
+            dstU[m] = u_r[j];
+            dstV[m] = v_r[j];
+            dstU[m + 4] = u_l[j];
+            dstV[m + 4] = v_l[j];
+        }
+    }
+    for (; i < width; i++) {
+        int g = src[0][i];
+        int b = src[1][i];
+        int r = src[2][i];
+
+        dstU[i] = (tem_ru * r + tem_gu * g + tem_bu * b + set) >> shift;
+        dstV[i] = (tem_rv * r + tem_gv * g + tem_bv * b + set) >> shift;
+    }
+}
+
+void planar_rgb_to_y_msa(uint8_t *_dst, const uint8_t *src[4], int width,
+                         int32_t *rgb2yuv)
+{
+    uint16_t *dst = (uint16_t *)_dst;
+    int32_t tem_ry = rgb2yuv[RY_IDX], tem_gy = rgb2yuv[GY_IDX];
+    int32_t tem_by = rgb2yuv[BY_IDX];
+    int len    = width & (~0x07);
+    int shift  = (RGB2YUV_SHIFT-6);
+    int set    = 0x801 << (RGB2YUV_SHIFT - 7);
+    int i;
+    v4i32 temp = (v4i32)__msa_fill_w(set);
+    v4i32 sra  = (v4i32)__msa_fill_w(shift);
+    v4i32 ry   = (v4i32)__msa_fill_w(tem_ry);
+    v4i32 gy   = (v4i32)__msa_fill_w(tem_gy);
+    v4i32 by   = (v4i32)__msa_fill_w(tem_by);
+    v16i8 zero = {0};
+
+    for (i = 0; i < len; i += 8) {
+        v16i8 _g, _b, _r;
+        v8i16 t_g, t_b, t_r;
+        v4i32 g_r, g_l, b_r, b_l, r_r, r_l;
+        v4i32 out_r, out_l;
+
+        _g    = LD_V(v16i8, src[0] + i);
+        _b    = LD_V(v16i8, src[1] + i);
+        _r    = LD_V(v16i8, src[2] + i);
+        t_g   = (v8i16)__msa_ilvr_b((v16i8)zero, (v16i8)_g);
+        t_b   = (v8i16)__msa_ilvr_b((v16i8)zero, (v16i8)_b);
+        t_r   = (v8i16)__msa_ilvr_b((v16i8)zero, (v16i8)_r);
+        g_r   = (v4i32)__msa_ilvr_h((v8i16)zero, (v8i16)t_g);
+        g_l   = (v4i32)__msa_ilvl_h((v8i16)zero, (v8i16)t_g);
+        b_r   = (v4i32)__msa_ilvr_h((v8i16)zero, (v8i16)t_b);
+        b_l   = (v4i32)__msa_ilvl_h((v8i16)zero, (v8i16)t_b);
+        r_r   = (v4i32)__msa_ilvr_h((v8i16)zero, (v8i16)t_r);
+        r_l   = (v4i32)__msa_ilvl_h((v8i16)zero, (v8i16)t_r);
+        out_r = (v4i32)__msa_mulv_w(r_r, ry);
+        out_l = (v4i32)__msa_mulv_w(r_l, ry);
+        out_r = (v4i32)__msa_maddv_w(g_r, gy, out_r);
+        out_l = (v4i32)__msa_maddv_w(g_l, gy, out_l);
+        out_r = (v4i32)__msa_maddv_w(b_r, by, out_r);
+        out_l = (v4i32)__msa_maddv_w(b_l, by, out_l);
+        out_r = (v4i32)__msa_addv_w(out_r, temp);
+        out_l = (v4i32)__msa_addv_w(out_l, temp);
+        out_r = (v4i32)__msa_sra_w(out_r, sra);
+        out_l = (v4i32)__msa_sra_w(out_l, sra);
+        for (int j = 0; j < 4; j++) {
+            int m = i + j;
+            dst[m] = out_r[j];
+            dst[m + 4] = out_l[j];
+        }
+    }
+    for (; i < width; i++) {
+        int g = src[0][i];
+        int b = src[1][i];
+        int r = src[2][i];
+
+        dst[i] = (tem_ry * r + tem_gy * g + tem_by * b + set) >> shift;
+    }
+}
diff --git a/libswscale/mips/yuv2rgb_msa.c b/libswscale/mips/yuv2rgb_msa.c
new file mode 100644
index 0000000000..1f9ef90804
--- /dev/null
+++ b/libswscale/mips/yuv2rgb_msa.c
@@ -0,0 +1,251 @@
+/*
+ * Copyright (C) 2023 Loongson Technology Co. Ltd.
+ * Contributed by jinbo(jinbo@loongson.cn)
+ * All rights reserved.
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include "swscale_mips.h"
+#include "libavutil/mips/generic_macros_msa.h"
+
+#define YUV2RGB_LOAD_COE                              \
+    /* Load x_offset */                               \
+    v8i16 y_offset = (v8i16)__msa_fill_d(c->yOffset); \
+    v8i16 u_offset = (v8i16)__msa_fill_d(c->uOffset); \
+    v8i16 v_offset = (v8i16)__msa_fill_d(c->vOffset); \
+    /*Load x_coeff */                                 \
+    v8i16 ug_coeff = (v8i16)__msa_fill_d(c->ugCoeff); \
+    v8i16 vg_coeff = (v8i16)__msa_fill_d(c->vgCoeff); \
+    v8i16 y_coeff  = (v8i16)__msa_fill_d(c->yCoeff);  \
+    v8i16 ub_coeff = (v8i16)__msa_fill_d(c->ubCoeff); \
+    v8i16 vr_coeff = (v8i16)__msa_fill_d(c->vrCoeff); \
+
+#define LOAD_YUV_16                                      \
+    m_y1   = LD_SB(py_1);                                \
+    m_y2   = LD_SB(py_2);                                \
+    m_u    = LD_SB(pu);                                  \
+    m_v    = LD_SB(pv);                                  \
+    m_u    = __msa_ilvr_b((v16i8)m_u, (v16i8)m_u);       \
+    m_v    = __msa_ilvr_b((v16i8)m_v, (v16i8)m_v);       \
+    m_u_l  = __msa_ilvl_b(zero, (v16i8)m_u);             \
+    m_u    = __msa_ilvr_b(zero, (v16i8)m_u);             \
+    m_v_l  = __msa_ilvl_b(zero, (v16i8)m_v);             \
+    m_v    = __msa_ilvr_b(zero, (v16i8)m_v);             \
+    m_y1_l = __msa_ilvl_b(zero, (v16i8)m_y1);            \
+    m_y1   = __msa_ilvr_b(zero, (v16i8)m_y1);            \
+    m_y2_l = __msa_ilvl_b(zero, (v16i8)m_y2);            \
+    m_y2   = __msa_ilvr_b(zero, (v16i8)m_y2);            \
+
+#define MUH_H(in0, in1, in2, in3, out0)                  \
+{                                                        \
+    v4i32 tmp0, tmp1;                                    \
+    tmp0 = in0 * in1;                                    \
+    tmp1 = in2 * in3;                                    \
+    out0 = __msa_pckod_h((v8i16)tmp1, (v8i16)tmp0);      \
+}
+
+#define YUV2RGB(y1, y2, u, v, r1, g1, b1, r2, g2, b2)   \
+{                                                       \
+    y1 = (v16i8)__msa_slli_h((v8i16)y1, 3);             \
+    y2 = (v16i8)__msa_slli_h((v8i16)y2, 3);             \
+    u  = (v16i8)__msa_slli_h((v8i16)u, 3);              \
+    v  = (v16i8)__msa_slli_h((v8i16)v, 3);              \
+    y1 = (v16i8)__msa_subv_h((v8i16)y1, y_offset);      \
+    y2 = (v16i8)__msa_subv_h((v8i16)y2, y_offset);      \
+    u  = (v16i8)__msa_subv_h((v8i16)u, u_offset);       \
+    v  = (v16i8)__msa_subv_h((v8i16)v, v_offset);       \
+    UNPCK_SH_SW(y_coeff, y_coeff_r, y_coeff_l);         \
+    UNPCK_SH_SW(y1, tmp_r, tmp_l);                      \
+    MUH_H(tmp_r, y_coeff_r, tmp_l, y_coeff_l, y_1_r);   \
+    UNPCK_SH_SW(y2, tmp_r, tmp_l);                      \
+    MUH_H(tmp_r, y_coeff_r, tmp_l, y_coeff_l, y_2_r);   \
+    UNPCK_SH_SW(ug_coeff, ug_coeff_r, ug_coeff_l);      \
+    UNPCK_SH_SW(ub_coeff, ub_coeff_r, ub_coeff_l);      \
+    UNPCK_SH_SW(u, tmp_r, tmp_l);                       \
+    MUH_H(tmp_r, ug_coeff_r, tmp_l, ug_coeff_l, u2g_r); \
+    MUH_H(tmp_r, ub_coeff_r, tmp_l, ub_coeff_l, u2b_r); \
+    UNPCK_SH_SW(vr_coeff, vr_coeff_r, vr_coeff_l);      \
+    UNPCK_SH_SW(vg_coeff, vg_coeff_r, vg_coeff_l);      \
+    UNPCK_SH_SW(v, tmp_r, tmp_l);                       \
+    MUH_H(tmp_r, vr_coeff_r, tmp_l, vr_coeff_l, v2r_r); \
+    MUH_H(tmp_r, vg_coeff_r, tmp_l, vg_coeff_l, v2g_r); \
+    r1    = __msa_adds_s_h(y_1_r, v2r_r);               \
+    v2g_r = __msa_adds_s_h(v2g_r, u2g_r);               \
+    g1    = __msa_adds_s_h(y_1_r, v2g_r);               \
+    b1    = __msa_adds_s_h(y_1_r, u2b_r);               \
+    r2    = __msa_adds_s_h(y_2_r, v2r_r);               \
+    g2    = __msa_adds_s_h(y_2_r, v2g_r);               \
+    b2    = __msa_adds_s_h(y_2_r, u2b_r);               \
+    CLIP_SH4_0_255(r1, g1, b1, r2);                     \
+    CLIP_SH2_0_255(g2, b2);                             \
+}
+
+#define RGB32_PACK(a, r, g, b, rgb_l, rgb_h)            \
+{                                                       \
+    v16i8 ra, bg;                                       \
+    ra    = __msa_ilvev_b((v16i8)r, (v16i8)a);          \
+    bg    = __msa_ilvev_b((v16i8)b, (v16i8)g);          \
+    rgb_l = __msa_ilvr_h((v8i16)bg, (v8i16)ra);         \
+    rgb_h = __msa_ilvl_h((v8i16)bg, (v8i16)ra);         \
+}
+
+#define RGB32_STORE(rgb_l, rgb_h, image)                \
+{                                                       \
+    ST_SH2(rgb_l, rgb_h, image, 4);                     \
+}
+
+#define YUV2RGBFUNC32(func_name, dst_type, alpha)                               \
+           int func_name(SwsContext *c, const uint8_t *src[],                   \
+                         int srcStride[], int srcSliceY, int srcSliceH,         \
+                         uint8_t *dst[], int dstStride[])                       \
+{                                                                               \
+    int x, y, h_size, vshift, res;                                              \
+    v16i8 m_y1, m_y2, m_u, m_v;                                                 \
+    v16i8 m_u_l, m_v_l, m_y1_l, m_y2_l;                                         \
+    v4i32 y_coeff_r, y_coeff_l, ug_coeff_r, ug_coeff_l;                         \
+    v4i32 ub_coeff_r, ub_coeff_l, vr_coeff_r, vr_coeff_l;                       \
+    v4i32 vg_coeff_r, vg_coeff_l, tmp_r, tmp_l;                                 \
+    v8i16 y_1_r, y_2_r, u2g_r, v2g_r, u2b_r, v2r_r, rgb1_l, rgb1_h;             \
+    v8i16 rgb2_l, rgb2_h, r1, g1, b1, r2, g2, b2;                               \
+    v16u8 a = (v16u8)__msa_fill_b(0xFF);                                        \
+    v16i8 zero = __msa_fill_b(0);                                               \
+                                                                                \
+    YUV2RGB_LOAD_COE                                                            \
+                                                                                \
+    h_size = c->dstW >> 4;                                                      \
+    res = (c->dstW & 15) >> 1;                                                  \
+    vshift = c->srcFormat != AV_PIX_FMT_YUV422P;                                \
+    for (y = 0; y < srcSliceH; y += 2) {                                        \
+        int yd = y + srcSliceY;                                                 \
+        dst_type av_unused *r, *g, *b;                                          \
+        dst_type *image1    = (dst_type *)(dst[0] + (yd)     * dstStride[0]);   \
+        dst_type *image2    = (dst_type *)(dst[0] + (yd + 1) * dstStride[0]);   \
+        const uint8_t *py_1 = src[0] +               y * srcStride[0];          \
+        const uint8_t *py_2 = py_1   +                   srcStride[0];          \
+        const uint8_t *pu   = src[1] +   (y >> vshift) * srcStride[1];          \
+        const uint8_t *pv   = src[2] +   (y >> vshift) * srcStride[2];          \
+        for(x = 0; x < h_size; x++) {                                           \
+
+#define DEALYUV2RGBREMAIN32                                                     \
+            py_1 += 16;                                                         \
+            py_2 += 16;                                                         \
+            pu += 8;                                                            \
+            pv += 8;                                                            \
+            image1 += 16;                                                       \
+            image2 += 16;                                                       \
+        }                                                                       \
+        for (x = 0; x < res; x++) {                                             \
+            int av_unused U, V, Y;                                              \
+            U = pu[0];                                                          \
+            V = pv[0];                                                          \
+            r = (void *)c->table_rV[V+YUVRGB_TABLE_HEADROOM];                   \
+            g = (void *)(c->table_gU[U+YUVRGB_TABLE_HEADROOM]                   \
+                       + c->table_gV[V+YUVRGB_TABLE_HEADROOM]);                 \
+            b = (void *)c->table_bU[U+YUVRGB_TABLE_HEADROOM];                   \
+
+#define PUTRGB(dst, src)                    \
+    Y      = src[0];                        \
+    dst[0] = r[Y] + g[Y] + b[Y];            \
+    Y      = src[1];                        \
+    dst[1] = r[Y] + g[Y] + b[Y];            \
+
+#define ENDRES32                            \
+    pu += 1;                                \
+    pv += 1;                                \
+    py_1 += 2;                              \
+    py_2 += 2;                              \
+    image1 += 2;                            \
+    image2 += 2;                            \
+
+#define END_FUNC()                          \
+        }                                   \
+    }                                       \
+    return srcSliceH;                       \
+}
+
+YUV2RGBFUNC32(yuv420_rgba32_msa, uint32_t, 0)
+    LOAD_YUV_16
+    YUV2RGB(m_y1, m_y2, m_u, m_v, r1, g1, b1, r2, g2, b2);
+    RGB32_PACK(r1, g1, b1, a, rgb1_l, rgb1_h);
+    RGB32_PACK(r2, g2, b2, a, rgb2_l, rgb2_h);
+    RGB32_STORE(rgb1_l, rgb1_h, image1);
+    RGB32_STORE(rgb2_l, rgb2_h, image2);
+    YUV2RGB(m_y1_l, m_y2_l, m_u_l, m_v_l, r1, g1, b1, r2, g2, b2);
+    RGB32_PACK(r1, g1, b1, a, rgb1_l, rgb1_h);
+    RGB32_PACK(r2, g2, b2, a, rgb2_l, rgb2_h);
+    RGB32_STORE(rgb1_l, rgb1_h, image1 + 8);
+    RGB32_STORE(rgb2_l, rgb2_h, image2 + 8);
+    DEALYUV2RGBREMAIN32
+    PUTRGB(image1, py_1);
+    PUTRGB(image2, py_2);
+    ENDRES32
+    END_FUNC()
+
+YUV2RGBFUNC32(yuv420_bgra32_msa, uint32_t, 0)
+    LOAD_YUV_16
+    YUV2RGB(m_y1, m_y2, m_u, m_v, r1, g1, b1, r2, g2, b2);
+    RGB32_PACK(b1, g1, r1, a, rgb1_l, rgb1_h);
+    RGB32_PACK(b2, g2, r2, a, rgb2_l, rgb2_h);
+    RGB32_STORE(rgb1_l, rgb1_h, image1);
+    RGB32_STORE(rgb2_l, rgb2_h, image2);
+    YUV2RGB(m_y1_l, m_y2_l, m_u_l, m_v_l, r1, g1, b1, r2, g2, b2);
+    RGB32_PACK(b1, g1, r1, a, rgb1_l, rgb1_h);
+    RGB32_PACK(b2, g2, r2, a, rgb2_l, rgb2_h);
+    RGB32_STORE(rgb1_l, rgb1_h, image1 + 8);
+    RGB32_STORE(rgb2_l, rgb2_h, image2 + 8);
+    DEALYUV2RGBREMAIN32
+    PUTRGB(image1, py_1);
+    PUTRGB(image2, py_2);
+    ENDRES32
+    END_FUNC()
+
+YUV2RGBFUNC32(yuv420_argb32_msa, uint32_t, 0)
+    LOAD_YUV_16
+    YUV2RGB(m_y1, m_y2, m_u, m_v, r1, g1, b1, r2, g2, b2);
+    RGB32_PACK(a, r1, g1, b1, rgb1_l, rgb1_h);
+    RGB32_PACK(a, r2, g2, b2, rgb2_l, rgb2_h);
+    RGB32_STORE(rgb1_l, rgb1_h, image1);
+    RGB32_STORE(rgb2_l, rgb2_h, image2);
+    YUV2RGB(m_y1_l, m_y2_l, m_u_l, m_v_l, r1, g1, b1, r2, g2, b2);
+    RGB32_PACK(a, r1, g1, b1, rgb1_l, rgb1_h);
+    RGB32_PACK(a, r2, g2, b2, rgb2_l, rgb2_h);
+    RGB32_STORE(rgb1_l, rgb1_h, image1 + 8);
+    RGB32_STORE(rgb2_l, rgb2_h, image2 + 8);
+    DEALYUV2RGBREMAIN32
+    PUTRGB(image1, py_1);
+    PUTRGB(image2, py_2);
+    ENDRES32
+    END_FUNC()
+
+YUV2RGBFUNC32(yuv420_abgr32_msa, uint32_t, 0)
+    LOAD_YUV_16
+    YUV2RGB(m_y1, m_y2, m_u, m_v, r1, g1, b1, r2, g2, b2);
+    RGB32_PACK(a, b1, g1, r1, rgb1_l, rgb1_h);
+    RGB32_PACK(a, b2, g2, r2, rgb2_l, rgb2_h);
+    RGB32_STORE(rgb1_l, rgb1_h, image1);
+    RGB32_STORE(rgb2_l, rgb2_h, image2);
+    YUV2RGB(m_y1_l, m_y2_l, m_u_l, m_v_l, r1, g1, b1, r2, g2, b2);
+    RGB32_PACK(a, b1, g1, r1, rgb1_l, rgb1_h);
+    RGB32_PACK(a, b2, g2, r2, rgb2_l, rgb2_h);
+    RGB32_STORE(rgb1_l, rgb1_h, image1 + 8);
+    RGB32_STORE(rgb2_l, rgb2_h, image2 + 8);
+    DEALYUV2RGBREMAIN32
+    PUTRGB(image1, py_1);
+    PUTRGB(image2, py_2);
+    ENDRES32
+    END_FUNC()
diff --git a/libswscale/rgb2rgb.c b/libswscale/rgb2rgb.c
index eab8e6aebb..b9cc922ec7 100644
--- a/libswscale/rgb2rgb.c
+++ b/libswscale/rgb2rgb.c
@@ -139,6 +139,10 @@ av_cold void ff_sws_rgb2rgb_init(void)
     rgb2rgb_init_c();
     if (ARCH_X86)
         rgb2rgb_init_x86();
+    if (ARCH_MIPS)
+        rgb2rgb_init_mips();
+    if (ARCH_LOONGARCH)
+        rgb2rgb_init_loongarch();
 }
 
 void rgb32to24(const uint8_t *src, uint8_t *dst, int src_size)
diff --git a/libswscale/rgb2rgb.h b/libswscale/rgb2rgb.h
index 3569254df9..c092f52a68 100644
--- a/libswscale/rgb2rgb.h
+++ b/libswscale/rgb2rgb.h
@@ -170,5 +170,7 @@ extern void (*yuyvtoyuv422)(uint8_t *ydst, uint8_t *udst, uint8_t *vdst, const u
 void ff_sws_rgb2rgb_init(void);
 
 void rgb2rgb_init_x86(void);
+void rgb2rgb_init_mips(void);
+void rgb2rgb_init_loongarch(void);
 
 #endif /* SWSCALE_RGB2RGB_H */
diff --git a/libswscale/swscale.c b/libswscale/swscale.c
index 40695503ad..e421e9f7a3 100644
--- a/libswscale/swscale.c
+++ b/libswscale/swscale.c
@@ -607,6 +607,10 @@ SwsFunc ff_getSwsFunc(SwsContext *c)
         ff_sws_init_swscale_aarch64(c);
     if (ARCH_ARM)
         ff_sws_init_swscale_arm(c);
+    if (ARCH_MIPS)
+        ff_sws_init_swscale_mips(c);
+    if (ARCH_LOONGARCH)
+        ff_sws_init_swscale_loongarch(c);
 
     return swscale;
 }
diff --git a/libswscale/swscale_internal.h b/libswscale/swscale_internal.h
index a59d12745a..50c2779364 100644
--- a/libswscale/swscale_internal.h
+++ b/libswscale/swscale_internal.h
@@ -642,6 +642,8 @@ av_cold void ff_sws_init_range_convert(SwsContext *c);
 
 SwsFunc ff_yuv2rgb_init_x86(SwsContext *c);
 SwsFunc ff_yuv2rgb_init_ppc(SwsContext *c);
+SwsFunc ff_yuv2rgb_init_loongarch(SwsContext *c);
+SwsFunc ff_yuv2rgb_init_mips(SwsContext *c);
 
 static av_always_inline int is16BPS(enum AVPixelFormat pix_fmt)
 {
@@ -872,6 +874,8 @@ void ff_sws_init_swscale_vsx(SwsContext *c);
 void ff_sws_init_swscale_x86(SwsContext *c);
 void ff_sws_init_swscale_aarch64(SwsContext *c);
 void ff_sws_init_swscale_arm(SwsContext *c);
+void ff_sws_init_swscale_mips(SwsContext *c);
+void ff_sws_init_swscale_loongarch(SwsContext *c);
 
 void ff_hyscale_fast_c(SwsContext *c, int16_t *dst, int dstWidth,
                        const uint8_t *src, int srcW, int xInc);
diff --git a/libswscale/utils.c b/libswscale/utils.c
index 1b1f779532..827e8338bc 100644
--- a/libswscale/utils.c
+++ b/libswscale/utils.c
@@ -53,6 +53,8 @@
 #include "libavutil/ppc/cpu.h"
 #include "libavutil/x86/asm.h"
 #include "libavutil/x86/cpu.h"
+#include "libavutil/mips/cpu.h"
+#include "libavutil/loongarch/cpu.h"
 
 // We have to implement deprecated functions until they are removed, this is the
 // simplest way to prevent warnings
@@ -595,6 +597,33 @@ static av_cold int initFilter(int16_t **outFilter, int32_t **filterPos,
             filterAlign = 1;
     }
 
+    if (have_msa(cpu_flags)) {
+        int reNum = minFilterSize & (0x07);
+
+        if (minFilterSize < 5)
+            filterAlign = 4;
+        if (reNum < 3)
+            filterAlign = 1;
+    }
+
+    if (have_lsx(cpu_flags)) {
+        int reNum = minFilterSize & (0x07);
+
+        if (minFilterSize < 5)
+            filterAlign = 4;
+        if (reNum < 3)
+            filterAlign = 1;
+    }
+
+    if (have_lasx(cpu_flags)) {
+        int reNum = minFilterSize & (0x07);
+
+        if (minFilterSize < 5)
+            filterAlign = 4;
+        if (reNum < 3)
+            filterAlign = 1;
+    }
+
     if (HAVE_MMX && cpu_flags & AV_CPU_FLAG_MMX) {
         // special case for unscaled vertical filtering
         if (minFilterSize == 1 && filterAlign == 2)
@@ -1671,7 +1700,10 @@ av_cold int sws_init_context(SwsContext *c, SwsFilter *srcFilter,
         {
             const int filterAlign = X86_MMX(cpu_flags)     ? 4 :
                                     PPC_ALTIVEC(cpu_flags) ? 8 :
-                                    have_neon(cpu_flags)   ? 8 : 1;
+                                    have_neon(cpu_flags)   ? 8 :
+                                    have_lsx(cpu_flags)    ? 8 :
+                                    have_lasx(cpu_flags)   ? 8 :
+                                    have_msa(cpu_flags)    ? 8 : 1;
 
             if ((ret = initFilter(&c->hLumFilter, &c->hLumFilterPos,
                            &c->hLumFilterSize, c->lumXInc,
diff --git a/libswscale/yuv2rgb.c b/libswscale/yuv2rgb.c
index d0df061e4d..d87b64a048 100644
--- a/libswscale/yuv2rgb.c
+++ b/libswscale/yuv2rgb.c
@@ -683,6 +683,10 @@ SwsFunc ff_yuv2rgb_get_func_ptr(SwsContext *c)
         t = ff_yuv2rgb_init_ppc(c);
     if (ARCH_X86)
         t = ff_yuv2rgb_init_x86(c);
+    if (ARCH_LOONGARCH)
+        t = ff_yuv2rgb_init_loongarch(c);
+    if (ARCH_MIPS)
+        t = ff_yuv2rgb_init_mips(c);
 
     if (t)
         return t;
diff --git a/tests/checkasm/checkasm.c b/tests/checkasm/checkasm.c
index 3e2ec377be..a5a72133de 100644
--- a/tests/checkasm/checkasm.c
+++ b/tests/checkasm/checkasm.c
@@ -206,6 +206,9 @@ static const struct {
     { "ALTIVEC",  "altivec",  AV_CPU_FLAG_ALTIVEC },
     { "VSX",      "vsx",      AV_CPU_FLAG_VSX },
     { "POWER8",   "power8",   AV_CPU_FLAG_POWER8 },
+#elif ARCH_MIPS
+    { "MMI",      "mmi",      AV_CPU_FLAG_MMI },
+    { "MSA",      "msa",      AV_CPU_FLAG_MSA },
 #elif ARCH_X86
     { "MMX",      "mmx",      AV_CPU_FLAG_MMX|AV_CPU_FLAG_CMOV },
     { "MMXEXT",   "mmxext",   AV_CPU_FLAG_MMXEXT },
@@ -224,6 +227,9 @@ static const struct {
     { "FMA4",     "fma4",     AV_CPU_FLAG_FMA4 },
     { "AVX2",     "avx2",     AV_CPU_FLAG_AVX2 },
     { "AVX-512",  "avx512",   AV_CPU_FLAG_AVX512 },
+#elif ARCH_LOONGARCH
+    { "LSX",      "lsx",      AV_CPU_FLAG_LSX },
+    { "LASX",     "lasx",     AV_CPU_FLAG_LASX },
 #endif
     { NULL }
 };
-- 
2.20.1

